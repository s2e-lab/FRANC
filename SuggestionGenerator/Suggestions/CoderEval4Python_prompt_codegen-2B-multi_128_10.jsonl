{"task_id": "62e60f43d76274f8a4026e28", "prompt": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\tif isinstance(nanoseconds, float):\n\t\tnanoseconds = int(nanoseconds)\n\treturn nn_time_of_day(nanoseconds, tz=tz)\n\t\n\ndef hydrate_local_date(date):\n\t\"\"\"\n\tHydrate date from isoformat date string. \n\t\t\n\t\t* date is either a string of the form YYYY-MM-DDTHH:MM:SS (see :ref:`ISO`)\n\t\tor a :any:`float` representing the number of nan"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif tz is None:\n\t\treturn datetime.fromtimestamp(nanoseconds / 1000000.0)\n\telse:\n\t\treturn datetime.fromtimestamp(nanoseconds / 1000000.0, tzinfo=tz)\n\t\ndef hydrate_timestamp(nanoseconds, tz=None):\n\t\"\"\"\n\tHydrator for `TimeStamp` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: TimeStamp\n\t\"\"\"\n\treturn hydrate_time("}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif isinstance(nanoseconds, time):\n\t\treturn nanoseconds\n\tif isinstance(nanoseconds, datetime):\n\t\treturn nanoseconds.timetuple()\n\treturn datetime.strptime(str(nanoseconds), '%Y-%m-%dT%H:%M:%S')\n\t\n# time formats\n\t\nISO_8601 = \"%Y-%m-%dT%H:%M:%S\"\n\t\n\t\t\ndef from_localtime(tz, time):\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\ttry:\n\t\treturn (datetime.datetime.utcfromtimestamp(nanoseconds / 1000000000)\n\t\t\t.replace(tzinfo=pytz.UTC))\n\texcept(TypeError, ValueError, pytz.UnknownTimeZoneError, ValueError):\n\t\treturn (datetime.datetime.utcfromtimestamp(nanoseconds / 1000000)\n\t\t\t.replace(tzinfo=pytz.UTC))\n\ndef convert_values_type(values, values_type):\n\t\"\"\"\n\tTransforms the contents to the required type.\n\t"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif isinstance(nanoseconds, int) or isinstance(nanoseconds, float):\n\t\treturn Time(nanoseconds, tz)\n\telif isinstance(nanoseconds, str) or isinstance(nanoseconds, unicode):\n\t\treturn LocalTime.from_string(nanoseconds, tz)\n\telse:\n\t\traise TypeError('Invalid type for timestring: %s' % type(nanoseconds))\n\ndef hydrate_date(year, month, day, tz=None):\n\t"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\th, m, s, ms = (nanoseconds // 1000000,\n\t\t\t\t   (nanoseconds // 1000) % 60,\n\t\t\t\t   (nanoseconds // 100) % 60,\n\t\t\t\t   nanoseconds % 1000)\t\t\t\n\t\t\t\n\tt = datetime.datetime(1970, 1, 1, tzinfo=tz)\n\t\n\t# FIXME: Timezone support needs to be fixed at the API level, currently, the \n\t# time is relative to UTC.\n\tif tz is not None:\n\t\tt = t."}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\treturn Time(nanoseconds, tz)\n\ndef hydrate_list(items, tz=None):\n\t\"\"\"\n\tHydrator for `List` of `Time`, `LocalTime`, and `Uint32` items.\n\t\n\t:param items:\n\t:param tz:\n\t:return: List\n\t\"\"\"\n\treturn List(map(hydrate_time, items), tz=tz)\n\ndef hydrate_dict(items, tz=None):\n\t\"\"\"\n\tHydrator for `Dict` of `Time`, `"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\t\n\tif isinstance(nanoseconds, int) or isinstance(nanoseconds, long): # nanoseconds\n\t\tif tz is not None:\n\t\t\tnanoseconds = datetime_to_time(datetime.fromtimestamp(nanoseconds // 1000000,\n\t\t\t                               tz=tz),\n\t\t\t                               local=tz).astimezone(utc)\n\t\telse:\n\t\t\tnanoseconds = Time(datetime.fromtimestamp(nanoseconds // 1e6,\n\t\t\t                               tz=utc).time())\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tif not tz:\n\t\treturn datetime.datetime.fromtimestamp(nanoseconds / 1e9)\n\n\treturn datetime.datetime.fromtimestamp(nanoseconds / 1e9 * 1000000000)\n\n\ndef hydrate_timestamp(nanoseconds, tz=None):\n\t\"\"\"\n\tHydrater for `TimeStamp` and `LocalTimeStamp` values.\n\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Timestamp\n\t\"\"\"\n\tif not tz:\n"}, {"generated_text": "def hydrate_time(nanoseconds, tz=None):\n\"\"\"\n\tHydrator for `Time` and `LocalTime` values.\n\t\n\t:param nanoseconds:\n\t:param tz:\n\t:return: Time\n\t\"\"\"\n\tseconds = nanoseconds // 1000000000\n\tnanoseconds -= seconds * 1000000000\n\thours = seconds // 3600\n\tseconds -= hours * 3600\n\tminutes = seconds // 60\n\tseconds -= minutes * 60\n\treturn Time(hours, minutes, seconds, nanoseconds)\n\ndef hydrate_timezone_offset(value):\n\tif value < -60000 or value > 60000:\n\t\traise ValueError(\"Out of range timezone offset: %s\" % value)\n\t\n\tminutes = int(value / 60000)\n"}]}
{"task_id": "62e60f3bd76274f8a4026e10", "prompt": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn value if value else None\n\t\n'''\n#\tDehydrator for `db-table` properties.\n#\tNOTE: Don't put this method in your Model class; the MetaClass will do that for you.\n#\t- **Use with django.db.models.fields.ForeignKeyField.m2m**\n#\t- **Don't depend on Django or ORM internals**\n#\t- `dj_model.objects.all(*models.db_names())`\n#\t- http://stackoverflow.com/questions/29251875"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\tif not value: return\n\treturn '\"%s\"' % value.days\n\ndehydrate_timedelta.allow_tags = True\ndehydrate_timedelta.short_description = '\"days\"'\n\n#: Decorator for `timedelta` fields. Allows full use of the \n#: field's config from Django's default settings module.\n#:\n#: For example:\n#:\n#:     from icalendar.fields import TimeDeltaField\n#:     class MyModel(models.Model):\n#:        ...\n#"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn value.days\n\n\t\t\t\n\t"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn value.__str__().split(\": \")[1]\n\ndef dehydrating_list(objects):\n\tl = list()\n\tfor obj in objects:\n\t\tif isinstance(obj, tuple):\n\t\t\tl.append(dehydrating(getattr(obj, obj[0])))\n\t\telif isinstance(obj, list):\n\t\t\tl.append(dehydrating_list(getattr(obj, obj[0])))\n\t\telif isinstance(obj, dict):\n\t\t\tl.append(dehydrating(getattr(obj, obj['"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\ttry:\n\t\treturn int(to_timedelta(value).total_seconds())\n\texcept (ValueError, TypeError):\n\t\treturn \"\"\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\n\treturn value.seconds + value.days * 24 * 3600\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\ttry:\n\t\tassert value.microseconds == 0\n\texcept AttributeError:\n\t\tpass\n\treturn value\n\t# return str(value)\n\n\ndef dehydrate_decimal(value):\n\t\"\"\"\n\tDehydrator for `decimal` values.\n\t\n\t:param value:\n\t:type value: decimal\n\t:return:\n\t\"\"\"\n\treturn value  # str(value)\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\n\tif (value == None):\n\t\treturn value\n\n\treturn datetime_to_timedelta(value)\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn value.total_seconds()\n\ndef validate_user(username, password):\n\t\"\"\"\n\t\tThis function validates the username and password.\n\t\n\t\t:param username:\n\t\t:param password:\n\t\n\t\"\"\"\n\tif __db_session:\n\n\t\tuser = __db_session.query(User).filter(User.username == username).first()\n\n\t\tif user is not None:\n\t\t\tif user.check_password(password):\n\n\t\t\t\tu = user\n\t\t\t\treturn u\n\t\treturn False\n\ndef authenticate(username, password):\n\t\"\"\"\n"}, {"generated_text": "def dehydrate_timedelta(value):\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\t\n\treturn value.seconds + (value.days * 86400)\n\n############\n# Validators #\n############\n\n@validate(validators={\n    \"timedelta\": validators.TimeDeltaValue(error_code=\"validate_bad_timedelta\"),\n    \"boolean\": validators.BooleanValue(),\n    \"regex\": validators.RegexValue(regex=re.compile(\".*\")),\n    \"regex_check\": validators.RegexValue(\n        regex=re.compile(\"."}]}
{"task_id": "62e60f37d76274f8a4026dfd", "prompt": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn _time_format(str(value))\n\n\"\"\"\n\n\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: TimeDelta\n\t:return:\n\t\"\"\"\n\treturn _time_format(str(value))\n\n\ndef _decode_base64(value):\n\t\"\"\"\n\tDecodes an encoded value.\n\t\n\t:param value:\n\t:type value: str\n\t:return:\n\t\"\"\"\n\treturn str(base64.b64decode(str(value).strip()"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif value is None: return None\n\treturn int((datetime.datetime.now() - value).total_seconds()*1000)\n\ndef dehydrate_rating(value):\n\tif value is not None or value!= 'None':\n\t\trating = (value - 30) / 5.0\n\telse:\n\t\trating = None\n\treturn rating\n\ndef dehydrate_user(value):\n\tif value is None: return None\n\treturn dict(id=value.id, name=value.get_profile().name)\n\ndef dehydrate_repo("}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tdate = value.to_datetime()\n\treturn date.strftime(DEHYDRATE_DATE_FORMAT)\n\n\n# Custom types\nclass BaseManager(models.Manager):\n\n\tdef contribute_to_class(self, cls, name):\n\t\t\"\"\"\n\t\tAdd manager methods to the provided class.\n\t\t\n\t\t:param class cls:\n\t\t:param string name:\n\t\t:return:\n\t\t\"\"\"\n\t\tsetattr(cls, name, self)\n\n##\n# @author Christopher S. Corley / chriscorley@us"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\t\n\tif value == None:\n\t\treturn None\n\t\n\tstr_format = \"%Y-%m-%d %H:%M:%S\"\n\t\n\treturn datetime.datetime.strptime(value.strftime(\"%s\"), str_format)\n\n@task\ndef generate_sample_account(name, description, owner, time):\n\tpass"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn value.time()\n\t\t\ndef dehydrate_datetime(value):\n\t\"\"\"\n\tDehydrator for `datetime` values.\n\n\t:param value:\n\t:type value: django.db.models.DateTimeField\n\t:return:\n\t:rtype: django.utils.timezone.pytz.timezone.AbstractTimeZone\n\t\"\"\"\n\tif not isinstance(value, datetime):\n\t\treturn value\n\n\treturn value.strftime('%Y-%m-%d %H:%M:%S')\n\n"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif value: return value.strftime('%H:%M')\n\telse: return ''\n\ndef serialize_time(data):\n\t\"\"\"\n\tSerializes `time` object to string.\n\t\n\t:param data:\n\t:return:\n\t\"\"\"\n\treturn data.isoformat() if data else ''\n\nclass DateTimeWithLocalTimezone(django.db.models.fields.DateTimeField):\n    \"\"\"\n    Custom field for dates.\n    \n    Supports automatic timezone detection and stores everything as UTC.\n    \n    * `"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn str(value)\n\ndef getattr_callable(obj, name):\n\tif name in [ \"id\", \"create_at\", \"update_at\", \"delete_at\" ]:\n\t\treturn obj.get(name)\n\t\ndef getattr_datetime(obj, name):\n\tif name in [ \"id\", \"create_at\", \"update_at\", \"delete_at\" ]:\n\t\treturn obj[name]\n\ndef getattr_attr(obj, name):\n\ttry:\n\t\treturn object.__getattribute__(obj,"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif not value:\n\t\treturn\n\treturn value.strftime(\"%Y-%m-%d %H:%M:%S\")\n\ndef dehydrate_datetime(value):\n\t\"\"\"\n\tDehydrator for `datetime` values.\n\t\n\t:param value:\n\t:type value: datetime.datetime\n\t:return:\n\t\"\"\"\n\tif not value:\n\t\treturn\n\treturn value\n"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\treturn timezone.localtime(value).isoformat().replace(':', '_')\n\n\"\"\"\n\tDehydrator for `datetime` values.\n\t\n\t:param value:\n\t:type value: datetime\n\t:return:\n\t\"\"\"\n\treturn value.isoformat().replace(':', '_')\n\n\"\"\"\n\tDehydrator for `timedelta` values.\n\t\n\t:param value:\n\t:type value: timedelta\n\t:return:\n\t\"\"\"\n\treturn str(value)\n\n\"\"\"\n\tDehydrator"}, {"generated_text": "def dehydrate_time(value):\n\"\"\"\n\tDehydrator for `time` values.\n\t\n\t:param value:\n\t:type value: Time\n\t:return:\n\t\"\"\"\n\tif not value:\n\t\treturn None\n\tvalue = pytz.timezone(config['default_timezone']).localize(value)\n\treturn value.isoformat().split(\".\")[0] + \"Z\" \n\nclass TimeField(models.DateTimeField):\n    dehydrate = dehydrate_time\n    \n    def contribute_to_class(self, cls, name):\n        super(TimeField, self).contribute_to_class(cls, name)\n\n        # TODO: We might need to add a metaclass in which"}]}
{"task_id": "62e60f33d76274f8a4026de9", "prompt": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value.wkt\n\nclass PointSerializer(serializers.ModelSerializer):\n\twkt = WktPointField(format=\"point\", required=False)\n\t\n\tclass Meta(object):\n\t\tmodel = Point\n\t\t\"\"\"\n\t\tA ModelSerializer will add a default order to the fields. In this\n\t\tcase, it's field is Point.id. We can define the order (or no\n\t\torder) by adding it after the name of the field. See\n\t\thttp://stackoverflow.com/a/19148968/611694\n\t\t"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\n\timport sys\n\tif sys.version_info >= (3, 0):\n\t\t# Python 3.x\n\t\treturn Point(value.x, value.y)\n\telse:\n\t\t# Python 2.x\n\t\treturn Point(value.getX(), value.getY())\n\n"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn \"%s-%s\" % (value.x, value.y)\n\nclass PointField(GeoPointField):\n\t\"\"\"\n\t\tPointField\n\t\"\"\"\n\tdehydrate_for_db = dehydrate_point\n"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\tif not isinstance(value, Point):\n\t\traise TypeError\n\n\treturn {\n\t\t\"x\": value.x,\n\t\t\"y\": value.y\n\t}\n\ndef dehydrate_linestring(value):\n\t\"\"\"\n\tDehydrator for LineString data.\n\t:param value:\n\t:type value: LineString\n\t:return:\n\t\"\"\"\n\tif not isinstance(value, LineString):\n\t\traise TypeError\n\n\treturn {\n\t\t\"geom\": GeoJSONWriter.dehydrate_geojson(value.geom"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\n\treturn {\n\t\t\"type\": value.geom_type,\n\t\t\"srid\": value.srid\n\t}\n\n\n@cache.cache_method\ndef dehydrate_polygon(value):\n\t\"\"\"\n\tDehydrator for Polygon data.\n\t\n\t:param value:\n\t:type value: Polygon\n\t:return:\n\t\"\"\"\n\t\n\treturn {\n\t\t\"type\": value.geom_type,\n\t\t\"srid\": value.srid,\n\t\t\"bbox\": [value.bbox.xMin,"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn {\n\t\t\"type\": \"Point\",\n\t\t\"coordinates\": [round(value[1],8), round(value[0],8)] \n\t}\n\n\"\"\"\n\t\n\tDehydrator that handles LineString data \n\t\n\tDehydrator for LineString data.\n\t\n\t:param value:\n\t:type value: LineString\n\t:return:\n\t\"\"\"\n\tdef dehydrate_point(value):\n\t\treturn {\n\t\t\t\"type\": \"Point\",\n\t\t\t\"coordinates\": [round(value.bounds[1],8"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\n\treturn value.as_wkt    \n\n@gefielder.value\ndef dehydrate_multipoint(value):\n\t\"\"\"\n\tDehydrator for Multipoint data.\n\t\n\t:param value:\n\t:type value: Multipoint\n\t:return:\n\t\"\"\"\n\t\n\treturn [point.as_wkt for point in value.as_list()]\n\n@gefielder.value\ndef dehydrate_linestring(value):\n\t\"\"\"\n\tDehydrator for LinearString data.\n\t\n\t\tWKT"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value.__dict__,\n\ndef set_point_from_dict(mapper, session, obj):\n\t\"\"\"\n\tSet Point from a dict.\n\t\n\t:param mapper:\n\t:type mapper: mapper\n\t:param session:\n\t:type session: Session\n\t:param obj:\n\t:type obj: Point\n\t:return:\n\t\"\"\"\n\ttry:\n\t\tobj.__dict__ = dict(obj.point_dict.__dict__)\n\texcept:\n\t\tobj.__dict__ = {}\n\ndef"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\treturn value.coordinates\n"}, {"generated_text": "def dehydrate_point(value):\n\"\"\"\n\tDehydrator for Point data.\n\t\n\t:param value:\n\t:type value: Point\n\t:return:\n\t\"\"\"\n\t\n\tif not isinstance(value, Point):\n\t\tvalue = Point(value)\n\t\n\treturn value.lat, value.lon\n\t"}]}
{"task_id": "62e60ed4d76274f8a4026da0", "prompt": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn [k for k in self._all_fields if k and k.islower()]\n\t\n\t\n\t\nclass RecordField(object):\n\t\n\tREQUIRED_FLAG = \"required\"\n\tPRI_KEY_FLAG = \"is_primary_key\"\n\tKEY_FLAG = \"key\"\n\t\n\t@classmethod\n\tdef get_valid_key_regexp(cls):\n\t\treturn cls.get_name().lower() + \"[0-9A-Za-z_@]*\"\n\n\t@classmethod\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.__keys\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tresult = []\n\tfor key in self.__meta__['keys']:\n\t\tresult.append(key)\n\t\n\treturn result\n\n\ndef values(self):\n\t\"\"\"\n\tReturn all values of the record.\n\t\n\t:return: list of values\n\t\"\"\"\n\t# TODO\n\t# return the values in the 'items' structure in a dict\n\treturn [self.__row__[key] for key in self._key_list]\n\n\ndef items(self):\n\t\"\"\"\n\tReturn all values associated with 'keys' in the record."}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\tkey = ['id', 'user_id', 'name', 'type', 'description']\n\t\n\tkey_list = list()\n\tfor _key in key:\n\t\tif self[_key]!= None:\n\t\t\tkey_list.append(_key)\n\n\treturn key_list\n\nclass records:\n\tdef __init__(self, file=''):\n\t\tself.records = {}\n\t\ttry:\n\t\t\t_file = open(file, \"rb\")\n\t\t\tself._file = _file\n\t\t\tline = _file.readline()\n\t\t\twhile line!="}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self._keys\n\n\t\nclass NoRecord(object):\n\t\"\"\"\n\tNo record is returned.\n\tA NoRecord object is returned by calling :meth:`loadRecord` when neither an object\n\twith the given keys and arguments would be found in the database or when you try\n\tto call method that only take ``*args`` for the given parameter names.\n\t\n\tNoRecord objects are not created directly, rather, you retrieve them with calling \n\t:meth:`NoRecord.load()` in your database's :meth:`DbManager.close()"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.fields.keys()\n\t\n\t\ndef keys1(self):\n\t\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key codes\n\t\"\"\"\n\treturn [ k(self,v) for k,v in self.fields]\n\t\n\t\ndef values(self,key):\n\t\"\"\"\n\tGet values for key\n\t\n\t:param key: key name\n\t:return: returns values for key\n\t\"\"\"\n\tif isinstance(key, tuple):\n\t\tkey,subkey=key\n\t\t\n\t\t"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\n\tkeys = []\n\t\n\tfor line in _get_stream(self._open_file(\"keys\"), self.encoding):\n\t\tkeys.append(line)\n\t\n\treturn keys\n\n# vim: tabstop=8 expandtab shiftwidth=4 softtabstop=4 \n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn self.fields.keys()\n\"\"\"\n\tReturns the value of the \"age\" attribute as a string, or False if not set.\n\"\"\"\ndef age(self):\n\tif hasattr(self, \"age\"):\n\t\tval = self.age\n\telse:\n\t\tval = None\n\treturn val\n\"\"\"\n\tReturns the value of the \"age\" attribute as an integer, or False if not set.\n\"\"\"\ndef age_int(self):\n\tif hasattr(self, \"age\"):\n\t\tval = self.age\n\t\tif val == \""}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\treturn keys(self)\n\n\t\nclass Table:\n\t\"\"\"\n\tA table is a list of records. Record order is preserved. Records can be accessed\n\tusing the keys (iterable). Records may be added and removed from the table using\n\tthe 'add' and'remove' methods. The 'iterate' method returns an iterator.\n\t\n\t\"\"\"\n\t\n\t\n\tdef __init__(self, init=[]):\n\t\t\"\"\"\n\t\t:param iterable init: initial data list (default: [])\n\t\t\"\"\"\n\t\tself._items = []\n\t\t\n"}, {"generated_text": "def keys(self):\n\"\"\"\n\tReturn the keys of the record.\n\t\n\t:return: list of key names\n\t\"\"\"\n\t\n\treturn self._keys\n\ndef key(self, key):\n\t\"\"\"\n\tReturn the value of the ``key`` field.\n\t\n\t:param key: key name\n\t:type key: string\n\t:return: value of the key, or ``None``\n\t:rtype: string\n\t\"\"\"\n\t#return self._record.get(key)\n\tfor k in self._keys:\n\t\tif k==key:\n\t\t\treturn self[key]\n\tprint('key not in record?')\n\tprint(self._record)\n\treturn None"}]}
{"task_id": "62e60ecfd76274f8a4026d6a", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\ttry:\n\t\tif protocol_version:\n\t\t\treturn cls._handler_registry_v2[protocol_version]\n\t\telif len(cls._handler_registry_v2) is 1:\n\t\t\treturn list(cls._handler_registry_v2)[0]\n\t\telse:\n\t\t\treturn list(cls._handler_registry_v2.values())\n\texcept KeyError:\n\t\traise ValueError(\"Unsupported protocol version %s\" % protocol_version)\n\t\t\n\t"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\treturn cls.protocol_handlers()\n\ttry:\n\t\thandler_class = cls.protocol_handlers().get(protocol_version)\n\t\treturn handler_class\n\texcept KeyError:\n\t\tif type(protocol_version) is not type(()):\n\t\t\traise TypeError(('protocol_handlers() must be passed a tuple for'+\n\t\t\t\t'a version tuple, not' + str(type(protocol_version))))\n\t\treturn None\n\nclass Protocol(object):\n\t\"\"\""}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\treturn cls.protocols(cls, protocol_version).items()\n\t\n## End of class definitions.\n\t\ndef protocol_handlers(protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available protocol handler handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    "}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t# Construct a generator function to create the class\n\tdef create(version=None):\t\t\n\t\t# Check if this is an explicit protocol version\n\t\tif version:\n\t\t\t# Yup, an explicit version\n\t\t\tif isinstance(version, tuple):\n\t\t\t\t# Ensure it's an actual tuple\n\t\t\t\tif version!= (protocol_version,):\n\t\t\t\t\traise TypeError(\"version must be tuple (or None) if specified\")\n\t\t\telse:\n\t\t\t\traise TypeError(\"expected tuple, but got %s instead\" % type(version))\n\t\telse:\n\t\t\t# No explicit version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\treturn cls._handlers\n\t\nif __name__ == '__main__':\n\tp = BoltProtocol(3, 5)\n\ts = BoltStream()\n\tassert_equal(s.version, (3, 5))\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\t# build/load protocol handler class\n\thandler_cls = {(3, 5)}\n\tfor idx, sver in cls._version_map.iteritems():\n\t    if protocol_version and (sver!= protocol_version):\n\t\tcontinue\n\t    handler_cls[sver] = cls\n\t\n\tdef all():\n\t    yield handler_cls\n\treturn all()\n\t\nclass BoltProtocol(object):\n\t\"\"\"\n\tBase class for all protocol drivers\n\t\"\"\"\n\t\n\tdef __init__("}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif not protocol_version:\n\t\treturn {cls.version: cls for cls in cls._protocol_handlers}\n\tif len(protocol_version)!= 2:\n\t\traise TypeError(\"protocol version must be a 2-tuple representing\" \n\t\t\t\"protocol version (e.g. (3, 5)\")\n\t\n\tif cls in cls._protocol_handlers:\n\t\treturn {protocol_version: cls}\n\treturn {}\n\n\nclass BoltProtocol(Protocol):\n\t_message_dict"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\thandlers = {}\n\t\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Provide a tuple as first argument\")\n\t\n\t# If not version, then return all versions available and None for protocol version\n\tif not protocol_version:\n\t\treturn handlers\n\t\n\t# If we already have a protocol version handler, then return that version\n\tfor key, handler in handlers.items():\n\t\t\n\t\t# Convert versions as necessary\n\t\told_version, new_version = key\n\t\t\n\t\tif old_version == protocol_version"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Expected protocol version as tuple\")\n\t\n\tprotocols = {}\n\tif protocol_version:\n\t\tif not (1 <= protocol_version[0] <= 3):\n\t\t\traise ValueError(\"Protocol version not supported\")\n\t\t\n\t\tif isinstance(protocol_version, tuple):\n\t\t\tprotocols[(3, 5)] = \"Bolt3.5\"\n\t\tprotocols[protocol_version] = \"Bolt%d\" % protocol_version[0]"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is None:\n\t\t# Return a list of versions where the code exists\n\t\tversions = list(iterkeys(cls.protocol_versions))\n\t\tif None in versions:\n\t\t\t# None is not a valid protocol version\n\t\t\tversions.remove(None)\n\t\treturn dict(zip(versions, versions))\n\n\tfor pv in cls.protocol_versions[protocol_version]:\n\t\tif isinstance(pv, tuple):\n\t\t\tcls._handlers[pv] = cls._create_handler(pv)\n\t\telse:"}]}
{"task_id": "62e60e49d76274f8a4026d25", "prompt": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t@decorator\n\tdef decorator_func(func):\n\t\tdef wrapped_func(*args, **kwargs):\n\t\t\t# create tx object\n\t\t\ttx = Transaction.create()\n\t\t\twith tx:\n\t\t\t\t# run function\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\t# get tx meta data\n\t\t\t\ttx_meta_data = tx.get_tx_meta_data()\n\t\t\t\t# create metadata from decorator parameters\n\t\t\t\tmetadata = metadata_from_decorator_params(kwargs) if kwargs else None\n\t\t\t\t# attach metadata to tx object"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef wrapper(func):\n\t\tfunc._unit_of_work = Metadata(metadata)\n\t\tfunc._timeout = timeout\n\t\treturn func\n\treturn wrapper\n\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef wrapper2(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkwargs['metadata'] = metadata\n\t\t\tkwargs['timeout'] = timeout\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn wrapper2\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\tdef wrapped(func):\n\t\tis_function = inspect.isfunction(func)\n\t\tif not is_function:\n\t\t\tis_method = inspect.ismethod(func)\n\t\ttry:\n\t\t\tif is_function or is_method:\n\t\t\t\tfunc = wrap_transaction(func)\n\t\texcept PendingTransaction:\n\t\t\tpass  # This will be handled with the wrap function.\n\t\tfunc.add_unit_of_work_marker()\n\t\treturn func\n\t\n\treturn wrapped\n\n\n# Helper decorators\n\nclass _PendingTransaction"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\tdef decor(f):\n\t\t\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\n\t\t\tif metadata is None:\n\t\t\t\tmetadata = {}\n\t\t\tif timeout is not None:\n\t\t\t\tmetadata[\"timeout\"] = float(timeout)\n\t\t\t\n\t\t\treturn f(*args, **kwargs)\n\t\t\n\t\treturn wrapper\n\t\n\treturn decor\n\n"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\tdef wrapper(func):\n\t\tif is_decorator(func):\n\t\t\tfunc.neo4j_unit_of_work = True\n\t\t\tfunc = func(neo_unit_of_work)\n\t\telse:\n\t\t\tfunc.neo4j_unit_of_work = True\n\t\t\tdef inner(*args, **kwargs):\n\t\t\t\tfunc(*args,**kwargs)\n\t\t\t\n\t\treturn inner\n\t\n\tif not callable(func):\n\t\treturn decorate(func, \"neo4j_unit_of_work\", True"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef decorate(f):\n\t\tif not timeout:\n\t\t\ttimeout = transaction_timeout\n\t\t# store the original function parameters\n\t\t# but replace them by a call that creates an internal transaction instead of a normal one.\n\t\t# This call is passed to new transaction, and is available to its methods as internal parameter 'tx'\n\t\toriginal_f = f\n\t\tdef inner(*args, **kwargs):\n\t\t\tfrom transaction import transaction as tx\n\n\t\t\t# create a new transaction instance\n\t\t\twith transaction():\n\t\t\t\treturn original_f(*args, **kwargs)\n\t\t# return a wrapper"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tif metadata:\n\t\treturn _transaction(metadata=metadata, timeout=timeout)(func)\n\telse:\n\t\treturn _transaction(timeout=timeout)(func)\n\t\ndef _transaction(metadata=None, timeout=None):\n\t\"\"\"\n\tRun a transaction function inside the transaction context.\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\t\n\t# check if transaction should be ignored or not\n\t# if it matches a pattern in the ignore list, return None\n\t\n\tdef f():\n\t\treturn fn()\n\t\n\t# check if transaction has timed out\n\t# if it has timed out, return None\n\t# otherwise execute the calling function and return result\n\t\n\tif timeout and (timeout <> 0):\n\t\ttry:\n\t\t\twith TransactionTimeoutMonitor(timeout) as monitor:\n\t\t\t\treturn fn()\n\t\texcept:\n\t\t\twith TransactionTimeoutMonitor(None) as monitor:\n\t\t\t\treturn None\n\telse"}, {"generated_text": "def unit_of_work(metadata=None, timeout=None):\n\"\"\"\n\tThis function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\t\n\tFor example, a timeout may be applied::\n\t\n\t    from neo4j import unit_of_work\n\t\n\t    @unit_of_work(timeout=100)\n\t    def count_people_tx(tx):\n\t        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n\t        record = result.single()\n\t        return record[\"persons\"]\n\t\n\t:param metadata:\n\t    a dictionary with metadata.\n\t    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n\t    It will also get logged to the ``query.log``.\n\t    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n\t:type metadata: dict\n\t\n\t:param timeout:\n\t    the transaction timeout in seconds.\n\t    Transactions that execute longer than the configured timeout will be terminated by the database.\n\t    This functionality allows to limit query/transaction execution time.\n\t    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n\t    Value should not represent a negative duration.\n\t    A zero duration will make the transaction execute indefinitely.\n\t    None will use the default timeout configured in the database.\n\t:type timeout: float or :const:`None`\n\t\"\"\"\n\tdef decorator(function):\n\t\tif isinstance(function, Transaction):\n\t\t\treturn function\n\t\telse:\n\t\t\treturn Transaction(function, metadata, timeout)\n\n\treturn decorator\n\nclass Transaction:\n\t\"\"\"\n\tThis class provides all features (start, run, suspend, resume, close and inspect) of neo4j-driver transaction execution system.\n\tIt consists of a function `with` that allows execution of a transaction by passing it a database session and some optional parameters.\n\t\n\t>>> tx = neo.Transaction()\n\t>>> tx.with(session).run(\"RET"}]}
{"task_id": "62e60e05d76274f8a4026cfd", "prompt": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tfor i, val in enumerate(self):\n\t\tif val[0] == key:\n\t\t\treturn i\n\t\t\n\treturn -1\n\t\n\t\n\t\ndef get(self, key):\n\t\"\"\"\n\t\tReturn the item associated with the given key.\n\t\t\n\t\t:param key: a key\n\t\t:return: item associated with the given key\n\t\t:rtype: List\n\t\"\"\"\n\treturn self[self.index(key)]\n\t\n\t\n\t\ndef keys(self):\n\t\"\"\"\n\t\tReturn all the keys in this list.\n\t\t"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.sorted[key].index\n\n\ndef get(self, key):\"\"\"\n\tGet the item corresponding to the given key.\n\t\n\tIf some key is not found, returns None.\n\t\n\t:param key: the key\n\t:returns: None or the value\n\t\"\"\"\n\treturn self.sorted[key]\n\n\ndef remove(self, key):\"\"\"\n\tRemove or delete the associated to the given key\n\t\n\t:param key: the key\n\t:rtype: bool\n\t:param self.\n\t\"\"\"\n\tif key"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.indices[key]"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.list.index(key)\n\t\n\t\ndef key(self, key):\n\t\tif key < 0 or key >= len(self.list):\n\t\t\traise out_of_range_error\n\n\treturn self.list[key]\n\n'''\ndef contains(self, key):\n\treturn key in self.list\n\t\n\ndef getitem(self, key):\n\treturn self.list[key]\n\ndef __deepcopy__(self, memo):\n\tcls = self.__class__\n\tresults = [None] * len("}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tif key in self:\n\t\treturn self.data.index(key)\n\ndef items(self):\n\t\"\"\"\n\tReturn a list of (key, obj) pairs in the tree that\n\tmatch the given filtering function.\n\t\n\t:param fn: filtering function\n\t:return: a list of (key, obj) pairs\n\t:rtype: list\n\t\"\"\"\n\treturn [(key, value) for (key, value) in self.data.iteritems() if fn(key, value)]\n\ndef keys(self):\n\t\"\"\"\n\tReturn a list"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tvalue = self.get(key)\n\tif value:\n\t\treturn value\n\telse:\n\t\treturn 0\n\t\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self.items[key]\n\n\ndef removeIndex(self, ikey):\n\t\"\"\"\n\tRemove the given index.\n\t\n\t:param ikey: an index\n\t\"\"\"\n\ti = self.findIndex(ikey)\n\tdel self.items[i]\n\n\n\n\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\tif self.isempty() or key not in self.keys(): return -1\n\treturn self.data.get(key, 0)\n\ndef set(self, key, value):\n\t\"\"\"\n\tSet a key's value to a new value\n\t\n\t:param key: a key\n\t:type key: int or str\n\t:param value: a new value\n\t:type value: any\n\t:return: nothing\n\t:rtype: None\n\t\"\"\"\n\tkey = checkKey(key)\n\tself.data[key] = value\n\ndef"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self._keyindex_to_rowindex[key]\ndef find(self, key):\"\"\"\n\tFinds and returns the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn self._keyindex_to_rowindex[key]\ndef find2(self, key):\"\"\"\n\tFinds and returns the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\t\n"}, {"generated_text": "def index(self, key):\n\"\"\"\n\tReturn the index of the given item.\n\t\n\t:param key: a key\n\t:return: index\n\t:rtype: int\n\t\"\"\"\n\treturn key.index(self.keys)\n\ndef get(key_index, default = None):\n\t\"\"\"\n\tReturn the value at the given index, with a default value when\n\tthe index does not point to a valid value.\n\t\n\t:param key_index: index of the key\n\t:param default: the default value\n\t:return: the value at the given index, the default when the\n\t       index does not point to a valid item\n\t:rtype: value\n\t\"\"\"\n\treturn default if self._data[key_index] is None else"}]}
{"task_id": "62e60da4d76274f8a4026cf1", "prompt": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif keys is None:\n\t\treturn sum([self.get_value(k) for k in self.keys()], [])\n\treturn self.get_value(tuple(keys))\n\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn [val for val in self.items.values(*keys) if not (keys and val.index < 0)]\n\n\nfrom.record import Record\n\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\t\n\treturn [self[key] for key in keys if key in self]\n\n#END ========================================================================="}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tquery = self.query\n\trecords = query.limit(limit, skip).all()\n\t\n\treturn [rec.values(*keys) for rec in records]\n\nclass Records(object):\n\t\"\"\"\n\tProvide a simple and easy way to retrieve the records from the query, using\n\tthe methods :meth:`select`, :meth:`where`, etc.\n\t\n\tExample usage:\n\t\n\t>>> import sqlalchemy.orm\n\t\n\t>>> query = engine.query(engine.User).filter(user.name == 'user1')"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif keys:\n\t\treturn [ item[keys[0]].value if type(item[keys[0]]) == pydasm.AsmRecordType else item for item in self.values() ]\n\treturn [ item.value for item in self.values() ]\n\t\t\nclass PydasmFileParser(object):\n\t\"\"\"\n\tParser for ``*.pydasm`` files. Each ``AsmRecord`` is parsed\n\tand contained in a ``AsmRecordSet`` object.\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\":return: a list"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif not keys:\n\t\treturn super(OrderedRecords, self).values()\n\tdata = super(OrderedRecords, self).values()\n\tfor key in keys:\n\t\ttry:\n\t\t\tdata = data[data.index(key)]\n\t\texcept (ValueError, TypeError):\n\t\t\traise KeyError(\"No %s records in %r\" % (key, self.name))\n\t\t\n\treturn data\n\nclass DictRecords(Records):\n\tdef __init__(self, name, **fields):\n\t\t\"\"\"\n\t\tInitialise with a"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn [self[key] for key in keys if key in self]\n\t\n\t\ndef keys(self, *keys):\n\t\"\"\"\n\tReturns the keys of the record; optionally only\n\tinclude certain keys by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: set of keys\n\t:rtype: set\n\t\"\"\"\n\tif keys:\n\t\treturn set([key for index, key in enumerate(keys) if index < len(self)"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tif keys:\n\t\treturn self._record.values(*keys)\n\telse:\n\t\treturn self._record.values()\n\nclass Record(object):\n\t\"\"\"\n\tAbstract class for record information classes.\n\t\"\"\"\n\t\n\tdef __init__(self, record={}):\n\t\t\"\"\"\n\t\tInitializer.\n\t\t\n\t\t:param record: dictionary of the record\n\t\t:type record: dict\n\t\t\"\"\"\n\t\tself._record = record\n\nclass RecordList(list):\n\t\"\"\"\n\tHandy list class that can operate over records.\n\t\"\"\"\n"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\tvalues = self._values[0]\n\tif len(keys) > 0:\n\t\tif isinstance(values[0], tuple):\n\t\t\treturn [x[0] for x in values if x[0] in keys]\n\t\treturn [values[0] for x in keys if x in values[0]]\n\treturn values\n\n@_reg.compat(0,6,2)\ndef _get_record(rec):\n\treturn list(_get_records(rec))[0]  \n\n@_reg.compat(0,6,2)"}, {"generated_text": "def values(self, *keys):\n\"\"\"\n\tReturn the values of the record, optionally filtering to\n\tinclude only certain values by index or key.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t             are provided, all values will be included\n\t:return: list of values\n\t:rtype: list\n\t\"\"\"\n\treturn self._c.cursor.execute(pipeline.make_pipeline(\n\t\tpipeline.values(*keys)\n\t)).fetchall()\n\n# =================================================================== Record\n\ndef to_record(cursor):\n\t\"\"\"\n\n\t    Generate a new :class:`Record` of the given :class:`Cursor` object.\n\n\t    To avoid data loss, it is strongly recommended that a proxy for `Cursor`'s\n\t    underlying file descriptor be passed to this function. Otherwise, every time\n\t    a new :class:"}]}
{"task_id": "62e60b10d76274f8a4026ccd", "prompt": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\td = {}\n\tfor k in keys:\n\t\tif isinstance(k, slice):\n\t\t\tk_start = k.start if k.start is not None else 0\n\t\t\tk_stop = k.stop if k.stop is not None else len(self._items)\n\t\t\tif not (0 <= k_start < k_stop):\n\t\t\t\traise IndexError(\"slice indices out of bounds\")\n\t\t\tk_stop = k_stop if k_stop < len(self._items) else None\n\t\telse:\n\t\t\ttry:\n\t\t\t\tk = int(k)\n\t\t\t\t"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\treturn dict(zip(keys, self.itervalues(*keys)))\n\n\n\n# ------------------------------------------------------------------------------\nclass Row(Record):\n\t\"\"\"\n\tDefines a single row in a database table, given the ``table``, the\n\t``rowid``, and the ``values`` of the row.\n\n\t:param table: table object\n\t:param rowid: rowid object\n\t:param values: an iterable containing the values of the row\n\n\tThis class is deprecated and will be removed in version 0.6; instead, use\n\t:class:`RowMapper <"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\td = dict()\n\tfor i, k in enumerate(self.recdata):\n\t\tif keys is None or i in (keys):\n\t\t\ttry:\n\t\t\t\td[k] = self.recdata[k]\n\t\t\texcept KeyError:\n\t\t\t\td[k] = _rec_default\n\t\t\t\tLOG.debug(\"Value '%s': None inserted\" % k)\n\t\telse:\n\t\t\tLOG.debug(\"Value '%s': %f replaced by _rec_default\" % (k, d[k]))\n\treturn d\n\n\t\n\t\n\t\n\t"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\treturn dict(zip(self._keys, self._data.values()))\n\n\t\n\n\n###\n# Exposing field properties via an interface (as opposed to direct attribute access)\n###def __getattr__(self, key):\n\t# Returns the _values of a field by index\n\t# - or -\n\t# Returns the value of a field by key\n\t# - or -\n\t# Returns the value of a field, if there is one; `None`\n\t# otherwise (with a warning, if the field cannot be found)\n\t# - or -\n\t#"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\n\tretval = {}\n\tkeys_to_use = self._keys_to_use\n\tfor key in keys or keys_to_use:\n\t\tvalue = self.get(key)\n\t\tvalue = self._validate_value(key, value)\n\t\tretval.update({key:value})\n\t\n\treturn retval\n\t\n\t\ndef all(self):\n\t\"\"\"\n\tReturn a tuple of all the keys and values in the record, including\n\tboth named and un-named keys and values.\n\t\n\tSee :meth:`."}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tresult = {}\n\twith _ensure_exception(self):\n\t\titems = list(self.items())\n\titems.extend(items[-i][:keys and len(keys) or self.n or 0]\n\t\tfor i in range(len(keys or [])) if keys and keys[i] is not None)\n\tfor item in items: result[item[0]] = item[1]\n\treturn result\n\n\t\n\nclass Record(Itemized):\n\t\"\"\"\n\tAn object representing the values of any number of key/value pairs.\n\n\t"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tif keys:\n\t\tr = {}\n\t\tfor k in keys:\n\t\t\ttry:\n\t\t\t\tr[k] = self._data[k]\n\t\t\texcept:\n\t\t\t\tr[k] = None\n\t\treturn r\n\telse:\n\t\treturn self._data\n\n\n"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tret = {}\n\tif self is None:\n\t\tpass\n\telif keys:\n\t\tfor i, k in enumerate(keys):\n\t\t\ttry:\n\t\t\t\tret[k] = self[k]\n\t\t\texcept IndexError:\n\t\t\t\tret[k] = None\n\t\treturn ret\n\telse:\n\t\treturn self.data\n\nclass SimpleRecord(Record):\n\t\"\"\"Provides an interface to easily store and retrieve simple string\n\tvalues based on a set of fields and their associated indexes.\n\t\"\"\"\n\t__slots__ = ()\n\t\n\tdef get_"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\t\n\treturn dict((self[str(idx)], val) for idx, val in enumerate(self)\n\t            if any([k in keys for k in self[idx]] - {None})\n\t)\n\t\n\t\nclass RecordIterator(object):\n\t\"\"\"\n\tIterate over records in a RecordSet. The iterator is iterable, so the\n\tfollowing:\n\t\n\t>>> for record in RecordIterator(records):\n\t>>> \tprint record\n\t\n\tis effectively the equivalent of iterating over records with the\n\tfollowing"}, {"generated_text": "def data(self, *keys):\n\"\"\"\n\tReturn the keys and values of this record as a dictionary,\n\toptionally including only certain values by index or key. Keys\n\tprovided in the items that are not in the record will be\n\tinserted with a value of :const:`None`; indexes provided\n\tthat are out of bounds will trigger an :exc:`IndexError`.\n\t\n\t:param keys: indexes or keys of the items to include; if none\n\t              are provided, all values will be included\n\t:return: dictionary of values, keyed by field name\n\t:raises: :exc:`IndexError` if an out-of-bounds index is specified\n\t\"\"\"\n\tkeys = (0, ) if keys is None else keys\n\trv = {}\n\tfor idx in keys:\n\n\t\t# Get the current value\n\t\tv = self.get(idx)\n\t\tif (idx in keys) or (v is None):\n\n\t\t\t# Skip the value\n\t\t\tpass\n\n\t\t# Add it to the output dict\n\t\telse:\n\t\t\trv[v[0]] = v[1]\n\n\treturn rv\n\n# Register\nRecord.register(Type)\n"}]}
{"task_id": "62e6087bd76274f8a4026bfa", "prompt": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t# Get data size before removing it.\n\tdata_size = len(self)/2 - 2\n\t# Remove data.\n\tself.set(self[-data_size:])\n\t# Put new data.\n\tself.push(unpack(\"=H\",\"\"+self.pop_u8()+self.pop_u8()))\n\t\ndef pop_u32(self):\n\t\"\"\"\n\tReturn the next four bytes of data from the queue as a big-endian\n\t32-bit unsigned integer.\n\t\"\"\"\n\tself.set(self"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn (ord(self._data_buffer[self.cur_pos:self.cur_pos+2]) << 8) + \\\n\t      ord(self._data_buffer[self.cur_pos+2:self.cur_pos+4])\n\t\n\ndef pop_u32(self):\n\t\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n\t32-bit-unsigned integer.\n\n\t>>> c64.pop_u32()\n\t\"\"\"\n\treturn ((ord(self._data_buffer[self.cur_"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn self.endian_u16(*self.pop_lend())\n\nclass DecodeStack_u16(DecoderStack):\n\tdef __init__(self):\n\t\tDecoderStack.__init__(self, data=0, endian=little8endian)\n\nclass EncodeData_u16(EncodeStack):\n\tdef __init__(self, endian=little8endian):\n\t\tEncodeStack.__init__(self, endian)\n\t\n\tdef push_u16(self, data):\n\t\t\"\"\"\n\t\t"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tpos = self.size()-2\n\tself.pop_bytes(2)\n\treturn struct.unpack('>H', self.get_bytes(2))[0]\n\n\nclass Packet(PacketBase):\n\t\"\"\"\n\tPacket: A wrapper for PacketBase, allowing a single\n\tsetter for the raw packet. Use packet.raw() if you are getting\n\ta real packet (use Packet.from_bytes() to get a PacketBase\n\tobject)\n\n\tSubclass of packet.Base, but as a class instead of a method of"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\traw = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn unpack(\">H\", raw)[0]\n\nclass Header:\n\t\"\"\"\n\tClass implementing the EIA-708 specification.\n\t\"\"\"\n\tdef __init__(self, data=None):\n\t\tif data is not None:\n\t\t\tself.parse(data)\n\n\tdef parse(self, data):\n\t\t\"\"\"\n\t\tImplements the parsing function described in the EIA-708 specification.\n\t\t\"\"\"\n\t\tself.version = pop_u16"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tdata = self.data\n\tdata = data[:-2]\n\treturn unpack(\">H\", data)[0]\n\t\ndef pop_u32(self):\n\t\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n\t32-bit unsigned integer.\n\t\"\"\"\n\tdata = self.data\n\tdata = data[:-4]\n\treturn unpack(\">I\", data)[0]\n\t\ndef pop_s16(self):\n\t\"\"\"\n\tRemove the last two bytes of data, returning them as a signed"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\t# TODO:\n\t# * The most significant 3 bytes are returned\n\t#     at little-endian.\n\t# * The least significant 3 bytes are returned\n\t#     at big-endian.\n\treturn self.pop()\n  \nclass Frame:\n\t\"\"\"\n\tEach function in the stack must return a new instance of this class\n\t(not a copy of it!) when it finishes. Frames are used by the interpreter\n\tto detect and handle protocol exceptions and the like.\n\t\"\"\"\n\n\tdef __init__(self, sender_addr, receiver_addr,"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\treturn struct.unpack('!H', self.buffer)[0]\n\ndef pop_f32(self):\n\t\"\"\"\n\tRemove the last four bytes of data, returning them as a floating\n\tpoint number.\n\t\"\"\"\n\treturn struct.unpack('!f', self.buffer)[0]\n\ndef pop_str(self):\n\t\"\"\"\n\tRemove the last 8 bytes of data, returning the bytes as a null-terminated\n\tstring encoded in UTF-8.\n\t\"\"\"\n\treturn self.buffer[:-2].decode('utf-8')"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tself._buff.lpop()\n\tself._buff.lpop()\n\treturn (self._buff.get_uint16() << 8) + self._buff.get_uint8(2)\n\n\n###################################################################\n# Debugging functions.\n###################################################################\n\ndef dumpdata(buff_obj):\n\t\"Debugging function.\"\n\tf = open(\"/tmp/packtul\", \"w\")\n\tfor i in buff_obj.data:\n\t\tif(i == 0):\n\t\t\tcontinue\n\t\telse:\n\t\t\tf.write(\"%x\\n"}, {"generated_text": "def pop_u16(self):\n\"\"\"\n\tRemove the last two bytes of data, returning them as a big-endian\n\t16-bit unsigned integer.\n\t\"\"\"\n\tb16 = struct.unpack(\"H\",self.data[-2:]+self.data[-1])[0]\n\tself.data = self.data[:-2]\n\treturn b16\n\ndef bcd_to_float(bcd):\n\t\"\"\"\n\tBCD representation to floating point.\n\n\tReturns\n\t-------\n\tfloat\n\t\t\tThe bcd integer converted to a float.\n\t\"\"\"\n\t# See http://docs.python.org/library/struct.html#formats\n\treturn struct.unpack(\">f\",struct.pack"}]}
{"task_id": "62e6087ad76274f8a4026bf2", "prompt": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t)\n\n\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t):\n        \n        \n        \n        \n\t\"\"\"\n\tDehydrate the object into a tuple (type, data)\n\t\n\t:param obj: The object to dehydrate\n\t:param hydrate_hooks: Map (class, dehydration function) of types to\n\t    hydration functions.\n\t:returns A tuple of <type, dehydrated value>\n\t\"\"\"\n\t# return Response(Message.DEHYDRATE(obj.value, dehydrate_hooks), handlers=handlers)\n        if dehydrate_hooks:\n            return Response("}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t):\n\tcursor = self.cursor()\n\tstart = cursor.lastrowid\n\tif n>0 and qid==-1:\n\t\tn = n+start\n\telif n==0 and qid==-1:\n\t\tpass\n\telse:\n\t\tn += start\n\tdisclaimer = '\\n[DISCARD {}:{}]'.format(n,qid)\n\tif p:\n\t\tif handlers:\n\t\t\thandler = p.handlers[p.class_]\n\t\telif class_:\n\t\t\thandler = p.hand"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t):\n\tdef getHooks(self):\n\t\traise Exception(\"This instance does not have any hooks\")\n\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\treturn DISCARD(n, qid, dehydration_hooks, hydration_hooks, handlers)\n\nclass MAPPING(message):\n\ttype = 'MAPPING'\n\t\"\"\"\n\tSends the MAPPING message to the output queue.  Hints the server to map\n\tfields from the MAPPING message to the fields of the response message class.\n\t\n\tFor example, if the message class is ClientRequest, one may map the field field_a\n\tof the MAPPING message to a field_a of the response message class.  The\n"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t)\n\n\t#: Return a response object from a MULTI / BRPOPLPUSH message\n\t@classmethod\n\tdef brpoplpush(cls, timeout=None, channel=0, value=1, *handlers):\n\t\t\"\"\"\n\t\t:return Response\n\t\t\n\t\tImplement M-LPUSH/MULTI messages\n\t\t\"\"\"\n\t\treturn Response(*packstream.brpoplpush(timeout=timeout, value=value, channel=channel),\n\t\t\t\t\t\t\t*handlers)\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturn the length of"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t):\n\t\"\"\"\n\t\n\tReturns a new `Response object` for the supplied arguments.\n\t\n\t:param n: number of records to discard, default =-1 (ALL)\n\t:param qid: query ID to discarte for, default =-1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t):\n\t\t\n\t\tif not dehydration_hooks:\n\t\t\tdehydration_hooks = {}\n\t\tif not hydration_hooks:\n\t\t\thydration_hooks = {}\n\t\tout = self.Response()\n\n\t\t# generate the discard message\n\t\tself.cmd_msg(\n\t\t\tpackstream.msg('discard', \n\t\t\tpackstream.int_type('n', n), \n\t\t\tpackstream.int_type('qid', qid),\n\t\t\tpackstream.dict_type(),\n\t\t\tpackstream.dict_type(dehydration_hooks"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t):\n\n\tlogger = logging.getLogger('%s.%s' % (\n            packstream.__module__,\n            packstream.__doc__))\n\n        try:\n            self._output = packstream\n            # Set the default output stream so we don't have to call\n            # setOutput() by accident and break any logging for it.\n            self._output = sys.stdout\n        except AttributeError:\n            raise PackStreamException(\n                'output cannot be set during initialization')\n        self._output.setOutput(sys.stdout)"}, {"generated_text": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a DISCARD message to the output queue.\n\t\n\t:param n: number of records to discard, default = -1 (ALL)\n\t:param qid: query ID to discard for, default = -1 (last query)\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\t)\n\t\t\n\tdef reset_response_hooks(self, handlers=None):\n\t\tself._response_hooks = handlers\n\t\n\t@typecheck\n\t@decorate_property(__doc__, \"appends DISCARD messages to the output queue.\")\n\t@decorate_method(None)\n\tdef append(self, n=-1, qid=-1, dehydration_hooks=None,\n\"\"\"\n\tAppends a RESET message to the output queue.\n\t\n\t:param n: number of records to reset, default = -1 (ALL"}]}
{"task_id": "62e60879d76274f8a4026bec", "prompt": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\n)\n\n@register_command(\"commit\")\ndef commit(self):\n\t\"\"\"\n\tAppears to work the same way as begin but performs the transaction and adds a COMMIT.\n\t\"\"\"\n\t\n@register_command(\"rollback\")\ndef rollback(self):\n\t\"\"\"\n\tAppears to work the same way as begin but performs the transaction and adds AUTO_ROLLBACK.\n\t\"\"\""}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tfrom bookstream.transaction import Transaction\n\n\tif timeout is None:\n\t\ttimeout = self.client.settings['connect_timeout']\n\tif mode is None:\n\t\tmode = self.client.settings['readwrite']\n\tif bookmarks is None:\n\t\tbookmarks = set()\n\tif metadata is None:\n\t\tmetadata = dict()\n\n\treturn Transaction(self.client.transactions.begin(\n\t\tmode,\n\t\tbookmarks,\n\t\tmetadata,\n\t\ttimeout,\n\t\tbookstream.client,\n\t\timp_user,\n\t\tdehydration_hook"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tif \"database\" in __builtin__.__dict__:\n\t\tdatabase = __builtin__.__dict__[\"database\"]\n\telif \"database\" in context.__dict__:\n\t\tdatabase = context.__dict__[\"database\"]\n\telse:\n\t\tdatabase = context.__dict__[\"session\"][\"database\"]\n\n\tif mode is None:\n\t\tmode = database.get(\"mode\")\n\n\tif bookmarks is None:\n\t\tbookmarks = database.get(\"bookmarks\", None)\n\n\tif metadata is None:\n\t\tmetadata = {}\n\n\t"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tpass\n\n@abc.abstractmethod\ndef commit(self, bookmarks=None, timeout=None, imp_user=None, metadata=None):\n\t\"\"\"\n\tAppends a Commit message to the output queue.\n\t\n\t:param bookmarks: iterable of bookmark values after which this txn committed\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param imp_user: the user to impersonate\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t\"\"\"\n\tpass\n\n@abc.abstractmethod\ndef rollback("}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t) -> Response:\n\t\timport boltdb\n\t\treturn self.call(BoltHandler.command(self, mode=mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout), handles)\n\n\tdef commit(self) -> Response:\n\t\timport boltdb\n\t\treturn self.call(BoltHandler.command(self, mode=\"COMMIT\"))\n\t\n\tdef abort(self) -> Response:\n\t\timport boltdb\n\t\treturn self.call(BoltHandler.command(self, mode=\"ABORT\"))\n\n\tdef exists(self, mode=None, book"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\treturn _TransactionResponse(\n\t\tself._queue, self._tx_client.begin(mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks, handlers)\n\t)\n\n\n@lru_cache(maxsize=2**14)\ndef commit(self, timeout=None):\n\t\"\"\"\n\tAppends a COMMIT message to the output queue.\n\t\n\t:param timeout: timeout for transaction execution (seconds)\n\t:return: Response object\n\t\"\"\"\n\treturn _TransactionResponse(\n\t\tself._queue"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\tdef write(self, data):\n\t\t\"\"\"\n\t\tAppends data to the write queue.\n\t\t:param data: data to be written\n\t\t:return:\n\t\t\"\"\"\n\t\tpass\n\nclass Response:\n\t\"\"\"\n\tA Response object contains data from the transaction\n\tIt can be serialised to a byte array, or read from it.\n\t\n\tIt can be returned from ``begin`` and can be used in any ``select()`` call.\n'''\n\t@abc.abstractmethod\n\tdef serialise(self):\n\t\t\"\"\"\n\t\tSerialises the response"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t) for mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks, handlers in zip(*[iter_modes(mode), iter_bookmarks(bookmarks), iter_metadata(metadata), iter_timeout(timeout), iter_db(db), iter_imp_user(imp_user), iter_dehydration_hooks(dehydration_hooks), iter_hydration_hooks(hydration_hooks)], 1)]\n\n    @manifest_message(messages.end)\n\tdef end(self"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t    \n    @coroutine\n    @wrap_exceptions\n    def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name"}, {"generated_text": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n\"\"\"\n\tAppends a BEGIN message to the output queue.\n\t\n\t:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n\t:param bookmarks: iterable of bookmark values after which this transaction should begin\n\t:param metadata: custom metadata dictionary to attach to the transaction\n\t:param timeout: timeout for transaction execution (seconds)\n\t:param db: name of the database against which to begin the transaction\n\t    Requires Bolt 4.0+.\n\t:param imp_user: the user to impersonate\n\t    Requires Bolt 4.4+\n\t:param dehydration_hooks:\n\t    Hooks to dehydrate types (dict from type (class) to dehydration\n\t    function). Dehydration functions receive the value and returns an\n\t    object of type understood by packstream.\n\t:param hydration_hooks:\n\t    Hooks to hydrate types (mapping from type (class) to\n\t    dehydration function). Dehydration functions receive the value of\n\t    type understood by packstream and are free to return anything.\n\t:param handlers: handler functions passed into the returned Response object\n\t:return: Response object\n\t\"\"\"\n\t\n\tdef __init__(self, **kwargs):\n\t\tsuper(begin, self).__init__(**kwargs)\n\n\tdef execute(self):\n\t\t\"\"\"executes the begin command given the supplied arguments\"\"\"\n\t\tsuper(begin, self).execute()\n\n\t\tif self[\"db\"]:\n\t\t\tself.client.connect()\n\t\t\tif not self.client.__is_valid_db(self[\"db\"]):\n\t\t\t\traise BoltError(\"unknown database\")\n# XXX This is in the Bolt namespace for now because it's not been written\n# in the protocol yet."}]}
{"task_id": "62e60723d76274f8a4026b75", "prompt": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn n\n\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n == None:\n\t\treturn None\n\tif n == 0:\n\t\treturn 0\n\tremaining_part = n - round(n)\n\tif remaining_part >= 0:\n\t\treturn round_half_to_even(n)\n\telse:\n\t\treturn round_half_to_even(n-1)\n\n\ndef round(number, decimal, integer):\n\tif(decimal == None):\n\t\treturn number\n\telse:\n\t\tmultiplier = 10**decimal\n\t\tinteger_part = number / multiplier\n\t\tdecimal_part = number"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tfloor_n = to_float(floor(n))\n\t\n\tif floor_n == n:\n\t\treturn n\n\n\tremainder_n = n - floor_n\n\n\tif abs(remainder_n) <= 0.5:\n\t\tremainder_n = round_half_to_even(floor_n + remainder_n)\n\telse:\n\t\tremainder_n = round_half_to_even(floor_n) + round_half_to_even(floor_n + remainder_n - 1.0)\n\n\treturn"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif ( n % 1.0 ) < 0.5:\n\t\treturn int( n )\n\telse:\n\t\treturn int( round( float( n ) + 0.5) )\n\n\nif __name__ == '__main__':\n\n\timport doctest\n\tdoctest.testmod()\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn int((n+0.5)*10)"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n < 0 or n > 5:\n\t\traise ValueError(\"Number must not exceed 0.5 and less than 5\")\n\t\n\toutput = n\n\t\n\tif n == 1:\n\t\treturn n*(int(math.ceil(n)))\n\t\n\telif n == 2:\n\t\treturn n*(int(math.floor(n)))+1\n\t\t\n\telif n == 3 or n == 4:\n\t\treturn n*(int(math.floor(n)))\n\t\n\telif n == 5:\n\t\treturn n*("}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n < 1:\n\t\tn += 0.5\n\tif n < 1:\n\t\treturn 1\n\n\tsign = 1\n\tif n >= 0.5:\n\t\tsign *= -1\n\n\tn *= 2\n\ti2 = n // 2\n\ti3 = 0\n\tif len(str(abs(n))) > 3:\n\t    i2 = n // 3\n\t    n %= 3\n\n\twhile i2!= i3 and n > 0.5:\n\t\ti3 = i2\n\t\tn = n - (n % n)\n"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\treturn int(n) / 2 if int(n) % 2 else int(n) / 2 + 1\n\t\t\ndef get_digit_in_decreasing_order(number, index, string):\n\t\"\"\"\n\t:param number:\n\t:param index:\n\t:param string:\n\t\tstring = abcdefghijklmnopqrstuvwxyz\n\t:return:\n\t\"\"\"\n\tdecreased_index = 0\n\twhile index + decreased_index <= len(string) - 1 and decreased_index <= index:\n\t\tif string"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\tif n % 0.5 == 0:\n\t\treturn int(n)\n\telse:\n\t\treturn round_half_to_even(n - 0.5 * math.floor(n/0.5) * 0.5)\n\n#\tround_half_to_even(3)\n#\t>>> round_half_to_even(3.2)\n#\t    3\n#\t    >>> round_half_to_even(3.5)\n#\t    4\n#\t    >>> round_half_to_even(3.7)\n#"}, {"generated_text": "def round_half_to_even(n):\n\"\"\"\n\t>>> round_half_to_even(3)\n\t    3\n\t    >>> round_half_to_even(3.2)\n\t    3\n\t    >>> round_half_to_even(3.5)\n\t    4\n\t    >>> round_half_to_even(3.7)\n\t    4\n\t    >>> round_half_to_even(4)\n\t    4\n\t    >>> round_half_to_even(4.2)\n\t    4\n\t    >>> round_half_to_even(4.5)\n\t    4\n\t    >>> round_half_to_even(4.7)\n\t    5\n\t\n\t:param n:\n\t:return:\n\t\"\"\"\n\t\n\t# n is int, else throws an error\n\t# int() takes a string or a number\n\ttry:\n\t\treturn int(n)\n\texcept ValueError:\n\t\t# If input string is a float then round the number to the nearest int\n\t\tif(round(float(n)) == n):\n\t\t\tn = round(float(n))\n\n\teven = int(math.floor(n)+0.5)\n\todd = int(math.floor(n))+1\n\tif(n == even):\n\t\treturn even\n\tif(n % even"}]}
{"task_id": "62e60707d76274f8a4026b69", "prompt": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t# Generate the Point subclasses:\n\tclasses = {}\n\tfor fid, fields in iteritems(fields):\n\t\tclass name(name, Point):\n\t\t\t__slots__ = ()\n\t\t\t__init__ = Point.__init__\n\t\t\tfields = fields\n\n\t\tclasses[fid] = name\n\t\n\t# Generate all the Point's derived point's:\n\tpoint_classes = {}\n\tfor fid, subclass in iteritems(classes):\n\t\tderived_classes = [subclass]\n\t\tif fid in srid_map:\n\t\t\tderived_srid ="}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tPointType = namedtuple('Point', fields)\n\n\t@classmethod\n\tdef point(cls, **kwargs):\n\t\t\"\"\"\n\t\tCreate a Point record without specifying the fields.\n\n\t\t>>> point('latitude', 'longitude', 45.123)\n\t\tPoint(latitude=45.123, longitude=90.321)\n\t\t\"\"\"\n\t\treturn PointType(**kwargs)\n\n\t@classmethod\n\tdef from_fields(cls, **kwargs):\n\t\t\"\"\"\n\t\tCreate a Point record, specifying the fields.\n\n\t\t>>> point.from"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tfrozen_fields = {}\n\tfrozen_fields.update(fields)\n\tclass Wrapper(GeoPoint, metaclass=type):\n\t\t__slots__ = ()\n\t\t__doc__ = __str__ = \"\"\"\\\n\n\t\t    %(srid_name)s %(srid_definition)s\n\n\t\t    >>> import srid# test\n\n\t\t``%(srid_name)s`` - the geometry class instance\n\t\t``%(geom_name)s`` - the attribute the geometry instance is bound to, or None if attribute is a geometry\n"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tclass DynamicPointSubclass(Point):\n\t\t\"\"\"\n\t\tA generic subclass of the ``Point`` that takes in a serial in\n\t\tthe correct format.\n\t\t\"\"\"\n\t\t__slots__ = fields\n\t\t\n\t\tdef __new__(cls, lat, lon, *args, **kwargs):\n\t\t\tif not isinstance(lat, float):\n\t\t\t\terror = 'Latitude must be a floating number, given:'+ '{}'.format(type(lat)) +'with:'+ '{}'.format(lat)\n\t\t\t\traise PointCreateError(error"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tclass_name = type(name, (Point,), {})\n\n\tclass_name.__doc__ = \"\"\" A subclass of the Point primitive for \n\tmanipulating coordinate sequences in PostGIS. \n\n\tArgs:\n\t\tname (str): Name of the point class\n\t\"\"\"\n\tfor field in fields:\n\t\tsetattr(class_name, field, SpatialPrimitiveField(srid_map[field], name))\n\n\treturn class_name\n\ndef poly_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically create"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tclass point(object):\n\t\t\"\"\"\n\t\t.. py:class::\n\n\t\t\tclass point(object):\n\n\t\t\t\t__metaclass__ = registry.registry(registrar)\n\n\t\t\t\tPoint = registry.register(point_mapping)\n\n\t\t\t\tdef __init__(self, *args):\n\n\t\t\t\t\t...\n\n\t\t\t\t@property\n\t\t\t\t\tdef _properties(self):\n\n\t\t\t\t\t...\n\n\t\t\t\t@property\n\t\t\t\t\tdef _bounds(self):\n\n\t\t\t\t\t...\n\n\t\t\t\t@property\n\t\t\t\t\tdef pk(self):...\n\t\t\"\"\"\n\n\t\tfields = fields\n\t\tregistry = dict(reg"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\n\tdef _make_point_type(name, fields, srid_map):\n\t\tPoint = namedtype(\"Point\", (\"x\", \"y\"), schema.Column(\"x\", \"float4\", nullable=False, server_default=\"0\"),\n\t\t\t\tschema.Column(\"y\", \"float4\", nullable=False, server_default=\"0\"))\n\t\tPoint.__name__ = name\n\t\tPoint.__module__ = __name__\n\t\tPoint.srid = srid_map.get(fields.get(\"srid\"))\n\t\tPoint.xsd = \""}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\t\n\tpoint_class = lambda **kwds: Point(**kwds)\n\n\tclass Point(geos_types.Point, object):\n            \"\"\"\n            A GeoJSON feature type Point class.\n            This class is used internally by :class:`Feature`, but may also serve as its own base type if desired.\n\n            :param `fields` (optional): An array of field-name/type pairs.\n            :param `srid_map`: A :class:`dict`, where each :class:`Point` subclass will have an entry for the given \n                              :"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\tPoint = type(name, (geo.Feature,), {\n\t\t'__metaclass__': PointsMetaClass(\n\t\t\t\t\"Point\",\n\t\t\t\tgeo.Feature,\n\t\t\t\tsrid_map\n\t\t\t).__name__})\n\n\tfor field in fields:\n\t\tfields_lookup[field.name] = field\n\t\tfields_type[field.name] = type(field.name, (point_field_type,), {\n\t\t\t\"__metaclass__\": field_meta_class(\n\t\t\t\t\tname=\"\".join((\"PointField\", field))))\n\n\t"}, {"generated_text": "def point_type(name, fields, srid_map):\n\"\"\"\n\tDynamically create a Point subclass.\n\t\"\"\"\n\timport inspect\n\tcl = type(name, tuple(dir(types.Point)), {'__truediv__': operator.truediv, 'xy': 0})\n\tfor field, srid in srid_map.items():\n\t\tfield_name = f\"__{name.lower()}_{field.lower()}__\"\n\t\tcl.__{field_name} = property(lambda self: getattr(self, field))\n\t\tcl.xy.__{field_name} = property(lambda self: operator.add(*getattr(self, self."}]}
{"task_id": "62e5dc9ed76274f8a4026b5b", "prompt": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\n\tdef _decorator(fn):\n\t\tdef _fn(*args, **kwargs):\n\t\t    warnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t    return fn(*args, **kwargs)\n\t\t_fn.__name__ = fn.__name__\n\t\t_fn.__doc__ = fn.__doc__\n\t\treturn _fn\n\treturn _decorator\n"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef deco(fct):\n\t\tfct.__deprecated__= \"Function %s has been deprecated in favour of %s\" % (fct.__name__, fct.__name__)\n\t\tif hasattr(fct, '__slots__'):\n\t\t\tfct.__slots__ = None\n\t\treturn fct\n\t\n\treturn deco\n\nclass ThreadingDeque:\n\t\"\"\"\n\tThreaded deque class.\n\t\n\tIt is actually a queue but you can use it like a queue because it's\n\tdesigned to"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tmessage = message.upper()\n\tdef wrappedFunction(func):\n\t\tfunc.__name__ = func.__name__.lower()\n\t\tfunc.__doc__ = \"%s has been deprecated in favour of %s\\n\\n\" % (message, message.__upper__()) + func.__doc__\n\t\tfunc.__deprecated__ = True\n\t\treturn func\n\treturn wrappedFunction\n\n\t\n# ------------------------------------------------------------------------\n#                                                        Public API\n# ------------------------------------------------------------------------\n\ndef isNotTensor(fn):\n\ttry: fn.nDim"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\n\tdef decorator(function):\n\t\t\n\t\tfunction.warnings = []\n\t\t\n\t\tdef decorated(*args, **kw):\n\t\t\tif function.warnings:\n\t\t\t\twarnings.warn(string.join(function.warnings,\"\\n\"))\n\t\t\t\tfunction.warnings = []\n\t\t\treturn function(*args, **kw)\n\t\tdecorated.deprecated = function\n\t\treturn decorated\n\t\n\treturn decorator\n\n"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef decorator(func):\n\t\tmessage = decorator._decorate(message)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, FutureWarning, stacklevel=5)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\ndef _decorate(message):\n\tif '@' in message and 'deprecated' in message:\n\t\tparts = message.split()\n\t\tnew = parts[1] + ':'\n\t\tpos = new.find('(')\n\t\tif pos!= -1:"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\n\tdef wrapper(f):\n\t\tdef deprecated(*args, **kwargs):\n\t\t\ts = f.__name__ if f.__name__ is not None else f.__class__.__name__\n\t\t\tmessage = \"DEPRECATED - {0}\".format(s)\n\t\t\twarnings.warn(message, DeprecationWarning)\n\t\t\treturn f(*args, **kwargs)\n\t\treturn deprecated\n\treturn wrapper"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef fdec(f):\n\t\tdef ddec(*args, **kw):\n\t\t\tmsg = \"%s has been deprecated in favour of %s\" % (f.__name__, message)\n\t\t\twarnings.warn(msg, DeprecationWarning, stacklevel=2)\n\t\t\treturn f(*args, **kw)\n\t\tddec.__name__ = f.__name__\n\t\tddec.__doc__ = message\n\t\treturn ddec\n\treturn fdec\n\n# For running tests using PyMongo's unittest framework\ndef _unittest(f"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\n\t@wraps(f)\n\tdef deprecate(*args,**kwargs):\n\t\treturn wrapped(*args,**kwargs)\n\t\n\treturn deprecate\n\ndef deprecated2(message):\n\t@wraps(f)\n\t@wraps(f)\n\tdef deprecate(x,y):\n\t\treturn wrapped(x,y)\n\treturn deprecate\n\t\ndef deprecate_dict(message):\n\t@wraps(f)\n\t@wraps(f)\n\tdef deprecate(**kw):"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\t\n\tdef __init__(self, message):\n\t\tself.message = message\n\t\n\tdef call(self, obj, *args, **kwargs):\n\t\timport warnings\n\t\tmsg = \"%s has been deprecated in favour of %s\" % (obj.__name__, message)\n\t\twarnings.warn(msg, DeprecationWarning)\n\t\treturn obj(*args, **kwargs)\n\ndef newfactory(name, *args, **kwargs):\n\t\"\"\"\n\tCreate a new class that inherits from the decorator class\n\t\"\"\"\n\t"}, {"generated_text": "def deprecated(message):\n\"\"\"\n\tDecorator for deprecating functions and methods.\n\t\n\t::\n\t\n\t    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n\t    def foo(x):\n\t        pass\n\t\"\"\"\n\tdef actual_decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef new_func(*args, **kwndw):\n\t\t\tif has_deprecated(func):\n\t\t\t\twarnings.warn(DEPRECATION_MSG % (func.__name__, message), stacklevel=7)\n\t\t\treturn func(*args, **kwndw)\n\t\treturn new_func\n\treturn actual_decorator\n\n@deprecated(\"'a' is deprecated. Use 'b' instead.\")\ndef use_moved_package(a,"}]}
{"task_id": "62e4fc3c85ea98643089041e", "prompt": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t# 'R:', 'R(foo):'\n\tif \"R:\\\\\" in code:\n\t\tcode = code.replace(\"R:\\\\\", \"R: \")\n\tif \"R(foo):\" in code:\n\t\tcmd = \"R CMD SH \"\n\t\tcode = code.replace(\"R(foo):\", \"\")\n\t\tcmd = \" \".join([cmd] + shlex.split(code))\n\t\tsubprocess.Popen(cmd, shell=True).communicate()\n\treturn code\n\n\ndef Rcmd(code: str):\n\t\"\"\""}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\topt = None\n\tif \"--no-save\" in code:\n\t\t# save.append(\"NoSave\")\n\t\topt = \"-save\"\n\telif \"--no-restore\" in code:\n\t\t# save.append(\"NoRestore\")\n\t\topt = \"-save -restore\"\n\telif \"--no-show-results\" in code:\n\t\t# show.append(\"NoShow\")\n\t\topt = \"-nolog_level\"\n\telif \"--no-color\" in code:\n\t\t# printcols.append(1)\n\t\t"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tenv.set('R_HOME', find_exe('R', env['PATH']))\n\n\treturn f'R_HOME=\"{env.get(\"R_HOME\")}\" '\n\n@bashfn\ndef _r_setup(code: str) -> str:\n\t\"\"\"\n\t\tA series of functions in this file that do things that should only \n\t\tbe done once R has started.\n\t\"\"\"\n\twith settings(display_output=False, env={'R_HOME':''}):\n\t\tif code == \"nodes\":\n\t\t\tassert r(\"library(igraph)"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\ts = \"library('optparse'); opts <- getopt('--version', '');\\\n\t\t\tsetOpt(opts, '--verbose', 'TRUE', '--vanilla', 'FALSE', '--no-save', 'FALSE', '--no-restore');\\\n\t\t\tinvisible(startRSession(\"~/.R/R-3.4.3\", \"--no-restore\"))\\\n\t\t\topts <- as.list(opts); setR(opts);\"\n\treturn s.replace(\"R\", rep.config[\"R_HOME"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn f\"\"\"\\\n[BOOLEAN_CASES]\n{code} = T\n{code} = F\n{code} = 0\n{code} = 1\n\"\"\"\n\n\ndef _inline_r_run(code: str) -> str:\n\t\"\"\"\n\t\tA function to execute inline R test cases.\n\t\"\"\"\n\treturn f\"\"\"\\\nset.seed(42)\n{code}\nsum(rbind(1, rnorm(1, sd=0, method='uniform')))\n\"\"\"\n\n\ndef _make_cases() -> Iterator"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn 'CXXFLAGS += -DR_XOPEN_VERSION=\"2\"'\n\n\n# Setup the default R options that should be loaded into R unless otherwise specified\ndefault_r_options = {\n\t'packages': 'utils,lubridate;'\n}\n\n\n# Setup the env variables to initialize R on load\nR_ENV_VARIABLES = {'PYTHON_ROOT': PythonRoot,\n\t\t\t\t\t'R_ROOT': RRoot}\n\n# Setup the env variables to specify the options that R will use by default\n# We could"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\treturn \"R_USE_IMAGE_CODECS: TRUE \"\\\n\t\t\t\t\t\t\t\t\t\t\t\t \"R_GAMMA_METHOD: 2 \"\\\n\t\t\t\t\t\t\t\t\t\t\t\t \"R_INIT: FALSE \"\\\n\t\t\t\t\t\t\t\t\t\t\t\t \"R_SHOW_PLOT: TRUE\";\n\n\ndef _inline_r_environment():\n\t\"\"\"\n\t\tInjects environment into R process as a local variable\n\t\t\"\"\"\n\t# this is a bit different from the environment setup for other things\n\t# like setting options in R code\n\timport os\n\timport sys\n\tfrom os.path import expanduser, join,"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tscript = \"\"\"\n# This file gets written during the R initialization process. Don't\n# modify it by hand!\n\noptions(R.data.path='%(r_library)s')\noptions(collapse.library=\"%(collapse_library)s\")\n\ndoit(\"%(code)s\")\n\t\"\"\"\n\n\tr_params = _config_params(code)\n\treturn script % {\n\t\t'r_library': os.path.join(os.environ['R_HOME'], 'library'),\n\t\t'collapse_library': _get"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tlines = \"\"\"\n# Set up environment variables for R\nold_package_paths=$export PROFDR_PYS=\"${RHOME}/lib/profile\"\nnew_package_paths=$export PROFDR_PYS=\"${RHOME}/lib/profileR\"\nold_environment_paths=$export PROFDR_ENV=\"${RHOME}/lib/ENV\"\nnew_environment_paths $export PROFDR_ENV=\"${RHOME}/lib/ENV_R\"\nold_R_paths=$export R_HOME"}, {"generated_text": "def _inline_r_setup(code: str) -> str:\n\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\n\tonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode ='source(\"/library/R/bin/R CMD BATCH\" ; silent ;\\n{0})'\\\n\t\t\t.format(code)\n\treturn code\n"}]}
{"task_id": "62e4fbda85ea986430890405", "prompt": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t,cmds,log)\n\nlog.info(\"xargs {0}, target concurrency: {1}\".format(cmds,target_concurrency))\n\ndef run_xargs(cmd_list, target_concurrency):\n\t\"\"\"Run cmd_list concurrently, with each part run against a number of processes.\n\t\"\"\"\n\tpool = multiprocessing.Pool(target_concurrency)\n\tpool.map(func=lambda x: x(cmd_list), iterable=range(target_concurrency))\n\tpool.close() \t\t\n\tpool"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tcolor = True\n\ttarget_concurrency = 4\n)\n\n\n\n\n\n# Define a list of things we can do from the commandline\ncmds = ('list', 'print', 'print-graph', 'print-log', 'print-log-file', \n\n# A python function:\n'-pyfunc', 'print-pyfunc', \n\n# A function from xargs:\n'-xargs', 'print-xargs', \n\n# An integer:\n'-con', 'target_concurrency', \n)\n\n\n\n# Make it up.\ncmd"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tdef xargs(iterable, target_concurrency):\n      \t    \titerable = iter(iterable)\n      \t        p = Pool(target_concurrency)\n      \t        n = len(iterable)\n      \t        p.map(iterable.next, range(n//target_concurrency + 1))\n\nif (c.mode == 'xargs'):\n       if c.color == True:\n           xargs(iterable, target_concurrency)\n       else:\n           x_cmd(iterable, target_concurrency)\nelse"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tcolor = ''\n\ttarget_concurrency = 1\n)\n\n@parfor(xargs, shell=True)\ndef parse_xargs(pf):\n\t(execpath, args) = pf.pop(0)\n\tcolor = ''\n\tif 'color' in execpath:\n\t\tcolor = execpath\n\t\texecpath = os.path.dirname(execpath)\n\txargs_cmd = '\"%s\" %s' % (execpath,''.join(pf))\n\toutput = sp.check_output('%s'"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\tp = subprocess.Popen(args, stdout=subprocess.PIPE)\n\n\tconcurrency = [None] * target_concurrency\n\t\n\tfor line in iter(p.stdout):\n\t\tline = line.rstrip()\n\n\t\tif line.startswith(\"/sbin/color\"):\n\t\t\timport signal\n\t\t\ttry:\n\t\t\t\tif os.name == \"nt\":\n\t\t\t\t\tos.system(\"color echo\")\n\t\t\t\telse:\n\t\t\t\t\t#print \"colorecho\"\n\t\t\t\t\tos.kill(os.tty_to_fd(0), signal"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t: color = bool(), # whether to use a pty or not\n\t: target_concurrency = None  # how many partitions to run concurrently for each machine\n) -> \"args\" -> None:\n\t# initialize\n\tmachines = machines()\n\tif not target_concurrency:\n\t\ttarget_concurrency = machines.concurrency\n\n\t# figure out the partition size and the number of partitions\n\tpart = ptypart.allocate(ttynum0 + 1)\n\t# set the number of partitions to run\n\t# XXX might be better to run just a subset,"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t# Setup\n\n\timport os\n\timport subprocess\n\n\t# Parse arguments\n\tparser = argparse.ArgParser()\n\n\tparser.add_argument(\"-v\", \"--version\",\"version\",\n\t\thelp=\"Version: %(prog)s \",\n\t\trequired=True)\n\n\tparser.add_argument(\"-c\", \"--color\",\n\t\ttype=bool,\n\t\thelp=\"color the output\",\n\t\trequired=False)\n\n\tparser.add_argument(\"-i\", \"--inherits\",\n\t\ttype=int,\n\t\thelp=\"Partition #"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t):\n\ttarget_concurrency = args()[0]\n\tif sys.platform.startswith(('nt', 'darwin')):\n\t\tdef xargs():\n\t\t\tfor f in args():\n\t\t\t\tp = Popen(f, shell=True)\n\t\t\t\n\t\t\tif hasattr(target_concurrency, '__iter__'):\n\t\t\t\ttarget_concurrency = [arg for arg in target_concurrency]\n\t\t\tif target_concurrency < 0:\n\t\t\t\ttarget_concurrency = p.returncode\n\t\t\t\n\t\t\tassert target_concurrency >= 0\n\t\t\t"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t) -> None:\n\tdef split(f: Callable[[str], Iterable[str]], *colors: List[List[Any]]) -> Iterable[Callable[[str], Iterable[str]]]:\n\t\tif not colors:\n\t\t\tyield lambda t: (lambda *parts: (t, parts))\n\n\t\tfor c in color(colors):\n\t\t\tfargs = [f]\n\t\t\tfor n in range(len(c)):\n\t\t\t\tfargs.append(operator.attrgetter(\"color_\" + str(n)))\n\n\t\t\tg = (lambda"}, {"generated_text": "def xargs(\n\"\"\"\n\tA simplified implementation of xargs.\n\t\n\tcolor: Make a pty if on a platform that supports it\n\ttarget_concurrency: Target number of partitions to run concurrently\n\t\"\"\"\n\t, **kwargs)\n\n\ndef pty_process(command, name=None, **kwargs):\n    # from os.path import join\n    from os import environ\n    # if name is None: name = environ['USER'] + \"_\" + command\n    # pty = environ['SHELL'] == \"bash\"\n    # subprocess.call(shutil.which(ptty))\n    # if name is None: name = environ['USER'] + \"_\" + command\n    from subprocess import call\n    call(command)\n    \n    "}]}
{"task_id": "62e4fbda85ea986430890403", "prompt": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tshuffled_seq = []\n\tfor el in seq:\n\t\tindice = random.randrange(len(seq))\n\t\tshuffled_seq.append(seq[indice])\n\treturn shuffled_seq\n\n\ndef _get_text_lines(text: str) -> list[str]:\n\t\"\"\"\n\tGet text into list lines for splitting\n\t\"\"\"\n\tlines = []\n\tfor line in text.splitlines():\n\t\tlines.append(line)\n\treturn lines\n\n\ndef _get_shuffled_text(text: str) ->"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn random.sample(seq, len(seq))\n\n\nclass Pairs():\n\t\"\"\"\n\tPairs a list of tuples\n\t\"\"\"\n\tdef __init__(self, *pairs: tuple):\n\t\tself._t, self._o = tuple(zip(*pairs))\n\n\tdef __getitem__(self, i: int) -> 'TItem':\n\t\treturn self._t[i]\n\n\tdef __iter__(self) -> 'TItemIterator':\n\t\treturn self._iter()\n\n\tdef __len__(self) -> int:\n\t\t"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\ta_list = list(seq)\n\trandom.shuffle(a_list)\n\treturn a_list\n\n\nclass RandomizedSequence(list):\n\trand_index = index = 0\n\tname = 'a_new_randstring: '\n\n\tdef __init__(self, seq_of_strings: Sequence[str]):\n\t\tsuper().__init__()\n\t\tself = _shuffled(seq_of_strings)\n\t\tself.name += '\\nseq_len={:d}, '.format(len(seq_of_strings))+"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn list(seq)[1:][::-1]\n\ndef shuffle_str(string: str) -> str:\n\t\"\"\"\n\tCreate a variant string from input argument\n\t:param string: string to be shuffled\n\t:return: shuffled string\n\t\"\"\"\n\trand_seq: list[str] = _shuffled(list(string))\n\treturn \"\".join(rand_seq).title()\n\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tshuffle = random.SystemRandom().shuffle\n\tshuffle(seq)\n\treturn seq # return a new list\n\n\ndef _reversed(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\t\tTake a sequence of strings and returns a reversed version\n\t\"\"\"\n\tseq_copy = list(seq)\n\tseq_copy.reverse()\n\treturn seq_copy\n\n\nif __name__==\"__main__\":\n\tmy_dict = {\n\t\t'joe': ['apple', 'banana', 'orange'],\n\t\t'paul"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\treturn sorted(seq)\n\n\n@lru_cache(maxsize = 50)\ndef _pregel_matrix(d: OrderedDict,\n                   cnt: int,\n                   size: int,\n                   cov_size: int) -> list[Mapping[int, float]]:\n\tmatrix: list[Mapping[int, float]] = _shuffled(d.items())\n\tmatrix = [Mapping(\n\t\ti, float((i * cnt) / size)) for i in range(cnt)]\n\treturn matrix\n\n"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tif not isinstance(seq, list): seq = list(seq)\n\trandom.shuffle(seq)\n\treturn seq\n\t\n\n\ndef build_tokens(word_freq: Optional[Dict[str,int]] = None) -> Generator[str,None,None]:\n\t\"\"\"\n\t>>> words = build_tokens()\n\t>>> list(words)\n\t['foo', 'bar']\n\t>>> def _tokens():\n\t...\tyield foo\n\t...\tyield bar\n\t>>> words = build_tokens(_"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tfor _ in range(0, 10):\n\t\trandom.shuffle(seq)\n\t# return seq\n\treturn [v for v,d in sorted(enumerate(seq), key = lambda x:x[1])]\n\n\ndef _shuffled_range(inp: int = 2) -> Sequence[str]:\n\t\"\"\"\n\t\tInput:\n\t\t\t{\n\t\t\t\"range\": 0,\n\t\t\t\"shuffled\":\"yes\"\n\t\t\t}\n\t\"\"\"\n\tinp = int(inp)\n\tyield from _shuffled(range(in"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\t# noinspection PyShadowingNames\n\treturn list(seq)\n\n\nclass RandomNameGenerator:\n\n\tdelim = \"_\"\n\t\n\tdef __init__(self, names, charset: str=None):\n\t\t\"\"\"\n\t\t:param names: A list/tuple of all the names\n\t\t:param charset: Character set to create names\n\t\t\"\"\"\n\t\tif charset is None:\n\t\t\tcharset = \"\"\n\t\tfor idx, i in enumerate(range(0, len(names))):\n\t\t\tcharset = self.delim"}, {"generated_text": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\"\"\"\n\tDeterministically shuffle\n\t\"\"\"\n\tshuffled = list(seq)\n\trandom.shuffle(shuffled)\n\treturn shuffled\n\ndef _check_tweet_id(tweet_id: str) -> bool:\n\t\"\"\"\n\tCheck tweet id\n\t\"\"\"\n\tif not tweet_id:\n\t\treturn False\n\telif len(tweet_id) == 8:\n\t\treturn False\n\telif len(tweet_id) <= 4:\n\t\treturn False\n\treturn True\n\n\ndef _check_text(text: str) -> bool:\n\t\"\"\"\n\t"}]}
{"task_id": "62e4fb6585ea98643089032b", "prompt": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\t# split and count the version number parts\n\tparts = []\n\tfor v in s.split('.'):\n\t\tif v.isdigit():\n\t\t\tparts.append(int(v))\n\t\telse:\n\t\t\tparts.append(-1)\n\n\tassert len(parts) >= 2\n\n\tmajor = parts[0]\n\tminor = 1\n\tmicro = 1\n\trelease = 0\n\n\t# look for version numbers in format\n\t# xxxxx (x.y.z.p), and update values\n\tfor part in parts[1:]:\n\t\tif part"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\ta, b, c = s.split('.')\n\treturn [int(c)] + [int(x) for x in map(int, (a, b))]\n\n\ndef make_version(ver_tup: tuple, sver_tup: tuple):\n\tif sver_tup < ver_tup:\n\t\treturn str(sver_tup) + '.' + ''.join(str(a) for a in ver_tup) + '.' + str(c)\n\nclass VersioningTest(TestCase):\n\tdef"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\ttokens: list[str]\n\ttokens = s.split('.')\n\treturn tuple(map(int, tokens))\n\nclass TestVersion(unittest.TestCase):\n\n\tdef setUp(self):\n\t    self.min_vers_list = []\n\t    # list of 3-tuple (min_maj_vers, min_min_vers, minor_min_vers)\n\t    # with versions not greater than 2.2.0\n\t    for v in [\n\t                # major, minor, minor, minor, minor"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tpieces = s.split(\".\")\n\ts1 = int(pieces[0])\n\ts2 = int(pieces[1].rstrip(\"patch\"))\n\treturn s1, s2, pieces[2]\n\ndef _patch_version(ver: tuple[int,...]) -> str:\n\t\"\"\"\n\t\tformat patch version to the standard form\n\t\t\"\"\"\n\tret = \"\"\n\tfor num in ver:\n\t\t# if number is greater than 20, break the version into two components\n\t\t# since there is no way to define >= 2.0.0 and < 2"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tv = s.split('-')[0]\n\tv2d = v[:7] # only need 2 digits\n\t_v = 0\n\tfor _v in map(\n\t\t\tlambda d1, d2: min(int(d1), int(d2)),\n\t\t\tv2d[1:4],\n\t\t\tv2d[:-2]):\n\t\t_v += int(v2d[7:]) * 10\n\treturn (int(v2d[3]), _v)\n\ndef check(**k:str) -> bool:\n\t"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tl = s.split('.')\n\tif len(l) < 2:\n\t\treturn (0, 0, 0)\n\tret = 0\n\twhile l:\n\t\ti = 0\n\t\twhile i < len(l):\n\t\t\tret = ret * 10 + int(l[i])\n\t\t\ti += 1\n\t\tl = l[i:]\n\treturn (ret, l)\n\n\ndef version_query(f: str) -> tuple[str,...]:\n\t\"\"\"\n\t:param f:\n\t:return:\n\t\"\"\"\n\tfor f in ["}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tassert not isinstance(s, str) # TODO: only works on text\n\tret, sep, rest = s.partition('.')\n\tret = int(ret )\n\tsep = sep.lower()\n\trest = rest.split('.')\n\t# for f in s.split('.'):\n\t# \tprint('{} -> {}'.format(f, int(f)))\n\tfor i, f in enumerate([ f for f in rest if not f.isdigit()] + rest):\n\t\tif not f.isdigit():"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tm = re.match(\"(\\d+)\\.(\\d+)\\.(\\d+)\", s)\n\tif m is None:\n        \traise ValueError(\"Unable to parse version \" + s)\n\n\treturn tuple(map(int, m.groups()))\n\nclass ConfigParser:\n\n\t\"\"\"\n\tRead a configuration file\n\t\"\"\"\n\tdef __init__(self, file: str, encoding: str = default_encode, debug: bool = False):\n\t\tself.file = Path(file, encoding)\n\t\tself.debug = debug\n\n\tdef validate"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\treturn tuple([int(a) for a in s.split(\".\") if a.isdigit()])\n\ndef main():\n\tsys.stdout = subprocess.PIPE\n\n\tprog_desc = descrption_from_file('smsprog.txt')\n\tprint(prog_desc)\n\ttime.sleep(2)\n\tprint(str(subprocess.Popen('smsprog.py 2', shell=True)))\n\ttime.sleep(30)\n\n\tinput(\"Press ENTER to continue...\")\n\nif __name__"}, {"generated_text": "def parse_version(s: str) -> tuple[int, ...]:\n\"\"\"\n\tpoor man's version comparison\n\t\"\"\"\n\tversion = s.split(\".\")\n\tif len(version) == 2:\n\t\t# major.minor\n\t\treturn tuple(int(v) for v in version)\n\telif len(version) == 3:\n\t\t# major.minor.patch\n\t\treturn tuple(int(v) for v in version)\n\telse:\n\t\traise ValueError(f\"{s} is not a valid version\")\n\ndef check_version(from_version: str, to_version: str) -> bool:\n\t\"\"\"\n\t\tcompares the two versions\n\t\"\"\""}]}
{"task_id": "62e4fb4d85ea9864308902e7", "prompt": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tif sys.platform == \"win32\":\n\t\tcmd = _fix_windows_shebang(cmd)\n\t\tcmd = _make_windows_safe_path(os.path.abspath(cmd))\n\n\treturn tuple(map(_remove_dupe_cmd, cmd))\n"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tif isinstance(cmd, str):\n\t\tcmd = [cmd]\n\n\tif '<:' in''.join(cmd):\n\t\tcmd[0] = '<'+cmd[0]\n\n\tif isinstance(cmd, tuple) and '<' in''.join(cmd) and not isinstance(cmd, str):\n\t\tcmd = ['\"%s\"' % f for f in cmd]\n\t\treturn (cmd[0] if len(cmd) == 1 else cmd)\n\n\treturn cmd\n\ndef fix_python_path(command: (str, list[str"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t\n\tfrom string import Template\n\t\n\t# normalize cmd arguements\n\tcmd_fixer = Template('${0:00}' * 1)\n\tif \" \" in cmd:\n\t\t\tcmd = cmd_fixer.substitute(cmd))\n\tcmd = cmd.split()\n\t\n\t\n\t### this stuff is not necessary\n\t# replace shebang\n\tre_shebang = re.compile(r'#! /bin/csh')\n\tproc_path = os.path.realpath(os.path.normpath(sys."}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tpath = os.path.normpath(\"\\\\\".join(cmd))\n\tif sys.platform.startswith(\"win\"):\n\t\t# Make paths such as \"C:\\foo\\bar\" to just \"bar\"\n\t\tpath = os.path.normpath(path)\n\t\tif path.startswith(r\"\\\\\") and os.path.isdir(path):\n\t\t\tpath = path[1:]\n\t\tpath = os.path.normcase(path)\n\t\tif path.endswith(r\":\\\\\"):\n\t\t\t# This is possibly a deeply-"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tif isinstance(cmd, types.StringTypes):\n\t\tcmd = cmd.split()\n\tis_normal = True if cmd and os.path.isfile(cmd[0]) else False\n\tnew_cmd = []\n\tshell = 'cmd' if is_normal else 'cmd.exe'\n\tcmd = cmd\n\tfor item in cmd:\n\t\tif is_normal and is_shebang(item):\n\t\t\titem = item.split('\"')[0]\n\t\telif item.strip().startswith('!'):\n\t\t\tif is_normal:\n\t\t\t\t"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\t# Make sure the command exists\n\tif len(cmd) == 1 and os.path.exists(cmd[0]):\n\t\tcmd = cmd[0]\n\t\tcmd[0] = os.path.realpath(cmd[0])\n\t\t\n\tif cmd and is_exe(cmd[0]):\n\t\treturn cmd\n\n\telif cmd and os.path.basename(cmd[0]) == \"node\":\n\t\tcmd_name = os.path.basename(cmd[1])\n\t\troot = os.path.dirname(cmd[1])\n"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\tnew_cmd = []\n\tif isinstance(cmd, str):\n\t\tnew_cmd = [cmd]\t\t\t\t\t# simple case, no path translation required\n\telse:\n\t\tfor part in cmd:\t\t\t\t\t\t# need to determine what to do for each part\n\t\t\tif isinstance(part, str):\t\t# string\n\t\t\t\tnew_cmd += glob_windows_path(part)\t# handle path translation\n\t\t\telse:\t\t\t\t\t\t# no path (maybe an entire object), so just continue processing\n\t\t\t\tnew_cmd += part\n\treturn tuple(new_cmd)\n\n#---------------------------------------------------------------------------\n#"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\ts = str(cmd)\n\n\tif s.startswith(('C:\\\\', 'D:')) and not s.startswith(('C:/', 'D:/')): # not a full path\n\t\ts = os.path.normpath(s)\n\t\n\tif s.startswith(('/', '\\\\')): # absolute path\n\t\ts = s.lstrip(r'/').lstrip('\\\\')\n\t\t\n\treturn (s, )\n\n\ndef _remove_encode_from_cmd(cmd: str) -> str:\n    "}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\timport os\n\tif cmd:\n\t\tif '=' not in cmd and ':' not in cmd:\n\t\t\tif '|' in cmd or os.pathsep in cmd or os.linesep in cmd #||'/' in cmd #FIXME: windows does not normalize the pwd\n\t\t\t\t# normalize pathname\n\t\t\t\tcmd = os.path.abspath(cmd)\n\n\t\t\t# Normalize shebang if present and on windows\n\t\t\tif cmd.startswith('#!'):\n\t\t\t\tcmd = (os.path.splitext(os.path.split(cmd"}, {"generated_text": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\"\"\"\n\tFixes for the following issues on windows\n\t- https://bugs.python.org/issue8557\n\t- windows does not parse shebangs\n\t\n\tThis function also makes deep-path shebangs work just fine\n\t\"\"\"\n\told_cmd = cmd\n\tif sys.platform[:3] == \"win\":\n\t\tif isinstance(cmd, (tuple, list)) and cmd[0].lower() == \"python\":\n\t\t\tcmd = [sys.executable, cmd[1]] + cmd[2:]\n\t\told_cmd = cmd\n\treturn old_cmd\n\ndef is_win_command_line(cmdline: Tuple[str,...]) -> bool:\n    \"\"\"\n\tChecks whether the provided command should be run from the command line, or from subprocess.\n\n\tThe only"}]}
{"task_id": "62b8d27a48ba5a41d1c3f4c6", "prompt": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tif lock is True:\n\t\t# cache.clear() if the decorator is applied more than\n\t\t# once to the same function.\n\t\tlock = threading.Lock()\n\n\tcache.clear()\n\tcache.data = {}\n\tcache.lock = lock\n\tcache.key = key\n\n\tdef _memoizer(*args, **kwargs):\n\t\t# the inner function of the cache decorator.\n\t\tkey = (args, tuple(sorted(kwargs.items())))\n\t\ttry:\n\t\t\t# check if this key is already cached\n\t\t\tif key in"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tlock = lock or threading.RLock()\n\tcache_name = \"__{key}__{time}__{thread}\".format(key=key,\n\t\t\t\t\t\t\ttime=time.time(), thread=_thread.get_ident())\n\n\t@functools.wraps(func)\n\tdef wrapper(*args, **kwargs):\n\t\tkeystr = (\"%s\" % (key,)).encode(\"utf-8\")\n\t\tlock.acquire()\n\t\ttry:\n\t\t\thits, miss = cache.get(cache_name, [0, 0])"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tcache = func.__cache_ = cache\n\t\tif cache:\n\t\t\t@wraps(func)\n\t\t\tdef memoizer(*args, **keyargs):\n\t\t\t\tif not lock.acquire(False):\n\t\t\t\t\treturn cache.get(key(*args, **keyargs), func=func)\n\t\t\t\tresult = func(*args, **keyargs)\n\t\t\t\tif not lock.acquire(False):\n\t\t\t\t\tcache[key].pop(key, None)\n\t\t\t\t\tcache.get(key(*args, **keyargs), func=func) = result\n\t\t\t\t\t"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\t__utils__[\"cache.cached\"] = lambda self, cache=None, key=hashkey, lock=None: cached(cache, key, lock or self)\n\t\treturn wrap(func)\n\n\treturn decorator\n\n# Backwards compat hack\ndef cached_instancemethod(func):\n\t\"\"\"\n\tDecorator to wrap an instancemethod and make it work\n\tas a function that can be cached instead.\n\t\"\"\"\n\tdef decorator(klass):\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tinst = k"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorator(func):\n\t   def wrapped(*args, **kw):\n\t       if lock is None:\n\t           value = cache.get(key(func, *args, **kw), None)\n\t       else:\n\t           with lock:\n\t               value = cache.get(key(func, *args, **kw), None)\n\t           if value is None:\n\t               value = func(*args, **kw)\n\t               cache[key(func, *args, **kw)] = value\n\t       return value\n\t   return wrapped\n\treturn"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tcache = cache or defaultdict(dict)\n\t\t\n\tdef _func(self):\n\t\tif lock:\n\t\t\tlock.acquire()\n\t\tcacheresult = cache[key][self.id]\n\t\tif not cacheresult:\n\t\t\tcacheresult = self.__func__()\n\t\t\tcache[key][self.id] = cacheresult\n\t\tif lock:\n\t\t\tlock.release()\n\t\treturn cacheresult\n\treturn _func\n\t\ndef cached_func(cache, key=hashkey, lock=None):\n\t\"\"\"Decorator wrapper to wrap a function with a"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tlock = lock or RLock()\n\n\t@functools.wraps(func)\n\tdef memoizer(*args, **kwargs):\n\t\tif lock.acquire(False):\n\t\t\t# Make sure this is a unique key\n\t\t\tkey = cache_key(key, *args, **kwargs)\n\n\t\t\tif key in cache:\n\t\t\t\tret = cache[key]\n\t\t\telse:\n\t\t\t\t# Calculate the return value\n\t\t\t\ttry:\n\t\t\t\t\tret = func(*args, **kwargs)\n\t\t\t\texcept:\n\t\t\t\t\timport traceback\n\t\t\t\t\tprint(\"Exception raised during cache"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef deco(func):\n\t\tcache.set(key(func), func, lock=lock)\n\t\treturn func\n\treturn deco\n\ndef memoize(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef deco(func):\n\t\tcache.set(key(func), func, lock=lock)\n\t\treturn cache.get(key(func), func)\n\treturn deco\n\n#.............................................................................\n\n"}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\t@decorator\n\t\t\tThe decorator can be used as a normal function call, but now\n\t\t\tthe result is returned from the cache or the original function.\n\t\t\t\"\"\"\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# Get a hash of the key to save it in the cache.\n\t\t\tkey = hashkey(key, args, kwargs)\n\t\t\t# Try to get the cache object\n\t\t\tcache = args[0].cache\n\t\t\t# Try to get the results from the cache."}, {"generated_text": "def cached(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tresults in a cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\t\t@decorator(func)\n\t\t\tdef foo(a, b):\n\t\t\t\tprint 'foo', a, b\n\t\t\t\treturn a + b\n\t\t\t\"\"\"\n\t\t@functools.wraps( func )\n\t\tdef memoizer(*args, **kwargs):\n\t\t\t# Check the cache, remember if the key is already in the cache\n\t\t\tkey = key or \"%s%s\" % (hashkey(func, *args, **kwargs), hashkey(*kwargs))\n\t\t\tif lock:\n\t\t\t\tlock.cancel_"}]}
{"task_id": "62b8d24048ba5a41d1c3f49f", "prompt": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = {}\n\tdef decor(func):\n\t\t@functools.wraps(func)\n\t\tdef memoizer(*args, **kw):\n\t\t\tif typed:\n\t\t\t\tresult = func(*args, **kw)\n\t\t\telse:\n\t\t\t\tresult = func(*args, **kw)\n\t\t\tval = cache.get(result, None)\n\t\t\tif val is None:\n\t\t\t\tcache[result] = result\n\t\t\t\tval = result\n\t\t\tif ttl is None:\n\t\t\t\treturn val\n\t\t\tif time.monotonic() - timer > maxsize:\n\t\t\t\tfor k in"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tif typed:\n\t\treturn typedmemcache(maxsize, ttl, timer)\n\telse:\n\t\treturn timememcache(maxsize, ttl, timer)\n\nmemcache = ttl_cache()\n\"\"\"\n\tCached version of :func:`memcache`, with a per-item TTL of two hours.\n\t\"\"\"\n"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorated(cache, fn, func, typed=typed):\n\t\t@wraps(fn)\n\t\tdef _fn(*args, **kwargs):\n\t\t\tkey = fn.__name__, tuple(args), tuple(kwargs.items())\n\t\t\ttry:\n\t\t\t\treturn cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tresult = fn(*args, **kwargs)\n\t\t\t\tif len(result) > maxsize:\n\t\t\t\t\tcache.popitem()\n\t\t\t\tcache[key] = result\n\t\t\t\tif ttl:\n\t\t\t\t\tcache.setitem(key, time.time"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\t\n\tdef __init__(self, fn, typed=False):\n\t\tself.fn = fn\n\t\tself.memoize_expiryrange = None\n\t\tself.ttl = ttl\n\t\tself.memoize_ttl = None\n\t\tself.typed = typed\n\n\tdef __call__(self, *args, **kwargs):\n\t\tkwargs.update({\"memodict_typed\": self.typed})\n\t\treturn self.cache(*args, **kwargs)\n\n\tdef cache(self, *args, **kwargs):"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef lrucache(func):\n\t\tcache = {}\n\t\tdef lruc(*vargs, **kwargs):\n\t\t\tkey = tuple(vargs) + tuple(kwargs.items())\n\t\t\ttry:\n\t\t\t\treturn cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*vargs, **kwargs)\n\t\t\t\tif typed and isinstance(result, tuple): cache[key] = tuple(vartype(r, vartype) for r, vartype in zip(result, vartype))\t\n\t\t\t\telse: cache[key] = result"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache  = dict()\n\tqueue  = deque()\n\tmaxsize = maxsize and maxsize*16 or 1024**6\n\ttl     = ttl\n\ttyped  = typed\n\n\tdef lru_cache(fget, maxsize=maxsize, ttl=ttl):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = getrepr(args, kwargs)\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = fget(*args, **kwargs)\n\t\t\t\tqueue.append(key)\n\t\t\t\tif len("}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(*args, **kwargs):\n\t\tnow = timer()\n\t\tkey = make_key(*args, **kwargs)\n\t\ttry:\n\t\t\tvalue = cache.get(key, None)\n\t\texcept TypeError:\n\t\t\tpass\n\t\telse:\n\t\t\tif isinstance(value, timedelta) and value > maxsize * ttl:\n\t\t\t\tcache.__delitem__(key)\n\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif args:\n\t\t\t\tkey = make_key(*args, **kwargs)\n\t\t\t\ttry:\n\t\t\t\t\tvalue ="}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef save_fn(fn=None, maxsize=128, ttl=600, typed=None):\n\t\tnonlocal fn\n\t\tassert fn is None\n\t\tassert maxsize is not None\n\t\tassert ttl is not None\n\t\tfn = ttl_cache_factory(maxsize, ttl, timer, typed)\n\t\treturn fn\n\treturn save_fn\n\n\n@cache\ndef ttl_cache_factory(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"Factory function that returns a Memo"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(func):\n\t\tif typed:\n\t\t\treturn cachedtyped(func, maxsize=maxsize)\n\t\telse:\n\t\t\treturn cached(func, maxsize=maxsize)\n\t\t\n\tdef newfunc(*a, **kw):\n\t\tdef call_under_concurrency(*args):\n\t\t\tif maxsize == 0:  # pragma: no cover\n\t\t\t\treturn func(*args, **kw)\n\t\t\n\t\t\tkey = u\"%s:%s:%s\" % (func.__name__, repr(a), repr(kw))  #"}, {"generated_text": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef ttl_cache_decorator(f):\n\t\tdef wrapped_f(*args, **kw):\n\t\t\tdef _cachekey():\n\t\t\t\treturn args, kw\n\t\t\ttry:\n\t\t\t\treturn cache[_cachekey()]\n\t\t\texcept KeyError:\n\n\t\t\t\tres = f(*args, **kw)\n\t\t\t\t# cache[cachekey] = cache_tuple (tuple of 3 items)\n\t\t\t\t# (args, kw, ttl_value) => (cache_tuple, expire_time in milliseconds)\n\t\t\t\tcache[_cachekey()] = (res, timer"}]}
{"task_id": "62b8d23b48ba5a41d1c3f49a", "prompt": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef wrapper(f):\n\t\tunwrap = _mru_method_wrapper(f)\n\t\tbackup = None\n\t\t@wraps(f)\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tcache_key = make_cache_key(f, args, kwargs)\n\t\t\ttry:\n\t\t\t\treturn get(cache_key)\n\t\t\texcept cache.Client.Miss as e:\n\t\t\t\tcache_value = f(*args, **kwargs)\n\t\t\t\tbackup = get(cache_key, lambda: None)\n\t\t\t\tsave(cache_key, cache_"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\t\n\tif not typed:\n        \tcache = dict.fromkeys(map(lambda x:x.get_id(), filter(lambda x:x.has_cache(), d.objects.values()))).keys()\n        else:\n        \tcache = list.from_iterable(filter(lambda x:x.has_cache(), d.objects.values()))\n        \n\tdef decorator(func):\n        \t#@functools.wraps(func)\n        \t@functools.wraps(func)\n        \t@marshal.cache"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tif maxsize <= 0:\n        \traise ValueError(\"maxsize must be greater than 0.\")\n\tclass _HashedSeq:\n\t\tdef __init__(self, x):\n\t\t\tself.contents = x\n\t\t\ttry:\n\t\t\t\tself.hashcontents = hash(x)\n\t\t\texcept TypeError:\n\t\t\t\tself.hashcontents = self.__hash_function(x)\n\t\t\tself.index = 0\n\t\tdef __hash__(self):\n\t\t\treturn self.hashcontents\n\t\tdef __lt__(self, other):\n                \treturn self"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tassert maxsize >0, \"mru_cache's maxsize must be at least 1\"\n\n\tcache={}\n\tdef decorator(fnc):\n\t\tlru={}\n\t\t@functools.wraps(fnc)\n\t\tdef wrapped(*args, **kwds):\n\t\t\tkey=fnc.__call__(*args, **kwds)\n\t\t\tif key not in lru:\n\t\t\t\tlru[key]=1\n\t\t\t\tcache[key]=fnc(*args, **kwds)\n\t\t\telse:\n\t\t\t\tlru[key]+=1\n\t\t\t\t"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorating_function(func):\n\t\tcache = dict()\n\t\tcache_f = None\n\t\tif not typed:\n\t\t\tcache_f = func\n\n\t\t@functools.wraps(func)\n\t\tdef memoizer(*params, **kwargs):\n\t\t\tcache_key = params\n\t\t\tif len(kwargs) > 0:\n\t\t\t\tcache_key += tuple([k + \"=\" + repr(v) for k, v in kwargs.items()])\n\t\t\tkey_cache_result = cache.get(cache_key, None)\n\t\t\tif key_"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tmaxsize = mru_nums[maxsize]\n\t\thits = {}\n\t\tlru = [None]*maxsize\n\t\tdef func(*a):\n\t\t\tkey = (id(a),)+(a,)\n\t\t\tif key in hits:\n\t\t\t\treturn hits[key]\n\t\t\tresult = func(*a)\n\t\t\thits[key] = result\n\t\t\tdel lru[lru.index(result)]\n\t\t\tlru[-1] = key\n\t\t\twhile len(lru) > maxsize:\n\t\t\t\tdel lru"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef decor(target):\n\t\t@functools.wraps(target)\n\t\t@wraps(target)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = str(target) + str(args) + str(kwargs)\n\t\t\tret = cache.get(key, None)\n\t\t\tif not ret:\n\t\t\t\tret = target(*args, **kwargs)\n\t\t\t\tcache.set(key, ret, maxsize)\n\t\t\treturn ret\n\t\treturn wrapper\n\tif typed:\n\t\twrapper = decor\n\telse:\n\t\twrapper = decor(target)"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tcache = mru_cache_setdefault(maxsize, typed)\n\t\tdef memoized(*args):\n\t\t\tresult = cache.get(args, None)\n\t\t\tif result is None:\n\t\t\t\tresult = func(*args)\n\t\t\t\tcache[args] = result\n\t\t\treturn result\n\t\treturn memoized\n\treturn decorator\n\nmemoize = functools.partial(mru_cache, typed=True)\nmemoized = mru_cache()\n\ndef simple_memoize(maxsize=128, typed=False"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(fun):\n\t\tdef cache(*args):\n\t\t\tkey = hash(args)\n\t\t\trval = cache.ttl[key]\n\t\t\tif rval is None:\n\t\t\t\trval = fun(*args)\n\t\t\t\tcache.ttl[key] = rval\n\t\t\treturn rval\n\t\tcache = collections.defaultdict(lambda: lambda: {'value': None, 'ttl': maxsize, 'tt': time.time()})\n\t\tif typed:\n\t\t\tcache = LRUCacheType(maxsize, typed)(cache)\n\t\tcache = util"}, {"generated_text": "def mru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Most Recently Used (MRU)\n\talgorithm.\n\t\"\"\"\n\tcache = OrderedDict()\n\n\tdef decorating_function(func):\n\t\tcache_name = '_cache_' + func.func_name\n\n\t\t@wraps(func)\n\t\tdef wrapper_function(*args):\n\t\t\tif cache_name not in cache:\n\t\t\t\tcache[cache_name] = []\n\t\t\t\ttry:\n\t\t\t\t\tcache_lru = LRU(maxsize)\n\t\t\t\t\tcache[cache_name] = cache_lru.lru_refresh(cache[cache_name])\n\t\t\t\texcept TypeError:\n\t\t\t\t\tcache_lru = LR"}]}
{"task_id": "62b8d23948ba5a41d1c3f498", "prompt": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\t\n\tdef decorating_function(func):\n        \n\t\tcache = {}\n\t\tlock = threading.RLock()                    #  Global locking to prevent race condition w/\n                                                    #  another thread that could add/update an entry\n\t\t@wraps(func)                                #  while we loop the function calls\n\t\tdef wrapper(*args, **kwargs):                #  (wraps the call as well)\n\t\t\tif len(cache) == maxsize:                #  If cache is full, remove LRU element\n\t\t\t\tdel cache[cache.keys()[0"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorating_function(function_to_wrap):\n\t\tdef memoizing_function(*args):\n\t\t\tkey = make_cache_key(args)\n\t\t\tcached_value = _MemoizeCache.get(key)\n\t\t\tif cached_value is not None:\n\t\t\t\treturn cached_value\n\n\t\t\tvalue = function_to_wrap(*args)\n\t\t\t_MemoizeCache.set(key, value)\n\t\t\treturn value\n\t\treturn memoizing_function\n\tif typed:\n\t\treturn decorating_function\n\telse:\n\t\treturn decorating_function("}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tif typed:\n\t\t\tdef memoizer(*args, **kwargs):\n\t\t\t\treturn _CacheNode(func, args, kwargs)\n\t\telse:\n\t\t\tdef memoizer(*args, **kwargs):\n\t\t\t\treturn FuncCache(func, args, kwargs)\n\n\t\tmemoizer.func_cache = _CacheMaster(maxsize)\n\t\treturn memoizer\n\treturn decorator\n\n\ncache_clear = _CacheMaster.clear\ncache_get = _CacheMaster.get\ncache_set = _CacheMaster.set\n"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tcache = {} if typed else {}\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args):\n\t\t\tkey = tuple(args)\n\t\t\tif key not in cache: cache[key] = func(*(args))\n\t\t\tresult = cache[key]\n\t\t\tdel cache[key]\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function):\n\t\tglobal _cache\n\t\tif isinstance(maxsize, int) and maxsize > 0:\n\t\t\t@functools.wraps(user_function)\n\t\t\tdef cache_getter(*args, **kwargs):\n\t\t\t\tkey = (args, tuple(sorted(kwargs.items())))\n\t\t\t\tresult = _cache.get(key, None)\n\t\t\t\tif result:\n\t\t\t\t\tresultstring = result[1] + repr(maxsize)\n\t\t\t\t\tif len(resultstring) > maxsize:\n\t\t\t\t\t\t_cache."}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef actual(func):\n\t\tdef mem_of_func(self, *args, **kwargs):\n\t\t\tkey = [hash(item) for item in args, tuple(kwargs.items())]\n\t\t\ttry:\n\t\t\t\treturn LRU[key]\n\t\t\texcept KeyError:\n\t\t\t\tvalue = func(self, *args, **kwargs)\n\t\t\t\tif maxsize is None:\n\t\t\t\t\tLRU.put(key, value)\n\t\t\t\telse:\n\t\t\t\t\tif len(LRU) > maxsize:\n\t\t\t\t\t\tLRU.popitem(False)\n\t\t\t\t\tLRU."}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(f):\n\t\t\"\"\"\n\t\tThe actual decorator for limiting results.\n\t\t\"\"\"\n\t\tcache=dict()\n\t\t@functools.wraps(f)\n\t\tdef wrapper(*args, **kw):\n\t\t\t\"\"\"\n\t\t\tThe actual memoized function.\n\t\t\t\"\"\"\n\t\t\tkey=make_key(args, kw)\n\t\t\tif key in cache:\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tresult=f(*args, **kw)\n\t\t\t\tif not typed:\n\t\t\t\t\tresult=result._eval_type()\n\t\t\t\tcache[key]="}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tif typed:\n\t\tdef decorator(f):\n\t\t\t@functools.wraps(f)\n\t\t\tdef wrapped(*args):\n\t\t\t\treturn _lru_cache(f, maxsize, *args)\n\n\t\t\treturn wrapped\n\n\t\treturn decorator\n\telse:\n\t\tdef decorator(f):\n\t\t\t@functools.wraps(f)\n\t\t\tdef wrapped(*args):\n\t\t\t\treturn _lru_cache(f, maxsize)\n\n\t\t\treturn wrapped\n\n\t\treturn decorator\n\n\n# Example\nif __name__ == '__main__':\n\t@"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tif maxsize<0:\n\t\treturn _lru_cache_no_maxsize(typed)\n\tdef wrapper(functor):\n\t\treturn _lru_cache(maxsize,typed,functor)\n\treturn wrapper\n\n@lru_cache(3)\ndef _lru_cache(maxsize,typed,functor):\n\t# TODO: make functions memoable\n\tdef memoize(self):\n\t\tif not hasattr(self, \"_cached_value\"):\n\t\t\tself._cached_value = functor(self)"}, {"generated_text": "def lru_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Recently Used (LRU)\n\talgorithm.\n\t\"\"\"\n\tdef decorating_function(user_function, cache_name = None):\n\t\t\"\"\"\n\t\tReturns a wrapped function with LRU caches and the provided\n\t\tmaxsize of the cache.\n\n\t\tThe new cache is a dictionary that knows how to update the\n\t\tcache with a ``.update()`` method that will save back\n\t\tup to the maximum number of elements the cache can store.\n\n\t\tThe call will take some time longer because the call is not\n\t\tmade immediately until a function update() method is\n\t\tapplied to the dictionary.\n\n\t\tThe decorator will wrap a function so that you"}]}
{"task_id": "62b8d23748ba5a41d1c3f496", "prompt": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tif typed:\n\t\tdata = dict()\n\telif typed is not None:\n\t\tdata = {}\n\t_cache = {}\n\n\tdef make_wrapper(*args, **kwargs):\n\t\t\"\"\"\n\t\tMake a closure that calls the wrapped function `fn` and memoizes\n\t\tthe result.\n\t\t\"\"\"\n\t\tdef cached_fn(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\tkey = tuple(args +  tuple(sorted(kwargs.items())))\n\t\t\texcept TypeError:\n\t\t\t\tkey = '_INVALID_KEY_'\n\t\t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\timport collections\n\tMAXSIZE = maxsize\n\n\tif len(maxsize)!= 6 or not maxsize[-1] == ':':\n\t\traise Exception('lfu_cache(maxsize) requires a \":NN\" format string')\n\n\tdef decorator(func):\n\t\tcache = collections.OrderedDict()\n\n\t\tif not typed:\n\t\t\tcache = collections.defaultdict(lambda: collections.OrderedDict())\n\t\t\tcache = collections.defaultdict(lambda: collections.OrderedDict())\n\n\t\t@wraps(func)\n\t\tdef memoizer(*"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\t\n\t# TODO change return type depending on typed arg;\n\t# Currently type(object()) is needed\n\t\n\t# TODO could cache cache[hash] too, and have a delete()\n\t\n\tmaxsize = max(1, maxsize)\n\t\n\t@functools.wraps(func)\n\tdef cached_func(*args,**kwargs):\n\n\t\tif not kwargs.has_key('typed'):\n\t\t\tkwargs['typed'] = False\n\t\t\t\n\t\tfunc_name = kwargs.get('__name__',"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef _cache(wrapper):\n\t\t@functools.wraps(wrapper)\n\t\tdef _lfu_wrapper(*args, **kwargs):\n\t\t\t_result = wrapper(*args, **kwargs)\n\t\t\tif not hasattr(_result, \"_cached\"):\n\t\t\t\t_lfu_cache = set()\n\t\t\t\t_lfu_result = wrapper(*args, **kwargs)\n\t\t\t\tif _lfu_result is not _lfu_result:\n\t\t\t\t\t# we need to track the result here so both the\n\t\t\t\t\t# cached and uncached versions have the same\n\t\t\t\t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tcache = {}\n\tdef memoized(func):\n\t\tdef cache_get(key, default):\n\t\t\tif not key in cache:\n\t\t\t\tcache[key] = func(key)\n\t\t\treturn cache[key]\n\t\tdef cache_set(key, value):\n\t\t\tcache[key] = value\n\t\tdef cache_del(key):\n\t\t\tif key in cache:\n\t\t\t\tdel cache[key]\n\t\tif typed:\n\t\t\tcache[default(cache_get, lambda: type(str)(key).encode()) for key in range(maxsize)] = func\n\t\t"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tfrom collections import deque\n\n\tif not typed:\n\t\t@functools.wraps(func)\n\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tkey = self.__key(self, args, kwargs)\n\n\t\t\t# get the key to this item\n\t\t\tresults = self.lfu_data.get(key, [])\n\n\t\t\t# get the item, if it's new\n\t\t\tif not any(item for item in results if item == item.key):\n\t\t\t\t# save the new item\n\t\t\t\tresults.append(func(self, *args"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tif typ is True:\n\t\tdef lfu_cache_typed(*args, **kw):\n\t\t\treturn lfu_cache_typed(*args, **kw)\n\telse:\n\t\tdef lfu_cache_dict(*args, **kw):\n\t\t\tdict_ = {}\n\t\t\tcache = {}\n\t\t\tdata = []\n\n\t\t\t# Call the function on the first pass\n\t\t\tfunction, args, kw = args[0], args[0:1], kw[0]\n\n\t\t\tcache[function.__name__] = (function, args, kw)\n\t\t\t#"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kw):\n\t\t\tcache = func.cache_\n\t\t\tkey = str(args) if not kw else str(args) + str(kw)\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = func(*args, **kw)\n\t\t\telse:\n\t\t\t\tcache_it = cache[key]\n\t\t\t\tif isinstance(cache_it, tuple):\n\t\t\t\t\treturn cache_it[0](*cache_it[1], **cache_it[2])\n\t\t\t\telse:\n\t\t\t\t\treturn cache[key]"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\tlru = collections.OrderedDict()\n\t\t\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = hash(str(args) + ''.join(map(str, kwargs.values())))\n\t\t\tif key in lru:\n\t\t\t\treturn lru[key]\n\t\t\tlru[key] = func(*args, **kwargs)\n\t\t\treturn lru[key]\n\n\t\tif typed:\n\t\t\treturn wrapper\n\n\t\twrapper.cache = collections.Ordered"}, {"generated_text": "def lfu_cache(maxsize=128, typed=False):\n\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\n\tup to `maxsize` results based on a Least Frequently Used (LFU)\n\talgorithm.\n\t\"\"\"\n\tclass LRUCache(object):\n\t\tdef __init__(self, maxsize):\n\t\t\tself.count = 0\n\t\t\tself.cache = dict.fromkeys(range(maxsize), None)\n\t\t\tself.maxsize = maxsize\n\n\t\tdef __call__(self, *args, **kwargs):\n\t\t\tkey = hash(args) + hash(kwargs)\n\t\t\tif key in self.cache:\n\t\t\t\tvalue = self.cache[key]\n\t\t\t\tself.count += 1\n\t\t\t\tif self.count <= self.maxsize:\n\t\t\t\t\treturn value\n\t\t\tvalue"}]}
{"task_id": "62b8d22f48ba5a41d1c3f488", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tself.cache.popitem(False)\n\t\t\n\tif not self.cache.empty():\n\t\tkey, value = self.cache.popitem(False)\n\t\treturn key, value\n\t\n\treturn\n\t\t\ndef clear(self):\n\tself.cache.clear()\n\tpass\n\ndef pop(self, key, default=None):\n\t\"\"\"\n\tRemoves and returns `value` associated with `key`, or raises `KeyError` if not found.\n\t\"\"\"\n\tif key in self.cache:\n\t\tvalue = self.cache.pop("}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\treturn self._pops.popitem()\n\nclass DictStack(collections.UserDict):\n\t\"\"\"\n\tThis is a subclass of collections.UserDict which maintains \n\ta stack of dictionaries.\n\n\tExample::\n\n\t\t>>> from collections import UserDict\n\t\t>>> from dictstack import DictStack \n\t\t>>> ds = DictStack()\n\t\t>>> ds['dictionary'] = {'key': 'value'}\n\t\t>>> ds.push(dictionary={})\n\t\t>>> ds.get('dictionary') == {}\n\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\ttry:\n\t\treturn self._store.popitem() # return oldest item\n\texcept IndexError:\n\t\treturn None\n\texcept KeyError:\n\t\t# it's not in the list, so return None\n\t\treturn None\n\n# XXX make this faster if keys have fewer than self._MAX_SIZE items\nclass LRUCache(dict, Store):\n\t\"\"\"\n\tLeast-Recency Cache - most recent added first.\n\tThis is a dictionary where the keys are sorted by insertion time in the store.\n\tAs a consequence, the least-recently added keys are removed first"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif len(self.data): \n\t\treturn self.data.popitem()\n\n\ndef iter_dict_items(self):\n\tfor k, v in self.data.items():\n\t\tyield k, v\n\ndict.iteritems = iter_dict_items\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tself._raise_if_closed()\n\t_item = self._deque.popleft()\n\treturn _item\n\t\n\n\n#\n\ndef _iteritems(self):\n    \"\"\" Iterate over the dictionary's items, as (key, value) pairs.\n\n    Same as ``iteritems`` defined on dictionaries in the standard library.\n\n    \"\"\"\n    self._raise_if_closed()\n    keys = self.keys()\n    values = [self[key] for key in keys]\n    return zip(keys, values)\n\ndict.__iter__"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\treturn self.__dict__.popitem()\n    \n    def pop(self, *args):\n        return self.__dict__.pop(*args)\n\"\"\"\n    Remove all the entries of the dictionary. Equivallent to deleting all\n    the items of it.\n    \"\"\"\n    def clear(self):\n\treturn self.__dict__.clear()\n\"\"\"\n    Check if the dictionary hasn't been modified.\n    \"\"\"\n    def is_modified(self):\n\treturn self.__dict__.is_modified()\n\"\"\"\n    Return all the entries"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\treturn self.dict.popitem()\n  \n  \nclass DictLocking(dict):\n  def __init__(self, init=None):\n\t    super(DictLocking, self).__init__(init)\n\t    self.lock = threading.RLock()\n\n  def lock_write(self):\n\t    self.lock.acquire()\n\n  def unlock_write(self):\n\t    self.lock.release()\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\treturn (self._dictionary.popitem())\n\t\nclass WeakRef(ref):\n\t\"\"\"\n\tBind an object and create a weak reference to it.\n\t\n\t:param object obj: Reference. \n\t:return: :py:class:`WeakRef`\n\t\"\"\"\n\tdef __init__(self, obj):\n\t\tref.__init__(self, obj, _get_reference_id())\n\t\nclass _WeakRefProxy(references):\n\t\"\"\"\n\tProxied weak references.\n\t\"\"\"\n\tdef __init__(self"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\tif not self:\n\t\t\traise NotImplementedError(\"popitem() is not implemented\")\n\treturn super(SortedOrderedDeque, self).popitem()\n\ndef insert(self, key, item):\n\t\"\"\"\n\tAdd `item` to the end of the deque.\n\n\t>>> q = SortedOrderedDeque() # empty deque\n\t>>> s = SortedOrderedDeque()\n\t>>> x, y = q, s\n\t>>> x.insert(2, 1) # push 1 to the left of 2 in q\n\t>>> x"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair first inserted.\n\t\"\"\"\n\ttry:\n\t\titem = self._table.poll()\n\t\tself._counters.pop(item[0])\n\t\treturn item\n\texcept TableEmpty:\n\t\treturn Null\n\ndef clear(self):\n\t\"\"\"\n\tRemove all `(key, value)` pairs from the hash table without\n\tnotifying observer(s).\n\t\"\"\"\n\ttry:\n\t\tif self._table:\n\t\t\tdel(self.__d)\n\t\t\tself.__counters.clear()\n\texcept AttributeError:\n\t\tpass\n\ndef count(self, key):\n\t"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47e", "prompt": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tif key in self._cache_:\n\t    return self._cache_[key]\n\treturn self._base.setdefault(key, default)\n\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\ttry:\n\t\treturn self.__getitem__(key)\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n\t\n# http://docs.python.org/2/library/configparser.html#configparser-objects\n# pylint: disable=R0904\nclass OrderedConfigParser(ConfigParser, Mapping):\n\t\"\"\"\n\tCustomized version of ConfigParser for using OrderedDict\n\t\"\"\"\n\tdef __iter__(self):\n\t\treturn iter(sorted(super(OrderedConfigParser, self).items"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn getattr(self, setdefault)\n\n\tdef setupdate(self, other, **kwargs):\n\t\t\"\"\"\n\t\tD.setdict(D,**kwargs) -> None.  Set D from dict/iterable.\n\t\tSet D's value for a key, if it wasn't previously set.\n\t\t\"\"\"\n\t\treturn setdefault(self, other.iterkeys(), {}).update(other, **kwargs)\n\n\tdef update(sD, sD2):\n\t\t\"\"\"\n\t\tD.update(items) -> None.  Update D from Items dict/iter"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn self.add(key,default)\n\n_t_setdefaultd = threading.local()\ndef setdefaultd(default, key1, key2=None):\n\t\"\"\"\n\tD.setdefaultd(default[,key1,key2=None]):\n\tSets key2 to default if specified, otherwise returns the same as setdefault(key1)\n\t\"\"\"\n\tif key2 is None:\n\t\tsetdefault(key1,default)\n\t\n\telse:\n\t\tsetdefault(key1,None)\n\t\tset(key1,"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\t\n\treturn self.get(key, default)\n\n# --------------------------------------------------------\n# --------------------------------------------------------\n# --------------------------------------------------------\n\nif __name__ == \"__main__\":\n\tif __name__ == \"__main__\":\n\t\tt = datetime.datetime.now()\n\t\tprint \"test start at %s\" % (t)\n\n\t\tl = [1, 2, 3, 4, 5, 6, 7, 9, 10, 24, 25, 26, 27, 28, 29, 30, 33, 34, 35, 37, 38, 40, 42, 43]\n\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn self[key] if key in self else default\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn self.get( key, default )\n\ndef setdefaults(self):\n\t\"like dict().update(D) except that unset values are replaced by default\"\n\tfor k, default in self.defaults.items(): self[k] = self.get(k, default)\n\ndef update(self, d):\n\t\"\"\"\n\tD.update(E) -> None.  Update D from E.\n\tE may be a dictionary or another mapping object.\n\t\"\"\"\n\tif type( d ) is dict: self.data.update( d )\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\treturn super().setdefault(key, self.__class__(default))\n\n\ndef popitem(self):\n\t\"\"\"\n\tD.popitem() -> (k,v), remove and return some (key, value) pair as a\n\t2-tuple; but raise KeyError if D is empty.\n\t\"\"\"\n\treturn super().popitem()\n\n\ndef update(self, dict):\n\t\"\"\"\n\tD.update(E) -> None.  Update D from E: for k in E: D[k] = E[k].\n\tSee the documentation of"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\n\treturn default\n\ndef copy(self):\n\t\"\"\"\n\tD.copy() -> a shallow copy of D\n\t\"\"\"\n\treturn dict(self)\n"}, {"generated_text": "def setdefault(self, key, default=None):\n\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n\t\"\"\"\n\tdef setdefaultself(key, default=None):\n\t        if key not in self:\n\t                self.__setattr__(key, default)\n\t                return default\n\t        else:\n\t                return self.get(key)\n\t# def default(self, key, default=None):\n\t#\tif key not in self:\n\t#\t\treturn default\n\t#\treturn self[key]\n\n\tself.setdefault = collections.abc.functools.wraps(setdefault)(setdefault)\n\tself.setdefaultself = collections.abc"}]}
{"task_id": "62b8d22948ba5a41d1c3f47c", "prompt": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn self.getdict(str(key),None)\n\ndef items(self, kw={}, allow_default=False):\n    \"\"\"\n\tD.items() -> a set-like object where each element has key, value\n\t\"\"\"\n    ret = []\n    [ret.append(self[k]) for k in self]\n    return ret\n\t\n\t\n\t\n\t"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\t# set first to avoid exception on KeyError\n\treturn self.getitem(key, default)\n\nclass UserDict(MutableMapping):\n\t\"\"\"\n\tGeneric dictionary implementation using __getitem__, update, setitem, delitem\n\t\"\"\"\n\n\tdef __init__(self, source=None, **kwargs):\n\t\t\"\"\" Create a dictionary object\n\t\t\"\"\"\n\t\t# add keyword arguments to instance-attributes for faster lookup\n\t\tfor k in kwargs:\n\t\t\tsetattr(self, k, kwargs[k])\n\n\t\t# initialise dict"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn self.__getitem__(key) if key in self else default\ndef getWithDeps(self, key, default=None, dependencies=None, cache=True):\n        \"\"\"\n        D.getWithDeps(k[,d], dependencies):\n            search key in order, return first value not in dependencies\n        \"\"\"\n        if dependencies is None:\n            if cache:\n                return self.__getitem__(key)\n            else:\n                return self.__getitem__(key) if key in self else default\n        \n        if key not in"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn self.data[key] if key in \n\nD = SortedDict()\n\nif __name__ == '__main__':\n\tl = [5, 3, 7, 1, 2, 9]\n\tl2 = [0, 2, 1, 9, 3, 5]\n\tD.add('name', 'Vishnu','s')\n\tD.add('age', 22, '+')\n\tD.add('height', 5.1, '-')\n\tD.add('dob', 2007, '#')\n\tprint"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn self.items.get(key, default)\n\n\ndef setdefault(key, default=None):\n\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D and d\n\tis not None\n\tReturns D.get(key, default), like dict.get(x, default) except\n\tD.get is overridden and may be called with D[key]=default.\n\t\"\"\"\n\t_get = self.get\n\tif key not in self:\n\t\tself"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\tif key in self.data:\n\t\treturn self.data[key]\n\telse:\n\t\treturn default\n\n@add_meta_path('basket', 'key:basket')\nclass KeyBasket:\n\t\"\"\"\n\tD.keybasket([v[,d]]) -> KeyBasket(d)\n\n\tD.keybasket([v[,d]], init={'b':'foo', 'c':'bar'}) -> KeyBasket(v, d)\n\t\"\"\"\n\tdata = {}\n\n\tdef __init__(self, bag"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry:\n\t\tif isinstance(key, str):\n\t\t\tif key in self._data:\n\t\t\t\treturn self._data.get(key)\n\t\treturn default\n\texcept Exception as e:\n\t\traise e\n\t\t\n\t\n\n\n\n\n\n\n"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\ttry: return self.cache[key]\n\texcept KeyError: return default\n\t\t\n\n\t\ndef put(self, key, value):\n\t\"\"\"\n\tD.put(key, value) -- Associate key with value.\n\t\"\"\"\n\tself.cache[key] = value\n\n\ndef has_key(self, key):\n\t\"\"\" Return True if D has a key key, else False \"\"\"\n\treturn key in self.cache\n\t\t\n\n\t\nif __name__ == '__main__':\n\td = DMap()\n\td.put('foo', 1"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn self[key] if key in self else default"}, {"generated_text": "def get(self, key, default=None):\n\"\"\"\n\tD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n\t\"\"\"\n\treturn self.__getitem__(key, default)\n\n\n\tdef pop(self, key, *default):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given; otherwise KeyError is raised.\n\n\t:param key:\n\t:param default:\n\t\tdefault return value if key is not in dict.\n\t\"\"\"\n\treturn self.__getitem__(key, *default)\n\n\n\tdef setdefault(self, key, value):\n\t"}]}
{"task_id": "62b8d22548ba5a41d1c3f472", "prompt": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\ttry:\n\t\tfrom functools import wraps\n\texcept:\n\t\tfrom django.utils.functional import wraps\n\t\n\tdef helper(func):\n\t\tdef cache_and_return(obj, *args, **kw):\n\t\t\tlock = cache.get(key(kw))\n\t\t\tif lock is not None:\n\t\t\t\tif lock.acquire(False):\n\t\t\t\t\tresult = lock.cache\n\t\t\t\t\tlock.release()\n\t\t\t\telse:\n\t\t\t\t\traise ValueError(\"Cache lock is busy\")\n\t\t\telse:\n\t\t\t\tlock = Lock()\n\t\t\t\tlock.acquire()\n\t\t\t\t"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tlock = lock or RLock()\n\n\tdef decorator(*args, **kwargs):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twith lock:\n\t\t\t\tkey = makekey(key, *args, **kwargs)\n\t\t\t\tif key in cache:\n\t\t\t\t\tresult = cache[key]\n\t\t\t\t\treturn result\n\t\t\t\telse:\n\t\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\t\tcache[key] = result\n\t\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n#==============================================================================\n\nclass"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef _cachedmethod(self, cache=cache):\n\t\t@wraps(method)\n\t\tdef __wrapped(self, *args, **kw):\n\t\t\tkey = key or hash\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tcachekey = getattr(self, cache)\n\t\t\t\t\tif cachekey not in cache:\n\t\t\t\t\t\tcache[cachekey] = method(self, *args, **kw)\n\t\t\t\t\t\treturn cache[cachekey]\n\t\t\t\t\telse:\n\t\t\t\t\t\t# return the value without calling the wrapped function again\n\t\t\t\t\t\tkey = '%s"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tlock = lock or RLock()\n\n\tclass memoize(object):\n\t\n\t\tdef __init__(self, func, lock):\n\t\t\tself._lock = lock\n\t\t\tself._cache = {}\n\t\t\tself._func = func\n\t\n\t\tdef __get__(self, obj, objtype=None):\n\t\t\tobj._lock.acquire()\n\t\t\ttry:\n\t\t\t\ttry:\n\t\t\t\t\tif obj._cache and obj._cache.has_key(self._func.__name__):\n\t\t\t\t\t\treturn obj._cache[self._func.__name__]\n\t\t\t\texcept Key"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tlock = lock or thread.allocate_lock()\n\n\tdef wrap(func):\n\t\tlock.acquire()\n\t\ttry:\n\t\t\td = {}\n\t\t\tif type(cache) is types.DictionaryType:\n\t\t\t\td = cache.setdefault(func,{}) # pylint:disable = E1125\n\t\t\telse:\n\t\t\t\td[key] = ref(cache.setdefault(func,{}),lock.__exit__)\n\t\t\tif func in d:\n\t\t\t\treturn d[func]\n\t\t\telse:\n\t\t\t\tdef call(*args, **kw):"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tif cache is not None:\n\t\tdef _decorated(func):\n    \t\t\t\"\"\"\n    \t\t\tSave memoized results in a cache.\n    \t\t\t\"\"\"\n\t\t\tdef _innercall(*args, **kwargs):\n\t\t\t\tif not isinstance(cache, dict):\n\t\t\t\t\treturn func(*args, **kwargs)\n        \t\t\tdef memo(*args, **kwargs):\n\t\t\t\ttry:\n\t\t\t\t\tret = cache[key]\n\t\t\t\texcept:\n\t\t\t\t\tret = func(*args, **kwargs)\n\t\t\t\t\tcache[key] = ret\n\t\t\t\treturn ret\n\t\t\t\t\n\t\t\tdef memo"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef decorating(func):\n\t\tfunc.cached_method = cachemethod(cache, key, lock)\n\t\treturn func\n\treturn decorating\n\n# ------------------------------------------------------------\n# Global function cache\n# ------------------------------------------------------------\n\n# The global cache for modules, classes, functions\n# and constants\nfunctioncache = {}\n# The lock for the above cache\nlock = threading.Lock()\n# Existen module methods and module attributes.\n_globals_meta = {}\n# module attributes\n_module_attributes = {}\n# object attributes\n_object_attributes"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tif not callable(cache):\n\t\tcache = cache.default_factory    \n\tdef wrapper(self, *args, **kwargs):\n\t\tkey = key if callable(key) else key(self, *args, **kwargs)\n\t\tif lock:\n\t\t\twith lock:\n\t\t\t\tresult = cache.get(key, self)\n\t\t\t\tif result is not None:\n\t\t\t\t\treturn result\n\t\t\t\tresult = func(self, *args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\treturn result\n\t\telse:\n\t\t\treturn func(self, *"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tif lock is None:\n\t\tdef lockfunc(instance):\n\t\t\tkeyvalue = instance.__dict__.get(key)\n\t\t\tif keyvalue is not None:\n\t\t\t\t# We need to add'self' to the key value (for now) for\n\t\t\t\t# thread safety purposes.\n\t\t\t\tkeyvalue.append(self)\n\t\t\treturn keyvalue\n\telse:\n\t\tdef lockfunc(instance):\n\t\t\tif lock:\n\t\t\t\tkeyvalue = instance.__dict__.get(key)\n\t\t\t\tif lock == keyvalue:\n\t\t\t\t\treturn lock\n\t\t\telse:\n\t\t\t\tlock"}, {"generated_text": "def cachedmethod(cache, key=hashkey, lock=None):\n\"\"\"\n\tDecorator to wrap a class or instance method with a memoizing\n\tcallable that saves results in a cache.\n\t\"\"\"\n\tdef decorating_function(func):\n\t\tdef memoizing_method(*args, **kw):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\tvalue = cache.get(key(func, args, kw), None)\n\t\t\tif value is None:\n\t\t\t\tvalue = func(*args, **kw)\n\t\t\t\tcache[key(func, args, kw)] = value\n\t\t\tif lock:\n\t\t\t\tlock.release()\n\t\t\treturn value\n\t\tmemoizing_method.func_name = func.func_name\n\t\tmemoizing_method.cache = cache"}]}
{"task_id": "62b8c517e0d34b282c18122e", "prompt": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\treturn cls.__getstr_ext(e.msg, e, max_level, max_path_level)\n\n\nclass FileNotFoundError(Exception):\n\t\"\"\"File not found exception\"\"\"\n\tpass\n\n\nclass FileAlreadyExistsError(Exception):\n\t\"\"\"File already exists exception\"\"\"\n\tpass\n\n\nclass InvalidFileExtensionError(Exception):\n\t\"\"\"Invalid file extension exception\"\"\"\n\tpass\n\n\nclass IOError(Exception):\n\t\"\"\"IO error\"\"\"\n\tpass\n\n"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tif isinstance(e, str):\n\t\treturn \"%s\" % e\n\tstring = \"%s: %s\" % (type(e).__name__, e)\n\tif hasattr(e, '_stackTrace') and e._stackTrace:\n\t\tst = list(traceback.extract_stack(e._stackTrace))\n\t\tst = st[:-2]\n\t\tstring = \"%s\\n   in %s\" % (string, ':'.join([\"%s:%s\" % (path[0:max_path_level"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\ts = \"%s: %s\" % (e.__class__.__name__, e)\n\ttry:\n\t\ttrace = traceback.extract_tb(sys.exc_info()[2])\n\t\ttrace_levels = []\n\t\tfor x in trace:\n\t\t\tif x[0] == max_level + 2:\n\t\t\t\ttrace_levels.append(x[0])\n\t\ttrace_levels.reverse()\n\t\tfor x in trace_levels:\n\t\t\tif x == max_level + 1:\n\t\t\t\ts += \"\\nTrace starts from this code"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tdef _(cls, max_level=max_level, max_path_level=max_path_level):\n\t\t\"\"\"\n\t\tThis is a closure, and therefore all its free variables are\n\t\tshared between the inner function and the outer one.\n\t\tCalled at most once when the function is entered for the first time and\n\t\tat most once when the function leaves.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tlines = list()\n\t\t\tfor level in range(0, max_path_level):\n\t\t\t\tlines.append(\" ..\")\n\t\t\tlines.append('')  "}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\textbuf=[cls.__name__]\n\tif isinstance(e, Exception):\n\t\tp=inspect.currentframe().f_back.f_lineno\n\t\tcls_name=extbuf.pop(0)\n\t\te_classname=cls_name\n\t\ttry:\n\t\t\tif len(cls_name) > max_path_level:\n\t\t\t\te_classname = cls_name[max_path_level+1:]\n\t\t\tlog.debug(\"Call to {}\".format(inspect.getmodule(e).__name__))"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tcls_name = cls.__name__\n\tif not hasattr(e, \"message\"):\n\t\treturn \"%s: Unable to find message in %s\" % (e, cls_name)\n\n\tif e.message == None: return \"%s: Unable to find message in %s\" % (e, cls_name)\n\n\t# Remove trailing newlines\n\te_msg = str(e).replace('\\n',' ')\n\n\tif isinstance(e.message, Exception):\n\t\t\t# Recursive exception - create backtrace\n\t\t\treturn ext"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\treturn '%s' % cls._render(e, max_level, True, max_path_level)\n\n\ndef extostr_only_max_level(cls, e, max_level=30, max_path_level=5):\n\treturn '%s' % cls._render(e, max_level, False, max_path_level)\n\n\ndef render(e):\n\t\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance\n\t:type e: Exception\n\t:return The exception readable string\n\t:r"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\treturn str(cls(cls.ex_format_value(e), max_level=max_level, max_path_level=max_path_level))\n\n\ndef ex_format_value(e):\n\t\"\"\"\n\tFormat an exception. This version is used in the constructor of the exceptions.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:return The string representation of the exception passed in args\n\t:rtype str\n\t\"\"\"\n\treturn str(e)\n\n\nclass ExType(ExceptionType):\n\t\"\"\"\n\t"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\t# FIXME make format_exception work for all subclasses \n\treturn cls.format_exception(e, max_level, max_path_level)\n\ndef format_exception(ex, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat an exception.\n\t:param ex: Any exception instance.\n\t:type ex: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5"}, {"generated_text": "def extostr(cls, e, max_level=30, max_path_level=5):\n\"\"\"\n\tFormat an exception.\n\t:param e: Any exception instance.\n\t:type e: Exception\n\t:param max_level: Maximum call stack level (default 30)\n\t:type max_level: int\n\t:param max_path_level: Maximum path level (default 5)\n\t:type max_path_level: int\n\t:return The exception readable string\n\t:rtype str\n\t\"\"\"\n\tif sys.platform.find('linux')!= -1:\n\t\ttb = (rstrtb(e) if sys.version_info.major == 3 else rstrtb(e.tb))\n\telse:\t\t\n\t\ttb = e.__traceback__\n\t\tif not tb:\n\t\t\traise Exception('No exception')\n\t\n\tlines = []\n\tlines += [ '** Stack trace: **' ]\n\tlines += [ '' ]\n\tfor line in tb:\n\t\tlines += [ '' ]\n\t\t\n\t\tlevel = 0\n"}]}
{"task_id": "62b8bbbfe0d34b282c181210", "prompt": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif not isinstance(file_name, str):\n\t\treturn 0\n\n\tif not os.path.exists(os.path.dirname(file_name)) and not overwrite:\n\t\traise Exception(\"Create a directory for the file %s\" % os.path.dirname(file_name))\n\n\twith open(file_name, 'wb') as f:\n\t\tf.write(text_buffer)\n\n\treturn len(text_buffer)\n\n# ----------------------------------------------------------------------------------------------------------------------\n\ndef append_text_in_file(file_name, text_buffer, encoding"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif not os.path.isfile(file_name):\n\t\ttry:\n\t\t\ttext_file = open(file_name, 'w', 0666)\n\t\t\tif encoding is None:\n\t\t\t\ttext_file.write(text_buffer)\n\t\t\telse:\n\t\t\t\tfor line in text_buffer.decode(encoding).split('\\n'):\n\t\t\t\t\ttext_file.write('{}\\r\\n'.format(line))\n\t\t\t\ttext_file.flush()\n\t\t\t\ttext_file.close()\n\t\texcept Exception as err:\n\t\t\tprint('Error in file {}"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\terror_code = 0\n\tfile_name = os.path.abspath(file_name)\n\tif not os.path.exists(file_name) or overwrite:\n\t\ttry:\n\t\t\tfile_handle = open(file_name, 'w')\n\t\t\tfile_handle.write(text_buffer.encode(encoding))\n\t\t\terror_code = file_handle.close()\n\t\texcept Exception as e:\n\t\t\terror_code = e\n\n\treturn error_code\n\n\n# TODO\ndef get_output_directory():\n\tpass\n\n\n"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif not os.path.exists(file_name):\n\t\ttry:\n\t\t\tfile(file_name, 'w', encoding)\n\t\texcept IOError, e:\n\t\t\tprint(e)\n\t\t\treturn -1\n\n\tlength = len(text_buffer)\n\tfile_obj = open(file_name, 'wb+', True)\n\ttry:\n\t\tfile_obj.write(text_buffer)\n\texcept IOError, e:\n\t\tprint(e)\n\t\treturn -1\n\n\tfile_obj.close()\n\tos.chmod(file"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\ttry:\n\t\twith utils.unlikely_lock_guard(encoding=encoding):\n\t\t\tfile_to_write = io.open(file_name, 'wb')\n\t\t\tfile_to_write.seek(0)\n\t\t\tif overwrite or not file_to_write.tell():\n\t\t\t\tfile_to_write.write(text_buffer)\n\t\t\t\tfile_to_write.truncate()\n\t\t\telse:\n\t\t\t\tfile_to_write.write(text_buffer)\n\t\t\t# close the file\n\t\t\tfile_to_write.close()\n\t\t\t"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif type_is_str(file_name):\n\t\tif not os.path.isfile(file_name):\n\t\t\t_logger.debug('File is not exist: %s', file_name)\n\t\t\tif overwrite:\n\t\t\t\twith open(file_name, 'wb') as file:\n\t\t\t\t\tfile.write(text_buffer)\n\t\t\t\t_logger.debug('File is created: %s', file_name)\n\t\t\telse:\n\t\t\t\twith open(file_name, 'a') as file:\n\t\t\t\t\tfile.write(text_buffer)\n\t\t\t\t_log"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tencoding = normalize_charset(encoding)\n\tif encoding is None and encoding!=\"iso-8859-1\":\n\t\tencoding = \"utf-8\"\n\ttext_buffer=text_buffer.decode(encoding)\n\tif os.path.isdir(file_name):\n\t\traise IOError, '\"%s\" Is a directory' % file_name\n\tif os.path.isdir(text_buffer.replace(\"\\n\",\"\")):\n\t\traise IOError, '\"%s\" Is a directory' % file_"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\ttry:\n\t\ttry:\n\t\t\twith open(file_name, 'a' if overwrite else 'w', encoding=encoding) as file_handler:\n\t\t\t\tfile_handler.write(text_buffer)\n\t\t\t\tfile_handler.close()\n\t\texcept IOError as error:\n\t\t\tlogger.error(error)\n\t\t\treturn 0\n\t\telse:\n\t\t\treturn len(text_buffer)\n\texcept IOError as error:\n\t\tlogger.error(error)\n\t\treturn 0\n\n\ndef write(file_path_1, file_path_2, file_"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tif file_name.endswith(\".pdb\"):\n\t\t_append_pdb(file_name, text_buffer, encoding, overwrite)\n\telif file_name.endswith(\".cfg\"):\n\t\t_append_config(file_name, text_buffer, encoding, overwrite)\n\telse:\n\t\tlogger.error(\"Format not supported by 'append_text_to_file.append_text_to_file': %s\", file_name)\n\treturn 0\n\ndef _append_pdb(file_name, text_buffer"}, {"generated_text": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\"\"\"\n\tWrite to the specified filename, the provided binary buffer\n\tCreate the file if required.\n\t:param file_name:  File name.\n\t:type file_name: str\n\t:param text_buffer: Text buffer to write.\n\t:type text_buffer: str\n\t:param encoding: The encoding to use.\n\t:type encoding: str\n\t:param overwrite: If true, file is overwritten.\n\t:type overwrite: bool\n\t:return: The number of bytes written or lt 0 if error.\n\t:rtype int\n\t\"\"\"\n\tbytes_written = 0  # Counter of appended bytes.\n\terror = False  # Errors from io.FileIO is a boolean.\n\n\ttry:\n\t\ttext_file = io.FileIO(file_name, \"w\", encoding=encoding)\n\t\tbytes_written = text_file.write(text_buffer)\n\texcept IOError:\n\t\ttext_file = None\n\t\terror = True\n\tfinally:\n\t\ttry:\n\t\t\tif text_file:\n\t\t\t\ttext_file.close()\n\t\texcept Exception:  # For now, ignore.  "}]}
{"task_id": "62b8bbbfe0d34b282c18120f", "prompt": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\twith codecs.open(file_name, encoding=encoding) as text_file:\n\t\t\ttext_buffer = text_file.read()\n\t\treturn text_buffer\n\texcept Exception as err:\n\t\tprint('\\nCannot read file: {file_name}\\n{err}\\n'.format(file_name=file_name, err=err))\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tinput_text = None\n\ttry:\n\t\tinput_text = codecs.open(file_name, encoding=encoding).readlines()\n\texcept IOError:\n\t\tlogging.error(\"Unable to read file \\\"\" + file_name + \"\\\", with encoding \\\"\" + encoding + \"\\\".\")\n\t\tsys.exit(1)\n\tif input_text == \"\":\n\t\treturn None\n\telse:\n\t\treturn input_text\n\ndef file_to_textlines(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\tf = codecs.open(file_name, 'r', encoding)\n\t\ttxt = StringIO(f.read())\n\t\tf.close()\n\t\tb = TextFileBuffer(txt)\n\t\treturn (b)\n\texcept Exception as e:\n\t\tlog_exception(e)\n\t\treturn None\n\ndef textbuffer_to_file(txtobj, filename=\"dummy\", overwrite=True):\n\t\"\"\"\n\tSave a text buffer towards a file.\n\tWarning : This file WILL be created on a disk!\n\t:param txtobj"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\twith io.open(os.path.expanduser(file_name), \"rt\", encoding=encoding) as f_:\n\t\t\ttext_buffer = f_.readlines()\n\texcept py2vs.UnicodeDecodeError:\n\t\treturn None\n\n\treturn text_buffer\n\ndef is_executable_file(full_file_name):\n\t\"\"\"\n\tCheck a file is an executable file (file must not be a directory)\n\t:param full_file_name: A file name to check.\n\t:type full_file_"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\twith codecs.open(file_name, \"r\", encoding=encoding) as f:\n\t\t\ttext_out = f.read()\n\t\tf.close()\n\texcept UnicodeDecodeError: #if the encoding to read is not supported.\n\t\tprint \"Error reading and decoding file : \" + file_name\n\t\tprint \"The file was not loaded because it does not support the encoding {0}\".format(encoding)\n\t\ttext_out = None\n\texcept IOError: #if the file to read is not found\n\t\tprint \"Error reading file"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tif os.path.isfile(file_name) == False:\n\t\treturn None\n\n\twith codecs.open(file_name, 'r', encoding) as f_file:\n\t\ttext = f_file.read()\n\n\treturn text\n\n\ndef parse_keywords(keywords_str, delimiter=\" \"):\n\t\"\"\"\n\tPasra keywords on a string in a list.\n\tCAUTION : This will split the string IN A LIST - no space is allowded.\n\t:param keywords_str: The string to parse\n\t:type keywords_str"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'r', encoding=encoding) as f:\n\t\t\treturn f.read()\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn \"\""}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\tbuffer = None\n\ttry:\n\t\t\timport codecs\n\t\t\tbuffer = codecs.open(file_name, 'r', encoding)\n\t\t\timport _memoryview\n\t\t\tresult = _memoryview.buffer_to_text(buffer.read())\n\t\tfinally:\n\t\t\tbuffer.close()\t\t\n\t\treturn result\n\texcept:\n\t\treturn None\n\n"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\twith open(file_name, encoding=encoding) as file_to_read:\n\t\ttext_buffer = file_to_read.read()\n\t\treturn text_buffer\n\n\ndef file_name_and_path(file_name):\n\t\"\"\"\n\t\tRetrieves the file name from file.\n\t\t(i.e. \"test.txt\" => \"path/to/file\")\n\t\t:param file_name: \n\t\t:type file_name: str\n\t\t:return: File full path.\n\t\t:rtype : str\n\t\"\"\"\n\tfull"}, {"generated_text": "def file_to_textbuffer(file_name, encoding):\n\"\"\"\n\tLoad a file toward a text buffer (UTF-8), using the specify encoding while reading.\n\tCAUTION : This will read the whole file IN MEMORY.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:param encoding: Encoding to use.\n\t:type encoding: str\n\t:return: A text buffer or None in case of error.\n\t:rtype str\n\t\"\"\"\n\ttry:\n\t\twith codecs.open(file_name, encoding=encoding, mode=\"r\") as file_desc:\n\t\t\treturn text_buffer(file_desc.read())\n\texcept:\n\t\tlogger.error('Cannot load file \"{file_name}\" due to an error.'.format(file_name=file_name))\n\t\treturn None\n"}]}
{"task_id": "62b8bbbce0d34b282c18120d", "prompt": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif os.path.isfile(file_name) is True:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\ndef get_current_directory():\n\t\"\"\"\n\tGet current file path.\n\n\t:return: Current's file path\n\t:rtype str\n\t\"\"\"\n\treturn os.path.dirname(os.path.abspath(__file__))\n\n\ndef get_all_dir():\n\t\"\"\"\n\tGet all files under current directory.\n\n\t:return: return all files under current directory.\n\t\"\"\"\n\tall_files ="}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tpath=os.path\n\treturn path.exists(file_name) or path.isfile(file_name)\n\ndef is_file_empty(file_name):\n\t\"\"\"\n\tCheck if file name exists and is not empty.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (file is not empty), false (file is empty)\n\t:rtype bool\n\t\"\"\"\n\tf=open(file_name, 'rb')\n\tresult=True\n\ttry:\n\t\tf"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\ttry:\n\t\tf = open(file_name, 'r')\n\t\tf.close()\n\t\treturn True\n\texcept IOError:\n\t\treturn False\n\ndef gen_uuid():\n\t\"\"\"\n\tReturn random string of 32 characters\n\t:return: Random String\n\t:rtype: str\n\t\"\"\"\n\tchars = '0123456789abcdefghijklmnopqrstuvwxyz'\n\tret = ''\n\tfor c in uuid.uuid4().bytes:\n\t\tret += chars[c >> 4 & 15]"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\ttry:\n\t\treturn file_name[-1]!= '/'\n\texcept TypeError:\n\t\treturn False\n\ndef get_file_name(file_uri, root_dir):\n\t\"\"\"\n\tGiven file URI, get the root file name and the relative file name.\n\tIn example :\n\tfile_uri = 'http://www.w3schools.com/img/my_image.png'\n\troot_dir = 'data1'\n\treturn('http://www.w3schools.com/img/my_image.png','my"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif os.path.exists(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef read_file(file_name):\n\t\"\"\"\n\tRead file from a file path.\n\t:param file_name: File path\n\t:type file_name: str\n\t:return: Output string\n\t:rtype str\n\t\"\"\"\n\tf = open(file_name)\n\tcontent = f.read()\n\tf.close()\n\treturn content\n\ndef write_file(file_name, content):\n\t\"\"\"\n\t"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif not file_name: return False\n\tif not os.path.exists(file_name): return False\n\treturn True\n\nif __name__ == \"__main__\":\n\tif not is_file_exist(\"tmp\"): os.mkdir(\"tmp\")"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\treturn os.path.exists(file_name)\n\n\ndef read_only_file(file_name):\n\t\"\"\"\n\tTry to read file while it's write protected.\n\tIf it's not found return None\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: File content(as str).\n\t:rtype str\n\t\"\"\"\n\tf = open(file_name, 'r')\n\tf_content = f.read()\n\tf_content = unicodedata.normalize('NFKD"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\ttry:\n\t\tif os.path.isfile(file_name):\n\t\t\treturn True\n\t\treturn False\n\texcept:\n\t\treturn False\n\n\ndef make_sure_path_exist(path):\n\t\"\"\"\n\tMake sure if path not exist, create it.\n\t:param path: Path.\n\t:type path: str\n\t:return: Return True (path exist), False (path do not exist)\n\t:rtype bool\n\t\"\"\"\n\tif not os.path.exists(path):\n\t\ttry:\n\t\t\tos.makedirs(path"}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tif not os.path.exists(file_name): return False\n\n\tfull_file_name = os.path.join(os.path.abspath('.'), file_name)\n\tif not os.path.exists(full_file_name): return False\n\n\treturn True\n\ndef getfilelist(path):\n\t\"\"\"\n\tGets list of files in directory path.\n\t:param path: Directory\n\t:type path: str\n\t:return: List of file names in absolute path\n\t:rtype: List[str]\n\t\"\"\""}, {"generated_text": "def is_file_exist(file_name):\n\"\"\"\n\tCheck if file name exist.\n\t:param file_name: File name.\n\t:type file_name: str\n\t:return: Return true (exist), false (do not exist, or invalid file name)\n\t:rtype bool\n\t\"\"\"\n\tfile_abs_path = os.path.abspath(__file__)\n\tfile_abs_path = os.path.dirname(file_abs_path)\n\tfile_abs_path = os.path.dirname(file_abs_path)\n\tfile_abs_path = os.path.dirname(file_abs_path)\n\tfile_abs_path = os.path.dirname(file_abs_path)\n\tif file_abs_path!= \"\":\n\t\tfile_abs_path += \"/\"\n\tfile_abs"}]}
{"task_id": "62b8b99de0d34b282c1811f8", "prompt": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tlog_files_cache = {}\n\n\t# Clear logging\n\tfor name, logging_function in logging_registry.items():\n\t\tlogging_function.disabled = True\n\t\tlogging_function.logging = False\n\n\t# Clear config\n\tconfig = {}\n\n\t# Reset config\n\tcls.config = config\n\n\t# Reset file logging\n\tcls.log_files_cache = log_files_cache\n\n\t# Reset logging in classes\n\tcls.logging = False\n\n\t_log_debug('Reset logging done')\n\n\nclass LoggingError"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls._logging = None\n\n\n###############################################################################\n# Logging class\n###############################################################################\n\n_logging = None\n\n_init_logging = False\n\nLOGGER_MODULE = \"mvpn_manager\"\n\nLOG_PROCESS ='mvpn_manager.process'\n\n\nclass BaseLogging:\n\n\tdef __init__(self, module):\n\t\t\"\"\"\n\t\tConstructor\n\t\t\"\"\"\n\t\tself._module = module\n\t\treturn\n\n\tdef configure(self):\n\t\t\"\"\"\n\t\tConfigure the process of"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls._log = logging.getLogger('app')\n\tcls._log.setLevel(logging.NOTSET)\n\thandler = logging.StreamHandler()\n\tformatter = logging.Formatter('[PID:{0}] {1} %(levelname)-5s : %(message)s'.format(os.getpid(), cls.__name__))\n\thandler.setFormatter(formatter)\n\tcls._log.addHandler(handler)\n\tcls._log.info('Reset to INFO level')\n\n\tfor lvl"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls.logger = log.getLogger(__name__)\n\n@classmethod\ndef init_logging(cls, name='log.dbg'):\n\t\"\"\"\n\t\tInitialize the logging class before running tests with this class.\n\t\t\n\t\tSee documentation of log.initLogging() for more information.\n\t\t\n\t\t>>> class MyClass(BaseTestCase):\n\t\t...   def testSomething(self):\n\t\t...     log.debug('saying: %s', 'hello' )\n\t\t\n\t\t:param name: the logging level\n\t\t:"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tif cls._log_enabled:\n\t\tsalt.log.setup_logger(cls.log,\n\t\t                      salt.utils.system_encoding.get_syslog_charset())\n\n\ndef get_mock(mocked_type):\n\t\"\"\"\n\tReturn a mocked object of type @given_type\n\t\"\"\"\n\treturn salt.loader.module_return({\n\t\t\"loaded_modules\": {\n\t\t\tmocked_type: \"mock object\"\n\t\t}\n\t})\n\n\ndef start_mock_loop(mock_"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tif hasattr(cls, '_log'):\n\t\tdelattr(cls, '_log')\n\tif hasattr(cls, '_logfh'):\n\t\tclose(cls._logfh)\n\t\tdelattr(cls, '_logfh')\n\tcls._log = ''\n\tsetattr(cls, '_log', '')\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls.log_enabled = False\n\n\tdef set_log_enabled(self, enable):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\t# Set log_enabled\n\t\tself.log_enabled = enable\n\n\tdef _print_logger(self, message):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\tif self.log_enabled:\n\t\t\tlogger.info(message)\n\n\tdef log(self):\n\t\t\"\"\"\n\t\t\"\"\"\n\t\t# Setup\n\t\tself._print_logger(\"================================================\")\n\t\tself._print_logger(\"Starting Logging for {}\".format(self))"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tcls.log_handlers = OrderedDict()\n\tcls.logging_level = 2\n\ndef _set_logging(cls):\n\tlevel_str = str(cls.logging_level)\n\n\tfor name, handler in cls.log_handlers.items():\n\t\tlogging.getLogger(name).setLevel(getattr(logging, level_str))\n\t\tlogging.getLogger(name).addHandler(handler)\n\n\ndef setup_logging(level=logging.INFO):\n\t\"\"\"\n"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tlevel.reset()\n\n\ndef _set_verbose(verbose):\n\t\"\"\"\n\t\tSet verbose state\n\t\"\"\"\n\tfrom. import level as l\n\tl.verbose = verbose\n\n\ndef _show(msg, end=\"\\n\"):\n\t\"\"\"\n\t\tDisplay message\n\t\"\"\"\n\tcls = class_from_name(\"pycharm\", \"log\")\n\tcls.writeln(msg)\n\n\ndef _show_info(msg):\n\t\"\"\"\n\t\tDisplay warning message\n\t\"\"\"\n\tcls = class_from_name"}, {"generated_text": "def _reset_logging(cls):\n\"\"\"\n\tReset\n\t\"\"\"\n\tprint(\"DEBUG\", \"Reset log file\")\n\tcls._log = None\n"}]}
{"task_id": "62b8b59feb7e40a82d2d1291", "prompt": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn 'Py' + self._getFallbackClass()\n\nclass CClass(Object):\n\t\"\"\"\n\t\tA special base class holding all the\n\t\tmeta information for a C API Type object\n\t\"\"\"\n\t@classmethod\n\tdef _getTargetClass(self):\n\t\tname = self.getName()\n\t\tmodule = self.getModule()\n\t\tself.module = name\n\t\t# self.cobj = cffi.caresign.new_func(name, cffi_module, flags=cffi_flags)\n\t\tpyfunc = 'c"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\tif self.name in ['Py',]:\n\t\treturn self.impl\n\t_ = self.__doc__.split('(')[0]\n\treturn 'Py.'+_[:-2].lower()+'.impl'\n\ndef _findTargetClass(self):\n\t\"\"\"\n\tFind the _impl class, either the implementation class itself, or the fallback fallback class.\n\t\"\"\"\n\tif self.name in ['Py',]:\n\t\treturn self\n\tpathname = os.path.join(os.path.dirname(self.__module__), '_"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise RuntimeError, \"Unknown class \" + self.__class__.__name__\n\n\t# Could use the class name, or __name__\n\t#if isFallback():\t\treturn \"MyFallbackClass\"\n\t#if isPy():\t\t\t\treturn \"MyPyClass\"\n\treturn _getTargetClass()\n\nif __debug__:\n\t_enableDebug( debugMode )\nelse:\n\t_disableDebug()\n\t\n## EOF"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\ttup=type_tuple(self)\n\ttry:\n\t\treturn importClass(f'_sctp.{self.name}')\n\texcept ImportError:\n\t\treturn Fallback._getTargetClass()\n\n#===============================================================================\n\nclass SCTPConnection(_SSCTPConnectionBase, _SCTPConnection); pass\nclass SCTPSimpleConnection(_SSCTPSimpleConnectionBase, _SCTPSimpleConnection); pass\n\n#===============================================================================\n\n#\n# Utilities\n#\n\ndef getAvailableSCTPCryptoTransform(_"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn self._implMap.get('Py',\n\"\"\"\n@PyType(klass=%(klass)s)\nclass %(klass)s:\n    \"\"\" % locals() )\n\n\n\ndef _getModuleFunction(self, n):\n\treturn self._implMap.get('Py',\n'''\n@module_function(self, ctypes.c_uint32)\ndef %(method)s(self, %(args)s):\n''' % locals() )\n\n\n\ndef _getCMethodImpl():\n\treturn '''\ndef %"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn \"Py\"+self._target.__name__\n\n\nclass PyPluginClassLoader(PluginClassLoader, BasePluginClassLoader):\n\tpass\n\n # vim:ts=4:sw=4:et\n"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\timport pifacecommon\n\treturn pifacecommon\n\n__all__ = [\n\t\"__init__\",\n\t\"AbstractDevice\",\n\t\"AbstractVirtualDevice\",\n\t\"AbstractInterface\",\n\t\"VirtualDeviceMixin\",\n\t\"VirtualInterfaceMixin\",\n]\n"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn 'Py'\n"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\treturn str\n######################################\n"}, {"generated_text": "def _getTargetClass(self):\n\"\"\"\n\tDefine this to return the implementation in use,\n\twithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise AttributeError(\"Don't know how to get a target class\")\n\ndef _getTarget(cls):\n\t\"\"\"\n\tReturns a _Base implementation that corresponds to\n\tthe given class, which must extend _Base. This function\n\tis automatically called by Pyro4 whenever PyroProxy\n\thas to send this class, to determine what function\n\timplementation is used.\n\n\tThis function works like this: if a proxy has already been\n\tinitialized for an instance with this class, that proxy is\n\tused, else the method _getTargetClass() is called and a\n"}]}
{"task_id": "62b8b590eb7e40a82d2d1275", "prompt": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tif not orderings:\n\t\treturn []\n\telse:\n\t\treturn [o for o in orderings if o]\n\ndef _stripEmptyStrings(s):\n\tfor i, e in enumerate(s):\n\t\tif '' == e:\n\t\t\treturn s[:i]\n\treturn s\n\n_STRIP_INVALID_CHARS    = '\\n\\t\\v'.split()\n_STRIP_WHITESPACE       = re.compile('[\\s\\n]+')\n_LITERAL_STRINGS        = re."}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\timport collections\n\tresult = collections.OrderedDict()\n\tfor ordering in orderings:\n\t\tfor obj in ordering.keys():\n\t\t\tsuffix = ordering[obj]\n\t\t\tif suffix[0] not in list(result.keys()):\n\t\t\t\tresult[suffix[0]] = [suffix]\n\t\t\telse:\n\t\t\t\tif suffix[0] in result[suffix[0]]:\n\t\t\t\t\t# This ordering is already done\n\t\t\t\t\tcontinue\n\t\t\t\t# Find the ordering we want\n\t\t\t\tkeys = [o[0] for o in result[suffix[0]]]"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tfinal = []\n\tfor ordering in orderings:\n\t\tfor suffix in ordering:\n\t\t\tif suffix not in final:\n\t\t\t\tfinal.append(suffix)\n\treturn final\n##\n\ndef getOrderedDAG(objectList, _minOrdering = None):\n\t\"\"\"\n\tGiven the complete object DAG (the set of objects\n\tordered by its predecessor and successor) and a minimum\n\tordering,\n\t\n\treturn the DAG with at most one object per ordering\n\t\"\"\"\n\tif _minOrdering is None:\n\t\t_minOrdering = []"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\treturn merge.getMergeFunction(orderings)()\n\t\ndef _extractOrderings(input, extractFrom):\n\t\"\"\"\n\tExtract elements from a list based on the first element in an order definition\n\t\n\tFor example:\n\t\n\t>>> _extractOrderings([1,2,3,4,5], ['x','z'])\n\t[1, 3, 5]\n\t\"\"\"\n\t\n\tfirst = extractFrom[0]\n\tif first.count(first) > 1:\n\t\traise IndexError(\"Invalid extract ordering:"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\t# Make the ordering lowercase\n\torderings = map(str.lower, orderings)\n\t\n\t# Compute how \"big\" the ordering is\n\tif len(orderings) >= 2:\n\t\tbiggest = len(orderings) * [None]\n\t\tfor o in orders:\n\t\t\tfor i in xrange(len(o)):\n\t\t\t\tbiggest[i] = max(biggest[i], o[i]==orderings[i])\n\telse:\n\t\tbiggest = []\n\t\n\t# Make an empty sequence\n\tl"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tif not orderings:\n\t\treturn []\n\telse:\n\t\treturn [max([id for id, n in orderings], key=idToSuffix)] + orderings[0:-1]\n\n@log_changes\ndef _convertToFingermarks(converted_fingermarks):\n\t\"\"\"\n\tConvert a list of items (sorted by _id) into a list of\n\tfingermarks (strings) where a given item (such as a table) is\n\tincluded in the resulting list as a hash: '1' | 'foo'\n\t\n"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tassert len(orderings) > 0\n\t_mergeOrderings_aux(orderings, set(), None, 1, [])\n\tresult = [ordering[0] for ordering in orderings]\n\tfor index in range(1, len(orderings[0])):\n\t\t_mergeOrderings_aux(orderings, set(), index, result)\n\tassert is_vertex_ordering(result)\n\treturn result\n\t\n\t\ndef _mergeOrderings_aux(orderings, history, prefix, result, seen):\n\torder_lengths ="}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\tneworderings = []\n\tfor ordering in orderings:\n\t\tneworderings += ordering\n\t\tif len(neworderings) >= 2 and ordering[0].startswith('(') == \\\n\t\t\tordering[-1].startswith('('):\n\t\t\tneworderings[-1] += ',' + ordering[-1]\n\t\t\tneworderings[-1] = neworderings[-1][:-1] + ordering[-1]\n\t\t\tneworderings = neworderings[:-1]\n\tneworderings = [ordering for ordering in orderings if"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\treturn reduce(_appendOrdering, orderings, list(orderings[0]))\n\ndef _iterOrderedKeys(dic):\n    \"\"\"\n\tIterate over keys of dic with order.\n\t\n\t>>> list(_iterOrderedKeys(dict(x=1, y=2, a=3, b=4)))\n\t['a', 'b', 'x', 'y']\n\t\"\"\"\n    yield from _iterkeys(dic)\n    yield from _itervalues(dic)\n    yield from _itervalues"}, {"generated_text": "def _legacy_mergeOrderings(orderings):\n\"\"\"\n\tMerge multiple orderings so that within-ordering order is preserved\n\t\n\tOrderings are constrained in such a way that if an object appears\n\tin two or more orderings, then the suffix that begins with the\n\tobject must be in both orderings.\n\t\n\tFor example:\n\t\n\t>>> _mergeOrderings([\n\t... ['x', 'y', 'z'],\n\t... ['q', 'z'],\n\t... [1, 3, 5],\n\t... ['z']\n\t... ])\n\t['x', 'y', 'q', 1, 3, 5, 'z']\n\t\"\"\"\n\t\n\torderings = [i for i in sort([o.get() for o in orderings])]\n\tremaining = ([o['suffix'] for o in orderings])\n\twhile len(remaining) > 0:\n\t\tfor ordering in orderings:\n\t\t\tnew = []\n\t\t\tprev = None\n\t\t\tfor o in remaining:\n\t\t\t\tif o == ordering['suffix']:\n\t\t\t\t\tnew.append(o)\n\t\t\t\telif prev == o:\n\t\t\t\t\tfor e in new:\n\t\t\t\t\t\tremaining.remove(e)\n\t\torderings.append(new)"}]}
{"task_id": "62b8b58deb7e40a82d2d1269", "prompt": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tifaceList = []\n\tif getattr(object, '__providedBy__', None) is not None:\n\t\tifaceList.extend(object.__providedBy__)\n\tif getattr(object, 'ifacesProvided', None) is not None:\n\t\tfor iface in object.ifacesProvided:\n\t\t\tif not isinstance(iface, interfaces.Attribute) and not isinstance(instanceOf(iface)):\n\t\t\t\tifaceList.append(iface)\n\tif getattr(object, '__of__', None) is not"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn declarator.providedBy(object)\n\ndef directlyProvides(required): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn declarator.directlyProvides(required)\n\ndef directlyRequiredBy(provided): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tDirectly required by the given provided interface\n\t\"\"\"\n\tfor required in"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tinterfaces = object.__class__.meta_interfaces\n\twhile interfaces is not None:\n\t\tfor interface in interfaces:\n\t\t\tfor providedInterface in interface.providedBy(object):\n\t\t\t\tyield providedInterface\n\t\t\t# We need to check every declared metaclass\n\t\t\tinterfaces = interfaces.interfaces\n\n# See documentation of `DirectlyProvidedBy` method\nDirectlyProvidedBy.providedBy = directlyProvidedBy\n\n# --- Directly provided interfaces ---\n\nclass ZopeInterfaceMeta(type): \n\t\"\"\"This metaclass extracts and registers the interfaces"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom.interfaces import directlyProvidedBy\n\n\treturn directlyProvidedBy(object)\n\n\ndef usedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly used by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom.interfaces import usedBy\n\n\treturn usedBy(object)\n\n\nclass IInterfaceRegistrationManager(\n    zope.component.interfaces.InterfacesRegistrationManager\n): # pylint:disable=too"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\treturn _declarations(object.__provides__())\n\nclass ModuleInterfaceProvider(InterfaceProvider):\n\t__module__ = 'zope.interface'\n\ndef providedBy(object): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.DirectlyProvidedBy`.\n\t\"\"\"\n\n\tif hasattr(object, '__providedBy__'):\n\t\treturn DirectlyProvidedBy(object.__providedBy__"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\tinterfaces = [zope.interface.implementedBy(object.implements) for object in inspect.getmro(object)]\n\tinterfaces = [interface for subinterfaces in interfaces for interface in subinterfaces]\n\treturn interfaces\n\ndef hasDirectlyProvidedInterfaces(object):\n# pylint:disable=redefined-builtin,w0122\n\t\"\"\"\n\tReturn whether the given object directly provides any interfaces\n\t\"\"\"\n\treturn (not (False in [directlyProvidedBy(object) is None for object in inspect."}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t\n\tif (implementer is None and \n\t    clsType is None and \n\t    provider is None):\n\t\treturn IDeclarationsOnly(object)\n\t\n\telif implementer is not None:\n\t\tif isinstance(implementer, interfaces.InterfaceClass):\n\t\t\timplementer = implementer._implementations\n\t\treturn interfaces.DeclarationFilter(object, implementer)\n\t\n\telif clsType is not None:\n\t\treturn interfaces.DeclarationFilter(object, interfaces.directlyProvidedBy(clsType"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\treturn _getProvides(object)\n\ndef implementedBy(object):\n\t\"\"\"\n\tProvide information on interfaces which a given object provides and also\n\tdirectly on the object itself.\n\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfor iface in _getProvides(object):\n\t\tyield iface, object\n\tyield _getProvides(object)\n\ndef _getProvides(object):\n\tif isPurelyBuiltin(object):\n\t\treturn builtinProvides\n"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tif not isinstance(object, IDeclaration):\n\t\t# We only handle objects as provided interfaces\n\t\traise TypeError('directlyProvidedBy called on a non-IDeclaration object') # @NilPerformingObject\n\treturn object.__providedBy__()\n\nclass Attribute(object):\n\t\"\"\"\n\tAttribute.__new__(class_=type)\n\t(from http://www.tampal.org/repos/zope/interface-tutorial/html-src/interface/Attribute.html)\n\t\"\"\"\n\tdef __init"}, {"generated_text": "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the interfaces directly provided by the given object\n\t\n\tThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t## @private\n\tgetProvidedBy = None\n\t\n\tgetProvidedBy = getProvidedBy # pylint:disable=redefined-builtin\n\t\n\tdef provide(machina, *interfaces): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tProvide the given interfaces recursively via the passed\n\t\tmachina object.\n\t\t\"\"\"\n\t\tob = machina\n\t\t# Loop over the interfaces the machina directly provides\n\t\t# and if the interface is not already provided, continue the recursion\n\t\tfor i in"}]}
{"task_id": "62b8b559eb7e40a82d2d11f8", "prompt": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tif not classes:\n\t\treturn classes\n  \n\telif classes[0] in classes:\n\t\tminClasses = [classes[0]]\n\t\tfor c in classes[1:]:\n\t\t\tif c not in minClasses:\n\t\t\t\tminClasses.append(c)\n\t\treturn minClasses\n\telse:\n\t\treturn classes\n"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tclassesSet = []\n\tfor c in classes:\n\t\tfor cls in classesSet:\n\t\t\tif cls[0] < c[0]:\n\t\t\t\tclassesSet.append([cls,c])\n\t\t\tif cls[0] == c[0] and cls[1] < c[1]:\n\t\t\t\tclassesSet.append([cls,c])\n\treturn classesSet\n\nif __name__ == \"__main__\":\n\t## Check args\n\tif len(sys.argv) <= 1:\n\t\tprint \"Error: Please specify a list of class"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tfrom operator import attrgetter\n\tfrom fractions import Fraction\n\treturn min(classes, key=attrgetter('depth','name'))\n\ndef checkClass(c,d):\n\treturn c.isChildOf(d)\n\ndef getMinimalABI(classes):\n\t\"\"\"\n\tReturns abi of the most minimal list of base class that the given\n\tclasses have in common, or None if no classes were found\n\t\"\"\"\n\tfrom fractions import Fraction\n\tminimal = []\n\tfor i in range(len(classes)):\n\t\tcheck"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\treturn [\n\t\t\"%s<=%s\" % (\n\t\t\tstr(c[0].__name__), \n\t\t\tstr(c[1].__name__),\n\t\t\t) \n\t\t\tfor c in classes\n\t]\n\nclass ClassMeta(type):\n\t\"\"\"\n\tA metaclass in python that, when the class is created\n\t\"\"\"\n\tdef __new__(cls, classname, bases, dict):\n\t\tdict[\"bases\"] = bases \n\t\treturn type.__new__(cls, classname, bases, dict)\n"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\treturn min(classes, key=classes.count)\n\n# ------------------------------------------------------------------------------\n\ndef minimalSubclass(oldclass, newclass):\n\t\"\"\"\n\tReturns the minimal subclass of oldclass that is equivalent with newclass\n\t\"\"\"\n\tsubclasses = getSubclasses(oldclass)\n\tfor subclass in subclasses:\n\t\tif classIsEqualOrSubclass(newclass, subclass) \\\n\t\t\t\tand isSameType(oldclass, subclass):\n\t\t\treturn subclass\n\treturn None\n\n# ------------------------------------------------------------------------------\ndef findMatching(className):\n\t\"\"\"\n\tFind matching"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\treturn  _reduceToMinimalBases(classes, lambda c: c.__bases__)\n\ndef fullBase(classes):\n\t\"\"\"\n\tCollecting all direct bases of all classes, excluding already collected bases\n\t\"\"\"\n\treturn  _gatherBasesForFullBase(classes)\n\nif __name__ == \"__main__\":\n\tfor path in ['src/a/b','src/c/__init__']:\n\t\tclasses = fullBase([__import__(path + \".Class\")])\n\t\tprint \"A new class appears: {"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tfrom copy import copy\n\tclasses = copy(map(type, classes))  # make a copy of the list\n\tminclasses = copy(classes)         # and its minimum equivalent\n\n\tfor i in range(len(classes)):\n\t\tfor j in range(i):             # find minimal equivalence\n\t\t\tif classes[i] == classes[j]:     # if there is a minimal class\n\t\t\t\tminclasses.remove(classes[j]) # it will be removed\n\t\t\t\tbreak                       # and the equivalence is removed\n\t\telse:                         # if there is no minimal class\n\t\t\tmin"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tres = classes[0]\n\tfor classe in classes:\n\t\tif classe < res:\n\t\t\tres = classe\n\treturn res\n\n\nclass IBase(object):\n\tdef __str__(self):\n\t\treturn repr(self.__dict__)\n\n\tdef __iter__(self):\n\t\treturn iter(self.__dict__)\n\n\tdef __getitem__(self, key):\n\t\treturn self.__dict__[key]\n\n\tdef __setitem__(self, key, value):\n\t\tself.__dict__[key] ="}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\timport os\t\n\tfrom itertools import groupby\n\t\n\tcls = {}\n\tfor i in sorted(classes):\n\t\tcls[i] = []\n\tfor i in sorted(classes):\n\t\tc = classes[i]\n\t\tfor j in range(len(c)):\n\t\t\tk = c[j]\n\t\t\tif k[:1] == ':' or k in cls: continue\n\t\t\tg = tuple(c[:j:1])\n\t\t\tif g not in cls:\n\t\t\t\tcls[g] = []\n\t\t\tcls"}, {"generated_text": "def minimalBases(classes):\n\"\"\"\n\tReduce a list of base classes to its ordered minimum equivalent\n\t\"\"\"\n\tresult = []\n\tclsCount = len(classes)\n\ti = 0\n\tcurMin = classes[0]\n\tfor c in classes:\n\t\tif c!= curMin:\n\t\t\tresult.append(curMin)\n\t\t\tcurMin = c\n\t\t\ti += 1\n\t\telif i!= 0 and i!= clsCount:\n\t\t\tresult.append(classes[i])\n\t\t\ti += 1\n\tif result[0]!= \"base\" and (result[len(result) - 1]!= \"base\" or i == clsCount):\n\t\tresult."}]}
{"task_id": "62b8b4b9eb7e40a82d2d1134", "prompt": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif all is False:\n\t\treturn self._attributes\n\n\tattributes = []\n\tfor name, info in self._meths.items():\n\t\tattributes.append(PY_ATTRIBUTE % (info.name, info.desc))\n\tfor name in dir(self):\n\t\tif isinstance(getattr(self, name), types.FunctionType):\n\t\t\tinfo = self._meths.get(name, None)\n\t\t\tif info is not None:\n\t\t\t\tattributes.append(PY_ATTRIBUTE_TYPE % (info."}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif all:\n\t\treturn dict( (a,self.__getitem__(a)) for a in _dict_names(self.__info__, ['required', 'optional']))\n\telse:\n\t\treturn dict(_info_item_names(self.__info__, ['required', 'optional']), **{a: self.__getitem__(a)})\n\ndef names(self, all=False): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn attribute names defined by interface, plus a list of\n\tattributes of parent"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tattrnames = []\n\tfor f in self.instance._interfaces:\n\t\tattrnames.extend(f.namesAndDescriptions())\n\tif not all:\n\t\treturn attrnames\n\treturn list(set(attrnames))\n\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tinfo = {'desc_extended': (\n\t\t\"Return attribute names and descriptions extended for this class.\"),\n\t\t'help': \"Returns 'names_and_descriptions' as a dictionary.\",\n\t\t}\n\tnames_and_descriptions = self.getNamesAndDescriptions(all=all)\n\tif not all and len(names_and_descriptions) == 1:\n      \tdesc_name = names_and_descriptions[0][0]\n\telse:\n\t\tdesc_name = self.__class__.__name__\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tret = self.interface()\n\tif all == True:\n\t\tret = ret['attrs']\n\treturn ret\n\nif __name__ == '__main__':\n\tfrom vistrails.core.modules.module_registry import get_module_registry\n\timport sys\n\tfrom optparse import OptionParser\n\tparser = OptionParser(usage=(\"\"\"%prog [options] <module> [...]\"\"\"))\n\tparser.add_option(\n\t\t\"-l\", \"--list\",\n\t\taction=\"store_true\", dest=\"list\", default=False"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tattributes = []\n\tattributes.append('-')\n\tname_re = re.compile(r\"^[a-z0-9_]+$\")\n\tfor value in self.attributes:\n\t\tif type(value) in (unicode,str):\n\t\t\tif not name_re.match(value):\n\t\t\t\twarn(\"non-standard attribute %s defined by class.\" % value)\n\t\t\telse:\n\t\t\t\tattributes.append(value)\n\t\telif type(value) is tuple:\n\t\t\tassert len(value) == 2\n\t\t\tif type"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\t# pylint: disable=no-self-use\n\treturn (\"\\n\".join(i.attributes.dtype.names) + \" (description: \" +\n\t\t\t\"\\n\".join(i.attributes.dtype.desc.values()) + ')',\n\t\t\tself.attributes.dtype.desc.values())\n\n\t# pylint: disable=missing-return-type, missing-return-doc, undefined-variable, unused-argument\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tif isinstance(all, str):\n\t\tall = any\n\tif not isinstance(all, bool): # 'True' will be false\n\t\traise TypeError(\"all argument expected as bool, got %r\" % (type(all),))\n\n\tattribs = [getattr(i,i.attrib) for i in self]\n\tif not all:\n\t\tattribs = [i for i in attribs if i not in self]\n\n\tnamesAndDescs = zip(attribs,attribs[1:]) # pylint:disable"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tdescriptions = dict()\n\tnames = dict()\n\tfor interface in self.__interfaces__.itervalues():\n\t\tnames[interface.__name__]\t= interface.__name__\n\t\tfor name, func in interface.__info__.iteritems():\n\t\t\tnamespace = name.split(\".\")[0]\n\t\t\tif names[namespace]:\n\t\t\t\tnames[namespace].append(name)\n\t\t\telse:\n\t\t\t\tnames[namespace] = [name]\n\t\t\tif not all and self.__options__[namespace]:\n\t\t\t\tcontinue\n"}, {"generated_text": "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn attribute names and descriptions defined by interface.\n\t\"\"\"\n\tdef get_all():\n\t\tif all:\n\t\t\treturn [i.fullname for i in get_implementers(self)\n\t\t\t\t\tif not i.fully_qualified_name.endswith(':<inherited>')]\n\t\telse:\n\t\t\treturn [i.fullname for i in get_implementors(self)]\n\treturn (get_all() if all else self.implementedby_).keys() + ['fullname']\n\n# Get implementation from module\nclass InstanceImplementation(object):\n\t\"\"\"\n\tInstances of this class"}]}
{"task_id": "62b8b416eb7e40a82d2d1129", "prompt": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\t\n\treturn (\n\t\tattribute_names +\n\t\t(all and [\n\t\t\t\"__abstract__\",\n\t\t\t\"__author__\",\n\t\t\t\"__attr_type__\",\n\t\t\t\"__attr_values__\",\n\t\t\t\"__basemodel__\",\n\t\t\t\"__bases__\",\n\t\t\t\"__category__\",\n\t\t\t\"__class_abstract__\",\n\t\t\t\"__class_init__\",\n\t\t\t\"__class_metaclass__\",\n\t\t\t\"__class_mro__\",\n\t\t\t\"__class__\",\n\t\t\t\"__del__\","}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn NamedAttribute.names(self)\n\n# ---\n\n\nclass NamedMethodDescriptor(NamedAttribute): # pylint:disable=too-many-public-methods\n\t\"\"\"\n\tDescriptor for a method.\n\t\"\"\"\n\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tInitialise the descriptor.\n\t\t\"\"\"\n\t\tif'method' in kwargs:\n\t\t\tself.method = kwargs['method']\n\t\t\tself.name = self.method.__name__\n\t\telse:\n\t\t\tNamed"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tall = self.boolify(all)\n\treturn [name for (name, ptype) in sorted(self.mapped().items())\n\t\tif ptype(all)]\n\ndef type(self, name): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tRetrieve the type of the given attribute.\n\t\"\"\"\n\trv = self.mapped()[name]\n\tif name in ('invalidated', 'value', 'valueChangedEvent', 'valueWillChange'):\n\t\treturn ptypes.mapped(rv)\n\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn self if not all else self.member_data\n\n  def name(self, all=False): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn the name of the interface as defined by the meta-data.\n\t\"\"\"\n\treturn self.name if not all else self.id\n\n  def __iter__(self): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tIterate over all attributes by calling ``for e in meta:...``\n\ton the underlying object returned by ``Meta.resolve()``.\n\t"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn self._base.get_member_names(all, self)\n\ndef method(self, method, *args, **kargs): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tCreate a new method. This method is not a normal method defined by the\n\tinterface, but it can be called by an inherited method call to create\n\ta new subcall.\n\n\tThis method accepts any arguments an can, and will throw a TypeError if\n\tit encounter an unexpected one::\n\n\t\t@implementer\n\t\tclass Example:\n\t\t\tdef say(self"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif all: # pragma: no cover - can't be used if not set.\n\t\treturn [attr.name for attr in cls]\n\n\tnames = {}\n\tfor prop in metamodel.get_all_properties(cls):\n\t\tfor attr_name in prop.attr.interface.names(all=True):\n\t\t\ttry: # We do it twice; on the first try the default value is resolved, on the second we check if it is an attribute at all.\n\t\t\t\tif attr_name: names[attr_name] = prop.attr."}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif not all:\n\t\treturn [ name for name in self.__slots__ if name in self.__dict__ ]\n\n\treturn list(self.__slots__)\n\nclass Decl(object):\n\t\"\"\"\n\tdeclaration object - type of object\n\t\tname\n\t\ttype\n\t\tparent\n\t\"\"\"\n\tdef __init__(self, name, tp, parent):\n\t\tself.name   = name\n\t\tself.type   = tp\n\nclass Type(Decl):\n\t\"\"\"\n\ttype object - name and types list\n\t\"\"\"\n\tdef"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tr = self._attributes\n\tif all:\n\t\treturn r.keys()\n\telse:\n\t\treturn r.copy()\n\ndef is_attribute(self, name): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn True if the interface defines an attribute.\n\t\"\"\"\n\tr = self._attributes\n\treturn name in r\n\n\ndef attributes(self, all=False): # pylint:disable=redefined-builtin\n\t\"\"\"\n\tReturn a list of the attributes as a tuple.\n\t\"\"\"\n\tatt"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\treturn (self._attrnames(), all and self.inherited)\n\t\n\t\ndef get(name, all=False, local=True): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturns a property named `name` or raises ValueError if not found.\n\tBoth local an inherited properties will be returned.\n\t\"\"\"\n\tattrs = self._attrnames(all)\n\tif name in attrs:\n\t\tattr = getattr(self, name)\n\t\tif local:\n\t\t\treturn attr\n\t\telse:\n\t\t\treturn attr"}, {"generated_text": "def names(self, all=False): # pylint:disable=redefined-builtin\n\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif all:\n\t\treturn super().names()\n\telse:\n\t\t_names = []\n\t\tfor attr in self.attrs():\n\t\t\t_names.append(attr.name)\n\t\treturn _names\n\n\tdef isinstance(self, *values): # pylint:disable=redefined-builtin\n\t\t\"\"\"\n\t\tcheck if the interface is in a specified set.\n\n\t\t:param object values: the attribute values to check for\n\n\t\t.. seealso:: :meth:`ofsp.interfaces.IPv6Interface.isinstance`\n\t\t\"\"\"\n\t\t"}]}
{"task_id": "62b8b3d6eb7e40a82d2d111c", "prompt": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif not isinstance(sequence, list):\n\t\tsequence = [sequence]\n\tresult = []\n\tfor i in sequence: \n\t\tr = []\n\t\tif isinstance(i, (string_type, builtin_types['TypeArguments'])):\n\t\t\tpass\n\t\telif isinstance(i, Declaration):\n\t\t\tr.append(i)\n\t\telse:\n\t\t\t# a single argument might be either a tuple of declarations\n\t\t\t# or a single interface class\n\t\t\tif (isinstance(i, tuple) and len(i)==2 and isinstance("}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tnormalized = []\n\tfor a in sequence:\n\t\tif(isinstance(a, Declaration) or isSequence(a)):\n\t\t\tnormalized.append(a)\n\t\telif(isinstance(a, Interface)):\n\t\t\tnormalized.append(expandInterface(a))\n\t\telif(isinstance(a, Spec)):\n\t\t\tnormalized.append(expandSpec(a))\n\t\telif(isinstance(a, tuple)):\n\t\t\tnormalized.append(a)\n\t\telif(type(a) in (list, tuple"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\tif output is None:\n\t\toutput = []\n\t\n\tif isinstance(sequence, Declarion):\n\t\tsequence = [ sequence ]\n\t\n\tif isinstance(sequence, Iterator):\n\t\tsequence = list(sequence)\n\t\t\n\t\tif not sequence: # Empty Iterator, use empty tuple\n\t\t\tsequence = ()\n\t\n\twhile sequence:\n\t\tfor i in range(0, len(sequence)):\n\t\t\titem = sequence[i]\n\t\t\tif isinstance(item, Iterator):\n\t\t\t\tsequence[i] = tuple(sequence[i])"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\tassert type(output) == type(None) or isinstance(output, list)\n\t\t\n\tif isinstance(sequence, IDLInterface):\n\t\t# [A] expand interfaces\n\t\tdecl = sequence\n\telse:\n\t\t# [B] expand declarations\n\t\tif isinstance(sequence, IDLDeclaration) and not isinstance(sequence, IDLType):\n\t\t\t# XXX interface\n\t\t\tdecl = sequence\n\t\telse:\n\t\t\t# interfaces cannot be resolved. use a list and sort\n\t\t\tdecl = list()\n\t\t\tfor a in sequence:\n\t\t\t\tif isinstance"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\toutput = []\n\t# Normalization of arguments sequence\n\tfor arg in sequence:\n\t\tif arg is None:\n\t\t\toutput.append(arg)\n\t\telif hasattr(arg, 'getInterfaces'):\n\t\t\toutput.extend(arg.getInterfaces())\n\t\telif not isinstance(arg, Tuple):\n\t\t\toutput.append(arg)\n\t\telse:\n\t\t\tfor element in arg:\n\t\t\t\tif isinstance(element, InterfaceSpec):\n\t\t\t\t\toutput.append(element)\n\t\t\t\telif isinstance(element, Tuple) and (element."}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tnewsequence = sequence[:]\n\t# replace with types\n\t\n\twhile sequence:\n\t\titem = sequence[0]\n\t\tif item is None or isinstance(item, self.Type):\n\t\t\tnewsequence.popleft()\n\t\t\tnewsequence.popleft()\n\t\telif isinstance(item, tuple):\n\t\t\tn = 0\n\t\t\twhile type(item) is tuple:\n\t\t\t\titem = item[0]\n\t\t\t\tn += 1\n\t\t\tnewsequence.popleft()\n\t\t\tnewsequence.popleft()\n\t\t\twhile n > 0:\n\t\t\t\tnew"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\tif isinstance(sequence, (list, tuple)):\n\t\treturn sequence\n\t\n\tassert isinstance(output, (list, tuple, dict))\n\t\n\tif isinstance(sequence, InterfaceDeclaration):\n\t\tsequence = sequence.definition\n\t\n\tassert isinstance(sequence, (dict, InterfaceDeclaration))\n\t\n\toutput = []\n\t\n\tdef append(key, value):\n\t\tif key == 'implements':\n\t\t\tkey = 'implements specification'\n\t\telif key == 'type':\n\t\t\tkey = 'interface'\n"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\t\n\t# Don't mutate the original list - use a temporary copy instead\n\ttmp = list(sequence)\n\t\n\t# Copy all declaration argument elements to a new list\n\tfor elem in sequence:\n\t\t\n\t\t# Check type is declaration or tuple\n\t\tif isinstance(elem, declarator):\n\t\t\t\n\t\t\t# Copy a single (single-component) interface declaration\n\t\t\tif elem.declaration.single:\n\t\t\t\t\n\t\t\t\ttmp.append(decl(elem.declaration))\n\t\t\t\n\t\t\t# Copy a tuple of interfaces\n\t\t\telse:\n\t\t\t\t"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\targ_list, i = [], 0\n\targ_indexes = {}\n\tif isinstance(sequence, (tuple, list)):\n\t\tfor idx, item in enumerate(sequence):\n\t\t\tif isinstance(item, tuple):\n\t\t\t\traise SyntaxError('tuples are not allowed as arguments')\n\t\t\t# If item is a Declaration, append it instead\n\t\t\tif isinstance(item, Declaration):\n\t\t\t\tif not bool(item.implements):\n\t\t\t\t\titem_list = [item]\n\t\t\t\telse:\n\t\t\t\t\titem_list = item.implements"}, {"generated_text": "def _normalizeargs(sequence, output=None):\n\"\"\"\n\tNormalize declaration arguments\n\t\n\tNormalization arguments might contain Declarions, tuples, or single\n\tinterfaces.\n\t\n\tAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tif output==None:\n\t\t\toutput = []\n\tfor a in sequence:\n\t\tif isinstance(a,interface.Declaration):\n\t\t\t# We cannot add declarations to the output list, because the\n\t\t\t# order of declarations matters\n\t\t\toutput.extend(Declaration._normalizeargs(str(a)))\n\t\telif isinstance(a,tuple):\n\t\t\toutput.extend(Declaration._normalizeargs(a.asList()))\n\t\telif not isinstance(a,interface.Interface):\n\t\t\t# We cannot add interaces to the output"}]}
{"task_id": "62b8b3d5eb7e40a82d2d1110", "prompt": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\t# This was the first version, there should be a better way...\n\tfrom coptimizations import *\n\t\n\treturn (hasattr(coptimizations, \"c\") and coptimizations.c)\n\n_python_optimizations_available = False\n\"\"\"\n\tFor internal use, indicate whether the C optimizations are available.\n\t\n\tWe can have both C and Python optimizations running at the same time.\n\t\n\tThis does not say whether the optimization should be enabled or not.\n\t\"\"\"\n\nif \"OPTIONS_PREFIX\" in globals():"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\tc = ctypes.CDLL(None)\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\tif hasattr(c, \"LIBR_C_H\"):\n\t\t\treturn True\n\treturn False\n\nif __name__ == \"__main__\": \n\tprint __doc__\n\ttry: \n\t\timport numpy \n\t\timport sys\n\texcept:\n\t\tif \"NUMPY_NO_NUMERIC\" in envs or \"NUMPY_NO_NUMERIC\" in os.environ:\n\t\t\t"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\ttry:\n\t\t__import__(\"coptimizations\")\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n\ndef _get_c_optimizations_path():\n\t\"\"\"\n\tReturn the (default) optimization path,\n\tfrom which optimization module can be loaded.\n\t\n\tNote that the path must be absolute!\n\t\"\"\"\n\t# This is an internal function, don't call it directly.\n\t# Instead, use the OptimizationModule variable named _optimization_module.\n\treturn \"\"\n\n_optimization_module=\"\"\"\n#define C_"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\tif _check_pybind(): return _pybind_optimizations_t()\n\n\ttry:\n\t\treturn OptimizeModule(_pybind_optimizations_t())\n\texcept ImportError:\n\t\tpass\n\n\treturn False\n\n\ndef _c_optimizations_enabled():\n\t\"\"\"\n\tReturn True if the C optimization module is available and\n\toptimization is enabled using pybind11\n\t\"\"\"\n\treturn _check_pybind() or _get_optimizations_available()\n\nif _c_optimizations_enabled():\n\n\tdef _c_optimization_default"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\treturn _get_optimization_module_name()\n\ndef _run(text, **kwargs):\n\t\"Run the C preamble, call the generated function, and run the exit code.\"\n\t\n\tfrom. import preamble\n\t\n\tpreamble.run(argv_text=text, **kwargs)\n\t\n\tkwargs.update(**kwargs)\n\t\n\t# We do the imports outside the call to run, because the\n\t# function may be imported elsewhere, like the Python\n\t# package, or a generated package"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\ttry:\n\t\timport compiler.pycode\n\t\t\n\t\timport optimizations as CO\n\t\tCO.optimizations()\n\t\treturn True\n\t\n\texcept ImportError:\n\t\treturn False\n\t\ndef _register_compiler_hooks():\n\t\"\"\"\n\tRegisters compiler hooks for optimization\n\t\"\"\"\n\t\n\tfrom compiler import hook\n\n\thook.emit_preamble()\n\n_register_compiler_hooks()\t\t\n"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\timport sys\n\timport imp\n\tc_name = 'pygame.mixer._c'\n\tif sys.platform.startswith('win'):\n\t\tc_name += '32'\n\t\t\n\tc_path = c_name + '.pyc'\n\ttry:\n\t\tc_path = imp.find_module(c_name)\n\texcept ImportError:\n\t\tpass\n\ttry:\n\t\tc = imp.load_module(c_name, *imp.find_module(c_path))\n\texcept ImportError:\n\t\tpass\n"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\n\timport Cython\n\t\n\ttry:\n\t\tif 'cython_include_directories' in Cython.__dict__:\n\t\t\treturn True\n\texcept AttributeError:\n\t\tpass\n\t\n\treturn False\n\ndef codegen_c(func, ctx_name='<local>'):\n\t\"\"\"\n\tGenerate C code from the given Function.\n\t\"\"\"\n\t\n\tfrom Cython import Code\n\tfrom Cython.Compiler.Main import compile\n\tfrom Cython.Compiler.Errors import CompileError\n\n\tif"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\t\"\"\"\n\ttry:\n\t\tfrom scipy.optimize import (\n\t\t\tfmin_bfgs,\n\t\t\tfmin_cg,\n\t\t\tfmin_ncg,\n\t\t\tNelderMead,\n\t\t\tBrent,\n\t\t\tminimize,\n\t\t\tfmin_L_BFGS_B_scipy,\n\t\t\tfmin_tnc_scipy,\n\t\t\tfmin_l_bfgs_b,\n\t\t)\n\t\tfrom scipy.optimize.approx_fprime import approx_fprime\n\t\tfrom scipy.optimize"}, {"generated_text": "def _c_optimizations_available():\n\"\"\"\n\tReturn the C optimization module, if available, otherwise\n\ta false value.\n\t\n\tIf the optimizations are required but not available, this\n\traises the ImportError.\n\t\n\tThis does not say whether they should be used or not.\n\t\"\"\"\n\tglobal _OPTIMIZERS\n\t\n\tif not _OPTIMIZERS:\n\t\t_OPTIMIZERS = optimize.get_optimization()\n\t\n\treturn _OPTIMIZERS\n\nif '_c_imports_available' in globals():\n\tdef _c_imports_available():\n\t\t\"\"\"\n\t\tReturn the C code generator, if available, otherwise\n\t\ta false value.\n\t\t\n\t\tIf the imports are required but not available, this\n\t\traises the ImportError.\n\t\t\n\t\tThis does not say whether they should"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110f", "prompt": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tshould = bool(_pyimp)\n\tif should:\n\t\tshould = should and (Cython.Constants.COMPILING_FROM_PYPY or not _use_c_impl)\n\treturn should\n\n\ndef _check_c_import():\n\tglobal _pyimp\n\t_pyimp = True\n\ttry:\n\t\timport __builtin__ as builtins\n\texcept ImportError:\n\t\timport builtins\nelse:\n\t_check_c_import()\n\n\n\n#------------------------------------------------------------------------\n\ndef optimize_loop(source_lines, filename,"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\treturn _use_pyimpl() and not _is_pypy()\n\ndef _c_to_py_func(name, args, arg_types, ret_type, arg_names, kw_names, default):\n\t\"\"\"\n\tReturns a function that converts a C object into a Python object.\n\t\n\t*name*, a ``str``, is the name of the C function that returns the\n\tPython object.\n\t\n\t*args*, a ``tuple`` of ``str``, is the C return type.\n\t\n\t*arg_types*,"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t# if we're on PyPy we don't want to use the C optimizations\n\tif env.has_key('PYPY') or env.has_key('NOPYROOT'):\n\t\treturn False\n\telse:\n\t\treturn _use_c_impl()\n\nclass c_impl(base_impl):\n\t\"\"\"\n\tImplementation of the C module interface with the builtin compiler.\n\n\tAll the exposed methods are documented below in a separate section.\n\t\"\"\"\n\ndef _try_add_c_impl():\n\t\"\"\"\n\tAttempt to import the"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\ttry:\n\t\t# PyPy supports these C optimizations, see\n\t\t# https://pypy.org/pypy/internal/api_duktape.html#duk_duk_p_optimize_function_call_for_builtins\n\t\tif PY_PYPY and os.environ.get('PYPY_CALL_C_IMPL_FUNCTION_CALLS') == '1':\n\t\t\treturn True\n\t\t\n\t\t# If running under pytest, allow\n\t\tif os.environ.get('PYTEST"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t# return True if we're on PyPy, or we're told explicitly not to use C\n\t# implementations.\n\treturn sys.pypy_version_info > (0, 0) or not _use_c_impl()\n\n_use_c_impl = None\n\"\"\"\n\tDefines the implementation for the various optimizations that we should\n\tattempt.\n\t\n\tThe value is a tuple of ``(name, implementation)``, where ``name`` is a\n\tstring describing the implementation's name and ``implementation`` is a\n\t``callable`` that accepts ``("}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif not PYPY:\n\t\treturn _use_c_impl()\n\telse:\n\t\timport __pypy__\n\t\treturn getattr(__pypy__, \"version_info\", (0, 0)) < (5, 5, 0)\n\ndef _c_function_wrapper(func):\n\tfrom.function import _c_function\n\treturn _c_function(func)\n\n@exportable\ndef make_wrapper(f):\n\t\"\"\"\n\tGiven a function *f*, a name which should be used to create an identical\n\tfunction at"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\treturn _should_attempt_c_optimizations_pypy(PYTHON_VERSION)\n\t\ndef _should_attempt_python_optimizations(incompatible_python_version=None):\n\t\"\"\"\n\tReturn a true value if we should attempt to use the pure Python\n\toptimizations.\n\t\n\tThis takes into account whether we're on PyPy, the value of the\n\t``PYTHON_VERSION`` environment variable, and the value of the\n\t``INCOMPATIBLE_PYTHON_VERSION`` variable, as defined"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\tif not PYPY:\n\t\treturn False\n\tif get_bool_env_var('PURE_PYTHON') or not get_bool_env_var('PYPY_XLA_COMPILER', False):\n\t\treturn False\n\t# On pypy, a C compier can be used only where the PyPy compiler is\n\t# available.\n\treturn True\n\n\ndef _use_c_impl():\n\t# If PYPY, PyPy C compilers are better than the regular ones on PY3.\n\tif PYPY"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t# Make sure the result is never actually ``False`` (for backwards\n\t# compatibility)\n\tif not use_c_optimizations:\n\t\treturn False\n\n\t# Always use the C optimizations, as we're on PyPy or not using JIT.\n\treturn (use_c_impl(env))\n\n\n\nif __name__ == \"__main__\":\n\timport doctest\n\tdoctest.testmod()\n\n"}, {"generated_text": "def _should_attempt_c_optimizations():\n\"\"\"\n\tReturn a true value if we should attempt to use the C optimizations.\n\t\n\tThis takes into account whether we're on PyPy and the value of the\n\t``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n\t\"\"\"\n\t\n\t# This is a terrible hack to account for the differences between CPython and PyPy.\n\t\n\tif os.getenv('PYPY', '0') == '1':\n\t\n\t\treturn False\n\t\n\t# Python 2.5 can return '1' for PYPY.\n\t\n\t# This hack is copied from PyPy's dist.py to get around the fact that\n\t# PYPY (as far as we can tell) returns a '1' when running under \"make cc\"\n\t# in a directory with CPython as its backend, and"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110e", "prompt": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn [x for x in _c_optimizations if x[0] == 1]\n\ndef _c_warnings_required():\n\t\"\"\"\n\tThe opposite of `_c_warnings_ignored`.\n\t\"\"\"\n\treturn [x for x in _c_warnings if x[0] < 0]\n\nC_OPTIONS = [\n\t('', 'c_optimizations_required', _c_required_warnings),\n\t('', 'c_optimizations_ignored', _c_optimizations_ignored),\n\t#('',"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn not (\n\t\tconfig.options.CXX and config.options.LINKARCH and not config.options.force_C++)\n\n\ndef _c_libs_ignored():\n\t\"\"\"\n\tThe opposite of `_c_libs_required`.\n\t\"\"\"\n\treturn not (\n\t\tconfig.options.CXX and config.options.LINKARCH and not config.options.force_cxx and not\n\t\tconfig.options.LINKARCH and not config.options.force_LINKARCH and not config.options.force_C)\n\n\ndef _"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn not _c_optimizations_required()\n\n\ndef _build_ignored():\n\t\"\"\"\n\tSee :func:`build_ignored`.\n\t\"\"\"\n\treturn _build_optimizations_ignored()\n\n\ndef _c_build_ignored():\n\t\"\"\"\n\tSee :func:`c_build_ignored`.\n\t\"\"\"\n\treturn _build_optimizations_ignored()\n\n\ndef _c_build_required():\n\t\"\"\"\n\tSee :func:`c_build_required`.\n\t\"\"\"\n\treturn not _build_"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tconfig['c_optimizations'] = boolNot(config['c_optimizations'])\nelse:\n\tfor f in ('py_optimizations_enabled', 'c_optimizations_enabled'):\n\t\t\tconfig[f] = bool(config[f])\n\ntry:\n\timport _cffi_src\nexcept ImportError:\n\t_cffi_src = None\nelse:\n\timport _cffi_src  # type: ignore\n\nffi = FFI()\n\nffi.set_source(\"_cffi_src\", \"\"\"\n"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn\n\n# =========================================================================\n# __C_OPTIMIZATIONS\n# =========================================================================\n\n# ____________________________________________________________\ndef _c_optimizations():\n\t\"\"\"\n\tDeciding is done through the variable `__C_OPTIMIZATIONS`, its\n\tconfiguration file `_c_optimizations.cfg`, and the global variables\n\t`C_OPTIONS`.\n\t\"\"\"\n\tglobal __C_OPTIMIZATIONS\n\tglobal _c_optimizations_required\n\tglobal _c_optimizations_ignored\n\n\tif not os.path."}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn not any(e.flags & Optimizations.C)\n"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\treturn not _c_optimizations_required() is None\n\n\n# Some common options for c compilers.\n# The idea behind this is that the compiler and language combination chosen\n# here makes sense for the language we're optimizing (Java, for example).\n_compiler_options = {\n\t'compiler_cc': {\n\t\t'opt_level': '-O0',\n\t\t'max_heapsize': '0',\n\t},\n\t'compiler_cgcc': {\n\t\t'opt_level': '-O0',\n\t\t'max_heaps"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tdef __init__(self, l):\n\t\tsuper().__init__(l)\n\t\tself.predictable = set()\n\tdef requires_optimization(self, t):\n\t\t\"\"\"Whether |t| requires optimization for this backend.\"\"\"\n\t\tif not isinstance(t, CType):\n\t\t\treturn False\n\t\tif not self.predictable:\n\t\t\tself.predictable.add(self)\n\t\t\treturn False\n\t\treturn t in self.predictable\n\tdef _optimize_conversion(self, _, src_type, dst_type"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tfrom numba.cuda.cudadrv import lower_signature\n\n\tif not cudadrv_required:\n\t\t@lower_signature(callee_registerer=callee_registerer)\n\t\tdef _c_optimizations(*args):\n\t\t\tassert_compiled()\n\n\tif not cuda_cudadrv:\n\t\treturn\n\n\t@lower_signature(callee_registerer=callee_registerer)\n\tdef _c_optimizations_enabled(*args):\n\t\tfrom cudadrv"}, {"generated_text": "def _c_optimizations_ignored():\n\"\"\"\n\tThe opposite of `_c_optimizations_required`.\n\t\"\"\"\n\tglobal _optimizations_required\n\tassert _optimizations_required == {'all'}, \"Already set\"\n\tassert _optimizations_required!= {'none'}, \"Can't revert to 'none'\"\n\t_optimizations_required = {'none'}\n\n\n@c_function('SDL_COMPILED')\ndef _c_optimizations_required(*args):\n\t\"\"\"\n\tThis has to return one of the following values:\n\t\t-> 'none':\n\t\t\tno code optimizations need to be applied\n\t\t-> 'all':\n\t\t\tall code optimizations are"}]}
{"task_id": "62b8b3d4eb7e40a82d2d110d", "prompt": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif use_pyc:\n\t\t# Cython is required for pure Python mode.\n\t\treturn True\n\treturn False\n\n\ndef _use_c(c_implementation, c_implementation_path):\n\t\"\"\"\n\tInitialize the C-related runtime functions.\n\t\n\tThe C-related runtime functions may be called from the Python interpreter if \n\tthey are compiled into Cython (and _c_optinalizations_required() returns\n\ttrue). They may also be provided by Cython if not compiled into pure Python\n\t(and _use_pure_mode()"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn bool(_use_c_impl(force_c_impl=False))\n\n\ndef _use_c_impl(force_c_impl=False, c_libs=[], c_exts=[]):\n\t\"\"\"\n\tReturns a bool value if Python is using the C implementation (the fast\n\tCython implementation). This is not the C libs or C extensions that were\n\tused with Python.\n\n\tThis may be overridden for testing for a specific version of Cython\n\tusing ``_c_optimizations_required``.\n\t\"\"\"\n\t# Check if"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\t\n\treturn _use_c_impl()\n\t\ndef _c_optimizations_not_allowed():\n\t\"\"\"\n\tReturn a false value if the C optimizations are not allowed.\n\t\n\tThis uses the ``NO_C_PYTHON`` variable as documented in `_allow_c_impl`.\n\t\"\"\"\n\t\n\treturn _allow_c_impl()\n\n##################################################################\n\nif not _enable_cpp:\n\t_c_definitions = False\nelse:\n\t_c_definitions = _c_optimizations_required()\n"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\toptimizing = False\n\tfor opt in _C_OPTIMIZATIONS:\n\t\tif _use_c_impl(opt): optimizing = True\n\treturn optimizing\n\n\n_is_c_extension_required = _c_extensions_required\n\n\ndef _cpp_extensions_required():\n\t\"\"\"\n\tReturn true if no extension is required to compile the C++ code.\n\t\n\tThis uses the ``CPP_UNSUPPORTED`` variable as documented in `\n\t_c_extensions_required`.\n\t\"\"\"\n\tfor ext in CPP_"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\timport cffi\n\tfrom pypyjs.configuration import USE_C_IMPL\n\treturn bool(getattr(cffi, \"__cffi_user_c_optimizations_required__\", False) and USE_C_IMPL)\n\ndef _computeClassName(cname, classinfo):\n\t\"\"\"\n\t\"\"\"\n\tif cname is None:\n\t\tcname = \"jsclass_%s\" % (classinfo.jsclass_name)\n\treturn cname\n\n\ndef buildBinary(pypy, code,"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif PURE_PYTHON_FALSE not in os.environ:\n\t\tif _c_lib_included() is not None:\n\t\t\treturn _c_optimizations_required_base()\n\t\treturn False\n\treturn True\n\ndef _c_lib_included():\n\t\"\"\"\n\tGiven that C is enabled we know we can do the right imports as we pass\n\tthese to the compiler.\n\t\"\"\"\n\tfrom pure_python_staging import import_namespace, module_names\n\tfrom _pyco_c_lib import *\n\t"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\tif (sys.version_info[0] == 3 and sys.version_info[1] >= 3) and settings.PURE_PYTHON: return True\n\tif sys.platform == \"darwin\": return True\n\treturn settings.MACOS_X_10_5 and sys.version_info[:3] >= (2, 7, 9)\n\n########################\n# Helper Functions\n########################\n\n##########################################\n# Helper functions and constants\n##########################################\n\nif False and '__cplusplus__' in dir(__builtins__"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn _use_c_impl is True\n\t\n\t\n\t    \n\t"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn _c_impl() or _use_c_impl()\n\n\n#=============================================================================\n# __all__\n#=============================================================================\n__all__ = [\"_PYTHON\", \"-PYTHON\", \"pyoptix\", \"enable_optimizations\"]\n"}, {"generated_text": "def _c_optimizations_required():\n\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\n\tThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n\t\"\"\"\n\treturn not hasattr(sys, 'frozen') and __use_c_impl == 'c'\n\n\ndef _add_py_runnable_dependencies(dep):\n\t\"\"\"Add a list of names of dependencies to the `INSTALLED_APPS\n\tlist`.\n\t\"\"\"\n\tif dep:\n\t\tif isinstance(dep, str):\n\t\t\tdep = [dep]\n\t\tfor dname in dep:\n\t\t\tif dname not in _add_py_runnable_dependencies.app_names:\n\t\t\t\t_add_py_runnable"}]}
{"task_id": "62b87b989a0c4fa8b80b35ee", "prompt": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tfor key, val in self.ctx.iteritems():\n\t\tself.add(key, val)\n\ndef merge_context(a, b):\n\t\"\"\"\n\tCombines two contexts.\n\t\n\tThe context is only appended after the old one finishes.\n\t\"\"\"\n\tc = Context()\n\tfor key, val in a.iteritems():\n\t\tc[key] = val\n\tfor key, val in b.iteritems():\n\t\tc[key] = val\n\treturn c\n\ndef split_context(ctx):\n\t\"\"\"\n\tSplits"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\treturn self.context.update(initial_value=self.default_value,\n\t                           make_bins=self.make_bins)\n\"\"\"\n\tReset the histogram to the *initial_value* of the context.\n\t\n\tInitialize *self.bin_count* with the *bin_count*\n\tparameter.\n\t\n\tAlso *del self.bin_count* and recreate *self.bin* in *self.bin_count*.\n\t\n\tThen, *initial_value* of the current context are updated to the new initial_value"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\tself._hist_dict.clear()\n\t\n\tfor name in self:\n\t\tbins = self.get(name, make_bins())\n\t\tfor bin in bins:\n\t\t\tself._hist_dict[bin] = []\n\n\nclass Table(dict):\n\t\n\t\"\"\"\n\tA Dictionary which remembers the last value.\n\n\tEvery time you make an update the value will be remembered.\n\tWhen you make a Get or GetAll, you will get the last value by value, instead of using the dictionary's\n\t*get()*  or *getAll()"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.reset_hist(initial_value=self.params.initial_value,\n\t\tmake_bins=self.params.make_bins)\n\n\treturn self.reset_hist\n\n\t\n\t\n        #@profile\n        #def estimate_size_bins(self,\n        #        min_value=self.params.min_value,\n        #        max_value=self.params.max_value):\n\n        #    \"\"\"\n        #    Estimate a minimal number of bins to use\n        #    \"\"\"\n        #    self.estimate_"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.context = ContextDict()\n\"\"\"\n\nReturns\n-------\nhist : Histogram\n\nSee Also\n---------\n\"\"\"\n\treturn Histogram(self.initial_value, self.make_bins, *ctx)\n"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\treturn self._hist.reset()\n\t\ndef show(self):\n\t\"\"\"\n\tPrint out the histogram bins, values and ranges\n\t(if the *print_hist* context is *True*).\n\tThis is a shorthand method for *hist_to_dict(hist)* and calling\n\t*hist.show()* with the initial context (which in turn calls\n\t*hist.__setitem__(), *hist.reset()*, *hist.set_title()* and\n\t*hist.set_unit()*).\n\t\"\"\"\n\treturn hist_to_"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.clear()\n\t\n\t\n\tself.current_context = Context({})\n\t\n\tif isinstance(self.initial_value, dict):\n\t\tself.bin_data = {}\n\t\t# self.make_bins()\n\telse:\n\t\tself.bin_data = self.initial_value\n\t\t\t\n\t\t\t\n\t\ndef plot_histogram(ax, bin_data, bin_ids, n_bins = 100):\n\t\"\"\"\n\tCreate a (filled) histogram.\n\t\n\t*ax* can represent a matplotlib."}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.context = {}\n\tself.bins = self._make_bins(self.initial_value)\n\tctx_iter = sorted(self.context)\n\tfor _ctx, val in ctx_iter:\n\t\tself.bins[_ctx] = [val]\n\n## Histogram interface\n\n# histogram getters\n@staticmethod\ndef _get_value(self, bins):\n\t\"\"\"\n\tFinds the value that the histogram contains.\n\t\"\"\"\t\n\tif len(bins) == 0:\n\t\treturn 0\n\treturn"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\tself.__dict__ = {}\n\tfor i in xrange(self.bin_count):\n\t\tself.__dict__['bins['+str(i)+']'] = {\n\t\t\t'values': [],\n\t\t\t'widths': [],\n\t\t\t'count': 0,\n\t\t\t'values_count': 0,\n\t\t\t'percentiles': {}}\n\tself.__dict__['count'] = -1\n\tself.__dict__['initial_value'] = self.initial_value\n\tself.__dict__['initial_percentile'] = self.initial"}, {"generated_text": "def reset(self):\n\"\"\"\n\tReset the histogram.\n\t\n\tCurrent context is reset to an empty dict.\n\tBins are reinitialized with the *initial_value*\n\tor with *make_bins()* (depending on the initialization).\n\t\"\"\"\n\t\n\n\tself._context = {}\n\tself._counting = False\n\tself._initial_value = float(0)\n\tself._make_bins = lambda: _Hist.bin_default_values[len(self)]\n\n\treturn self\n"}]}
{"task_id": "62b87b859a0c4fa8b80b35d7", "prompt": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\t# Graphs can be multidimensional,\n\t# so convert them to tabular form\n\tif not hasattr(self,'shape'): return\n\t\n\tif header:\n\t\tdata = [separator.join(\n\t\t\t[str(z) for z in self.get_coord(zname, zdim, ['']*(self.ndim-1)) \n\t\t\t\tfor zname in names][:-1])\n\t\tfor names, dims in list(zip(self.names+self.header, self.dims))\n\t\t]\n\t\t"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\tlena.Log.error(\"to_csv is deprecated. Call graph.to_csv with format='table' instead\")\n\t\n\t\n\nclass ToMatrix(lena.Process):\n\t\n\tdef execute(self, data, format, separator):\n\t\tif format =='matrix':\n\t\t\tyield from self.to_matrix(data, separator)\n\t\telse:\t\n\t\t\ttry:\n\t\t\t\tyield from self.to_list(data, separator, format)\n\t\t\texcept TypeError:\n\t\t\t\tyield from self.to_list"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\t# TODO: This should go into Graphing\n\t\n\tcsv = StringIO()\n\tg_t = Table()\n\tfor i in range(0, self.get_points()):\n\t\tg_i = self.get_data()\n\t\t\n\t\ti_str = \"((\"\n\t\tfor f in range(0, self.get_n_fields()):\n\t\t\ti_str += str(g_i[f][i]) + \",\"\n\t\ti_str += \")))\"\n\t\t\n\t\tif header is None:\n\t\t\ti_str"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tif hasattr(self, \"as_tables\"):\n\t\treturn self._to_csv(separator, header)\n\telse:\n\t\tmsg = \"Graph '%s' is not a graph object.\" % str(self).split(\"\\n\")[0]\n\t\traise TypeError(msg)\n\n\nclass FromCSV:\n\t\"\"\"\n\t.. deprecated:: 1.0 in 0.5 added as iterable to create graphs.\n\t\n\tConvert from CSV.\n\t\n\tThis helper method tries to convert a graph file (*.csv* or *.text"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise Exception(\n\t\t\"to_csv is deprecated: %s->%s\" % (\n\t\t        graph.to_string(), self.to_string()\n\t\t)\n\t)\n\t\n\t\ndef get_vertex_attributes(G):\n\t\"\"\"\n\tGet attribtues of vertices of the graph.\n\t\n\t*G* : :class:`Graph`\n\t\n\t**Returns** a :class:`dict` mapping attribute names to :class:`str`'s.\n\t\"\"\"\n\tattribs = {}\n\tfor"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom lena.utils.csv import table_to_csv\n\timport numpy as np\n\n\tif header is not None:\n        # convert tuple to np.array and take first item\n        data = table_to_csv(np.array(self), **{\"header\" : header})\n        # add header to data\n        header = data[0]\n        # make columns string separated (otherwise, last column is not preserved)\n        data = [\" \".join(row) for row in data]\n        data.insert(0, header)\n        # add last column and"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t\n\timport warnings\n\t\n\twarnings.warn(\"Use to_csv since 0.3.10 in lenas 0.6 to_csv is not used. Iterables are converted to tables.\",\n\t\tDeprecationWarning)\n\t\n\tfrom lena.graph import Graph as _Graph\n\tfrom lena.sgraph import SGraph as _SGraph\n\n\tif header is not None:\n\t\tif not isinstance(header, basestring):\n\t\t\theader = \",\".join(header)\n\t\treturn \"\\n\".join(header.split())\n\t\n\tg"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\t# To CSV is called for graph's points only.\n\tif not isinstance(self, Graph): return\n\t# TODO\n\treturn [self.to_point().__str__().split(str(separator)).pop(0)] + [self.to_point().__str__()[:-1]]\n\n# TODO\n@staticmethod\ndef to_csv_table(*points):\n\t\"\"\"\n\t\tConverts a list of points to CSV.\n\t\t\n\t\tConverts points to CSV.\n\t\t\n\t\tExamples\n\t\t========\n\t\t\n\t\t>>> from lena"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tif self.length not in range(1,1001):\n\t\traise ValueError('Lena can not handle graphs of lengths '+\\\n\t\t\t\t\t\t  'above 100 nodes.')\n\tg = self\n\t# header if given\n\tif header is not None:\n\t\tif not isinstance(header, str): header = separator.join(header)\n\t\theader += \"\\n\"\n\t# conversion to CSV for a single dimension\n\tif g.ndim == 1:\n\t\tc = g.keys()\n\t\tif g.ismultigraph():\n\t\t\tfor u,v,"}, {"generated_text": "def to_csv(self, separator=\",\", header=None):\n\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t      Iterables are converted to tables.\n\t\n\tConvert graph's points to CSV.\n\t\n\t*separator* delimits values, the default is comma.\n\t\n\t*header*, if not ``None``, is the first string of the output\n\t(new line is added automatically).\n\t\n\tSince a graph can be multidimensional,\n\tfor each point first its coordinate is converted to string\n\t(separated by *separator*), then each part of its value.\n\t\n\tTo convert :class:`Graph` to CSV inside a Lena sequence,\n\tuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\treturn self.lena_output.outputs.ToCSV(self.points, header, separator)\n\n\nclass Node:\n\t\"\"\" A node in a graph. \"\"\"\n\tdef __init__(self, id=None, index=None, name=None, color=None, radius=None):\n\t\tif (id is None and index is not None) or (index is None and id is not None):\n\t\t\traise ValueError('id is not None and index is not None when initialization!')\n\t\tif color is None and radius is not None or name is None and id"}]}
{"task_id": "62b87b839a0c4fa8b80b35cb", "prompt": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tif self.__type == \"3d\":\n\t\txcoords = None\n\t\tycoords = None\n\t\tzcoords = None\n\t\tymin, xmin = 0, 0\n\n\t\tif coord_name == \"x\": # X axis\n\t\t\txcoords = RangeList(self._x.get_range())\n\t\t\tycoords = RangeList(self._y.get_range())\n\t\t\tzcoords = RangeList(self._z.get_range())\n\t\t\tymin = 0\n\t\telif coord_name == \"y\": # Y axis\n\t\t\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tfor idm in self.idmismatches:\n\t\tidx = np.where(self.idmismatches[idm] == coord_name)[0]\n\n\t\tif len(idx) > 1:\n\t\t\traise RuntimeError(\"More than one mismatch for coordinate %s (%s, %s)\"%(idm, coord_name, idx))\n\n\t\tif len(idx) == 0: raise RuntimeError(\"No mismatch for coordinate %s (%s, %s)\"%(idm, coord_name, idx))\n\n\t\treturn self._get_mism"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tindices = []\n\tif coordinate.Coordinate.is_scalar(coord_name):\n\t\tindices = [0]\n\telse:\n\t\tshape = self._ndims[coord_name]\n\t\tif coordinate.Coordinate.is_1d(coord_name):\n\t\t\tindices = [0]\n\t\telse:\n\t\t\tindices = [slice(None,)*len(shape) for i in range(tuple(shape) + (1,))[:-1]]\n\t\t\tindices[-1] += 1\n\treturn indices\n\n\n########################"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tdims = self.shape[coord_name]\n\terr_indices = range(dims)\n\tfor idim in range(dims, 0, -1):\n\t\terr_idx = idim\n\t\twhile err_idx not in err_indices:\n\t\t\terr_idx -= 1\n\t\t\terr_indices.append(err_idx)\n\treturn err_indices\n\n\t\nclass TestError_Index_Tester(unittest.TestCase):\n\t\"\"\"\n\t\"\"\"\n\tdef __init__(self):\n\t\tsuper"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\ti_f = self[coord_name][\"fidx\"]\n\ti_err = self[coord_name][\"ierr\"]\n\treturn i_f, i_err"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tif not self._shape:\n\t\treturn numpy.array(0, dtype=coord_name)\n\tif coord_name not in self._shapes[0]:\n\t\treturn numpy.array(0, dtype=int)\n\treturn numpy.arange(self._shape[0])\n\n\nclass CoordArrayField(ArrayField, CoordBaseType, numpy.ma.core.MaskedArray):\n\t\"\"\"\n\tAn array that is backed by an N-d array of a single coordinate.\n\t\"\"\"\n\tdef __init__(self, arraysh"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\tif coord_name in [\"X\", \"Y\", \"Z\"]:\n\t\tr = np.arange(len(self.y))\n\t\tc = self.error_matrix[self.col_coord, [coord_name]].flatten()\n\t\treturn r, c\n\telse:\n\t\traise InputError(\"Could not find the coordinates: {}\".format(coord_name))\n\t\n\t"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\t#coord_index = self.get_coord_index(name)\n\t#if coord_index is not None:\n\t\t#return [coord_index]\n\treturn [0]\n\n#def get_err_indices(self):\n\t#return [[0]]\n\ndef _get_err_indices_xyz(self, coord_name):\n\txy = self.get_coord_index(self.xyz[coord_name])\n\tif xy is not None:\n\t\treturn xy\n\treturn [0]\n\ndef _get_"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\treturn self.get_coord_err_inds(coord_map = {coord_name:1})\n\ndef _get_indices_in_map(lst0, map0, coord_name):\n\t\"\"\"\n\tGet indices corresponding to the coordinates named in `coord_name` in all the data in `lst0`.\n\t\"\"\"\n\tlst1 = [list(map0(i)) for i in lst0]\n\tlst2 = [list(map0(i)) for i in lst0]\n\treturn list(set([i"}, {"generated_text": "def _get_err_indices(self, coord_name):\n\"\"\"\n\tGet error indices corresponding to a coordinate.\n\t\"\"\"\n\ttry:\n\t\terr_indices = [self._err[c][coord_name] for c in range(self.ndim)]\n\texcept KeyError: error_string = \"unknown dimension for coord_name = \" + str(coord_name) + \", in \" + self.short_name()\n\tfor c in self.ndim: assert len(err_indices) == c, error_string\n\treturn err_indices\n\nclass Grid:\n\t\"\"\"\n\tThe Grid class holds an Array instance along with additional information required to access its data.\n\t\"\"\""}]}
{"task_id": "62b87b7e9a0c4fa8b80b35bc", "prompt": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tfor s, t, name in context.value:\n\t\tif name == \"error\":\n\t\t\tsubcontexts = context[name]\n\t\t\twhile s not in subcontexts:\n\t\t\t\tsubcontexts[s] = dict()\n\t\t\tif t in subcontexts:\n\t\t\t\tsubcontexts[t][\"index\"] = \\\n\t\t\t\t\tlen(subcontexts[t][\"index\"]) + 1\n\n\n@registry.add\nclass ToCSV:\n\t\"\"\"\n\tSave a graph of a given type to a CSV file.\n\t\n\tNote that you almost never need"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif self._error is not None:\n\t\tcontext.error = {error : subcontext or {} for error, subcontext in self._error.items()}\n\tif self._value is not None:\n\t\tcontext.value = {value : subcontext or {} for value, subcontext in self._value.items()}\n\ndef _get_graph_type(self):\n\tt = self._get_type()\n\treturn types[t[0]]\n\ngraph_type = property(_get_graph_type)\n\ndef update_subcontext(self, obj):\n"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tassert isinstance(context, dict)\n\tassert isinstance(self, Graph)\n\t\n\tdef _get_errors(x_or_y):\n\t\tif 'error_'+x_or_y in context:\n\t\t\treturn [x['error_'+x_or_y] for x in context['error']]\n\t\telse:\n\t\t\treturn []\n\t\t\t\t\n\tfor x in Graph._fields_desc:\n\t\ty_or_z = x.rpartition('_')[::2]\n\t\ty = y_or_z[0]\n"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif not context.verbose:\n\t\treturn\n\tfor k, v in self.graph.to_context().items():\n\t\tsub = context.to_dict(k).pop(\"subcontext\")\n\t\tif sub:\n\t\t\tsub.update(v) # recursion\n\t\telse:\n\t\t\tcontext.to_dict(k).update(v) # add new context\n\tcontext[\"error\"].append(\n\t\t{\"x_low\": {\"index\": self.error_x_low(self.err_low)}},\n\t\tcheck=True\n\t) # append error by"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t_ = context.error\n\tctx = context\n\tctx.values['error'] = \\\n\t{ k:v['x'].copy() for k,v in ctx['error']['*_low'].items() }\n\n\tctx.subcontexts['error'] = \\\n\t{ k: v['y'].copy() for k,v in ctx['error']['x_low'].items() }\n\n\nclass FromCSV(Graph):\n\t\"\"\"\n\tFromCSV(path_to_CSV,...) -> graph.Graph\n"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tsuper(_Graph2D, self)._update_context(context)\n\n\tif isinstance(context.value, tuple):\n\t\tif len(context.value) > 4:\n\t\t\tcontext.error = tuple(context.error + tuple(context.value[4:]))\n\t\tcontext.value = context.value[:4]\n\tif isinstance(context.values, Mapping):\n\t\tif len(context.values) > 4:\n\t\t\tcontext.error = tuple(context.error + tuple(context.values[4:]))\n\t\tcontext.values = tuple(context"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif \"error\" in context.value:\n\t\t#TODO: do we want to include more indices?\n\t\ts=[]\n\t\td={}\n\t\tif \"y\" in context.value.error:\n\t\t\td.update({\"x_low\": {\"index\": 0}})\n\t\tif \"z\" in context.value.error:\n\t\t\td.update({\"x_low\": {\"index\": 1}})\n\t\tif \"y\" in context.value.error:\n\t\t\td.update({\"y_low\": {\"index\": 2}})\n\t\tif \""}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif isinstance(context, dict):\n\t\treturn\n\tif not hasattr(context, '_errors'):\n\t\treturn\n\tfor type, indices in sorted(context._errors.items()):\n\t\tcontext[type] = {'values': {'index': 2}, 'index': 2}\n\t\tif len(indices) > 1:\n\t\t\tcontext[type+':e:low'] = {'values': {'index': indices[0]},\n\t\t\t\t\t\t\t\t\t  'index': indices[0]}\n\t\tif len(indices) > 2:\n\t\t\tcontext[type"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\t\"\"\"Add error (with only the first line) to the context\"\"\"\n\t#if error_indices:\n\tif self._error:\n\t\tctx_error = copy(context.error=context.error)\n\t\tctx_error[\"x_low\"]  = {\n\t\t\t\"index\": self._error_indices[0]\n\t\t}\n\telse:\n\t\tctx_error = None\n\tnew_context = copy(context=context)\n\tnew_context.error = ctx_error\n\t#for c in new_context.value:\n\t#\tprint"}, {"generated_text": "def _update_context(self, context):\n\"\"\"\n\tUpdate *context* with the properties of this graph.\n\t\n\t*context.error* is appended with indices of errors.\n\tExample subcontext for a graph with fields \"E,t,error_E_low\":\n\t{\"error\": {\"x_low\": {\"index\": 2}}}.\n\tNote that error names are called \"x\", \"y\" and \"z\"\n\t(this corresponds to first three coordinates,\n\tif they are present), which allows to simplify plotting.\n\tExisting values are not removed\n\tfrom *context.value* and its subcontexts.\n\t\n\tCalled on \"destruction\" of the graph (for example,\n\tin :class:`.ToCSV`). By destruction we mean conversion\n\tto another structure (like text) in the flow.\n\tThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tif self.t is not None:\n\t\tcontext.add_context(\"t\", self.t)\n\tcontext.add_context(\"E\", dict((k, v) for k, v in self.E.items()))\n\tif self.error is not None:\n\t\tcontext.add_context(\".error\", self.error)\n\tif self.E_low is not None:\n\t\tcontext.add_context(\"error_E_low\", {name: v for name, v in self.E_low})\n\n\tif self.has_scatter() and self"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3580", "prompt": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t# bins is a (bins,nbins)-tuple\n\tdelta = (edges[-1] - edges[0])/len(bins[0])\n\n\t# Histogram is of type nx1 or nxN\n\th = numpy.array([histogram(x, bins) for x in edges])\n\n\t# Reshape N*M to NxM\n\th = h.reshape(-1,h.shape[1])\n\n\t# We have nxM. Each row gives a function value sampled\n\t# at every bin, which"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\tintegral = 0.0\n\tfor ibin in xrange(0, histogram._h_numbins - 0 + 1):\n\t\tintegral += (edges[ibin + 1] - edges[ibin]) * histogram._h_weights[ibin]\n\treturn integral\n\n\ndef mean (bins, edges):\n\t\"\"\"\n\tCompute mean\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tfor bin in range( len(edges) - 1 ):\n\t\t\n\t\tbins[bin+1] += bins[bin] * (edges[bin+1] - edges[bin]) \\\n\t\t\t* (edges[bin+1] - edges[bin]) * (1/ (edges[bin] - edges[bin-1]))\n\treturn bins\n\ndef integral(bins, edges, low=0., high=0.):\n\t\"\"\"\n\tCalculate integrals inside the range [low,high) using the given mesh.\n\t\n\t*"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tn = 0\n\tfor i in xrange(len(edges)-1):\n\t\tif (edges[i+1]>bins[i]):\n\t\t\tn += 1\n\treturn n\n\ndef integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\treturn sum(s*n for s,n in zip("}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\treturn sum(h(b) * dx(b) for b in bins)\n\n\ndef integral(edges):\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*edges* form the mesh\n\tfor the integral.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t\n\tdx = step(edges)\n\t\n\treturn sum(h(b) * dx(b) for b in range(len(edges)) if b!= edges[-1])\n\ndef scale(n,"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\th, l = histogram(bins, edges)\t# generate histogram info\n\treturn h[-1]\t# return integral\n\ndef integral_over_interval(bins, edges, nbin=10):\n\t\"\"\"\n\tCompute integral over a given interval.\n\t\n\t*bins*, *edges* provide the mesh for the integration.\n\tFor a histogram, the histogram must be calculated first,\n\tas *edges* provides the x-values for the mesh.\n\t\n\tThis function allows to select an area of interest using "}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\treturn sum(vals) - 1\n\ndef integral(bins, edges):\n\t\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\tbins.sort()\n\tn = len(bins)\n\ti = 0.\n\twhile i < float(n - 1):\n\t\ti += len(edges[i] - edges[i + 1])\n"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\th = histogram.create(bins,1)\n\n\"\"\"\n.. autofunction:: integral\n\n\"\"\"\n"}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\treturn sum ((bin[1] - bin[0]) * edges[1] for bin in bins)\n\n\ndef area(edges, bins):\n\tr\"\"\"\n\tCalculate the area under the cumulative \n\tdistribution function defined by ``edges``\n\tcontained in the ndarray *bins*.\n\t\n\t:Examples:\n\n\t>>> # (the test file is included in documentation).\n\t>>> import sys\n\t>>> import pylab as pl\n\t>>> from.. import histogram\n\t>>> x = pl.linspace(0.0, 20."}, {"generated_text": "def integral(bins, edges):\n\"\"\"\n\tCompute integral (scale for a histogram).\n\t\n\t*bins* contain values, and *edges* form the mesh\n\tfor the integration.\n\tTheir format is defined in :class:`.histogram` description.\n\t\"\"\"\n\t# count values in bins\n\ttotal = 0.0\n\tfor j in range(len(bins)-1):\n\t\tc1 = edges[j]\n\t\tc2 = edges[j+1]\n\t\ttotal += bins[j]*c2\n\treturn total\n\ndef ndim(a):\n\t\"\"\"\n\tGet the numbers of independent (i.e. independent\n\tat different dimensions) dimensions in an array\n\t\n\t:argument: array to analyze the data\n\t:return: integer\n\t\"\"\"\n\t# create an empty array\n\tna = [];"}]}
{"task_id": "62b87b199a0c4fa8b80b354e", "prompt": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn isinstance(seq,FillRequestSeq) or \\\n\t    (isinstance(seq,SourceSeq)\n\t\tand not isinstance(seq,Source))\n\n\nclass FillRequestSeqList(Base):\n\tpass\n\nclass FillRequestSeq(Base):\n\t\"\"\"\n\tContainer for the list of FillRequest elements that generated this\n\t*seq*.\n\t\"\"\"\n\tdef __init__(self,*args):\n\t\t\"\"\"\n\t\tTake 0 or more FillRequest elements as arguments.\n\t\t\"\"\"\n\t\tself.items = args\n\n\n# ----------------------------------------------------------------------\n"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\treturn type(seq.id) is not Source and type(seq.id) is not str\n\ndef is_fill_request_sep(sep):\n\t\"\"\"\n\tTest whether *sep* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element, and an empty Source.\n\t\"\"\"\n\treturn sep.id.type == \"source\" and len(sep.id.id_space_abs) == 0\n\ndef is_full_seq(seq):\n\t\"\"\"\n\tTest whether *seq* can"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\ttry:\n\t\treturn len(seq)>0 and \\\n\t\t\tisinstance(seq,FillRequestSeq) or \\\n\t\t\tany(isinstance(x,FillRequest) for x in seq)\n\texcept Exception,e:\n\t\treturn False\n\n## Sequence containing only non-source FillRequest elements\ndef non_source_fill_requests(seq):\n\treturn filter(lambda fr: not isinstance(x, FillRequest), seq) \\\n\t\t\t\n## Sequence containing only source elements\ndef source_fill_requests(seq):\n\treturn sum((\n\t\t"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\tif seq.dtype().type is not _FillRequestType: return False\n\tif len(seq) == 0: return False\n\tif _is_mixed(seq[0]): return False\n\tif _is_source(seq[0]) == False: return True\n\treturn False\n\ndef is_source_seq(seq):\n\t\"\"\"\n\tTest whether *seq* can be converted to a SourceSeq.\n\t\n\tTrue only if it is a Source element or no element is\n\tor it is a Source element and is not a sequence of fills"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\treturn isinstance(seq,FillRequestSeq) or \\\n\t\tany(is_fill_request_seq(s) for s in seq.sequences)\n\n\n# The following variables are used to control garbage collection\n# of sequences from a sequence generator\nseq_garbage_collected = []\nseq_garbage_gen = IteratorSequence(generator=seq_garbage_collected)\n\n\"\"\"\nThis is useful for testing and testing\n\ndef get_sequences_from(data):\n\tif data.seq_path:\n\t\t_, f"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\tif not isinstance(seq, FillRequestSeq):\n\t\treturn False\n\n\t# We could have more checking here...\n\treturn True\n\n################################################################################\n#\n# Public\n#\n################################################################################\n\ndef is_request_seq(seq):\n\t\"\"\"\n\tTest whether *seq* can be converted to a RequestSeq.\n\n\tTrue only if it is a Request element\n\t\t\t\t\tor contains at least one such\n\t\t\t\t\tor is a Source sequence.\n\n\t*seq* can be *None*,\n\tif this indicates that processing was aborted,\n\tin which"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\tseq =''.join([str(seq), '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'])\n\treturn len(Filter(lambda x: x,\n\t\t\t\t\t\t   (IsFillRequest(),\n\t\t\t\t\t\t    IsFillRequestSeq(seq))\n\t\t\t\t\t\t)('', seq)) > 0\n\t\n\t\ndef _seq2fils(seq, _filter=lambda x:x, add_to_db=db.add_object):\n\t\"\"\"_seq2fils(_filter=lambda x:"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t# TODO: also check for Source\n\tfor r in seq:'fill request']:\n\t\t\treturn True\n\treturn False\n\ndef is_fill_request(r):\n\t\"\"\n\t# TODO: should I care if (is_fill_request_seq(res)?)\n\t\"\"\"\n\treturn r.tag == XsdFillRequest.tag or (isinstance(r, dict) and any(is_fill_request_seq(x) for x in r))\n\ndef is_source(r):\n\treturn isinstance(r, dict) and all("}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t# check whether the sequence is a FillRequestSeq\n\treturn Match(\"*.FillRequestSeq\", seq)\n"}, {"generated_text": "def is_fill_request_seq(seq):\n\"\"\"\n\tTest whether *seq* can be converted to a FillRequestSeq.\n\t\n\tTrue only if it is a FillRequest element\n\tor contains at least one such,\n\tand it is not a Source sequence.\n\t\"\"\"\n\t\n\tfor x in seq:\n\t\tif isinstance(x, Sequence):\n\t\t\tif not x.is_source():\n\t\t\t\treturn True\n\t\telse:\n\t\t\treturn x in _fill_requests\n\treturn False\n\t\ndef sequence_to_fill_request_seq(seq):\n\t\"\"\"\n\tConvert a sequence to a list of objects from the FillRequestSeq.\n\n\t*seq* can be any Sequence, except that it must not be a \n\tSourceSequence.\n\t\"\"\"\n\t\n\t# We will be generating a list here, with the"}]}
{"task_id": "62b87b099a0c4fa8b80b3538", "prompt": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn len(getattr(obj, 'fill', [])) == 2 and len(getattr(obj,'request', [])) == 1\n"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn hasattr(obj,'request') and hasattr(obj, 'fill')\n\t\ndef get_fill_request(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill' and'request'.\n\t\"\"\"\n\treturn obj.fill, obj.request\n\t\ndef make_fill_request():\n\t\"\"\"\n\tMakes a fill request object.\n\t\n\treturns:  the fill request object\n\t\"\"\"\n\ttry:\n\t\tfrom fill.requests import fillRequest\n\t\tfillRequests = fillRequest(fill_mode='request')\n\texcept"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tassert isinstance(obj, type(fill(Request()), dict)) or isinstance(obj, type(fill_(req)))\n\treturn True\n\n\ndef is_fill_requests_el(obj):\n\t\"\"\"\n\tObject contains 'fill' method.\n\t\"\"\"\n\treturn isinstance(obj, type(fill([]))) or isinstance(obj, type(fill_([])))\n\ndef is_fill_request_listel(obj):\n\t\"\"\"\n\tObject contains'request' method.\n\t\"\"\"\n\treturn isinstance(obj, type([])) or"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tif hasattr(obj, 'fill'):\n\t\treturn\n#\tif hasattr(obj,'request_obj'):\n#\t\treturn obj.request_obj\n\tif hasattr(obj, 'params'):\n\t\treturn type(obj.params['el']) == list\n\treturn False\n\ndef is_request_obj(obj):\n\t\"\"\"\n\tObject contains method'request'.\n\t\"\"\"\n\tif hasattr(obj,'request'):\n\t\treturn True\n#\tif hasattr(obj,'request_obj'):\n#\t\treturn obj"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn ('request' in dir(obj) and\n\t\t\t('fill' in dir(obj) or'request' in obj \n\t\t\t or ('request' in obj and None in dir(obj['request']))))\n\n\ndef filter_fill(items, obj):\n\t\"\"\"\n\tFilter items which obj has methods'request' and 'fill'.\n\n\t:param iterator items: iterator with items for filtering.\n\t:param object obj: object to filter all found 'fill'-method.\n\t\"\"\"\n\tfor item in items:\n\t\tif  is_fill_request_el(item)"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn'request' in dir(obj) and 'fill' in dir(obj)\n\ndef get_request_method(obj):\n\t\"\"\"\n\tObtain method'request' from obj.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(obj,'request')\n\texcept:\n\t\treturn None\n\ndef get_fill_method(obj):\n\t\"\"\"\n\tObtain method 'fill' from obj.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(obj, 'fill')\n\texcept:\n\t\treturn getattr(obj, '_fill',"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn (not callable(obj.fill) and\n\t\t\tnot callable(obj.request) and\n\t\t\thasattr(obj, '_request'))\n"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"request\")\n\ndef is_request_request_el(obj):\n\t\"\"\"\n\tObject contains functions 'get' and 'post'.\n\t\"\"\"\n\treturn hasattr(obj, \"get\") and hasattr(obj, \"post\")\n\ndef is_api_request(obj):\n\t\"\"\"\n\tObject contains function'request'.\n\t\"\"\"\n\treturn hasattr(obj, \"request\")\n\ndef execute(request, method, url, params, user_ip = None):\n\t\"\"\"\n"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\tfor method in ('fill','request'):\n\t\tif hasattr(obj, method):\n\t\t\treturn True\n\n\t\t#if hasattr(obj, 'im_func'):\n\t\t\t#if isinstance(obj.im_func, MethodType):\n\t\t\t\t#return True\n\n\treturn False\n\ndef is_method(obj):\n\t\"\"\"\n\tobject has the same name as a python function\n\t\"\"\"\n\tif hasattr(obj, '__name__') and obj.__name__ == obj.func_name:\n\t\treturn True\n\n\t#if hasattr(obj"}, {"generated_text": "def is_fill_request_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'request'.\n\t\"\"\"\n\treturn (type(obj) is types.MethodType) and (hasattr(obj,'request')) and (hasattr(obj, 'fill'))\n\ndef is_fill_request_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill' and'request'.\n\t\"\"\"\n\treturn (type(obj) is types.MethodType) and (hasattr(obj,'request')) and (hasattr(obj, 'fill'))\n\nclass RequestEvaluator:\n\n\t_EACH_FILL_MARKER = '__EACH"}]}
{"task_id": "62b87af99a0c4fa8b80b3524", "prompt": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\ndef is_subclass(cls, cls_to_search):\n    \"\"\"\n    Return a boolean value: True if `cls` is a subclass of `cls_to_search`, False otherwise.\n    Arguments:\n    `cls` class: the class to check\n    `cls_to_search` class: the class to search `cls` in\n    \"\"\"\n    return issubclass(cls, cls_to_search)\n\ndef get_list_of_methods_"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn getattr(obj, 'run', None) is not None \n\ndef is_run_func(obj):\n    \"\"\"\n    Object contains python function 'run'.\n    \"\"\"\n    return getattr(obj, 'run', None) is not None\n\ndef filter_funcs(funcs, obj=None):\n    \"\"\"\n    Function filters func.\n    :param funcs: list of filter functions\n    :param obj: filter only for specified object (as string name!)\n    :return: filtered func for function list\n    \"\"\"\n    if not isinstance("}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\tobj_ = obj\n\tif inspect.ismethod(obj):\n\t\tobj_ = obj.__self__\n\t__type__ = type(obj_).__name__\n\n\ttry:\n\t\t_method = getattr(obj_, 'run')\n\texcept (AttributeError, TypeError):\n\t\tpass\n\telse:\n\t\tif inspect.ismethod(obj):\n\t\t\t__type__ = obj_.__self__.__class__.__name__\n\treturn __type__\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn 'run' in dir(obj)\n\n\ndef is_valid_name(name):\n  \"\"\"Returns True if the given name is valid.\"\"\"\n\tif type(name) == str:\n\t\treturn u\"A\" <= name <= u\"Z\".find(name)\n\telse:\n\t\tfor c in name:\n\t\t\tif type(c)!= str and not 'A' <= c <= 'Z'.find(c):\n\t\t\t\treturn False\n\t\treturn True\n\n\ndef is_invalid_name(name):\n\treturn not is_valid_name(name)"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn (find_method(obj.implements, is_async, is_run) is None)\n\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn ((type(obj) == run and\n\t   type(exec_method('is_run_el')) == bool)\n\t   or (type(obj) == python_types.FunctionType and\n\t       exec_method('is_run_el')))\n\n\"\"\"\n\tExecute method 'exec_method(self, obj)' in the given object.\n\"\"\"\ndef exec_method(obj, exec_method):\n\tif is_run_el(obj):\n\t\treturn exec_method('run_el', obj)\n\telse:\n\t\treturn exec_"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn hasattr(obj, 'run')\n\n\nclass ErrorInExecutable(Exception):\n\t\"\"\"\n\tException throws by exec instance inside main of executable.\n\t\"\"\"\n\tpass\n\n\nclass ExecuteError(Exception):\n\t\"\"\"\n\tException for all errors within exec instance.\n\t\"\"\"\n\tpass\n\n\n# =============================================================================\n# =============================================================================\nclass Executable(ExecutableMixin, object):\n\t\"\"\"\n\tWrapper class for instance of :code:`exec` or other executable.\n\t\"\"\"\n\tdef run(self):\n\t\t\"\"\"\n\t\tExecute instance"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\texecutable_methods=[Method.run,Method.debug]\n\treturn isinstance(obj,Method) and isinstance(obj.__doc__,str) and str(obj.__doc__).count('run') > 0 and executable_methods.count(obj) > 0\n\ndef is_runnable_func(func):\n\t\"\"\"\n\tReturns bool when given function is suitable for execution\n\t\"\"\"\n\ttry:\n\t\texecfunc=getattr(func,Method.run)\n\t\treturn inspect.ismethod(execfunc)\n\texcept:\n"}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\t__metaclass__ = MetaIsRun\n\"\"\"\n\nclass Run:\n\tdef run(self, ctx):\n\t\t...\n\nclass TestRun:\n\tdef is_run_el(obj):\n\t\tobj.run = run\n\"\"\"\ndef cmp(o1, o2):\n\t\"\"\"\n\tSimple compare function for objects, not including attributes.\n\t\"\"\"\n\tif o1 is None or o2 is None:\n\t\treturn 0\n\tif type(o1) is type(o2):\n\t\treturn cmp(type(o1), type("}, {"generated_text": "def is_run_el(obj):\n\"\"\"\n\tObject contains executable method 'run'.\n\t\"\"\"\n\treturn hasattr(obj, \"run\") and callable(obj.run)\n\ndef is_runnable_el(obj):\n\t\"\"\"\n\tObject has an executable method 'get_runnable_command'.\n\tIf so, it must take at least arg 1, which is a function object. Must accept\n\tan execution method.\n\t\"\"\"\n\treturn hasattr(obj, \"get_runnable_command\") and callable(obj.get_runnable_command)\n\ndef is_executable(obj):\n\t\"\"\"\n\tObject can be"}]}
{"task_id": "62b87af69a0c4fa8b80b351a", "prompt": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif (type(obj) is ComputeElementType and len(obj.__init__) == 1 and 'fill' in obj.__init__):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\"\"\"\nThe following methods implements the interface methods of ComputeElement.\n\"\"\"\n\ndef get_fill_compute_el(obj):\n\t\"\"\"\n\tReturns the method used for filling a compound element.\n\n\tParameters\n\t----------\n\tobj : ComputeElement object\n\n\tReturns\n\t-------\n\tmethod : function\n\t\tmethod used to fill a ComputeElement object"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute') \n\ndef is_valid_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return is_fill_compute_el(obj) and isinstance(obj.fill, (func, methodcaller, Function)) and isinstance(obj.compute, (func, methodcaller, Function)) \n\ndef extract_fill_compute_el(obj):\n\t\"\"\"\n    Extract list"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn obj.is_fill\ndef is_fill_el(obj):\n\treturn obj.is_fill\ndef is_fill_compute_el(obj):\n\treturn obj.is_compute\ndef is_compute_el(obj):\n\treturn obj.is_compute\n\n# is_read_el\ndef is_read_el(obj):\n\treturn obj.is_read\ndef is_read_compute_el(obj):\n\treturn obj.is_compute\n# is_exec_el\ndef is_exec_el"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tf_name = obj.__qualname__\n\tret=f_name[0:2] == 'FX'\n\tif not ret:\n\t\tf_name = obj.__name__\n\t\tret=f_name[0:2] == 'FX'\n\treturn ret\n\ndef fill_value(obj, fill, v, comp=None):\n\n\t\"\"\"\n\tGet the value to be filled.\n\t\"\"\"\n\n\tval = [None  for i in range(2)]\n\n\tif isinstance(obj, list):\n\t\tfor i in range(len("}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\tif (hasattr(obj, 'fill')):\n\t\treturn True\n\telif (hasattr(obj, 'compute')):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\ndef is_fill_compute_mfe(obj):\n\t\"\"\"\n\tObject contains MFE functions (mfe, mfe_grad, mfe_hess, mfe_compute).\n\t\"\"\"\n\tif (hasattr(obj,'mfe') and hasattr(obj,'mfe_grad') and hasattr(obj,'mfe_hess')):"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn is_func_or_var(obj, 'fill');\n\ndef is_fill_compute_el_with_cache(obj):\n\t\"\"\"\n\tObject with the property 'fill' does not.\n\t\"\"\"\n\treturn is_func_or_var(obj, 'fill') \\\n\t        and not is_func_or_var(obj, 'compute');\n\ndef is_data_compute_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'load' and 'compute'.\n\t\"\"\"\n\treturn is_var_or"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\t#@profile\n\tfor attr in dir(obj):\n\t\tm = inspect.getmembers(obj, inspect.ismethod)\n\t\tfor method_name in m:\n\t\t\tif callable(m[method_name][0]) and method_name.startswith('fill'):\n\t\t\t\treturn True\n\treturn False\n\ndef is_compute_el(obj):\n\t\"\"\"\"\n\tObject contains executable methods 'fill', 'compute' or 'compute_all'\n\t\"\"\"\n\t#@profile\n\tfor attr in dir(obj):\n"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn hasattr(obj, 'compute') and hasattr(obj, 'fill')\n\t\ndef run_compute_el(obj):\n\t\"\"\"\n\tMethod that does the actual computation.\n\t\"\"\"\n\tobj.compute('calibrated')\n\n\ndef load_calibration(file_path, unit = None):\n\t\"\"\"\n\tLoads calibration from file and returns it. \n\tfile_path: full relative path to the newton calibration file\n\tunit: newton units (defaults to default units set in the configuration file)\n\traises"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\ndef is_isocode_compute_el(obj):\n\t\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn hasattr(obj, 'code') and hasattr(obj, 'compute')\n\n\ndef get_attributes(obj, attributes):\n\tattributes = set(attributes.split(','))\n\tret = {}\n\tfor prop in attributes:\n\t\tattr = obj.__dict__.get(prop,"}, {"generated_text": "def is_fill_compute_el(obj):\n\"\"\"\n\tObject contains executable methods 'fill' and 'compute'.\n\t\"\"\"\n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"compute\")\n\ndef is_numpy_struct_tuple(obj):\n\t\"\"\"\n\tObject is a numpy struct, tuple or a structure of objects that all contain numpy structs.\n\t\"\"\"\n\treturn isinstance(obj, (tuple, structure)) and all(isinstance(e, (struct, tuple, structure)) for e in obj)\n\ndef is_numpy_tuple(obj):\n\treturn isinstance(obj, tuple)\n\n# TODO add other checks"}]}
{"task_id": "62b87af19a0c4fa8b80b34f7", "prompt": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\td1, d2 = _dict_pair_dicts(d1, d2)\n\ttry:\n\t\tlevel = int(level)\n\texcept:\n\t\traise ValueError('invalid keyword argument {0}'.format(level))\t\t\n\tif level == 0:\n\t\treturn _dictdiff(d1, d2, True, True)\n\tc = d1.copy()\n\tsublevel = 2\n\td = _dictdiff(d1, d2, True, False)\n\td.update(sublevel - level)\n\td.update"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif isinstance(d1, Dict):\n\t\td1 = d1.copy()\n\tif isinstance(d2, Dict):\n\t\td2 = d2.copy()\n\tif level == -1:\n\t\treturn dictutil.difference(d1, d2)\n\telse:\n\t\tresult = {}\n\t\tfor key in set(d2.keys()) | set(d1.keys()):\n\t\t\ta = d1.get(key, UNDEFINED)\n\t\t\tb = d2.get(key, UNDEFINED)\n"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif level == -1:\n\t\tlevel = sys.maxsize\n\tret = {}\n\tfor k, v1 in d1.iteritems():\n\t\tif k not in d2 or not deep_equals(v1, d2[k], level) or k in ret:\n\t\t\tret[k] = v1\n\treturn ret\n###\n# Functions for comparing objects.\n###\n\ndef deep_equals(v1, v2, level=sys.maxsize):\n\t\"\"\"\n\tReturn True if two elements *v1* and *v2* are"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tfrom copy import deepcopy\n\treturn deepcopy(utils.difference(d1, d2, level))\n\n#-------------------------------------------------------------------------\n#\n# get_lang_data\n#\n#-------------------------------------------------------------------------\ndef get_lang_data(lang, name):\n\t\"\"\"\n\tReturn dictionary containing name=value pairs for the \n\tformatting information for language *lang*. In the returned\n\tdictionary *name* is the name of the required data item.\n\n\tTo find the data set the variable *name* equal to the desired\n\tvalue from the localized UI string.\n\t\n"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tresult = d1.copy()\n\tlevels = [(k, d1[k]) for k in d1.keys()]\n\tcount, iter = 0, iter(levels)\n\twhile count < level:\n\t\tk, v = next(iter)\n\t\tresult[k] = v\n\t\tdel result[k][d2[k]]\n\t\tcount += 1\n\treturn result\n\ndef difference_update(d1, d2, update_func):\n\t\"\"\"Perform an update for given difference between *d1* and *d2*.\n\t\n\tThis"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tresult = {}\n\tfor key, value in d1.iteritems():\n\t\tif key in d2:\n\t\t\tif level > 0:\n\t\t\t\tif d1[key] == d2[key]:\n\t\t\t\t\tlevel -= 1\n\t\t\t\t\tdel result[key]\n\t\t\t\t\tcontinue\n\t\t\tif value == d2[key]:\n\t\t\t\tdel d2[key]\n\t\t\t\tcontinue\n\t\telif value is not d2:\n\t\t\tcontinue\n\t\tresult[key] = value\n\tfor key in d2:\n\t\tif level > 0:\n\t\t\tif d2[key] is not d"}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tif level == -1:\n\t    _result = {}\n\t    keys = d1.keys()\n\t    if isinstance(d2.get(keys[0]), dict):\n\t        sub = d1[keys[0]]\n\t    for k in keys:\n\t        if k not in d2:\n\t            _result[k] = None\n\t        elif isinstance(d2[k], dict) and isinstance(sub[k], dict):\n\t            _result[k] = difference(sub[k], d2[k], level="}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tdiffn = {}\n\td1copy = copy.deepcopy(d1)\n\tfor k,v in d2.items():\n\t\tif k in d1:\n\t\t\td1v = d1[k]\n\t\t\tif eq(d1v,v): continue\n\t\t\tdiffn[k] = v\n\t\t\tdel d1copy[k]\n\t\t\ts1 = set(d1copy.iterkeys())\n\t\t\ts2 = set(d2.iterkeys())\n\t\t\tfor s in s2.intersection(s1):\n\t\t\t\tdiffn[s] ="}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\t_assert(hasattr(d1, \"iteritems\"))\n\t_assert(hasattr(d2, \"iteritems\"))\n\tresult = {}\n\tfor k in d1.iterkeys():\n\t\tif k in d2:\n\t\t\tif k not in result:\n\t\t\t\tresult[k] = d1[k]\n\t\telse:\n\t\t\t# If d2 does not contain k, only insert k into result.\n\t\t\tresult[k] = d1[k]\n\tfor k in d2.iterkeys():\n\t\tif k not in result:\n\t\t\tresult."}, {"generated_text": "def difference(d1, d2, level=-1):\n\"\"\"\n\tReturn a dictionary with items from *d1* not contained in *d2*.\n\t\n\t*level* sets the maximum depth of recursion. For infinite recursion,\n\tset that to -1. For level 1,\n\tif a key is present both in *d1* and *d2* but has different values,\n\tit is included into the difference.\n\tSee :func:`intersection` for more details.\n\t\n\t*d1* and *d2* remain unchanged. However, *d1* or some of its\n\tsubdictionaries may be returned directly.\n\tMake a deep copy of the result when appropriate.\n\t\n\t.. versionadded:: 0.5\n\t   add keyword argument *level*.\n\t\"\"\"\n\tret = {}\n\tdiff = d1.copy()\n\tif level < 0:\n\t\tdif = None\n\telif level == 0:\n\t\tdif = _d0(d1, d2)\n\telse:\n\t\tdif = []\n\twhile dif:\n\t\tdif = dif[0]\n\t\tif dif._tup in d2:\n\t\t\tif dif in d2:\n\t\t\t\tif not dif._items:\n\t\t\t\t\t# dif is the same on both sides, no difference\n\t\t\t\t\tcontinue\n\t\t\t\t# dif is non"}]}
{"task_id": "62b87af09a0c4fa8b80b34f1", "prompt": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tw = np.sum(weight)\n\txy = np.empty((w, 2))\n\txy[:, 0] = np.cumsum(weight)\n\txy[:, 1] = xy[:, 0] / w\n\t\n\tif np.max(xy[:, 1]) > 1.:\n\t\traise ValueError(\"Invalid values in xy. Max(xy[:,1])>1.1.\")\n\tself.data[np.argmin(xy[:,0]) : np.argmin(xy[:,0] - 1), :] += coord *"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.set(self.x[coord], self.y[coord], weight)\n\t\ndef normalize(self):\n\t\"\"\"\n\tNormalize the histogram to unity density, then fill missing bins.\n\t\"\"\"\n\thmax = np.max(np.abs(self.__data))\n\tself.__data /= hmax\n\tfor i in range(len(self)):\n\t\tself[i,0] = 0\n\tself.normalize_()\n\ndef get_bin_indices(self, width):\n\t\"\"\"\n\tDivide"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif self._dims < 2:\n\t\traise ValueError(\"not a histogram: no values stored to \"\n\t\t\t\"fill a histogram with\")\n\t\n\tassert coord * weight > 0, \"zero or negative weights are not allowed\"\n\t\n\t# get the bin index corresponding to the coordinate\n\t# and add the weighted value to the bins count\n\tif len(self._entries) > 1:\n\t\tfor i, bin in enumerate(self._entries):\n\t\t\tif bin[\"coord\"] < coord and bin[\"coord\"] + (self._interval - 1"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\n\ti, j = self.dim(coord)\n\tn = self.bins[i,j]\n\tif n == 0: return\n\n\tfor k in range(n):\n\t\t\tself.data[i,j][k] *= weight\n\n\treturn self\n\n#____________________________________________________________\nclass LinearHistogram(Histogram):\n\"\"\"\n\tLinearHistograms are histograms which map each coordinate in the range of the histogram to the bin\n\tcontaining as much data as possible.\n\t\n\t:param min_coord, max_coord: the current range ("}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tx, y = coord\n\n\tif x < 0.0 or x > self._width or y < 0.0 or y > self._height:\n\t\treturn\n\n\t_pixel = (int(round(y * self._height)), int(round(x * self._width)))\n\tself.data[_pixel] += weight\n\treturn\n\ndef to_pixels(self, coord):\n\t\"\"\"\n\tConvert the given *coord* to pixel coordinates.\n\t\n\tCoordinates outside the histogram edges are wrapped.\n\t\"\"\"\n\tx, y = Coord."}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif not self.edges.contains(coord):\n\t\treturn\n\n\tfor c, counts in self._counts.items():\n\t\tif abs(coord[0] - c[0]) > 1 or abs(coord[1] - c[1]) > 1:\n\t\t\tcontinue\n\n\t\tself._counts[c] += weight\n\nclass SummedHisto(HistoBase):\n\tdef __init__(self, edges=None, data=None):\n\t\tsuper(SummedHisto, self).__init__(edges, data)\n\t\t\n\t\tself"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\t\n\th = self.hist_at(coord)\n\th.fill(weight)\n\treturn h\n\n\ndef plot(self, axis=None, binminv=None, binwidth=None, title=None):\n\t\"\"\"\n\tPlot the histogram.\n\n\t:param axis: *axis* to add the plot to. Defaults to the\n\t\tcurrent active axis if *None*.\n\t:type axis: :class:`pywcs.utils.Axis`\n\t:param list binminv: *binminv* to be added to the axes. Def"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.__fill((coord,  weight))\n\ndef __fill(args):\n\t(coord, weight) = args\n\tx = int(round(coord[0]))\n\ty = int(round(coord[1]))\n\tsize = coord[0] < coord[1]\n\thist = [0 for j in range(256)]\n\tfor val in range(0, 256*256):\n\t\tif size:\n\t\t\tindex = size * y + x\n\t\t\tif index >= 0 and index < 256*256: hist[val] += weight\n\t\telse"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tassert self.histogram.shape == []\n\t_shape = self.histogram.shape\n\tif not (0 <= weight <= 1).all():\n\t\traise ValueError(\"Weight must be between 0 and 1\")\n\tif not (0 < coord <= 1).all():\n\t\traise ValueError(\"Coordinates must be between 0 and 1\")\n\t# weight*(x-range_x)\n\tfor axis in _shape:\n\t\tif not 0 <= coord[axis] <= 1:\n\t\t\tcontinue\n\t\tself.histogram[(slice(None),)*axis+slice"}, {"generated_text": "def fill(self, coord, weight=1):\n\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\t\n\tCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tif coord >= self.maxCoord: return;\n\tl = self.minCoord;\n\tr = self.maxCoord;\n\twhile l < r:\n\t\tif coord < l: l = coord;\n\t\tif coord > r: r = coord;\n\t\tl0 = r1 = (l+r)/2;\n\t\tself.fill_part(l0, r1, weight);\n\t\tl = l0;\n\t\tr = r1;\n\"\"\"\nfill_weights(self, weights, coord)\n\tFill histogram at *coord* with"}]}
{"task_id": "62b86aa3b4d922cb0e688d36", "prompt": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tm = {}\n\tfor k, v in labels.items():\n\t\tif type(k)!= str:\n\t\t\traise ValidationError(\"Could not validate labels, key '{}' is not valid.\".format(k))\n\t\telif type(v) == list:\n\t\t\tif not all(r.search(k) for r in regexes):\n\t\t\t\tm[k] = \"One or multiple '{}' regular expressions did not match pattern '{}'.\".format(\n\t\t\t\t\tregexes_string,\n\t\t\t\t\tk,\n\t\t\t\t)\n\t\telse:\n\t\t\t"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\terrors = []\n\tfor key, values in list(labels.items()):\n\t\tfor value in values:\n\t\t\tif not re.match(regex, value):\n\t\t\t\terrors.append({\"%s\": value})\n\tif errors and raise_if_any_false:\n\t\traise ValidationError(list(chain.from_iterable(errors)))\n\n_validate_labels.__doc__ = \"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tSee `_validate_labels`"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\terrors_occured = False\n\t\n\t# check for empty labels\n\tif labels is None:\n\t\treturn\n\t\n\t# check for dictionary arguments\n\tif not isinstance(labels, dict):\n\t\traise ValidationError(\"Expected labels to be a dictionary, got: {0}\".format(type(labels)))\n\t\n\t# make a dictionary of unique keys from the labels dictionary, this is to avoid checking the same\n\t# key (label key or label value) multiple times.\n\tto_check_keys = {k if isinstance(k, bas"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tfor key, values in labels.items():\n\t\tfor value in values:\n\t\t\tif not isinstance(key, str) or not isinstance(value, str):\n\t\t\t\traise ValidationError('Label does not match regexes: key {key!r} value {value!r}'\n\t\t\t\t\t.format(**locals()))"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tfor key in labels:\n\t\tif not isinstance(labels[key], string_type):\n\t\t\traise ValidationError('Labels cannot contain non-string type')\n\t\tif not re.fullmatch(re.compile(LABELS_RE.pattern), labels[key]):\n\t\t\traise ValidationError(\n\t\t\t\t'Label key \"'\n\t\t\t\t+ key\n\t\t\t\t+ '\"'\n\t\t\t\t+'does not match the current regex '\n\t\t\t\t+ '\"' + re.escape(LABELS_RE.pattern) + '\"'\n\t\t\t)\n\n\nclass DbSyncServiceConfigParser"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tfor k, v in labels.items():\n\t\tif re.match(valid_label_regex, k) is None:\n\t\t\tif type(v) is list:\n\t\t\t\tmsg = \"Invalid [{:!r}], expected regex string, list or iterable, but received {:!r}\"\n\t\t\t\traise ValidationError(msg.format(k, v))\n\t\t\telse:\n\t\t\t\traise ValidationError(\"Invalid label for key '{:!s}': expected regex string\".format(k))\n\t\t\n"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\terrors = []\n\tfor key in labels.copy():\n\t\ttry:\n\t\t\tmatch = _validate_label_re(_RE_PATTERNS[_RE_LABEL_TYPES[key]], key, labels)\n\t\texcept _error.MultipleRegexErrors as err:\n\t\t\terrors += err.get_message()\n\t\telse:\n\t\t\tif not match:\n\t\t\t\terrors.append({'key': key, 'value': err})\n\treturn errors\n\n\ndef _validate_re(regexp, value):\n    \"\"\"\n    Validate the"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\terrors = {}\n\tfor key, regex in labels.items():\n\t\tif not isinstance(key, (str, bytes)):\n\t\t\t# raise an exception if no regex is defined\n\t\t\tif key!= bool:\n\t\t\t\traise ValidationError({\"key\": key})\n\t\tkey_re = re.compile(key)\n\t\tif key_re is None:\n\t\t\traise ValidationError({\"key\": key})\n\t\tif isinstance(regex, str) and isinstance(labels[key], bytes):\n\t\t\tregex = re.compile(regex."}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\terrors = []\n\tfor key in labels:\n\t\t# TODO: Validate labels against their regular expressions\n\t\tif len(labels[key]):\n\t\t\terrors.append(\n\t\t\t\t{\"value\": key,\n\t\t\t\t \"message\": 'expected string or bytes-like object',\n\t\t\t\t \"key\": key})\n\t\n\tif errors:\n\t\terr = ValidationError(errors)\n\t\t\n\t\t# TODO: use more specific messages\n\t\traise err\n\t\n\treturn True\n"}, {"generated_text": "def _validate_labels(labels):\n\"\"\"\n\tCheck that keys and values in the given labels match against their corresponding\n\tregular expressions.\n\t\n\tArgs:\n\t    labels (dict): the different labels to validate.\n\t\n\tRaises:\n\t    ValidationError: if any of the keys and labels does not match their respective\n\t        regular expression. The error contains as message the list of all errors\n\t        which occurred in the labels. Each element of the list is a dictionary with\n\t        one key-value pair:\n\t        - key: the label key or label value for which an error occurred as string.\n\t        - value: the error message.\n\t\n\t        .. code:: python\n\t\n\t            # Example:\n\t            labels = {\n\t                \"key1\": \"valid\",\n\t                \"key2\": [\"invalid\"],\n\t                \"$$\": \"invalid\",\n\t                True: True,\n\t            }\n\t            try:\n\t                _validate_labels(labels)\n\t            except ValidationError as err:\n\t                assert err.messages == [\n\t                    {\"['invalid']\": 'expected string or bytes-like object'},\n\t                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                    {'True': 'expected string or bytes-like object'},\n\t                ]\n\t\"\"\"\n\tfor k in labels:\n\t\t\n\t\tif isinstance(labels[k], bool):\n\t\t\tlabels[k] = labels[k].__name__\n\t\t\n\t\tif re.search(r'[[][(\\]]', labels[k]) is not None:\n\t\t\traise ValidationError({k: 'Label key \"{}\" does not match the regex [...]'.format(k)})\n\t\t\n\t\tif not isinstance(labels[k], string_types):\n\t\t\traise ValidationError({k: 'Label key \"{}\" is not a string'.format(k)})\n"}]}
{"task_id": "62b86a9eb4d922cb0e688d25", "prompt": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tif not _resource_name_regex:\n\t\trv = []\n\t\tfor (i,name) in enumerate((\"name\", \"title\", \"url\"),1):\n\t\t\trv.append(re.compile(r\"^(\\w{1,%d})(\\d{1,%d})?.*?$\" % (50,50)))\n\t\t\trv.append(re.compile(r\"^%s(?:\\s+|-)%s.*?$\" % (i,i+1)))\n\t\t\trv.append(re"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\t_re_resource_name_valid = 'a..zA..Z0..9_'\n\n\treturn re.compile(_re_resource_name_valid, re.I)\n\n\ndef _is_valid_resource_prefix(s_resource_prefix):\n    \"\"\"\n    \tCheck the parameter's_resource_prefix' if it is a valid prefix of the\n    \tresource name.\n\n    Args:\n    \ts_resource_prefix (str): the string to be checked\n\n    Returns:\n    \tbool: True if's_resource_prefix' is"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tif not resource_name_regex_builder.compiled:\n\t\tresource_name_regex_builder.compile()\n\treturn resource_name_regex_builder.pattern\n\t\n\t\ndef _get_resource_name_matches():\n\t\"\"\"\n\tGet the valid resource name regex strings as a dictionary.\n\n\tReturns:\n\t    Dict[str, Pattern]: a set containing the valid resource name\n\t    registry entries and compiled regexes\n\t\"\"\"\n\treturn {\n\t\tresource_name: _get_resource_name_regex().pattern"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn re.compile(\"^[a-zA-Z][a-zA-Z0-9]*$\")\n\n\n"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tpatterns = r\"(^[a-zA-Z0-9-_.]+$)\";\n\treturn re.compile(patterns)\n\t\ndef _get_resource_id_regex():\n\t\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe resource id.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource id.\n\t\"\"\"\n\tpatterns = r\"([a-zA-Z]+)([0-9]+)?$\"\n\treturn"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn re.compile(\n\t\tr'^(?!.*__)(?!.*_)(?!\\.tmp).*$',\n\t\tre.UNICODE)\n\n\ndef _remove_tmp(name):\n\t\"\"\"\n\tRemove the tmp marker from the resource name.\n\n\tThis is a convenience function to remove the tmp marker from the\n\tname.\n\n\tArgs:\n\t    name(str): the name to validate.\n\n\tReturns:\n\t    str: the validated name.\n\t\"\"\"\n\treturn _get_resource_name_regex().sub('', name)\n"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tresource_pattern = re.compile(\"^[a-zA-Z0-9_]{1,64}$\")\n\treturn resource_pattern\n\n_resource_name_regex = _get_resource_name_regex()\n\n\ndef validate_resource_name(resource_name):\n    \"\"\"\n    Validate the provided resource name.\n    \n    Args:\n        resource_name(string):    the resource name entered by the user\n        \n    Returns:\n        (bool):   True if the passed resource name is valid for\n                  a resource managed"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\treturn re.compile(r'^[0-9a-zA-Z-_.]{1,50}$')\n\t\nclass KrakeResources(object):\n\t\"\"\"\n\tSimple class to hold a list of resources available to the CLI\n\tthrough the CLI. It should remain as simple as possible in order\n\tnot to slow the CLI down.\n\t\"\"\"\n\t\n\tdef __init__(self):\n\t\tself.res = []\n\t\t\n\tdef __iter__(self):\n\t\treturn iter(self.res)\n\t\n\tdef __cont"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tglobal _regex\n\tif not _regex:\n\t\t_regex = re.compile(\"^([a-z][a-z0-9_.-]*)(:?)([a-z0-9_]+)$\")\n\t\t_regex_invalid = re.compile(\"^(?![a-z0-9_.-])\")\n\treturn _regex\n\ndef _get_resource_name_validator():\n\t\"\"\"\n\tBuilds a function that validates the resource name.\n\t\n\tReturns:\n\t    fun"}, {"generated_text": "def _get_resource_name_regex():\n\"\"\"\n\tBuild or return the regular expressions that are used to validate\n\tthe name of the Krake resources.\n\t\n\tReturns:\n\t    (re.Pattern): the compiled regular expressions, to validate\n\t    the resource name.\n\t\"\"\"\n\tresources_re = \"\"\"\n\t^\n\t(?:\n\t\t([_a-zA-Z\\.]+(\\b|\\b\\w|\\p{Han}))  # A local or global resource\n\t|\n\t\t(?:\n\t\t\t\\[\n\t\t\t\t_[\\w.?]*\n\t\t\t\\](?=\\w.?)  # Alias for an existing resource\n\t\t|\n\t\t\t\\b(?:[_a-zA-Z][a-zA-Z-_.]*)?  # Name of an existing resource\n\t\t|\n\t\t\t[_a"}]}
{"task_id": "62b86a4fb4d922cb0e688cf8", "prompt": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tvalue=str(value)\n\tregexpr=re.compile(value.strip()+\"(?<!\\\\p{Alpha}&&[^ ])+\")\n\tregexpr.search(value)\n\tif not regexpr.group():\n\t\traise ValidationError(\"value %s doesn't match regexp\" % value)\n\t\n\ndef validate_range(value, start=None, stop=None):\n\t\"\"\"\n\tValidate the given value is a value between two or more values.\n\tThe value can be a string or an numeric.\n\t\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tpass\n\t\n\t\n\t\n\t"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tre.compile(regexp).match(value)\n\ndef clean_name(name):\n\t\"\"\"\n\tClean the given name by stripping all spaces.\n\t\n\tArgs:\n\t    name: the string to clean\n\t\n\tReturns:\n\t    the cleaned string\n\t\"\"\"\n\treturn name.strip()\n\ndef is_number(string):\n\t\"\"\"\n\tCheck whether the given string is a number.\n\t\n\tArgs:\n\t    string: the string to check\n\t\n\tReturns:\n\t    True if the given string is a number"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match('^[a-zA-Z0-9\\-]+$', value):\n\t\traise ValidationError(\"Invalid email address '%s'\" % value)\n\nclass Email(Valid):\n\tdef __init__(self, *args, **kwargs):\n\t\tValid.__init__(self, *args, **kwargs)\n"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\ttry:\n\t\tif not re.match(validator[\"value\"], value):\n\t\t\traise ValidationError\n\texcept KeyError:\n\t\traise Exception(\"Value {} has no matching regular expression.\".format(value))\n\n\n\"\"\"\n    Validate a given model object attribute\n    \n    Args:\n        obj: the model object\n        attr: the attribute name to validate\n    \n    Raises:\n        ValidationError: if no matching regex for the given regular expression exists.\n\"\"\"\ndef validate_get_and_set(value, obj, attr):"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tassert isinstance(value, str)\n\n\tdef check_length():\n\t\tif len(value) > MAX_LENGTH:\n\t\t\traise ValidationError(\"%s is too long (%d chars max).\" % (value, MAX_LENGTH))\n\n\tdef check_pattern():\n\t\tif not bool(PATTERN.search(value)):\n\t\t\traise ValidationError(\"Invalid regular expression: %s\" % str(PATTERN))\n\n\tfor each_check in [check_length, check_pattern]:\n\t\ttry:\n\t\t\teach_check()\n\t\texcept Validation"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match(value, value):\n\t\traise ValidationError(\"Invalid value '%s': '%s' does not match the expected regular expression\" % (value, value))\n\ndef validate_int_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\t\tArgs:\n\t\t    value: the integer to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not a digit\n\t    ValueError: if the given value is not convertible to int.\n\t\"\"\"\n\tif not"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif re.match(Validator.regex, value) == value:\n\t\treturn True\n\traise ValidationError(value)\n\n\n\nclass ValidationError(Exception):\n\t\"\"\"\n\tErrors class representing validation exceptions.\n\t\n\tAttributes:\n\t    mismatch: a string describing the invalid value.\n\t    regex: the regular expression used to validate the value.\n\t    validator_class_name: the name of the validator class.\n\t\"\"\"\n\n\tdef __init__(self, value, regex=None, validator_class_name"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not isinstance(value, six.string_types):\n\t\traise ValidationError\n\n\tregex = getattr(patterns, 'VALUE_REGEX', patterns.VALUE_REGEX_DEFAULT)\n\tif not re.compile(regex).match(value):\n\t\traise ValidationError\n\n\n#------------------------------------------------------------------------------\n# Attribute Value validators\n#------------------------------------------------------------------------------\ndef validate_oneof(value, attrs):\n    \"\"\"Validate the given value against the corresponding one of the attrs.\n\t\n\tArgs:\n\t    value: the string to validate"}, {"generated_text": "def validate_value(value):\n\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the regular expression.\n\t\"\"\"\n\tif not value:\n\t\traise ValidationError('The value must not be empty.')\n\tif not re.match(\"^(a-z|A-Z|0-9)+(.,)?$\", value):\n\t\traise ValidationError('Wrong value: %s' % value)\ndef validate_length(value):\n\t\"\"\"\n\tValidate the given length value against the corresponding regular expression.\n\t\n\tArgs:\n\t    value: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given value is not conform to the"}]}
{"task_id": "62b86a4fb4d922cb0e688cf7", "prompt": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\timport re\n\ttest_key = key\n\ttest_key = test_key.replace(\" \", \"\").strip()\n\ttest_key = re.sub(\"[^0-9A-Za-z]\", \"\", test_key)\n\tif len(test_key) <= 0:\n\t\traise ValidationError(\"ERROR: '%s' does not contain a valid email.\" % key)\n\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match(validate.REGEXP_KEY, key):\n\t\traise ValidationError('Invalid key: %s' % key)\n\t\ndef validate_name(name):\n\t\"\"\"\n\tValidate the given name against the corresponding regexp.\n\t\n\tArgs:\n\t    name: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given name is not valid.\n\t\"\"\"\n\tif not re.match(validate.REGEXP_NAME, name):\n\t\traise ValidationError('Invalid name: %s"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\timport re\n\n\tpattern = r'^[0-9a-zA-Z]{6}$'\n\tmatches = re.match(pattern, key)\n\n\tif not matches:\n\t\traise ValidationError('The given key is not valid. It is %s.' % (key))"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif key is None:\n\t\treturn\n\tif re.match(\"[0-9a-zA-Z_]*\", key) is None:\n\t\traise ValidationError(\"Invalid key. Must contain at most one underscore and a maximum of 64 characters.\")\n\ndef validate_user(user):\n\t\"\"\"\n\tValidate the given user name.\n\t\n\tReturns:\n\t    None: if the given user is valid (no ``None`` values).\n\t    String: otherwise.\n\t\"\"\"\n\tif user is None:\n\t\treturn\n\ttry:\n\t\t"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match(r'(?!^)([A-Za-z0-9_]|^)', key):\n\t    raise ValidationError(ValidationError.BAD_CHAR, \"Bad character in key: {}\".format(key))\n\nclass KeyValue(object):\n\t\"\"\"\n\tClass representing a key value pair.\n\t\n\tEach KeyValue has the following attributes:\n\t* key -> The value's name\n\t* value -> The value's value\n\t\"\"\"\n\t\n\tdef __init__(self, key, value):\n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match(KEY_REGEXP, key):\n\t\traise ValidationError(\"Regular expression {} does not match the given key.\".format(KEY_REGEXP))\n\n\ndef validate_data(data):\n\t\"\"\"Verify that the given data is an array containing only strings or not.\n\t\n\tArgs:\n\t    data: The data that will be validated.\n\t\n\tRaises:\n\t    ValidationError: if the given data does not contain the keys or not an array.\n\t\"\"\"\n\tif isinstance(data, list):\n\t\tif not"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not key.strip():\n\t\traise ValidationError(\"Key cannot be an empty string.\")\n\t\n\ttry:\n\t\tre.compile(str(key))\n\texcept:\n\t\traise ValidationError(\"Key %s is not valid.\" % key)\n"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tregex = validate_key_regex.regex\n\tif regex.match(key) is not None:\n\t\treturn\n\traise ValidationError(gettext(\"The given key '%(key)s' does not match the specified validation regex '%(regex)s'\") % {\"key\": key, \"regex\": regex.pattern})\n\n\nif __name__ == \"__main__\":\n    test_regex()\n    test_validate_key()"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\tif not re.match(constants.REGEX_KEY, key):\n\t\traise validate.ValidationError('The format of given key is not as required.')"}, {"generated_text": "def validate_key(key):\n\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\n\tArgs:\n\t    key: the string to validate\n\t\n\tRaises:\n\t    ValidationError: if the given key is not conform to the regular expression.\n\t\"\"\"\n\ttry:\n\t\tre.match(regex, key)\n\texcept:\n\t\traise ValidationError('Invalid email [%s]. Please use the form \"email@email.com\".' % key)\n\nclass DummyForm(forms.Form):\n\t\"\"\"Dummy form, the default for user to input name or email and submit request.\"\"\"\n\tfrom_name_or_email = forms.CharField(widget=forms.TextInput(attrs={'class':'form-control', 'placeholder':'\u8bf7\u8f93\u5165'}))\n\tto"}]}
{"task_id": "62b86a01b4d922cb0e688ccc", "prompt": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\t# Copy \"manifest\" property to \"observer_schema\" property (required for serialization!)\n\tobserver_schema_dict = _copy_kubernetes_resource_manifest_properties(manifest_dict, 'observer_schema')\n\t\n\t# Generate each level, step by step, recursively\n\tlevels = ['level1', 'level2', 'level3']\n\t\n\tfor level in levels:\n\t\t# Convert a flatlist into a dictionary\n\t\tif 'items' in manifest_dict:\n\t\t\t# TOD"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t# Some checks for manifest_dict\n\tassert isinstance(manifest_dict, dict) and all(\n\t\t(isinstance(k, str) and isinstance(v, str) and k.isalnum()) for k, v in manifest_dict.items())\n\n\t# Convert list and dict to single dict\n\t# The key of the dictionary is the kubernetes field name\n\t# In case of first_level dictionary (complete observed schema), keys in\n\t# manifest_dict are used as kubernetes fields names and field values are replaced by\n\t# the manifest file"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\tif len(manifest_dict) > 0:\n\t\tobj_id = manifest_dict.pop('id')\n\t\tversion = manifest_dict.pop('apiVersion')\n\t\tkind = manifest_dict.pop('kind')\n\t\n\tif first_level:\n\t\t# Copy only identifying fields from manifest dict\n\t\tmanifest_dict = {key: value for key, value in manifest_dict.items() if value is not None}\n\telse:\n\t\t# Copy non-dict fields from manifest dict\n\t\tmanifest_dict = {key: value for key,"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\tobservation_schema = dict()\n\t\n\t# Generate observation_schema and replace non-dict/list values with None\n\tfor key, value in list(manifest_dict.items()):\n\t\t# If dict, check if we are at the \"observer_schema\" level and store directly\n\t\tif isinstance(value, dict):\n\t\t\t# Only look at first level or complete observer_schema for resources\n\t\t\tif (first_level\n\t\t\t    or value[\"kind\"]!= \"PodList\"\n\t\t\t    or value[\"metadata\"][\""}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t# TODO: Use OrderedDict\n\t# TODO: Handle nested resources\n\t# TODO: Handle lists of lists\n\tif first_level:\n\t\t# Copy identifying fields from the manifest file (as there is no\n\t\t#'resource_type' in each manifest file, a resource has to be created with\n\t\t# type 'Pod')\n\t\tif \"metadata\" in manifest_dict.keys():\n\t\t\tif \"name\" in manifest_dict[\"metadata\"]:\n\t\t\t\tobserver_schema = {\n\t\t\t\t\t\"name\":  manifest_dict[\"metadata\"][\""}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif \"kind\" not in manifest_dict.keys():\n\t\traise InvalidArgumentException(\"Cannot generate observer_schema object without a kind\")\n\t\n\t# If firstLevel is false, create new dict with current scope for later recursion\n\tif not first_level:\n\t\tnew_manifest = dict()\n\t\n\t# Recursion starts here\n\tfor k, v in manifest_dict.items():\n\t\tif (\"kind\" in k and k!= \"kind\") or (\"version\" in k and k!= \"version\") or (\"metadata\" in k and k!= \""}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\tobserver_schema_dict = {}\n\t\n\tif manifest_dict.get(\"name\"):\n\t\tobserver_schema_dict[\"name\"] = manifest_dict[\"name\"]\n\t\n\tif manifest_dict.get(\"kind\"):\n\t\tobserver_schema_dict[\"kind\"] = manifest_dict[\"kind\"]\n\t\n\tif manifest_dict.get(\"apiVersion\"):\n\t\tobserver_schema_dict[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n\t\n\tif not first_level:\n\t\tif manifest_"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\tif first_level:\n\t\tfor key in [\"kind\", \"apiVersion\", \"metadata\"]:\n\t\t\tif key in manifest_dict:\n\t\t\t\treturn manifest_dict\n\t\tassert False, \"Expected 'first_level' to be set to ``True``\"\n\tfor key in [\"type\", \"subtype\"]:\n\t\tif key in manifest_dict:\n\t\t\treturn {\n\t\t\t\t\"fields\": {\n\t\t\t\t\tkey: manifest_dict[key]\n\t\t\t\t}\n\t\t\t}\n\tassert False, \"Unknown key in ``manifest_dict``: {}\".format(repr("}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t# If the first one is not a list, convert it to a list\n\tif first_level:\n\t\tif not isinstance(manifest_dict, list):\n\t\t\tmanifest_dict = get_list_from_dict(manifest_dict)\n\tnew_dict = {}\n\tfor key in manifest_dict.keys():\n\t\tif not isinstance(manifest_dict[key], dict):\n\t\t\tnew_dict.update({key: manifest_dict[key]})\n\t\telse:\n\t\t\tnew_dict.update(generate_default_observer_sche"}, {"generated_text": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\"\"\"\n\tTogether with :func:``generate_default_observer_schema_list``, this function is\n\tcalled recursively to generate part of a default ``observer_schema`` from part of a\n\tKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\t\n\tArgs:\n\t    manifest_dict (dict): Partial Kubernetes resources\n\t    first_level (bool, optional): If True, indicates that the dictionary represents\n\t        the whole observer schema of a Kubernetes resource\n\t\n\tReturns:\n\t    dict: Generated partial observer_schema\n\t\n\tThis function creates a new dictionary from ``manifest_dict`` and replaces all\n\tnon-list and non-dict values by ``None``.\n\t\n\tIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n\tresource), the values of the identifying fields are copied from the manifest file.\n\t\"\"\"\n\t\n\tif first_level:\n\t\t# If a resource has a \"kind\", we must override the kind of the observer_schema\n\t\t# to avoid conflicts. But the manifest must not be a complete observer schema of\n\t\t# a Kubernetes resource (i.e. not a list nor a dict), because the key \"kind\" is\n\t\t# mandatory in that case.\n\t\tassert(\"kind\" not in manifest_dict or manifest_dict.get(\"kind\") is None)\n\t\t\n\t\t# If a resource has a \"spec\", we must copy the value of the manifest \""}]}
{"task_id": "62b869ebb4d922cb0e688cc6", "prompt": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tif len(response) == 0 or len(response[0]) == 0:\n\t\treturn\n\tif len(observer_schema) == 0:\n\t\treturn\n\tfor observer_schema_field in observer_schema:\n\t\tfor element in response:\n\t\t\tif element['metadata']['uid'] == observer_schema_field['id']:\n\t\t\t\tfor observer_field in observer_schema_field['fields']:\n\t\t\t\t\tif len(last_applied_manifest) == 0:\n\t\t\t\t\t\tif len(observer_field['value']) >"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\tfor field in current_fields_in_obs:\n\t\tif not field in last_applied_manifest:\n\t\t\t# This field was missing so we add it to the partial ``last_applied_manifest``\n\t\t\tif field!= last_applied:\n\t\t\t\t# We set its value, but we're not dealing with the rest of last_applied\n\t\t\t\t# so we set the rest as well as the value for the field\n\t\t\t\tlast_applied_manifest[field] = response[field]\n\tfor observed_key, observed_value in observer_sche"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tmanifest_schema = {\n\t\t'status':'ready',\n\t\t'items': [],  # empty list\n\t}\n\tinit_list_of_observed_manifests_not_in_k8s_schema(manifest_schema, False)\n\tinit_list_of_observed_manifests_not_in_k8s_schema(response, True)\n\tlast_applied_manifest_for_update = {\n\t\t'status':'ready',\n\t\t'items': [],  # empty"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor schema_json in observer_schema:\n\t\tif \"update_last_applied_manifest_list_from_resp\" not in schema_json:\n\t\t\tcontinue\n\n\t\ttry:\n\t\t\tschema_json[\"update_last_applied_manifest_list_from_resp\"](\n                    last_applied_manifest, response)\n\t\texcept KeyError:\n\t\t\tpass\n\n\ndef update_last_applied_manifest_dict_from_resp(\n        last_applied_manifest_dict, response, resp_json):\n    for"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field not in response:\n\t\t\tresponse[field] = last_applied_manifest[field]\n\tfor i in xrange(len(response)):\n\t\tif 'items' in response[i]:\n\t\t\tif isinstance(response[i]['items'], list):\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tresponse[i]['items'], observer_schema[i], response[:i+1])\n\t\t\tif isinstance(response[i]['items'"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema:\n\t\tif field not in response:\n\t\t\tif field is not None:\n\t\t\t\tcontinue\n\t\t\terror_msg = \"Missing `%s` from response:\\n%s\" % (field, pp_list(response))\n\t\t\traise AssertionError(error_msg)\n\t\t\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = response[field]\n\n\ndef update_last_applied_manifest_dict_from_resp(\n\t\"\"\"\n\tCaclulates"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t):\n\tfor field in observer_schema:\n\t\tif field not in response:\n\t\t\tif field in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field] += 1\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[field] = 1\n"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t# for i in response:\n\t# \tif i['action'] in ['ADDED', 'MODIFIED']:\n\t# \t\t# update last_applied_manifest\n\t# \t\tif i['type'] not in observer_schema:\n\t# \t\t\tlast_applied_manifest.append(dict(field=i['fieldName'], value='', old_value=''))\n\t# \t\telse:\n\t# \t\t\t# update last_applied_manifest\n\t# \t\t\tif i['type'] == 'INTEGER'"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor item in last_applied_manifest:\n\t\tfor observed_field in observer_schema:\n\t\t\tfor k8s_object in response:\n\t\t\t\tif observed_field in k8s_object.info:\n\t\t\t\t\titem[observed_field] = k8s_object.info[observed_field]\n\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\t# There are no known fields in the json-schema response,\n\t\t\t\t\t# thus we can not handle the case, i.e.: no more observed\n\t\t\t\t\t# fields in the JSON response, and hence, we add the"}, {"generated_text": "def update_last_applied_manifest_list_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (list): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (list): partial ``observer_schema``\n\t    response (list): partial response from the Kubernetes API.\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tmanifest := []\n\tfor p_field, p_name in observer_schema.items():\n\t\tif p_name not in (\"name\",):\n\t\t\tif p_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[p_name] = []\n\t\t\tlast_applied_manifest[p_name].append(p_field)\n\t\t\tif p_name in response:\n\t\t\t\tresponse_field = response[p_name].split(\",\")\n\t\t\t\tfor r_field in response_field:\n\t\t\t\t\tobserved_field"}]}
{"task_id": "62b869eab4d922cb0e688cc5", "prompt": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in observer_schema['required']:\n\t\tif isinstance(observer_schema['required'][field], list):\n\t\t\tfound_applicable = False\n\t\t\tfor schema_entry in observer_schema['required'][field]:\n\t\t\t\tif isinstance(schema_entry, basestring):\n\t\t\t\t\tvalid_field = schema_entry\n\t\t\t\t\tschema_found = True\n\t\t\t\telse:\n\t\t\t\t\t# not a string, assume this is a schema entry object\n\t\t\t\t\tschema_found = True\n\t\t\t\tif (\n\t\t\t\t\t\"schema\" in response\n\t\t\t\t\t"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field in response['metadata']['annotations']:\n\t\tif field not in (k for (k, g) in observer_schema if g == 'observed'):\n\t\t\tcontinue # skip fields that have not been observed\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = response\n\n\tlast_applied_manifest['manifest'].update(response['spec'])\n\tlast_applied_manifest['metadata']['labels'] = response['metadata']['labels']\n\n"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\tfor field in observed_fields:\n\t\tif last_applied_manifest.get(field) is None:\n\t\t\tvalue = response.get(field)\n\t\t\tif value is not None:\n\t\t\t\tlast_applied_manifest[field] = value\n\t\t\telse:\n\t\t\t\traise KeyError(\"No value for {} in {}\".format(field, response))\n\t\n\treturn last_applied_manifest\n\ndef update_last_applied_manifest_list_from_resp(\n\t\n\tresponse\n);\n"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t    # kubernetes observed fields. Used in update_last_applied_manifest_dict_from_resp\n\t    observed_fields = {\n              \"spec\": {\n                  \"target_ref\": {\n                      \"apiVersion\": \"v1\",\n                      \"kind\": \"ConfigMap\",\n                      \"name\": \"kube-apiserver-config-managed-kubeadm-control-plane\",\n                      \"namespace\": \"kube-system\",\n                  },\n                  \"idle_timeout_seconds\": {\"value\": 3600},\n              },"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor observed_field in observable_fields:\n\t\tif observed_field in response:\n\t\t\tobserved_field_type = type(observed_field)\n\t\t\tif observed_field_type is int:\n\t\t\t\tvalue = response[observed_field]\n\t\t\telif observed_field_type is str:\n\t\t\t\tvalue = observe(response[observed_field])\n\t\t\telse:\n\t\t\t\traise RuntimeError('Value type of \"%s\" is not supported for %s' %\n\t\t\t\t                      (observed_field, observed_field_type))\n\t\t\t# update the observed fields\n"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tdef _init_response_val_and_last_applied_manifest_dict(observed_field,\n\t                                                      response_val,\n\t                                                      last_applied_manifest_dict):\n\t\t_init_response_val_and_last_applied_manifest_dict(observed_field,\n\t\t                                                  response_val)\n\t\tif observed_field not in last_applied_manifest_dict:\n\t\t\tlast_applied_manifest_dict[observed_field]"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor observed_field in observed_fields:\n\t\ttry:\n\t\t\tlast_applied_manifest[observed_field] = observer_schema[observed_field]['value']\n\t\texcept KeyError:\n\t\t\traise KeyError('KeyError (observe field: \"{}\") not present in '\n\t\t\t   'schema.'.format(observed_field))\n\treturn\n\ndef remove_resource_spec(resource_spec):\n\n\tif resource_spec is not None:\n\t\ttry:\n\t\t\twith open(resource_spec, 'w') as fr"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t):\n        if (observed_field not in response) :\n            return\n\t\t\n        response[observed_field] = []\n\t\t\n        for _, observation in observer_schema.iteritems():\n            try:\n                response[observed_field].append(int(observation[last_applied_field]))\n                \n            except (TypeError,ValueError):\n                logger.info(\"{0}: skipped because observed field {1} has unknown value \"\n                            \"{2}\".format(cloned_instance[\"path\"],observed_"}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\t# update the observed field's value in the last_applied_manifest dict\n\tfor key_pair in LAST_APPLIED_MANIFEST_KEY_PAIR:\n\t\tobserved = key_pair[0].lower()\n\t\tnew_applied_path = key_pair[1]\n\t\tapplied = observer_schema.get_applied(observed)\n\t\tnew_applied = response.get(new_applied_path, None)\n\n\t\t# do not touch keys that are not in the Kubernetes schema\n\t\tif applied =="}, {"generated_text": "def update_last_applied_manifest_dict_from_resp(\n\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\n\tfunction is called recursively to update a partial ``last_applied_manifest``\n\tfrom a partial Kubernetes response\n\t\n\tArgs:\n\t    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n\t        updated\n\t    observer_schema (dict): partial ``observer_schema``\n\t    response (dict): partial response from the Kubernetes API.\n\t\n\tRaises:\n\t    KeyError: If the observed field is not present in the Kubernetes response\n\t\n\tThis function go through all observed fields, and initialized their value in\n\tlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfor field_name, field_value in observer_schema['schema'].items():\n\t\tif obs_var_name(field_name) == obs_var_name(field_name) and field_value['type'] == 'object':\n\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tif obs_var_name(field_name) not in response['manifests']['items']:\n\t\t\t\tcontinue \n\t\t\tlast_applied_manifest = update_last_applied_manifest_dict_from_resp(\n"}]}
{"task_id": "62b869eab4d922cb0e688cbf", "prompt": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tobserverents = set(app.observerents) - set(observer_keys)\n\tfor observerent in observerents:\n\t\tobserver_schema = app.observerent_schemas[observerent]\n\t\tfor schema_property in observer_schema.get_props():\n\t\t\tif schema_property not in app.prog_observer_schemas:\n\t\t\t\tapp.prog_observer_schemas[schema_property] = schema_property\n\t\tfor schema_property in observer_sche"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tfor resource, schema in resources_to_observer_schemas[app]:\n\t\tobserver_schema(app, {\n\t\t\tresource: schema\n\t\t})\n\ndef _parse_config_file(app, manifest_location):\n\t\"\"\"\n\tRead in the ``manifests/`` location for the specified K8s application and\n\tretrieve the location of the configuration file and resources.\n\n\tArgs:\n\t\tapp (krake.data.kubernetes.Application): The K8s application for which to\n\t\t\tretrieve configuration and config-file information\n"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tpass\n\n\ndef generate_default_observer_resources(app):\n\t\"\"\"\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        list of observer resources\n\t\"\"\"\n\tpass\n\n\ndef generate_default_observer_objects(app):\n\t\"\"\"\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        list of observer objects\n\t\"\"\"\n\tpass\n\n\ndef get_observers_path("}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tapp.model.manifest['spec'] += \"\\ntype : kubernetes_observer 'observer_generator';\\n\"\n\tapp.model.manifest['properties'] += generate_properties(app)\n\tapp.model.manifest += \"\"\"\n// Properties with the following options are only supported for JSON and YAML.\n//   Type: string (property type: object)\n//   Format: date-time (property key: metadata.creationTimestamp)\n//   Format: float (property key: spec.replicas)\n//   Format: int64"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tif app.observer.default_observer_schema:\n\t\treturn\n\n\tdef get_types(key):\n\t\t\"\"\"\n\t\tFind the types that a given kubernetes type is represented in the data model. The first value of the tuple is the `type` and the second is the `kind` of the resource.\n\n\t\tExample:\n\t\t\t{\n\t\t\t\t\"configMap\": [\"Pod\", \"v1.ConfigMap\"],\n\t\t\t\t\"deployment\": [\"Pod\", \"apps.v1.Deployment\"],\n\t\t\t}\n\n\t\t\tReturns:\n\t\t\t{\n\t\t\t\t\"Pod"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tresources_in_default_observer_schema = {resource for resource in _parse_resource_names(app.spec.manifest)\n\t\tif \"defaultObserver\" not in resource.lower() and resource not in resources_not_in_default_observer_schema}\n\n\tdefault_observer_schema = set()\n\tdef process_resource(resource):\n\t\tis_a_custom_default_observer = (\"defaultObserver\" not in resource.lower())\n\t\t\n\t\tif not is_a_custom_default_observer and resource not in"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tfor resource_type in [\"Service\", \"Deployment\", \"ReplicationController\", \"ReplicaSet\", \"Pod\", \"ResourceQuota\", \"Role\", \"RoleBinding\"]:\n\t\tapp.generate_observer_schema_for_resource(resource_type)\n\n\n@step(r'I generate the default subresource schema for each subresource present in (.+)\\.?')\ndef generate_default_subresource_schema(step, namespaces):\n\t\"\"\"\n\tGenerate the default observer subresource schema of a given resource type for\n\tspecified namespaces ("}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tif not hasattr(app,'schema_entries'):\n\t\treturn\n\n\tfor name, entry in app.schema_entries.items():\n\t\t# skip all existing observer schemas\n\t\tif 'observer' in entry.attributes:\n\t\t\tcontinue\n\n\t\t# generate default-only observer schema for the Kubernetes resource\n\t\tdefault_schema = {\n\t\t\t'type': 'object',\n\t\t\t'properties': {\n\t\t\t\t'attributes': {\n\t\t\t\t\t'type': 'object',\n\t\t\t\t\t'properties': dict(entry.attributes.items())\n"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\tfor namespace in filter(app.spec.manifest.filter(type=\"kubernetes\"), namespace_filter):\n\t\tfor name, resource in namespace.spec.manifest.filter(type=\"kubernetes\"):\n\t\t\tfor version, versioned_resource in resource.grouped.iteritems():\n\t\t\t\t# get the resource\n\t\t\t\tresource = app.spec.get_resource(type=\"kubernetes\", name=name, version=version)\n\t\t\t\tif resource is None or resource.schema is None:\n\t\t\t\t\tcontinue\n\t\t\t\t# find fields with default and extract and generate"}, {"generated_text": "def generate_default_observer_schema(app):\n\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in\n\t``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\n\tArgs:\n\t    app (krake.data.kubernetes.Application): The application for which to generate a\n\t        default observer schema\n\t\"\"\"\n\t\n\tdefault_observer_schemas = []\n\tdefault_observer_schema_name = '{}-default-observer-schema'\n\t\n\tfor resource in app.data.schemas:\n\t\t# Resources may not specify custom observer schemas\n\t\tif resource.get('observer_schema', None) is None:\n\t\t\tobserver_schema_name = default_observer_schema_name.format(resource['kubernetes'])\n\t\t\tapp.model.observers.add_schema(observer_"}]}
{"task_id": "62b43427903eeb48555d3ea5", "prompt": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tout_style_params_from_in_style = functools.partial(_format_params, param=True)\n\n\treturn sql, [\n\t\t(out_style_params_from_in_style(param) if params is None\n\t\t else out_style_params_from_in_style(param),) for param in params\n\t]\n\ndef build_params(params, names=None):\n\t\"\"\"(:meth:`.SQLParams.build_params` implementation)\n\t\n\tBuild :class:`~collections.abc.Mapping`"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tparam_string, params = _convert(params)\n\tparam_reformat = _reformat(in_style)\n\t_SQLParams_reformat.append(param_reformat)\n\treturn param_string, params\n        \ndef _convert(params):\n\tif isinstance(params, Mapping):\n\t\tif len(params) == 0 or not isinstance(params, Mapping):\n\t\t\traise ValueError('_convert: Expect a dictionary or a sequence of parameters.')\n\t\t\n\t    #if not isinstance(params, Mapping):"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t) as sql_format,\n\n# ----------------------------------------------------------------------\n# Parse a SQL query to find ':param...'' style parameter specifications\n# ----------------------------------------------------------------------\n\ndef param_parser(sql):\n\t\"\"\"\n\tReturns the positional parameters in the sql query.\n\t\n\tParameters are mapped from :class:`str` to :class:`int`\n\trepresentations of parameter values.\n\t\n\tReturns a :class:`set` (possibly empty).\n\t\"\"\"\n\tin_style  = ctypes.c_int()\n\tparam_dict = _Dict(None"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tif isinstance(params, Mapping):\n\t\tparams = params.items()\n\telif not params:\n\t\tparams = []\n\telse:\n\t\traise SQLParamsInvalid(\"Expected a sequence or mapping mapping\")\n\n\tparams = tuple(\n\t\t(p, None) for p in params\n\t)\n\n\tdef _format(sql):\n\t\tif isinstance(sql, bytes):\n\t\t\treturn sql.decode()\n\t\telif isinstance(sql, str):\n\t\t\treturn sql\n\t\telse:\n\t\t\traise TypeError(\"sql must be bytes or str\")\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t# In-style\n\tif (\n\t\tnot hasattr(params, '__len__') or\n\t\t(hasattr(params, '__iter__') and not isinstance(params, (basestring, bytes)))\n\t):\n\t\tif hasattr(params, 'keys'):\n\t\t\traise TypeError(\n\t\t\t\t'Params must be a mapping or an iterable, not {0}'\n\t\t\t\t.format(type(params))\n\t\t\t)\n\t\telif hasattr(params, 'get'):\n\t\t\tparams = dict(params)\n\t\telse:\n\t\t\t"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tassert \"sql\" not in kwargs\n\n\tif params is None:\n\t\treturn (\n\t\t\tsql,\n\t\t\tOrderedDict(),\n\t\t)\n\n\telse:\n\t\tif isinstance(params, dict):\n\t\t\tif (type(params) == OrderedDict) and (params.values() == []):\n\t\t\t\treturn (\n\t\t\t\t\tsql,\n\t\t\t\t\tOrderedDict(),\n\t\t\t\t)\n\n\t\t\tif (type(params) == OrderedDict) and (len(params) == 1):\n\t\t\t\tparams = params.values()[0]\n\n\t\t\telse:\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\t)\n\n@_typecheck(name=str)\ndef __new__(type, name):\n\treturn super().__new__(type)\n\n\nclass SQLParams(object):\n\t\"\"\"Params for SQL Query\"\"\"\n\n\t_dict = None\n\n\t@classmethod\n\tdef __new__(cls, *unpack):\n\t\tif not unpack:\n\t\t\traise E(None, _(\"parameters must not be provided\"))\n\n\t\tif unpack == [[]]:\n\t\t\t# Empty\n\t\t\treturn None\n\n\t\tif isinstance(unpack[0], ("}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tfrom six import b, string_types\n\tassert set(ins.keys()) <= set(params.keys())\n        for i, param in enumerate(params):\n            if isinstance(params, string_types):\n                params[i] = int(param)\n            else:\n                params[i] = ins[param]\n\treturn SQLParams.format(sql, params)\n\n@dec\ndef format_params(params):\n\t#print('params', params)\n\tparamset = set()\n\tfor i, param in enumerate(params):\n"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tfrom sqlalch.ast.utils import build_named_parameter_from_default_or_map\n\treturn sqlalchemy.__sqlalchemy__.parse(textwrap.dedent(sql).strip()), build_named_parameter_from_default_or_map(params)\n\n#: Default :class:`tuple` of positional and named :class:`tuple` objects.\nSQLParams.NAMED_PARAMETER = NamedParameter(\n\t'*{name}*',   #: :class:`str` for positional"}, {"generated_text": "def format(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\tcontains the set of in-style parameters. It maps each parameter\n\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t*params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       The set of converted out-style parameters (:class:`dict` or\n\t        :class:`list`).\n\t\"\"\"\n\tif not isinstance(sql, (bytes, str_type)):\n\t\traise TypeError('SQL query must be bytestring or string.')\n\t# Check if it is a named parameter query.\n\tif in_style == 'NAMED':\n\t\t# Prepare the map type.\n\t\tif not isinstance(params, dict):\n\t\t\traise TypeError('The SQL query parameter map must be a dict or list.')\n\t\t# Construct the NamedParameter query and append it.\n\t\tquery = '@NAMEDPARA@'.join(':' + k for k in"}]}
{"task_id": "62b43428903eeb48555d3eaa", "prompt": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tif not many_params: return sql, params\n\tif many_params.__class__ is bytes: many_params = unicode(many_params)\n\tparams = list(many_params.replace('?', None).replace(';',None).split('&'))\n\tif params[0][:1] not in ['\\'', '\"'] and params[0].find('\\'')!= -1 and params[0].find('\"')!= -1: params[0] = params[0][1:]\n\tif many_params.__class__ is unic"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tif isinstance(params, (Mapping, Sequence)):\n\t\treturn re_quote_plus.sub(ur'\\1\\1', formatmany(sql, list(params.items()))), list_to_dict(params)\n\telse:\n\t\treturn sql, params\n\nre_quote = re.compile(r'(\\\\n?(?=[\\[]|])|\")')\nre_quote_plus = re.compile(r\"(\\\\n?(?=[[]|,;])|\\\\u+)\")\n\ndef param_to_val("}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tparams_values = getattr(many_params, many_params.func.__name__, None)\n\tif isinstance(params_values, collections.abc.Mapping):\n\t\tparams_values = [params_values.copy() for _ in range(num)]\n\telse:\n\t\tparams_values = [params_values]*num\n\tassert len(params_values) == num, \\\n\t\t\"{} *params* must be a list whose length is equal to *num*\"\\\n\t\t.format(many_params.func.__name__)\n"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tif PY3:\n\t\ttext = sql.decode()\n\telse:\n\t\ttext = sql\n\tmany = _SQLParams.format(many_params) + \";\"\n\tsql = _SQLParams.parse_many(text) + many\n\treturn sql, many\n\n\nclass _SQLParams(collections.abc.MutableMapping):\n\t\"\"\"\n\tParameters converter.\n\t\n\t-       *in_style* (:class:`str` or :class:`~enum.Enum` with names:\n\t                   \"NAM"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tin_style = self._getboolean_parameter('in_style')\n\t_db = self._get_connection()\n\tparams = _db_call_wrapper(self._db_connection, _db.params)(self._db_connection, _SQL_FORMATMANY, sql, many_params)\n\treturn _format_params(params, in_style)\n\n########################################################################################################################\n\n########################################################################################################################\n\n########################################################################################################################\n\n########################################################################################################################\n\n########################################################################################################################\n\n########################################################################################################################\n"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tdef _format(\n\t\tsql:str,\n\t\tmany_params,\n\t\tparams,\n\t\t) -> Tuple[str]:\n\t\tif in_style == _Params.NAMED:\n\t\t\tmany_params_ = []\n\t\t\tfor many_params_item, many_params_keyset\\\n\t\t\t\tin _group_many(many_params, keyset=_keyset):\n\t\t\t\tmany_params_.extend(\n\t\t\t\t\t_list_many_params_out(\n\t\t\t\t\t\tvalues_ = _value_as_dicts(many_params_item),\n\t\t\t\t\t\t"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tres = bytearray()\n\tmany_params = []\n\tmany_params_raw = []\n\n\tsql = copy(sql)\n\n\tif many_params_raw is not None:\n\t\tmany_params_raw_l = []\n\n\t\tfor _many_params in many_params_raw:\n\t\t\tmany_params_raw_l.append(list(_many_params))\n\t\t\tmany_params_raw_l.append(many_params_raw_l[-1][1])\n\t\tmany_params_raw = many_params_raw_l\n\n\t\t"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\twith self.lock:\n\t\tquery, params = self.parse(request.form['sql'], many_params)\n\t\tlogger.debug('SQL Params: {query}'.format(**locals()))\n\t\tfrom sqlalchemy import types\n\n\t\tmany_params = list(many_params)\n\n\t\tif params is None:\n\t\t\treturn [query.statement], []\n\t\telse:\n\t\t\tvalues = {p: v for p, v in zip(params, many_params)}\n\t\t\tparam_map = {self.types[p]: v for p, v in"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\tout_params = []\n\tparam_count = 0\n\tfor param in many_params:\n\t\tif len(out_params) / param_count > 100:\n\t\t\tout_params.append([])\n\t\t\tparam_count = 1\n\t\ttry:\n\t\t\tout_params[param_count].append(param)\n\t\texcept IndexError: pass\n\t\tparam_count += 1\n\tout_params = tuple(out_params)\n\tsql = string.Template(formatstyle)\n\treturn sql.safe_substitute({'sql': out_params})\n"}, {"generated_text": "def formatmany(\n\"\"\"\n\tConvert the SQL query to use the out-style parameters instead of the\n\tin-style parameters.\n\t\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\t\n\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\tof in-style parameters (*params*).\n\t\n\t-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t        contains the set of in-style parameters. It maps each parameter\n\t        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t        *params* must be a :class:`~collections.abc.Sequence`.\n\t\n\tReturns a :class:`tuple` containing:\n\t\n\t-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\t\n\t-       A :class:`list` containing each set of converted out-style\n\t        parameters (:class:`dict` or :class:`list`).\n\t\"\"\"\n\treturn _sql_query(sql=sql, many_params=many_params, dict_type=list,\n\t\tout_style=True)\n\n# Format queries\n\n\ndef formats(sql, *many_params):\t\t\n\tr\"\"\"\n\tformat(sql, *many_params)\n\n\t*sql* (:class:`str` or :class:`bytes`) is the SQL query. The *many_params*\n\tare converted to a set of :class:`tuple`'s containing the\n\t:class:`dict`'s from *sql"}]}
{"task_id": "62b45df05108cfac7f2109ce", "prompt": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\treturn self.check(path)\n\ndef warnIfInvalid(self, path):\n\t\"\"\"\n\tGiven a path, return (ocfl, invalidate) (both booleans) if it is valid,\n\tand if not valid returns (False, None) where True was reported to be invalid\n\tand None is to be returned.\n\t\"\"\"\n\tocfl, invalidate = self.check(path)\n\tif invalidate:\n\t\treturn True, None\n\telse:\n\t\treturn False, None\n\ndef warnIfNotExists(self, path):\n\t"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\ttry:\n\t\tif self._load(path):\n\t\t\t# we don't need to do anything\n\t\t\treturn True\n\t\telse:\n\t\t\t# if we're missing data we'll get the data from OCFL\n\t\t\tpass\n\texcept Exception, e:\n\t\tlogging.error(str(e))\n\n\treturn False\n\n\n\t\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif not self.__osroot:\n\t\tprint(\"No OCFL root specified.\")\n\t\treturn False\n\tif self.__osroot!= ROOTD:\n\t\tself.__initdb()\n\tif self.__osroot == ROOTD:\n\t\tif not path:\n\t\t\tprint(\"No path specified.\")\n\t\t\treturn False\n\t\tresult = self.__validator.validate(self.__db, path)\n\t\tif not result or not result.values():\n\t\t\tprint(\"[WARNING] Path %s not valid (root %s)\"\n\t\t\t\t% (path,"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif self.parent == self.pyfs._root_dir:\n\t\t\tself.path = path\n\telse:\n\t\tself.path = os.path.abspath(path)\n\t\n\tfrom ocfl.core.validatebase import validate\n\n\tvalid = validate(self)\n\t\n\treturn valid\n\n@_method__get_base_path\ndef get_base_path(self):\n\t\"\"\"\n\tGet base path for OCFL object.\n\n\tReturns:\n\t\tpath: OCFL object's base path.\n\t\"\"\"\n\tif not has"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tif os.path.isabs(path) and len(path)==1:\n\t\tself.valid_path = True\n\t\tself.valid = True\n\t\treturn True\n\ttry:\n\t\tres = self.checkFile(path)\n\t\tif self.valid:\n\t\t\tself.valid_path = res\n\t\treturn res\n\texcept Exception as exc:\n\t\tlogger.error(\"OCFL Validate: \", exc)\n\t\tif isinstance(self.logger, type(logger)):\n\t\t\tself.__class__.logger = self.logger."}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tassert self.fs.isdir(path), \"validate() given non-directory path\"\n\treturn self.ocfl.validate(path)\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.ocfl.validate(path, 1, 0)\n\t\treturn True\n\texcept validate.OCFLValidationError:\n\t\treturn False\n\n\nclass OCFLPlugin(dcmk.plugin.Plugin):\n\t\"\"\"\n\tPlugin class that uses OCFL to validate data files.\n\t\"\"\"\n\tconfig = None\n\tdef __init__(self, pdmodule, config):\n\t\t\"\"\" \n\t\t\t@param pdmodule: PrefixDict to read plugin configuration from.\n\t\t\"\"\"\n\t\tsuper(OCFLPlugin, self).__init"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tfrom dlna import Validator\n\treturn Validator().validate(self, path)\n\ndef clean(path):\n\t\"\"\"\n\tRemove directory/file/contents.\n\t\n\tReturns True in case of successful removal, False otherwise.\n\t\"\"\"\n\tfrom dlna import Validator\n\ttry: Validator().clean(path)\n\texcept: pass\n\treturn False\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\tprint \"OCFL validation starting at %s\" % os.path.abspath(path)\n\tif self.is_pyfs:\n\t\treturn self.fs.valid(path)\n\treturn True\n"}, {"generated_text": "def validate(self, path):\n\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\t\n\tReturns True if valid (warnings permitted), False otherwise.\n\t\"\"\"\n\t\n\treturn PathValidator(path).isValid()\n'''\n"}]}
{"task_id": "62b45df15108cfac7f2109dc", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif self._is_invalid():\n\t\treturn 'invalid'\n\tif self._is_good():\n\t\treturn 'valid'\n\treturn 'partial'\n\n\nclass _Validate(object):\n\t\"\"\"\n\tProvide common validation routines.\n\t\"\"\"\n\n\tdef _get_path(self):\n\t\t\"\"\"\n\t\tThis will be a string of the form\n\t\t\"/path/to/parent/directory/myfile.myextension\".\n\t\tIf validation is successful, then the return value will match.\n\t\tNote: the 'path' value does not have a"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn '' if self.validate_all else prefix + 'OK' if not self.errors else prefix + 'ERROR'\n\n\nclass Validator(object):\n\t\"\"\"\n\tValidate a function, returning whether it passes any validation.\n\t\"\"\"\n\tValidator = type('Validator', (object,), {\n\t\t'test': False,\n\t\t'validate': set(),\n\t})\n\n\tdef __init__(self, test=False, *args, **kwargs):\n\t\tself.test = test\n\n\t@classmethod\n\tdef add(cls, *"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif self.check_status == VALIDATE_PASSED:\n\t\treturn prefix + 'VALIDATION PASSED'\n\telif self.check_status == VALIDATE_FAILED:\n\t\treturn prefix + 'VALIDATION FAILED'\n\telif self.check_status == VALIDATE_NOT_APPLICABLE:\n\t\treturn prefix + 'VALIDATION NOT APPLICABLE'\n\telif self.check_status == VALIDATE_UNKNOWN:\n\t\treturn prefix + 'VALIDATION UNKNOWN'\n\telif self.check_status"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif self.errors:\n\t\treturn '%s%s\\n%s' % (prefix, ', '.join(self.errors), prefix)\n\n#--------------------------------------------------------------------------\ndef is_validator(cls):\n\tif (not inspect.isclass(cls) \\\n\tor not issubclass(cls, ValidatedValidationMixin) \\\n\tor not hasattr(cls, 'validators') \\\n\tor len(cls.validators) == 0):\n\t\traise ValidationError('Expected class to be a Validation class, or to"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif self.is_valid():\n\t\treturn prefix + 'Valid'\n\telif self.is_validation_error():\n\t\treturn prefix + 'Validation error'\n\telif self.is_syntax_error():\n\t\treturn prefix + 'Syntax error'\n\telif self.is_validation_error():\n\t\treturn prefix + 'Validation error'\n\telse:\n\t\treturn prefix + 'Unknown'\n\n\ndef get_all_schemas():\n\treturn jsonschema.RefResolver.search(schema_files, __name"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn \"%s %s\" % (prefix, self.status)\n\ndef show_status(prefix=''):\n\t\"\"\"\n\tOutput all validators, with optional prefix.\n\t\"\"\"\n\tprint \"Validators:    %s\" % (prefix)\n\tfor validator in validators:\n\t\tprint \"%s:%s\" % (validator, validator_status(validator, prefix))\n\ndef get_validator_class():\n\t\"\"\"\n\tReturn the validator class with the highest value for the validity_level\n\tproperty. Returns None for invalid configurations"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn (prefix + \n\t    (self.errno and str(self.errno) or '') + \n\t    (self.errmsg and str(self.errmsg) or ''))\n\n\nif __name__ == '__main__':\n\timport doctest\n\tdoctest.testmod()\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif self.isValid():\n\t\treturn 'Valid'\n\telse:\n\t\treturn 'Invalid %s%s' % (prefix, self.error)\n\t\t\nclass Validator(object):\n\tregistry = {}\n\n\t# FIXME Use __metaclass__ for initilisation of the singleton\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tSet the list of validations to perform\n\t\t\"\"\"\n\t\tglobal registry\n\t\t\n\t\tself.error = None\n\t\tfor k in kwargs:\n\t\t\tsetattr(self"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\treturn prefix + (('-' * 2) +\n\t\t\t(('.' * 2) +\n\t\t\t self.state) if self.state else\n\t\t\t'').strip()\n\n\nclass ValidatorStatus(Validator):\n\t\"\"\"\n\tValidate status in case of validator failures.\n\n\tSee :py:class:`yade.utils.common.ValidatableTuple` for more details.\n\n\t.. attribute:: status\n\n\t\tTuple of \"state\" and \"exitStatus\" values,\n\t\tas reported by :py:meth:`yade.utils.common.validator"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string of validator status, with optional prefix.\n\t\"\"\"\n\tif self.fatal:\n\t\treturn prefix + \"fatal\"\n\telif self.err:\n\t\treturn prefix + \"error\"\n\telse:\n\t\treturn prefix + \"warning\"\n\t\t\n\n"}]}
{"task_id": "62b45df15108cfac7f2109dd", "prompt": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn '%s%s: %s\\n' % (prefix, type(self).__name__, self.repr_log())\n\nclass ValidationLog(metaclass=ValidationLogMeta):\n\n\tlog_name = \"validation log\"\n\t\"\"\"\n\tString name of log, used in log messages to identify this log.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tself.log()\n\n\tdef log(self, *items):\n\t\tstr_prefix = ''\n\t\tfor item in items:\n\t\t\tstr_prefix += item + '"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tret = \"\"\n\tif self.errmsg:\n\t\tret += prefix\n\t\tif self.errmsg[0] == 'W':\n\t\t\tret += \"WARN\"\n\t\t\tret += \" \"*len(prefix)\n\t\telif self.errmsg[0] == 'E':\n\t\t\tret += \"ERR   \"\n\t\t\tret += \"E*\"*len(prefix)\n\t\telif self.errmsg[0] == 'I':\n\t\t\tret += \"INFO  \"\n\t\t\tret += \"I*\"*len(prefix)\n\tret += str(self"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tlines = self.log_stream.splitlines()\n\tn = 0\n\twhile lines[n] not in self._validation_lines:\n\t\tn += 1\n\tif n > 0:\n\t\tlines = (lines[:n] + lines[n+1:])\n\tstream = prefix + '.'.join([('%(line)s (%(time)s).'% line) for line in lines])\n\tlog.debug(stream)\n\treturn stream\n\n\nclass ValidationContext(object):\n\t\"\"\"\n\tContext manager for performing (validation)"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn '%s %s' % (prefix, self.status_str_)\n\n# TODO\n# def get_fmt_str(status_str, prefix=''):\n# \treturn '%s %s' % (prefix, status_str)\n\ndef log_str(log_str, prefix=''):\n\t\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tdef write_str(str_, prefix=''):\n\t\tsys.stdout.write('%s%s' % (prefix, str_"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tif not isinstance(prefix, basestring):\n\t\tprefix = str(prefix)\n\tl = list()\n\tfor s in self.info.values():\n\t\tif prefix:\n\t\t\tl.append('  %s - %s' % (prefix, s))\n\t\telse:\n\t\t\tl.append(s)\n\treturn '\\n'.join(l)\n\n\nclass ValidationLog:\n\t\"\"\"\n\t\tCompatible base-class for validation log.\n\n\t\tCatchValidationLog is the common base class, it is used to generate an\n\t\tinstance of Val"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tif self.status.is_valid:\n\t\treturn \"%s: Validation OK\" % prefix\n\telif self.status.is_valid_warning:\n\t\treturn \"%s: Warning in validation\" % prefix\n\telif self.status.is_valid_info:\n\t\treturn \"%s: Info in validation\" % prefix\n\telse:\n\t\treturn \"%s: Failure in validation\" % prefix\n\ndef to_list(it):\n\t\"\"\"\n\tGiven iterator, return a list of all elements.\n\t\"\"\"\n\tl = []\n\tit = iter(it"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\treturn prefix + '\\n' + ''.join([str(e) for e in self.log])\n\n\nclass Invalidator(object):\n\t\"\"\"\n\tInterface for objects that will invalidate objects on a background thread.\n\t\"\"\"\n\n\tdef __init__(self, queue):\n\t\t\"\"\"\n\t\t@param queue: validation queue to use for asynchronous background processing.\n\t\t\"\"\"\n\t\tself.queue = queue\n\n\tdef invalidate(self, queue, validation, object):\n\t\t\"\"\"\n\t\tExecute validation process on specified object, to be done in a\n\t\tseparate thread"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tmsg = []\n\tfor name in dir(_valid):\n\t\tinfo = getattr(_valid, name)\n\t\tif isinstance(info, dict):\n\t\t\tcontinue\n\t\tif info.short_name:\n\t\t\tmsg.append('{} {}:'.format(prefix, name))\n\t\telse:\n\t\t\tmsg.append('{} {}'.format(prefix, name))\n\t\tfor k, v in vars(info).items():\n\t\t\tmsg.append('    {}: {}'.format(k, v))\n\treturn '\\n'.join(msg)\n\n\ndef status"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tif not hasattr(self, 'validations'):\n\t\treturn ''\n\n\tlog = '\\%s: %s' % (prefix, self.validations.values())\n\treturn log if len(log) > 0 else None\n\n"}, {"generated_text": "def status_str(self, prefix=''):\n\"\"\"\n\tReturn string representation of validation log, with optional prefix.\n\t\"\"\"\n\tpflg = 0\n\tif not prefix:\n\t\tprefix = ''\n\tdef __inner(self):\n\t\tpflg = 0\n\t\tif self.error:\n\t\t\tpflg = 1\n\t\tif self.warning:\n\t\t\tif pflg==0:\n\t\t\t\tyield' '*pflg + '(' + 'warning' + ') %s:'% prefix\n\t\t\telse:\n\t\t\t\tyield' '*pflg + '>'+ '%s:'% prefix\n\t\t\tyield self.message\n\t\telse:\n\t\t\t"}]}
{"task_id": "62b45e135108cfac7f2109f4", "prompt": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True #pylint: disable=not-callable # never raise a warning on this\n\n\n\n###\n###  Private methods.\n###\n\n\n\ndef _compute_identity_token(identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tCompute the given identifier, in order to be able to use it as a token in the database.\n\t\"\"\"\n\tif not isinstance(identifier, string_types):\n\t\traise TypeError(\"Identity token: given identifier is not a string\")\n\n\tif len(identifier) >"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\n@python_2_unicode_compatible\nclass BaseIDGenerator(IDGenerator, metaclass=abc.ABCMeta):\n\t\"\"\"\n\tIDGenerator mix-in that provides common functionality for BaseIDGenerators.\n\t\"\"\"\n\n\tdef __init__(self, identifier='id'):\n\t\t\"\"\"\n\t\tCreate a new IDGenerator instance with given identifier:\n\n\t\t* identifier: The identifier to use as the primary ID.\n\t\t\"\"\"\n\t\tself.identifier = identifier\n\n\tdef is_valid(self, identifier):  # pylint"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\nclass Validator(ValidatorInterface):  # pylint: disable=too-few-public-methods\n\t\"\"\"\n\tValidates a given field.\n\t\"\"\"\n\tdef __init__(self, schema, **kwargs):\n\t\t\"\"\"\n\t\tInitialize a Validator instance.\n\n\t\t:param schema: Schema to check the provided configuration against.\n\t\t:type schema: Schema\n\t\t:param kwargs: List of kwargs to pass to the ValidatorInterface constructor.\n\t\t:return:\n\t\t\"\"\"\n\t\tsuper(Validator"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\nclass ValidIdentifier(ValidIdentifierBase):\n\t\"\"\"\n\tBase class for validation logic that verifies if a string is a valid\n\tidentifier.\n\t\"\"\"\n\n\tdef is_valid(self, identifier):\n\t\t\"\"\"\n\t\tReturns True if str is an identifier.\n\t\t\"\"\"\n\t\tif ':' in identifier or '.' in identifier:\n\t\t\treturn False\n\t\treturn super(ValidIdentifier, self).is_valid(identifier)\n\n\nclass ValidBibs(ValidIdentifierBase):\n\t\"\"\"\n\tBase class for validation logic that ver"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\nif \"__main__\" == __name__:\n\t# Run doctests (un-mainline)\n\timport doctest\n\tdoctest.testmod()\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\n@register(True)\nclass IsTrue(BaseIsValid):\n\t\"\"\"\n\tIs True\n\t\"\"\"\n\n\tdef is_valid(self, identifier):\n\t\t\"\"\"\n\t\tReturn True if identifier is True, always True in this base implementation.\n\t\t\"\"\"\n\t\treturn True\n\n\n@register(False)\nclass IsFalse(BaseIsValid):\n\t\"\"\"\n\tIs False\n\t\"\"\"\n\n\tdef is_valid(self, identifier):\n\t\t\"\"\"\n\t\tReturn True if identifier is False, always True in this base implementation.\n\t\t\"\"\"\n\t\treturn False"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn False\n\n\nclass User(Person):\n\t\"\"\"\n\tSimple implementation of a simple user, without email or password.\n\t\"\"\"\n\tdef __init__(self, name):\n\t\t\"\"\"\n\t\tSet the username, from the name passed, as a string.\n\t\t\"\"\"\n\t\tsuper().__init__(\"email@email.com\")\n\t\tself._name = name\n\n\t@property\n\tdef name(self):\n\t\t\"\"\"\n\t\tThe user's name, from the name property.\n\t\t\"\"\"\n\t\treturn self._name\n\n\t@name.setter\n\tdef name"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\nclass NIST(BaseProvider):\n\t\"\"\"\n\tHandle NIST cryptographic domains, according to the Credential Domains\n\tstandard, with a particular emphasis on the US.\n\n\tThis provider will not be tested on the European implementations or any\n\tdomains outside of the US.\n\n\t:param domain: The domain to retrieve, e.g. \"SECP256RL\"\n\t:type domain: str or bytes\n\n\t:param issuer_id: The Credential Registry ID to authenticate with, or None\n\t:type issuer_id: str or bytes\n\n"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\nclass DefaultValidator(Validator):\n\t\"\"\"\n\tA default validator that returns True if the identifier is not valid according to other\n\tconfigured validators and returns False otherwise, except that if the identifier is valid,\n\tit also adds a warning to the validation messages. \n\n\tYou can pass several validators of this type to the Validator constructor (it's a chain).\n\t\"\"\"\n\t#pylint: disable=redefined-variable-type\n\tdef __init__(self, *validators):\n\t\t#pylint: disable=bad"}, {"generated_text": "def is_valid(self, identifier):  # pylint: disable=unused-argument\n\"\"\"\n\tReturn True if identifier is valid, always True in this base implementation.\n\t\"\"\"\n\treturn True\n\n\nclass Validator(BaseValidator):\n\t\"\"\"\n\tValidator implementation that validates the identity of names.\n\n\tThis extends :class:`.BaseValidator` by validating that the\n\tidentifier matches the regular expression :data:`config.IDENTIFIER`.\n\n\t:param str message: Message to display to the user\n\t:param str format: Format string\n\t\"\"\"\n\n\tdef __init__(self, message=\"must be an Identifier\", format=\"{value} is not a\"):\n\n\t\tsuper().__init__(\n\t\t\tdata=FORM"}]}
{"task_id": "62b45e145108cfac7f210a07", "prompt": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif not isinstance(inventory, dict):\n\t\terror(u'spec.yaml is not a valid inventory type')\n\tif extract_spec_version:\n\t\tself.spec_version = yaml.safe_load(yaml.dump({'version': int(version),'spec': self.spec}, Dumper=DictDumper))['version']\n\t\tif not isinstance(self.spec_version, int) or self.spec_version > SpecVersion.current():\n\t\t\terror(u'Incorrect specification version: %s' % \n\t\t\t\tSpecVersion."}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\t\n\tinventory_string = self.data_string(inventory)\n\tparse_result = yaml.safe_load(inventory_string)\n\tif parse_result is None:\n\t\treturn \n\t\n\tif isinstance(parse_result, str):\n\t\treturn\n\t\n\tself.validate_yml(parse_result, self.version, \"Inventory\", validate_type=validate_type)\n\t\n\tif extract_spec_version:\n\t\tself.validate_yml(parse_result, \"v\" + str(self.spec"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif extract_spec_version:\n\t\tif isinstance(self.type, basestring):\n\t\t\ttry:\n\t\t\t\tself.spec_version = SpecVersion(self.type)\n\t\t\texcept AssertionError:\n\t\t\t\treturn False\n\t\telif isinstance(self.type, SpecVersion):\n\t\t\tself.spec_version = self.type\n\t\telse:\n\t\t\traise TypeError('Expected type string, or SpecVersion, got %s' % type(self.type))\n\t\t\n\treturn self.spec_version not in INVALID_SPEC_VERSIONS\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif not self.spec_version or extract_spec_version:\n\t\tself.spec_version = self.get_spec_version(inventory)\n\tif inventory.type=='' or not is_valid_type(inventory.type):\n\t\terr_msg = '<' + self.tag + ': \"group\":'+ \"type_\" + '\"/' + \"contains\" + '\" not specified or invalid: '\n\t\tinventory.type = 'unassigned' + '.' + 'type'\n\t\tself.error = err_msg + error_str("}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif not isinstance(inventory, type(self.inventory_dict)):\n\t\tmsg = \"Expected an object of type '{0}', but it's actually a '{1}'\"\n\t\tLOG.critical(msg.format(type(self.inventory_dict).__name__, type(inventory).__name__))\n\t\treturn False\n\tif extract_spec_version:\n\t\tif not isinstance(self.spec_version, str):\n\t\t\tself.spec_version = self.get_from_inventory(self.inventory, \"spec_version\", None)\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tpass\n\n\n\t\nclass InventoryBase(object):\n\t\"\"\"\n\tBase class for all inventories.\n\t\n\tIt provides:\n\t- basic functionality for getting a copy of the inventory from a filename\n\t- a 'validate' function to look for missing fields\n\tIt adds following attributes to the instance:\n\t- type: the name of the inventory\n\t\"\"\"\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tCreate a new inventory object.\n\t\t\n\t\tFor every inventory, a reference will be kept to the object that created it.\n\t\tThis is used for"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tfor file_name in self.find_files(inventory):\n\t\t_ = inventory.get(file_name)\n\t\tfor task in self.filter(file_name, inventory):\n\t\t\tfor dep in task.deps:\n\t\t\t\tfor (module_name, module_version) in inventory.get(file_name).get(task.name).get('module',{}).items():\n\t\t\t\t\tself.verify_module_version(task, module_name, version=module_version)\n\t\t\t\tfor (task_name, task_version) in inventory.get(file_"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif (self.type is not None and not is_numeric(self.type)) or (self.source is not None and not is_numeric(self.source)):\n\t\traise ConfigError(\"Invalid type or source\")\n\t\t\n\telif self.type is not None and extract_spec_version:\n\t\tif not is_valid_spec_version(parse_spec_version(self.type)):\n\t\t\traise ConfigError(\"%s is not a valid specification version number\" % self.type)\n\t\t\n\t\tself.spec_version = parse_spec_version"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tlog.info('Tests validating %s' % self.name)\n\t\n\twith self.lock:\n\t\ttry:\n\t\t\tif not isinstance(inventory, list):\n\t\t\t\treturn False\n\t\t\tfound = False\n\t\t\tfor i in inventory:\n\t\t\t\tif i is not None and len(i) > 0 and 'uuid' in i:\n\t\t\t\t\tfound = True\n\t\t\tif found:\n\t\t\t\treturn True\n\t\texcept ESUCCESS:\n\t\t\treturn True\n\t\t\t\n\t\t# Inventory does not exist\n\t\treturn False\n"}, {"generated_text": "def validate(self, inventory, extract_spec_version=False):\n\"\"\"\n\tValidate a given inventory.\n\t\n\tIf extract_spec_version is True then will look at the type value to determine\n\tthe specification version. In the case that there is no type value or it isn't\n\tvalid, then other tests will be based on the version given in self.spec_version.\n\t\"\"\"\n\tif self.spec_version is None:\n\t\tself.set_spec_version(inventory, extract_spec_version)\n\n\telif extract_spec_version:\n\t\t# check if we're an inventory versioned against the current spec-version\n\t\tcurrent_version = Version(self.spec_version)\n\t\tinventory_version = self.get_version(inventory, 'inventory_id')\n\t\tif int(inventory_version) < current_version:\n\t\t\traise VersionError(\"Inventory version '%s' is greater than the current spec version: '%s'\" % ("}]}
{"task_id": "62b45e145108cfac7f210a09", "prompt": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tprint \"Digests used:\", digests_used\n\tmanifest_with_missing_digests = []\n\tfor f, ds in digests_used.items():\n\t\tself.expected_checksum_for_file(f, ds)\n\t\tif ds not in manifest_files:\n\t\t\tmanifest_with_missing_digests.append(f)\n\t\t\tprint '{} missing from manifest file list'.format(f)\n\t\telse:\n\t\t\tself.assert_checksum_for_file(f, ds)\n\tself.assertE"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tif digests_used is None:\n\t\tdigests_used = set()\n\n\t# If no digests required, don't do anything.\n\tif not manifest_files:\n\t\treturn digests_used\n\n\t# Check we have a manifest we can verify.\n\tfpath = (os.path.join(self.config['WORK_DIR'], manifest_files[0]))\n\tst = os.stat(fpath)\n\tif not stat.S_ISREG(st.st_mode):\n\t\traise RuntimeError('File %s is not a regular file"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\t_, manifest_digest = self.blobstore.check_data_blob_exists(generate_manifest_digest(json.dumps(self.manifest)))\n\t\tfor digest in digest_list:\n\t\t\tif digest in manifest_files[manifest_file][\"digests\"]:\n\t\t\t\tif digest not in digests_used:\n\t\t\t\t\tfor error in manifest_files[manifest_file][\"digests\"][digest][\"errors\"]:\n\t\t\t\t\t\tyield (\"Unused digest '{"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\t# get manifest file name that would be used if no digests present\n\t\t(dirpath, base_name) = os.path.split(manifest_file)\n\t\tif dirpath:\n\t\t\tself.log_info(\"removing dirpath:\", dirpath)\n\t\t\t# delete manifest if path exists\n\t\t\tif os.path.exists(dirpath):\n\t\t\t\tself.log_info(\"path exists\")\n\t\t\t\tos.rmdir(dirpath)\n\tassert os.path.exists(os.path.join"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor md_file in manifest_files:\n\t\tfor digest in str(md_file).split(':'):\n\t\t\tif not digest in digests_used:\n\t\t\t\tself.fatal(\n\t\t\t\t\t\"Digest %s is not present on the system.\\n Please install OpenSSL 1.0.1f and \"\n\t\t\t\t\t\"later or see %s/openSSL-1.0.1f-%s/README for instructions how to install them.\" %\n\t\t\t\t\t(digest, DIGEST_DIR, get_version(self._opts)))\n\n\tfor digest"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor file_manifest_name in manifest_files:\n\t\tfor digest in digests_used:\n\t\t\ttry:\n\t\t\t\treturn not manifest_files[file_manifest_name].get(digest, False)\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\treturn True\n\n\ndef read_strict_block_checksum_file(path):\n\t\"\"\"\n\tRead the checksum of a strict block checksum file\n\n\t@param path: Path to the strict block checksum file.\n\t@type path: str\n\t@return: The read checksum\n\t@"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tdigests_missing = (manifest_files[\"image_ids\"] |\n\t\t\t    set(manifest_files[\"image_refs_not_found\"]) -\n\t\t\t    set(digests_used))\n\tself.assertEqual(0, len(digests_missing),\n\t\t\t \"Needed manifest_digests are missing: %s\" %\n\t\t\t list(digests_missing))\n\n\tdigests_unused = (digests_used[1:] |\n\t\t\t   hashlib.algorithms_available -\n\t\t\t   digests_used[1:"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor manifest_file in manifest_files:\n\t\tdigests_from_manifest = [self.hash_name_to_digest(hash_name) for hash_name\n\t\t\tin manifest_file.get(\"Hashes\", {}).get(\"sha256\", [])]\n\t\tdigests_from_manifest.extend(manifest_file.get(\"hashes\", {}).get(\"sha256\", []))\n\t\tif digests_from_manifest == digests_used:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\treturn True\n\n\t\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tfor digest in reversed(digests_present):\n\t\tif digest in manifest_files:\n\t\t\tif digest in digests_used:\n\t\t\t\tyield digest\n"}, {"generated_text": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\"\"\"\n\tCheck all digests in manifest that are needed are present and used.\n\t\"\"\"\n\tall_digests = [f.digests for f in manifest_files]\n\tif self.force_checksum:\n\t\talldigests = all_digests[:]\n\t\talldigests.append(set(digests_used))\n\t\treturn sum(set(all_digests) & set(digests_used.keys())) == len(all_digests) and any(item in digests_used.values() for item in alldigests)\n\telse:\n\t\treturn len(all_digests) == len(set(all_dig"}]}
{"task_id": "62b45e165108cfac7f210a16", "prompt": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif not isinstance(prior, InventoryValidator):\n\t\tmessage = \"Prior parameter must be an InventoryValidator instance. \"\n\t\tmessage += \"Current object has type %s\" % str(type(self))\n\t\traise TypeError(message)\n\tvalid = True\n\tif self.priorversion!= prior.priorversion:\n\t\tmessage = 'Prior version %s is different than current version %s' %\\\n\t\t\t (prior.priorversion, self.priorversion)\n\t\tvalid = False\n\tif not valid:\n\t\tcurrent_inventories"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t# Check for prior version and for the right metadata\n\tif prior.version!= self.version: # 1.3.X -> 1.4.X\n\t\traise InputException('Incompatible prior version %s vs. %s' % (prior.validate_to_str(),self.__name__))\n\t# If self is prior, it is assumed to be validated by the prior; same if prior is self\n\ttry:\n\t\tif not isinstance(prior.metadata, MetaData):\n\t\t\traise InputException('Error in prior format: prior metadata is not an input object"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tself.validator.validate_prior_version(prior)\n\"\"\"\nEnd InventoryValidators.validate_as_prior_version\n===============================================================================\n\"\"\"\n"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tprior_ver = self._prior_version_to_string_and_error(prior)\n\tthis_ver = self.version\n\tif not this_ver:\n\t\treturn\n\tif this_ver.version!= prior_ver.version:\n\t\tprior_id = \"%s_%s\" % (prior_ver.key, prior_ver.value)\n\t\tthis_id = \"%s_%s\" % (this_ver.key, this_ver.value)\n\t\tinvalid_versions_keys_list = [\"%s_%"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\tassert isinstance(prior, InventoriesValidator)\n\tassert isinstance(prior, InventoryValidator)\n\t\n\t# Perform a quick check against prior's list of inventory types\n\t# In case the inventory types of prior and current differ, this check will\n\t# catch any possible inconsistencies\n\tif'mesh' in prior.type and'mesh' not in self.type: \n\t\tsys.exit('Inventory type mismatch: prior is type mesh but current has type tetgen')\n\tif'mesh3d' in prior.type and '"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif prior is None:\n\t\treturn True # there is always a prior version\n\t\n\tif prior.get_version() is None:\n\t\treturn False\n\t\n\tif prior.get_version() not in self.get_valid_versions():\n\t\treturn False\n\t\n\tprior_inventory = prior.convert()\n\t\n\t# if self was a prior version and prior has changed, this is a new version\n\tif prior.get_version() == self.get_version():\n\t\tself.set_version(None)\n\t\treturn False\n\t\n\t"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tassert(isinstance(prior, InventoryValidator))\n\t\n\tif prior.version > self.version:\n\t\tmsg = \"Validation error: prior version %d is greater than version %d\" % (prior.version, self.version)\n\telif prior.version < self.version:\n\t\tmsg = \"Validation error: prior version %d is greater than version %d\" % (prior.version, self.version)\n\telif prior.timestamp < self.timestamp:\n\t\tmsg = \"Validation error: prior timestamp %d is greater"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tif prior.product_map is not None:\n\t\tfor product_id, prior_version in prior.product_map.items():\n\t\t\tif isinstance(product_id, basestring):\n\t\t\t\tprior_version = prior.product_map[product_id]\n\t\t\t# A version of the product does exist - but if not the prior\n\t\t\t# version should also not exist\n\t\t\tif prior_version is None:\n\t\t\t\traise ValueError(\"The prior version in '%s' does not exist\" % prior)\n\t\t\tif product_id in self.product_map.keys"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\tpass"}, {"generated_text": "def validate_as_prior_version(self, prior):\n\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object.\n\t\n\tThe input variable prior is also expected to be an InventoryValidator object\n\tand both self and prior inventories are assumed to have been checked for\n\tinternal consistency.\n\t\"\"\"\n\t\n\tself.validators.append( prior.validate_as_prior_version)\n\treturn self.validate(self, prior)\n\ndef validate(self, prior=None):\n\t\"\"\"\n\tCheck the state of self and prior for internal consistency.\n\tIf previous validation has failed, return\n\ta tuple of validation error messages and errors.\n\t\n\tThe input prior is optional and if provided, validated against\n\tit as specified by the object's validate_as_prior_version.\n\t\"\"\"\n\t\n\tself_validators = []\n\t"}]}
{"task_id": "62b45e165108cfac7f210a17", "prompt": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t# Find logical paths in state for version in inventory\n\t# (This is equivalent to the output of\n\t#   show_state.get_inventory_matrix(inventory, version)\n\tlogical_path_map = {}\n\tfor logical_path in inventory.iter_logical_paths_for_version(version):\n\t\t\n\t\t# Get the set of files that are referenced by this directory in the\n\t\t# state.\n\t\treferenced_files = inventory.referred_files_of_logical_path(logical_path)\n\t\t\n\t\t#"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t# FIXME: We could make this a lazy load with something like a task\n\tlogical_path_map = {}\n\tfor version in inventory.versions(version):\n\t\tfor path, _file in version.get_files().items():\n\t\t\tlogical_path = path.join('/').replace('//', '/')\n\t\t\tlogical_path_map[logical_path] = set()\n\treturn logical_path_map\n\n\ndef _find_logical_paths(inventory, version, path, logical_path_map):\n\t\"\"\"\n\tFind all"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor path, state in inventory.get_logical_path_state(version).items():\n\t\tpath_parts = re.split(r'\\$', path)\n\t\tif len(path_parts) == 1:\n\t\t\t# This is a path relative to the manifest\n\t\t\trelative_path = path_parts[0]\n\t\t\tabsolute_path = os.path.join(manifest_directory, relative_path)\n\t\t\tif relative_path == path_parts[0]: # Is this path absolute?\n\t\t\t\treferenced_"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tassert version >= 0\n\tassert 0 < version < max_revision\n\n\tlogical_path_to_files = {} # logical_path_in_state -> set(content_files)\n\t\n\t# Walk through the revisions in reverse order, for efficiency.\n\tfor revision in range(version, -1, -1):\n\t\tif revision < 0:\n\t\t\traise StandardError(\"Error: get_logical_path_map() asked to traverse \"\n\t\t\t\t\t\t\t\t\"revision %d, but the inventory contains no version \"\n\t\t\t\t\t\t\t\t\"- can't happen...\" % (revision+"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tpathmap = {}\n\n\t# Iterate over every item in inventory.\n\tfor vse in inventory:\n\t\t# Get the state of all members of this version.\n\t\tpathmap_in_version = get_logical_path_map(vse, version)\n\t\t# Get pathnames of all files in all member states.\n\t\tpathmap_in_version = set(pathmap_in_version) - set([''])\n\t\t# Add to the map of logical paths.\n\t\tpathmap.update(pathmap_in_version)\n\n\t# Remove the empty string"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlogical_to_files = {}\n\t\n\t# Get a dictionary of logilogical to filesystem paths\n\tlogical_paths = inventory[version].get_all(File)\n\tlogical_paths_id = dict([(ln, fi.id) for ln, fi in logical_paths.items()])\n\t\n\t# Get all the file objects for all the logilogical objects\n\tfor fi in get_file_contents_from_state(inventory, version):\n\t\tlogging.debug(fi)\n\t\tlogical_path = fi"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tlogical_path_map = {}\n\tlogical_path_set = set()\n\tfor version_spec in version:\n\t\tversion_num = version_spec.find(\"=\")\n\t\tif version_num == -1:\n\t\t\tcontinue\n\t\tversion_number = version_spec[:version_num]\n\t\tversion_key = version_spec[version_num + 1:]\n\t\tversion_files = get_logical_path_files(inventory, version_number)\n\t\tlogical_path_map[version_key] = version_files\n\t\tlogical"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t# TODO: this is not as efficient as we could be.\n\t# TODO: there's a ton of duplication with this method...\n\tmapping = {}\n\tfor logical_path, file_states in inventory.items():\n\t\t# NOTE: this logic may have to change as we add new methods\n\t\t# for the inventory, esp. when we do subdirectories\n\t\t# I suppose the only correct logic would be that\n\t\t# the paths don't contain any elements which we want to collapse,\n\t\t# including the path. However, the paths will be the directory in which\n"}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\tvmap = {}\n\tfor resource in inventory:\n\t\tres_name = resource['name']\n\t\tif'versions' in resource:\n\t\t\tvmap[res_name] = {}\n\t\t\tfor version in resource['versions']:\n\t\t\t\tif version['name'] == version:\n\t\t\t\t\tif 'files' in version:\n\t\t\t\t\t\tvmap[res_name][res_name + \":\" + version['name']] = \\\n\t\t\t\t\t\t\tset([f['name'] for f in version['files']])\n\t\t\t\t\telse:\n\t\t\t\t\t\tvmap[res_name][res_name + \""}, {"generated_text": "def get_logical_path_map(inventory, version):\n\"\"\"\n\tGet a map of logical paths in state to files on disk for version in inventory.\n\t\n\tReturns a dictionary: logical_path_in_state -> set(content_files)\n\t\n\tThe set of content_files may includes references to duplicate files in\n\tlater versions than the version being described.\n\t\"\"\"\n\t\n\tlogical_path_map = { }\n\t\n\t# Build an inventory of files in inventory\n\tversion_files = [x[1] for x in inventory.files(version)]\n\t\n\t# Build a map of what's in inventory to what we have\n\tlogical_path_map = { }\n\tfor fcnt in os.listdir(os.path.join(inventory.output_dir, version, 'data')):\n\t\tfpath = os.path.join(inventory.output_dir, version, 'data', fcnt)\n"}]}
{"task_id": "62b45e175108cfac7f210a19", "prompt": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\tif fixity.scheme!= \"SHA1\": raise ValidationException(\"Fixity scheme is not SHA1\")\n\tif fixity.algorithm!= \"SHA1\": raise ValidationException(\"Fixity algorithm is not SHA1\")\n\t\n\tif not isinstance(fixity.digest, str): raise ValidationException(\"Fixity digest is not str\")\n\t\n\tif \"\\r\" in fixity.digest or \"\\n\" in fixity.digest or len(fixity.digest) < 2: raise ValidationException(\"Fixity digest is invalid\")"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\tfrom.util import FixityResult\n\n\tfrom.manifest import Manifest\n\n\tfrom.repo import GitRepo\n\tfrom.rebuild import GitRebuilder\n\t\n\tfrom.manifest import ManifestEntry\n\n\tif len(fixity)!= 1 or not isinstance(fixity, dict):\n\t\traise TypeError(\"'fixity' attribute should point to a single FixityBlock object\")\n\n\tassert len(manifest_files) == 1\n\tfor manifest_file in manifest_files:\n\t\tmatch = manifest_file.re.match(\"/\\*"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tpass\n\n\nclass Fixity(object):\n\t\"\"\"\n\tClass for validating and computing fixity values\n\t\"\"\"\n\tdef __init__(self, hashtype):\n\t\tself.hashval = \"\"\n\t\tself.hashtype = hashtype.lower()\n\n\tdef set_value_hashtype(self, hashtype):\n\t\t# TODO check hashtype\n\t\tself.hashval = hashtype\n\n\nclass HashType(object):\n\t\"\"\"\n\tHashType Enum\n\t\"\"\"\n\tSHA256 = \"sha256\"\n\tNAME = \"name\"\n\t"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\tself.validations.append('Validate fixity block')\n\n\tself.log.debug('Validating %s fixity.' % self.hash)\n\n\tvalidation_result = False\n\tvalidation_issues = []\n\tfor f in fixity:\n\n\t\tif self.config.options.fixity_disable or not f['md5']:\n\t\t\tcontinue\n\n\t\tself.log.debug(\"Validating file %s\" % f['location'])\n\n\t\t# Is the file mentioned in the manifest?\n\t\tif f['location'] not in manifest_files:\n"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tfixity_root = os.path.join(self.data_dir, 'fix')\n\tstructure_dir = os.path.abspath(os.path.join(fixity_root, 'inventories'))\n\tmanifest = Manifest(manifest_files)\n\tstructure_files = [f.path for f in manifest.values()]\n\tfor f in fixity.dir_paths:\n\t\tif f not in structure_files:\n\t\t\traise ValueError(\"Inventory file %s is not referred to by the fixity structure\" %"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tfor name in fixity:\n\t\tif '.' not in name:\n\t\t\traise LdpInventoryValidationException(\n\t\t\t\t\"invalid fixity entry '%s'\" % name)\n\t\tpath, checksum = name.split(':', 1)\n\t\tif path!='manifest.ldp':\n\t\t\traise LdpInventoryValidationException(\n\t\t\t\t\"invalid fixity entry '%s'\" % name)\n\t\tif checksum!='sha1' or '*' in manifest_files:\n\t\t\traise LdpInventoryValidationException(\n\t\t\t\t\"in"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\traise NotImplementedError()\n\n\n# EOF\n"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t\n\t# TODO: use the 'compiler = False', as indicated earlier\n\t# to prevent validation to run for all files in our manifests.\n\t# TODO: return a list of error messages, one for each file.\n\n\t# NOTE: this assumes that all files are listed in the manifest.\n\t# It is likely that this will not hold true since some\n\t# fixity checks do not reference files in the manifest.\n\t# We need to revisit this for 1.0.\n\t\n\tvalidation = validate_structure(fixity)\n\t# validate_st"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\t# 1) list files in the manifest and build a map of files\n\tmanifest_by_name = {}\n\tfor manifest_file in manifest_files:\n\t\tmanifest_file_path = manifest_file['path']\n\t\tmanifest_file_sha = manifest_file['sha']\n\t\tmanifest_file_name = os.path.basename(manifest_file_path)\n\t\tmanifest_by_name[manifest_file_name] = manifest_file_sha\n\tto_check = set([self.path])\n\twhile len(to"}, {"generated_text": "def validate_fixity(self, fixity, manifest_files):\n\"\"\"\n\tValidate fixity block in inventory.\n\t\n\tCheck the structure of the fixity block and makes sure that only files\n\tlisted in the manifest are referenced.\n\t\"\"\"\n\tif not self.config['enable-fixity-data-processing']:\n\t\treturn\n\trequired_tags = self.default_config['required-fixity-tags']\n\tmissing_tags = []\n\tfor tag in required_tags:\n\t\tif not fixity.filter(tag):\n\t\t\tmissing_tags.append(tag)\n\n\tif not missing_tags:\n\t\tfor tag in required_tags:\n\t\t\tfixity.valid_fixity(tag)\n\t\treturn\n\t\n\tlogging.warning(\"Not all required fixity tags are present: %s\""}]}
{"task_id": "62b463153879012d19481498", "prompt": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tassert path\n\n\treturn os.listdir(path)\n\ndef mkdir(*paths):\n\t\"\"\"\n\tCreate the given directories\n\t\"\"\"\n\tfor directory in paths:\n\t\tdirectory = os.path.join(directory, '')\n\t\tassert os.path.exists(directory) or os.path.isdir(directory)\n\t\tif not os.path.exists(directory):\n\t\t\tos.makedirs(directory)\n"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tfor root, dir_names, file_names in os.walk(path):\n\t\tfor file_name in file_names:\n\t\t\tyield os.path.join(root, file_name)"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn [f for f in os.listdir(path) if os.path.isfile(os.path.join(path,f))]\n\ndef find_data(name):\n\t\"\"\"\n\tFind the data files in the `data` sub-directory\n\t\"\"\"\n\treturn files_list(os.path.join(path,'data'))\n\ndef find_model(name,version):\n\t\"\"\"\n\tFind the model files in the `models` sub-directory\n\t\"\"\"\n\treturn files_list(os.path.join(path,'models',version"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tfor filename in listdir(path):\n\t\tif not filename.lower().endswith('.txt'):\n\t\t\tyield filename\n"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tfor filename in os.listdir(path):\n\t\tfull_ofilename = os.path.join(path, filename)\n\t\tstat_ofilename = os.stat(full_ofilename)\n\t\tif (stat_ofilename.st_mode & 0111)!= 0:\n\t\t    yield full_ofilename\n\nclass FilesVisitor(object):\n\tdef __init__(self, filelist):\n\t\tself.filelist = filelist\n\t#def get_files(self, paths):\n\tdef get_files(self):\n"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tdir = os.path.dirname(path)\n\tdir_list = []\n\tfor name in os.listdir(dir):\n\t\tif os.path.isfile(dir + os.sep+name):\n\t\t\tdir_list.append(os.path.join(dir,name))\n\n\treturn dir_list\n\ndef files_dict(path):\n\t\"\"\"\n\tReturns a dictionary containing the list of files at `path`.\n\t\t{ 'files' : [ 'file1',..., 'fileN'],\n\t\t  'folders' : [ '"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tpaths = []\n\n\tif os.path.isdir(path):\n\t\tfiles = os.listdir(path)\n\t\tfor f in files:\n\t\t\tcur_path = os.path.join(path, f)\n\t\t\tif is_binary(cur_path):\n\t\t\t\tcontinue\n\t\t\tpaths.append(cur_path)\n\n\telse:\n\t\tpaths.append(path)\n\n\treturn paths\n\n# -----------------------------------------------------------------------------------\n# Utility\n# -----------------------------------------------------------------------------------\ndef is_binary(filename):\n\t\"\"\"\n\tDetermine if `filename` is binary"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\tfor root, dirs, files in os.walk(path); files:\n\t\tpass\n\t# The directory traversal is not in the Python3.x and not in the\n\t# Python2.x.\n\t# \tdirs = [d for d in dirs if os.path.isdir(os.path.join(root, d))]\n\twith open(path) as f: data = f.readlines()\n\tresults = []\n\tfor line in data:\n\t\tif line[0] == \"#\": continue\n\t\tline = line[:-1"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn [ name for name in os.listdir(path)\n\t    if os.path.isfile(os.path.join(path,name))]\n\n\ndef runpath(r):\n\t\"\"\"\n\tRun `r` and display the output on screen without actually\n\tlaunching an interpreter.\n\t\"\"\"\n\tfrom pprint import pprint\n\tprint(\"Running: \")\n\tprint(\"$ \" + r)\n\ttry:\n\t\tcode.compile_command(r)\n\t\tprint(\"%s => %s\" %\n\t\t\t\t(r,\n\t\t\t\t"}, {"generated_text": "def files_list(path):\n\"\"\"\n\tReturn the files in `path`\n\t\"\"\"\n\treturn [line for line in os.listdir(path) if os.path.isfile(os.path.join(path, line))]\n\n\n# vim: ts=4 sw=4 syntax=python foldmethod=syntax foldback=foldmethod foldlevel=0 smarttab\n"}]}
{"task_id": "62b463153879012d1948149a", "prompt": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\tfor f in files:\n\t\tbase = os.path.basename(f).split('.')[0]\n\t\tif base in xmls:\n\t\t\tcontinue\n\t\txmls[base].append(f)\n\treturn xmls\n\ndef _get_data_with_xml_basename_files(files, basename='Package'):\n\t\n\tdata = {}\n\tfor f in files:\n\t\tbase = os.path.basename(f).split('.')[0]\n\t\tdata[f] = Package(data_filename=f, data_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\treturn {os.path.basename(xml_filename): _package_data_from_file(os.path.join(source, xml_filename), files) for xml_filename in xmls.keys()}\n\n#%% run\nif __name__==\"__main__\":\nif _os.environ['KAGGLE_ROOT']:\n\troot_path = _os.environ['KAGGLE_ROOT']\n\t_data_files =_group_files_by_name(root_path, '/Xml/packages/package_"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tt = time.time()\n\tm = Package()\n\tm.info['filemtime'] = [os.path.getmtime(file) for file in files]\n\tfor fname in files:\n\t\tf = path(fname)\n\t\t# We create the map only if there is at least one file with a particular XML name\n\t\tif f.name in xmls:\n\t\t\tif is_valid_name(f.name):\n\t\t\t\tname = f.name.replace('.xml', '')\n\t\t\t\tm.data['files'].append(name)\n"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\ttry:\n\t\tgrouped_files = {}\n\t\tfor package in package_from(source).packages:\n\t\t\tfor filen in package._files:\n\t\t\t\tif filen.base in xmls:\n\t\t\t\t\tgrouped_files[filen.base] = package\n\t\treturn grouped_files\n\texcept:\n\t\tprint(\"There is something wrong with the XML files\")\n"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tgrouped_files = {}\n\tfor xml_filename in xmls:\n\t\tif os.path.basename(xml_filename) not in [f.replace(\".xml\", \"\") for f in files]:\n\t\t\traise ValueError(\"{0} is not in the list of XML files {1}\".format(xml_filename, files))\n\t\tgrouped_files[os.path.basename(xml_filename)] = os.path.join(source, \"{\" + os.path.basename(xml_filename) + \".packages\"}\")\n\t\t\n\treturn grouped"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tfiles = list(files)\n\tfiles_dict = defaultdict(list)\n\tfor file_ in files:\n\t\tfile_name = os.path.basename(file_)\n\t\t\n\t\t# Get parent folder of file\n\t\tnew = file_\n\t\tnew_name = new\n\t\twhile True:\n\t\t\tnew = os.path.dirname(new)\n\t\t\tnew_name = os.path.basename(new)\n\t\t\tif not new_name:\n\t\t\t\tnew_name = '.'\n\t\t\t\n\t\t\tif file_name == new_name:\n\t\t\t\t"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\tpackage_map = {}\n\tfor filename in files:\n\t\t\txml_name = filename.replace('/', '_')\n\t\t\tif xml_name not in package_map:\n\t\t\t\tpackage_map[xml_name] = [filename]\n\t\t\telif file_is_new(filename, xml_name):\n\t\t\t\tpackage_map[xml_name].append(filename)\n\t\n\treturn package_map\n\nif __name__ == \"__main__\":\n\txmls = \"/Users/danielzeller/Documents/dataset_project/XML"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tdef collect_files(path, dirnames, filenames):\n\t\tfor filename in filenames:\n\t\t\tif xsd_excluded(filename):\n\t\t\t\tpass\n\t\t\telif path in os.path.split(filename):\n\t\t\t\twith open(filename,'r') as fi:\n\t\t\t\t\txml_filename = fi.readline().rstrip('\\r\\n')\n\t\t\t\t\tif xml_filename.startswith('<') and not xml_filename.startswith('<?xml'):\n\t\t\t\t\t\tname = os.path.splitext(filename)[0]\n\t\t\t\t\t\tif"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\tgroups = defaultdict(list)\n\tfor name in sorted(files):\n\t\txml_basename_root = os.path.join(source, name).split(os.sep)[-1]\n\t\tname_base = xml_basename_root.split(\".\")[:2]\n\t\tname_base.append(name)\n\t\tname_no_ext = os.path.splitext(name)[0]\n\t\tname_root = os.path.join(*name_base)\n\t\tsource_name = os.path.join(source, name)\n"}, {"generated_text": "def _group_files_by_xml_filename(source, xmls, files):\n\"\"\"\n\tGroup files by their XML basename\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\txml_filename : str\n\t    XML filenames\n\tfiles : list\n\t    list of files in the folder or zipfile\n\t\n\tReturns\n\t-------\n\tdict\n\t    key: name of the XML files\n\t    value: Package\n\t\"\"\"\n\t\n\t# get basenames of files and their associated packages as a dict\n\tbasenames = dict([(package.basename, package) for package in files])\n\t\n\t# split package basenames into directories and filenames\n\tdirectories = set([os.path.dirname(file) for file in files])\n\tfilenames = set(basenames.keys())\n\t\n\t# create sets for each directory in the project\n\tfor dir in directories - set(path for path in xmls):\n\t\t# add directories to the directory list\n\t\t"}]}
{"task_id": "62b463153879012d1948149b", "prompt": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn 'doc' in str(file_path) and str(file_path.replace(prefix, '')).replace('.doc', '') == str(prefix).replace('.doc', '') and prefix!= \"/\"\n\t\t\t\n\n@job('indexing pipeline')\ndef index_titles(title, document_base_path, corpus_names):\n\t\t\n\tprint(\"indexing for \", title)\n\t\n\tcorpus_document_tuples = []\n\t\n\tif ',' in corpus_names:\n\t\tcorpus_names = corpus_names."}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tprefix = os.path.split(prefix)[1] + '/'\n\tprefix_len = len(prefix)\n\ttry:\n\t\tif (file_path.endswith(prefix) | file_path.endswith(prefix.lower())):\n\t\t\treturn True\n\texcept:\n\t\tfile_path = os.path.normpath(file_path)\n\t\tif (file_path.endswith(prefix) | file_path.endswith(prefix.lower())):\n\t\t\treturn True\n\treturn False\n\n\ndef split_file_by_ext"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tprefix_folder, file_name = os.path.split(prefix)\n\tparts = list(pkg_resources.resource_filename(prefix_folder, file_name).rsplit(\"/\", 1))\n\treturn file_path.split('/')[-1].split('.')[0] == parts[-1]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\treturn os.path.basename(file_path).startswith(prefix + '_')\n\ndef parse_package_version(version_string):\n\t\"\"\"\n\tConverte a version String da forma `major.minor[.patch[.build]][-beta]`\n\tpara um nomea da versao string.\n\t\n\tParameters\n\t----------\n\tversion_string : str\n\t    Version string to convert\n\tReturns\n\t-------\n\tversion : str\n\t\tThe package's version string\n\t\"\"\"\n\t\n\tn"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tif file_path[0]!= '/':\n\t\tfile_path = '/%s' % file_path\n\t\n\tif re.search(('/%s$' % prefix).decode(\"utf-8\"), file_path):\n\t\treturn True\n\t\n\treturn False\n\n\ndef get_package_path(filename):\n\t\"\"\"\n\tEstablece os arquivos relativos das pastas que se encargam para a constru\u00e7\u00e3o\n\tde um pacote de livro/cap, retornando apenas um string correspondente \u00e0"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tdir_path = os.path.dirname(file_path)\n\tpattern = os.path.join(dir_path, \"%s*.*\" % prefix)\n\tif os.path.isdir(dir_path):\n\t\tresult = glob(pattern)\n\telse:\n\t\tresult = glob(pattern + \".*\")\t\n\treturn len(result) > 0\n\n\ndef match_file(file_path, files_to_check=False):\n    \"\"\"\n        Identify each file in `file_path` if it belongs to a package or not\n    "}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\treturn file_path.startswith(prefix)\n\n\ndef extract_version(file_path): r\"\"\"\n\tIdentify version from the filename (`file_path`) \n\t\n\tRetorna : 1st argument verison (eg: `version`)\n\t\t     0th argument version (eg: `1.3.6`)\n\t\n\tParameters\n\t----------\n\tfile_path : str\n\t    Filename\n\t    \n\tReturns\n\t-------\n\ttuple\n\t\tversion (eg: `(1, 3)`)\n\t\"\"\"\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\t\n\tfrom re import I \n\t\n\tpattern = I('.+' + prefix + '.*')\n\treturn bool(re.match(pattern, file_path))\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tif prefix.isdigit() and len(prefix)==3:\n\t\tprefix = str(int(prefix)+1)\n\tfile_path = os.path.normcase(os.path.abspath(file_path))\n\tfor package in [prefix+i for i in range(1,26)]:\n\t\tfilename = os.path.normcase(os.path.join(file_path, package+'.pdf'))\n\t\tif filename == file_path:\n\t\t\treturn True\n\treturn False\n"}, {"generated_text": "def match_file_by_prefix(prefix, file_path):\n\"\"\"\n\tIdentify if a `file_path` belongs to a document package by a given `prefix`\n\t\n\tRetorna `True` para documentos pertencentes a um pacote.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfile_path : str\n\t    File path\n\tReturns\n\t-------\n\tbool\n\t    True - file belongs to the package\n\t\"\"\"\n\tif (file_path.strip('/')).find(prefix) >= 0:\t\t# Si onde estava\n\t\treturn True\n\t\t\n\treturn False\t\n\n# Fun\u00e7ao recupera arquivo\ndef get_file(basename, dir_path):\n\t\"\"\"\n\tIdentify if a `basename` corresponds to an existing file in a given `dir_path`\n\t\n\tParameters\n\t----------\n\tbasename : str\n\t    Filename to match to\n\tdir_path : str\n\t    Directory where search for `basename`\n\t"}]}
{"task_id": "62b463153879012d1948149c", "prompt": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn filter(lambda fil: fil.endswith(prefix), files)\n\ndef select_filenames_in_pkg(pkg, files):\n\t\"\"\"\n\tReturn files list which belongs to a document package.\n\n\tParameters\n\t----------\n\tpkg : dict\n\t    Document package\n\tfiles : str list\n\t    files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to package.txt\n\t\"\"\"\n\tpkg_lines = pkg[\"pkg\"][\"lines\"]\n\treturn [file[len(pkg_lines[\""}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfiltered_files = filter(lambda f: f.startswith(prefix) and len(f) > len(prefix) and f.endswith(\".pdf\"), files)\n\tfiltered_files_nome = map(lambda f: f[len(prefix):-4], filtered_files)\n\treturn sorted(filtered_files_nome)\n\n\ndef write_list_to_file(l, filepath):\n\t\"\"\"\n\tWrite a list to a file.\n\t\n\tWrites a list `l` to a file named `filepath`"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\tprefix_len = prefix.__len__()\n\t\n\tfiles_filenames = []\n\t\n\tfor file in files:\n\t\t#get filename basename\n\t\tbase_name = os.path.basename(file)\n\t\t\n\t\tif base_name.find(prefix) == 0 and base_name.__len__() == prefix_len:\n\t\t\tfiles_filenames.append(file)\n\t\n\treturn files_filenames\n\n\ndef find_all_package_files(directory, files=None, prefix=None, extensions="}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t_debug('Entering _select_files() for prefix=\"{}\"'.format(prefix))\n\t_debug('  files=', repr(files))\n\t\n\tif len(files) == 0:\n\t\t_debug('  files list is empty, returning empty list', 0)\n\t\treturn []\n\t\t\t\t\t\t\n\tif isinstance(files, str):\n\t\tfiles_ = [files]\n\t\t\n\telif not isinstance(files, list):\n\t\traise ValueError('Invalid value in `files` parameter, must be of type string, list of strings, or a list of"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\t\n\treturn [fn for fn in files if fn.startswith(prefix)]\n\ndef select_file_as_document_package(filename):\n\t\"\"\"\n\tDecide which is a file which is in document package.\n\t\n\tReturn True if the file is in document package, False if not.\n\t\n\tParameters\n\t----------\n\tfilename : str\n\t    File path\n\tReturns\n\t-------\n\tBoolean\n\t    True if the file is in document package, False if not\n\t\"\"\"\n\t\n\tis_document_package_ = os.path"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tfiles_list = [os.path.basename(file)[:len(prefix)] for file in files]\n\treturn [element for element in files_list if element in files_list]\n\n\t\ndef get_last_docfiles_paths_prefixes(prefix, filenames_list):\n\t\"\"\"\n\tGet docfiles paths, fileprefixes and fileextensions for a document package.\n\t\n\tRetorna os arquivos da lista `files`, arqprefixes, arqexts\n\t\n\tParameters\n\t----------\n\tprefix :"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn [f for f in files if f.startswith(prefix)]\n\ndef sort_filenames_by_prefix(files, sort_key=\"pk\"):\n\t\"\"\"\n\tsort a list of filenames by their package paths. The sort key is\n\tassumed to be a model property containing the package's primary key.\n\t\n\tThis is a convenient way to get the path of sorted packages.\n\t\n\tParameters\n\t----------\n\tfiles : list\n\t    Filenames to be sorted\n\tsort_key : str \n\t    Column to"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn filter(lambda x: x.startswith(prefix), files) if files else list()\n\t\ndef main():\n\timport argparse\n\tfrom helpers.excecoes import excecao_arquivo_invalido\n\tfrom helpers.excecoes import excecao_execucao_erro\n\t\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-p', '--prefix', \t\t\ttype=str, \trequired=True, \thelp=\"Filename prefix\")\n\tparser.add_argument('"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\treturn [ x for x in files if x.filename.startswith(prefix) ]\n\ndef files_by_type(dirname, filenames):\n\t\"\"\"\n\tSelect only arquivos from filename:\n\t    \n\tRetorna a lista dos arquivos cujos tipos de enderecitos sao diferentes:\n\t>>> files_by_type('/tmp/pdf','/tmp/html/index.html','/tmp/html/chapter1.html')\n\t'html'\n\n\tParameters\n\t----------\n\t"}, {"generated_text": "def select_filenames_by_prefix(prefix, files):\n\"\"\"\n\tGet files which belongs to a document package.\n\t\n\tRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    Filename prefix\n\tfiles : str list\n\t    Files paths\n\tReturns\n\t-------\n\tlist\n\t    files paths which basename files matches to prefix\n\t\"\"\"\n\tprefix = os.path.basename(prefix)\n\treturn [file for file in files if file.startswith(prefix)]\n\n\ndef is_doc_file(filename, dir_list=[DOC_DIR], prefix=None, file_count=0):\n\t\"\"\"\n\tAuxiliary function on `filtering` that uses filename to decide whether\n\tfile belongs to a document package or not.\n\t\n\tDetermines if `filename` points for a path which is a valid\n\tdocuments file. File is considered a doc file if it is a doc\n\tfile"}]}
{"task_id": "62b463153879012d1948149d", "prompt": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tdata = {}\n\tfiles = {}\n\t\n\tfull_prefix = os.path.dirname(folder).split('./', 1)[1]\n\n\tfor root, _, files_ in os.walk(folder):\n\t\tfull_path = os.path.join(root, '*/*.xml').split('/')[1:]\n\t\tfiles[full_path] = os.path.join(*full_path).split('/')[1:]\n\n\tfor full_prefix in sorted(files): #sort by file prefix (excluding folder itself)\n\t\t"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t# Get basename of package\n\tp_basename = os.path.basename(folder).replace(BASEDIR + os.sep, '')\n\t\n\t# Get the number of subfolders\n\tn_subfolders = os.scandir(folder).__len__() if pyper.is_py3 else len(os.scandir(folder))\n\n\t# Check that the folder is a valid folder\n\ttry:\n\t\tfor subdir in os.scandir(folder):\n\t\t\tif subdir.name == 'info':"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfiles = [f for f in os.listdir(folder)\n\t    if os.path.isfile(os.path.join(folder, f))]\n\tpkginfo = dict()\n\tfor f in files:\n\t\tpkginfo[f] = cPickle.load(open(os.path.join(folder, f), 'rb'))\n\treturn pkginfo\n\n\ndef _store_pkginfo(pkginfod, filename, folder):\n    \"\"\"\n\tStore packages' data to file\n\t\n\tS"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfiles = [x for x in os.listdir(folder) if x[0]!=\"#\"]\n\tnames = [(os.path.join(folder, file), file.replace('XML', '').replace('-', '').split('.')[-1]) for file in files]\n\tresult = {}\n\tfor (path, name) in names:\n\t\t\t(info, ) = _read_file(path)\n\t\t\tif info:\n\t\t\t\tresult[name] = info\n\treturn result\n\ndef _get_version_info(folder):\n\t\"\"\"\n\t"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tgrouped = groupby(lambda f:f.basename(), os.listdir(folder))\n\treturn dict([(item[0], [item[1]]) for item in grouped])\n\nif __name__ == \"__main__\":\n\n\tif len(sys.argv) < 2:\n\t\tsys.exit(\"[Error] Not enough arguments: <path_to_root_dir>\")\n\n\tfolder = sys.argv[1]\n\tpkgls = _explore_folder(folder)\n\n\tfor key, values in pkgls."}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\toutput = {}\n\tfor f in glob.glob(str(folder+'/*XML*')):\n\t\tbasename = os.path.basename(f)\n\t\t\n\t\tif(basename in output):\n\t\t\t# already loaded, append\n\t\t\toutput[basename] = json.loads(output[basename])\n\t\telse:\n\t\t\t# load and save\n\t\t\t# this is pretty hacky:\n\t\t\t# we are converting XML file into JSON string, then we need to decode it back.\n\t\t\ttry:\n\t\t\t\toutput[basename] = json.loads("}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfiles = {}\n\tfor filename in os.listdir(folder):\n\t\troot, ext = os.path.splitext(filename)\n\t\tif (ext == '.xml'):\n\t\t\tpackages = pkg.load_all(folder = folder, root = root[:-5])\n\t\t\tfiles[root] = packages\n\treturn files\n\"\"\"\n\n\n##############################################################################\n# Helper functions and classes\nclass DnfQuery(object):\n    \"\"\"\n    Helper class for easy management of a package manager command.\n    \n    Parameters\n    ----------\n    manager :"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tfiles = []\n\t\n\tfor filename in os.listdir(folder):\n\t\tfiles.append(filename)\n\t\t\n\tgrouped = {}\n\tfor filename in files:\n\t\tp = re.match(\"(.*).xml\", filename)\n\t\t\n\t\tif p:\n\t\t\tkey = p.group(1)\n\t\t\t\n\t\t\tgrouped_list = grouped.get(key, [])\n\t\t\tgrouped_list.append(filename)\n\t\t\tgrouped[key] = grouped_list\n\t\t\t\n\treturn grouped\n\t\ndef _merge_file"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t# Get basename and directory of package\n\tpkg_name = os.path.basename(folder)\n\tpkg_dir = os.path.dirname(folder)\n\t\n\t# Get package contents\n\txml_files = fnmatch.filter(os.listdir(pkg_dir), '*.xml')\n\t\n\t# Convert to dict\n\tdata_list = []\n\tfor file in xml_files:\n\t\tdata_list.append(dict(file=file, dir=pkg_dir))\n\t\n\treturn data_list\n\ndef _parse_"}, {"generated_text": "def _explore_folder(folder):\n\"\"\"\n\tGet packages' data from folder\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tfolder : str\n\t    Folder of the package\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tpackages = {}\n\tfor file in os.listdir(folder):\n\t\t(fullname,basename) = os.path.split(file)\n\t\tif basename.endswith(\".pom\"): #ignore folder name and pom files\n\t\t\txml_name = basename[:basename.rfind(\".\")]+\".xml\"\n\t\t\tif xml_name in packages:\n\t\t\t\tpackages[xml_name][\"name\"]+=1\n\t\t\telse:\n\t\t\t\tpackages[xml_name]={\n\t\t\t\t\t\"name\":1,\n\t\t\t\t\t\"name_with_version"}]}
{"task_id": "62b463153879012d1948149f", "prompt": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\ttry:\n\t\tdoc = ET.parse(os.path.join(_data_dir, package_name, filename))\n\t\troot_path = root\n\texcept BaseException as exc:\n\t\t\n\t\tif isinstance(exc, FileNotFound):\n\t\t\ttry:\n\t\t\t\tdoc = _eval_file(prefix, filename)\n\t\t\t\tpackages[\"renditions\"] = {\n\t\t\t\t\t\"extension\": os.path.splitext(filename)[1][1:],\n\t\t\t\t\t\"path\": os.path.dirname(filename),\n\t\t\t\t\t\"relative\": os.path.dirname("}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tlog.debug(\"_eval_files %s %s\", prefix, file_path)\n\t\n\t_type = \"\"\n\t_asset = None\n\t_rendition = None\n\t\n\tif prefix.endswith(\".asset\"):\n\t\t_type = \"asset\"\n\t\t_asset = open(file_path).read()\n\telif prefix.endswith(\".rendition\"):\n\t\t_type = \"rendition\"\n\t\t_rendition = open(file_path).read()\n\t\n\tif not _rendition:"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\txml_root = ET.parse(file_path).getroot()\n\tnode_asset = [n for n in xml_root.findall('asset') if prefix in n.attrib.get('id','')]\n\tnode_rendition = [n for n in xml_root.findall('rendition') if prefix in n.attrib.get('id','')]\n\t\n\tpackages[prefix] = {} if node_asset else True\n\tif node_asset:\n\t\tpackages[prefix]['class'] = node_asset"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t# Carrega o pacote\n\tpackages = {'asset'}\n\t\n\tfile_object = open(file_path, encoding='utf-8', mode='r')\n\tif \"xml\" in file_object.readline():\n\t\tasset_data, asset_path = _xml_handler.readfile(file_object)\n\t\tpackages.add(\"asset\")\n\t\tpackages.add(asset_path)\n\telif (\".js\" in file_path) and (\".css\" not in file_path) and (\".map\" not"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t#identificando o tipo de arquivo\n\tif prefix=='asset':\n\t\t#verifica no arquivo que chama\n\t\tpackage_path = os.path.join(file_path,'package.xml')\n\t\t#verifica se arquivo foi encontrado\n\t\tif os.path.exists(package_path) :\n\t\t\t#processamento do pacote\n\t\t\t(asset_type,asset_path) = _asset_parse_xml(package_path)\n\t\t\tif len(asset_path"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tpackage_types = {\n\t\t\"asset\": _get_asset,\n\t\t\"rendition\": _get_rendition,\n\t}\n\tfile_path = os.path.join(file_path, prefix + \".xml\") \n\tif os.path.exists(file_path):\n\t\tpackage_type = package_types[file_type(file_path)]\n\telse:\n\t\tpackage_type = _get_asset\n\t\tif os.path.exists(file_path[:-8] + \"renderity.xml\"):\n\t\t\t"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tprefix_lower = prefix.lower()\n\tif prefix_lower in _asset_type:\n\t\tpackage_type = 'asset'\n\t\t# _packages[file_name] = {'package': package_type}\n\telif prefix_lower in _rendition_type:\n\t\tpackage_type ='rendition'\n\t\t# _packages[file_name] = {'package': package_type}\n\telse:\n\t\traise Exception(\"Invalid package: {0}\".format(prefix))\n\treturn package_type\n\n\n\nif __name__ == '"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\t# busca no index o tipo\n\ttipo = _get_file_type(file_path)\n\t\n\t# armazena tipo do arquivo no `packages`\n\tpackages[\"type\"] = tipo\n\t\n\t# determina o prefixo do arquivo, para montar os pacotes\n\tprefix_paths_prefix = re.sub('^([A-Za-z0-9_.]+)', '', prefix)\n\t\n\t# identifica o tipo de arquivo com o pacote\n\tpac"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tresult = {}\n\tif prefix.endswith('/asset/'):\n\t\t_file_type = 'asset'\n\telif prefix.endswith('/rendition/'):\n\t\t_file_type ='rendition'\n\telse:\n\t\treturn result\n\tif '/' not in prefix:\n\t\tfilename = prefix\n\telse:\n\t\tfilename = os.path.split(prefix)[1]\n\t_folder = os.path.split(file_path)[0]\n\tresult['type'] = _file_type\n\tresult['"}, {"generated_text": "def _eval_file(prefix, file_path):\n\"\"\"\n\tIdentifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\t\n\tIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n\to endere\u00e7o do arquivo em an\u00e1lise.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem extens\u00e3o\n\tfilename : str\n\t    filename\n\tfile_folder : str\n\t    file folder\n\t\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t# Identifica o tipo do arquivo\n\t_eval_type(file_path)\n\t\n\t# Atualiza os packages\n\tpackages[prefix] = file_path\n\n# Inicializa o status do pacote das caracter\u00edsticas criavel\ndef _init_caracteristic(prefix):\n\t\"\"\"\n\tInicializa o status do pacote das caracter\u00edsticas como `caracteristic_unlocked`.\n\t\n\tParameters\n\t----------\n\tprefix : str\n\t    nome do arquivo XML sem"}]}
{"task_id": "62b463153879012d194814a1", "prompt": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\trendition_model = add_rendition(lang, file_path)\n\trender_rendition(rendition_model)\n\ndef render_rendition(render_model):\n\trendition = Rendition.objects.get(pk=render_model['id'])\n\tfor rend in rendition.renditions.all():\n\t\tif rend.base_image_file.path:\n\t\t\t# render and save\n\t\t\trend.base_image_file.save(rend.file_path, None, False)\n\t\t\trend.save()\n\n# def get_rend"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\ttry:\n\t\t_item = Item()\n\t\t_item.store(lang)\n\t\t\n\t\twith open(file_path, 'rb') as _file:\n\t\t\t_fileobj = MemoryFile(_file)\n\t\t\t_item.addFile(_fileobj)\n\n\t\tresponse = redirect(reverse('index'))\n\t\tresponse.set_cookie('item', _item.id)\n\t\treturn response\t\n\texcept:\n\t\treturn self.get(request, *args, **kwargs)\n\n\t#TODO: error log\n\t\n\nclass Show(List):\n"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tself.request.response.set_content_type(\"application/json\")\n\tfilename = file_path.split('\\\\')[-1]\n\tjson_rendition = {\n\t\t'original': filename,\n\t\tlang: filename,\n\t\t'en': filename\n\t}\n\tself.request.response.write(json.dumps(json_rendition).encode('utf8'))\n\n\ndef error(self, status, message):\n    self.set_status(status)\n    self.set_header(\n        'Location',  # HTTP code 200"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tlogger.info(\"add_rendition \"+file_path)\n\trenditions = {}\n\twith io.open(file_path, 'rb') as f:\n\t\tdata = json.load(f)\n\t\tfor file in data[\"files\"]:\n\t\t\ttry:\n\t\t\t\twith io.open(file[\"path\"], 'rb') as f:\n\t\t\t\t\trenderable_data=json.loads(f.read())\n\t\t\t\t\tr= self.add_rendition(lang, renderable_data)\n\t\t\t\t\tid=r[\"id\"]\n\t\t\t\t\trenditions[id]=renderable"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tif not isinstance(file_path, dict) or not len(file_path) == 2 or file_path[\"en\"] not in self.files.keys() or self.files[file_path[\"en\"]]:\n\t\traise NotConfigured(\"This plugin require a config with the field %s\" % self.config[\"files\"])\n\n\trendition = RenditionFactory(file_path, default_lang=\"en\")\n\tid_rendition = str(uuid.uuid4())\n\n\tself.files[self.files[file_path[\"en\"]]] = id"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tret = None\n\tfor path in self.paths_to_renditions[lang]:\n\t\tif os.path.basename(path) == file_path:\n\t\t\tret = path\n\t\t\tbreak\n    \treturn ret\n\n\n\n\n\n\n"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\ttry:\n\t\tidfile = re.findall(r'/{1}(?P<idfile>[0-9a-z|_A-Z][0-9|A-Z|_|:|\\.|\\-]{2})$/', file_path)[0]\n\texcept IndexError:\n\t\traise AssertionError( 'Erro ao criar o arquivo.')\n\n\tif idfile not in self.repository:\n\t\tself.repository[idfile] = {}\n\twith open(file_path, \""}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tif DEBUG:\n\t\tprint u\">>>> add_rendition('%s')\" % lang\n\n\trenditions_to_add = renditions_to_add.get(lang)\n\tif not renditions_to_add:\n\t\trenditions_to_add = []\n\t\trenditions_to_add = {\n\t\t\tr_id: r for r_id, r in renditions_to_add.items()\n\t\t\tif (not r) or (r.extension == r_id)\n\t\t\tor (r.extension == r_id.replace('_"}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tprint(\"\\n\\n\\n\\n***INI:add_rendition ***\\n\\n\\n\")\n\twith open('renditions.json','r') as jsonfile:\n\t\trenditionsdata = json.load(jsonfile)\n\tif(len(renditionsdata) > 0):\n\t\tif(lang not in renditionsdata):\n\t\t\trenditionsdata[lang] = []\n\t\trenditionsdata[lang].append(file_path)\n\telse:\n\t\trenditionsdata = {lang:[]}\n\t\trenditionsdata[lang].append("}, {"generated_text": "def add_rendition(self, lang, file_path):\n\"\"\"\n\t{\n\t    \"original\": \"artigo02.pdf\",\n\t    \"en\": \"artigo02-en.pdf\",\n\t}\n\t\"\"\"\n\tcontent = json.dumps({\"filename\": file_path}  + ['en' if lang == 'en_US' else lang], indent=4)\n\theaders = {'Content-Type': 'application/json', 'Content-Length': str(len(content)), 'Accept': 'application/json'}\n\treturn requests.post(self._base_url, data=content, headers=headers)\n\ndef get_rendition(self, file_path):\"\"\"\n\treturns rendition dict from request result\n\t\"\"\"\n\tdata = requests.get(self._base"}]}
{"task_id": "62b463163879012d194814a2", "prompt": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tself.db_session.add(Asset(basename=basename, file_path=file_path))\n\n\n\n\nclass Product(ModelDeclarativeBase):\n\t__tablename__ = 'products'\n\n\tid = Column(Integer, primary_key=True)\n\tproduct_name = Column(String(32))\n\tprice_list = Column(JSONB)\n\n\n\tdef __init__(self, product_name, price_list=None):\n\t\tself.product_name = product_name\n\t\tself.price_list = price_list\n\n\n"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tasset = {basename: file_path}\n\n\treturn asset\n\nclass PDFImagePage(object):\n\t\"\"\"\n\tInherit from this class to create your own PDF extension (if you do, we highly recommend using \n\ta super class called \"PDFImagePage\", since the logic here depends on super class!)\n\t\"\"\"\n\tdef to_pdf_stream(self, filename = None, image = None):\n\t\tif not filename:\n\t\t\t# self.file_name is populated by parent class\n\t\t\tfilename = self.get_filename()\n\n\t\tf = open("}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tself.asset_data[basename] = file_path\n\t\ndef del_asset(self, basename):\n\tif basename in self.asset_data:\n\t\tdel self.asset_data[basename]\n\ndef get_asset(self, basename):\n\treturn self.asset_data.get(basename)\n\n\t\n"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tassets[basename] = file_path\n\n\ndef prepare_json(json):\n\treturn json_struct_from_simple_json(json)"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif basename not in self._assets_by_basename:\n\t    raise ValueError(\"The provided asset '{}' does not exist\".format(basename))\n\t_, extension = os.path.splitext(self._assets_by_basename[basename])\n\tfile_path = os.path.abspath(file_path)\n\n\twith open(file_path, 'r') as file_stream:\n\t    self.add_stream(basename, file_stream)\n\n\t#self.add_bytes(basename, file_"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif not os.path.exists(file_path):\n\t\tlogging.error(\"Can't find the file '%s'\", file_path)\n\t\treturn False\n\t\n\tfor i in self._asset:\n\t\tif i == basename:\n\t\t\tlogging.warning(\"Asset already created for filename '%s'\", basename)\n\t\t\treturn False\n\telse:\n\t\tfor i in self._asset:\n\t\t\tif i.find(basename)!= -1:\n\t\t\t\tif len(self._asset[i].split(\"/\")) >= len"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tfile_path,basename = os.path.split(file_path)\n\text = os.path.splitext(basename)[1]\n\tif ext == \"mhd\":\n\t\tcommand = \"imread(\\\"{}{}\\\")\".format(file_path,ext)\n\telif ext in [\"tif\", \"tiff\", \"png\"]:\n\t\tcommand = \"{}\".format(file_path)\n\telif ext in [\"jpg\",\"jpeg\",\"png\"]:\n\t\tcommand = \"caffe.io.load_image(\\\"{}\\\")"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tasset_list = self.get_asset(basename)\n\tif not asset_list:\n\t    asset_list = {}\n\tpath = file_path\n\tbasename, extension = os.path.splitext(basename)\n\tif basename not in asset_list:\n\t    # print(\"add_asset, basename {0}, path {1}\".format(basename, path))\n\t    asset_list[basename] = path\n\t    self.set_asset(basename, path_dict=asset"}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif basename in self.asset_map:\n\n\t\tif file_path in self.asset_map[basename]:\n\t\t\treturn\n\n\t\twith open(os.path.join(self.assets_dir_path, basename)) as f:\n\t\t\tself.asset_map[basename].add(f.read().strip())\n\n\t\tif basename not in self.asset_order:\n\t\t\tself.asset_order.append(basename)\n\n\telse:\n\t\tself.asset_map[basename] = set([])\n\t\tself."}, {"generated_text": "def add_asset(self, basename, file_path):\n\"\"\"\n\t\"{\n\t    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n\t    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n\t    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n\t}\n\t\"\"\"\n\tif not os.path.exists(file_path):\n\t\traise Exception(file_path+\" is not exist.\")\n\tself.asset_store[\"{}-{}\".format(basename, os.getcwd().split('/')[-1])] = '\"{}\" \"{}\" \"{}\"'.format(\n\t\tbasename, file_path, os.path.getsize(file_path))\n\n\treturn self.asset_store\n\n\n# ------------------------------------------------------------\n# ------------------------------------------------------------\n# ------------------------------------------------------------\n# ------------------------------------------------------------\n\n# Get"}]}
{"task_id": "62b463163879012d194814a4", "prompt": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\tfiles = dict()\n\ttry:\n\t\tif not zipfile.is_zipfile(zip_path):\n\t\t\traise IOError('{0} is not a zipfile'.format(zip_path))\n\t\telse:\n\t\t\tf_zip = zipfile.ZipFile(zip_path)\n\texcept:\n\t\traise IOError('Cannot determine whether {0} is a zipfile; invalid'\n\t\t\t   'file'.format(zip_path))\n\t\n\tfor zinfo in f_zip.infolist():\n\t    try:\n\t        if"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tdata = {}   # result\n\tfor filename_zipfile in _get_files_zip(zip_path):\n\t\tif filename_zipfile.endswith(\".xml\"):\n\t\t\tfilename_tar = filename_zipfile[:-4]\n\t\t\tdata[filename_tar], data[filename_tar + \".csv\"] = _get_package(filename_zipfile)\n\treturn data\n\ndef main():\n\t# print(_explore_zipfile(\"/home/alex/test.zip\"))\n\twith zipfile.ZipFile(\"/home/alex/"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\timport os\n\timport zipfile\n\t_dirs = set()\n\twith zipfile.ZipFile(zip_path, 'r') as zf:\n\t\tnames = zipfile.system_normalize_name\n\t\tfor line in zf.namelist():\n\t\t\tbasename = (names(line, '/'))\n\t\t\tif type(basename) is type(b''):\n\t\t\t\tbasename = basename.decode('utf-8')\n\t\t\t_dirs.add(basename)\n\t\tdirs = sorted(list(_dirs))\n\t\t\n\t\t"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tretVal = {'group_ids': {}, 'files': {}}\n\tgroup_ids, fnames = [], collections.defaultdict(list)\n\n\twith zf.ZipFile(zip_path) as zf:\n\t\t# Iterate xml files.\n\t\tzip_entries = zf.infolist()\n\t\tfor entry in zip_entries:\n\t\t\txml_basename = entry.filename.rpartition('.')[0]\n\n\t\t\t# Skip empty files.\n\t\t\tif entry.file_size == 0:\n\t\t\t\tcontinue\n\t\t\t# Check the"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t\n\ttry :\n\t\txml_basename = os.path.basename(zip_path)\n\t\txml_base_dir = os.path.dirname(os.path.dirname(zip_path))\n\texcept TypeError : #zip_path is not a string\n\t\terr = \"Path {!r} is not a string.\".format(zip_path)\n\t\tprint(err)\n\t\traise TypeError(err)\n\t\n\troot = zipio.ZipFile(zip_path)\n\tpackages = {}\n\t\n\tfor fname in root"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tret = {}\n\tfor item in get_package_items(zip_path):\n\t\tbasename = item['full_path'].split('/')[-1]\n\t\tret[basename] = {\n\t\t\t'attributes': item['attributes'].items(),\n\t\t\t'recent': item['recent_updates'],\n\t\t\t'downloads': item['downloads'],\n\t\t\t'files': item[list_of_files[0]],\n\t\t\t'details': item[details[0]]\n\t\t}\n\treturn ret\n\ndef _explore_"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tfiles = {}\n\tzf = zipfile.ZipFile(zip_path)\n\trootdir = \"xsd\"  # FIXME: This is probably the best fix to\n\t\t\t\t\t# avoid having to use the default rootdir in\n\t\t\t\t\t# xsdfiles.py\n\tfor n in zf.namelist():\n\t\t# zipfiles have paths like '/path/to/file.xml' as well as\n\t\t# paths like '' (to represent the root directory)\n\t\t# n is something like '/path/to/file.xml'\n\t\t\n\t\tfile_name = z"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\t# Get packages data from zip\n\ttry:\n\t\tzip_info = ZipFile(zip_path).namelist()\n\texcept Exception as e:\n\t\traise Error(\"Error: can not read {0} : {1}\".format(zip_path, repr(str(e))))\n\t\n\t# Check if 'pkg_data' was set before\n\tif (not pkg_data):\n\t\traise ValueError(\"'pkg_data' must be set.\")\n\t\n\t# Get all files names to read packages files\n\tpkg_names = []\n\tfor file_"}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\tret = dict()\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfor mod_file in zf.namelist():\n\t\t\tmod_name = mod_file.split(\"/\")[-1]\n\t\t\tmod_name = re.sub('\\.pyc|\\.pyo', '', mod_name)\n\t\t\tif mod_name in ret:\n\t\t\t\tret[mod_name] = ret[mod_name] + zf.read(mod_file)\n\t\t\telse:\n\t\t\t\tret[mod_name] = zf.read("}, {"generated_text": "def _explore_zipfile(zip_path):\n\"\"\"\n\tGet packages' data from zip_path\n\t\n\tGroups files by their XML basename and returns data in dict format.\n\t\n\tParameters\n\t----------\n\tzip_path : str\n\t    zip file path\n\tReturns\n\t-------\n\tdict\n\t\"\"\"\n\txml = os.path.join(zip_path,'metadata.xml')\n\tdata_dict = {}\n\twith zipfile.ZipFile(xml) as z:\n\t\tfilelist = z.namelist()\n\t\tfor name in filelist:\n\t\t\t_, basename = os.path.split(name)\n\t\t\tkey = basename.split('/')\n\t\t\tif '.' in basename:\n\t\t\t\tkey.append(basename)\n\t\t\telse:\n\t\t\t\tcontinue\n\t\t\tpth = os.path.join(*key)\n\t\t\tdata_dict.update({"}]}
{"task_id": "62b463163879012d194814a6", "prompt": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tfrom os.path import basename # path name\n\twith zipfile.ZipFile(zip_path) as z:\n\t\tfor zinfo in z.infolist():\n\t\t\tfilename = basename(zinfo.filename)\n\t\t\tyield filename\n\n######################################\n\nif __name__=='__main__':\n\timport sys\n\tif len(sys.argv)!=2:\n\t\tprint('Only one file path argument allowed!')\n\t\tsys.exit(1)\n\textracted_path = files_list_from_zipfile(sys.arg"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\timport re\n\tf = open(zip_path, mode=\"rU\")\n\tfiles = f.readlines()\n\tif 0 < len(files):\n\t\tfile_in_zip = re.compile('[-\\d]+', re.IGNORECASE)\n\t\tfiltered_files = [file_in_zip.sub('', x) for x in files]\n\t\treturn filtered_files\n\telse:\n\t\treturn None\n\nif __name__ == '__main__':\n\tfiles = files_list_from_zipfile\n\t# import p"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\twith ZipFile(zip_path, 'r') as my_zip: \n\t\treturn my_zip.namelist()\n\n\ndef is_gzip(name):\n\t\"\"\"\n\t\n\tExample:\n\n\t```\n\t'2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf'\n\tFalse\n\t\n\t'1/f'\n\tTrue\n\t```\n\t\"\"\"\n\tif '/gf_' in name:\n\t\treturn True\n\telse:\n\t\treturn"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\tfiles_dict = {}\n\twith zipfile.ZipFile(zip_path,'r') as f:\n\t\tfilelist = f.namelist()\n\t\tfor x in filelist:\n\t\t\tprint x\n\t\t\tfiles_dict[x] = f.open(x)\n\t\n\treturn files_dict\n\t\nif __name__ == '__main__':\n\tzip_path = 'p:\\\\p_austeintube\\\\1.7-s1-c3-m1-t10-e24\\\\'\n\tfiles_dict = files"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\tfrom os import listdir\n\tfrom os.path import join as _join\n\tfrom os.path import isfile,splitext as _splitext\n\tfrom sys import stdout\n\t\n\tfrom cnfformula.cnfformula import CNF\n\tfrom cnfformula.cnf import read_cnf\n\tfrom cnfformula.cnfformula import parse_cnffile\n\n\tdef walk(startdir):\n\t\t\"\"\"\n\t\tGenerate all files in a directory, including subdirectories.\n\t\t\"\"\"\n\t\tfrom"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\ttry:\n\t\treturn dict([(f, os.path.join(zip_path, f)) for f in os.listdir(zip_path)])\n\texcept:\n\t\treturn None\n\n\t\nfile_list_from_zipfile = files_list_from_zipfile\n\n\ndef path_for(zip_path, item, **kwargs):\n\t\"\"\"Takes the path to the `base_path` inside the `zip_path` and looks up the given path. Returns\n\tthe full path to the `item` as well as the full path to files it"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t# check if input path is a zipfile or not\n\tif not zipfile.is_zipfile(zip_path):\n\t\traise ValueError(f\"input must be a zipfile path: {zip_path}\")\n\t\n\t# Open zipfile and read all entries\n\twith zipfile.ZipFile(file=zip_path, \"r\") as f:\n\t\t# get the file names\n\t\tnames = f.namelist()\n\t\n\t# return names\n\treturn names\n\n\ndef download_images_files_to_current_path(ids, path,"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\twith open(zip_path, 'rb') as f:\n\t\t\tzfp = zipfile.ZipFile(f)\n\t\t\tret = []\n\t\t\tfor fname in filter(lambda x: x.startswith('2318-') and x.endswith('.pdf'), zfp.namelist()):\n\t\t\t\tret.append(zfp.read(fname))\n\treturn ret\n\n\ndef filter_list_from_mimelist(mimelist):\n\treturn [r.decode('utf-8') for r in mimelist.split(',')]"}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\t\n\twith zipfile.ZipFile(zip_path, 'r') as p_zip:\n\t\tfor f in p_zip.infolist():\n\t\t\t# ignore files with extension in \".zip\", \".bak\"\n\t\t\text = utils.get_file_extension(f.filename)\n\n\t\t\t# ignore files from TILU folder\n\t\t\tif ext == 'tgz':\n\t\t\t\tcontinue\n\n\t\t\t# ignore files starting with '.'\n\t\t\t# for example, file 'gf.tif' is not supported, because the files\n\t\t\t# are not created by TILL."}, {"generated_text": "def files_list_from_zipfile(zip_path):\n\"\"\"\n\tReturn the files in `zip_path`\n\t\n\tExample:\n\t\n\t```\n\t[\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n\t    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n\t]\n\t```\n\t\"\"\"\n\tdef zipfile_files(zpath):\n\t\twith closing(zipfile.ZipFile(zpath)) as zfp:\n\t\t    for _file in zfp.namelist():\n\t\t        yield zfp.getinfo(_file)\n\n\t\treturn [f for f in map(lambda x: (x[2], [os.path.join(zpath, f) for f in os.listdir(zpath)]), zipfile_files(zpath))]\n\n\treturn files_list(zipfile_files(zip_path))\n\n\n\n\t\n\n\t\n\n\n\n\n\n"}]}
{"task_id": "62b4631b3879012d194814dd", "prompt": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tdef fix(m):\n\t\treturn 'w%sst=\"' % m.group(1)\n\treturn re.sub(r\"w:st=['\\\"]\", fix, content, flags=re.M)\n\ndef convert_xmlns(content):\n\t\"\"\"\n\tConvert the xmlns attributes to prefixes.\n\t\"\"\"\n\n\tdef replace(m):\n\t\treturn \"xmlns%s='%s'\" % (m.group(1), m.group(2))\n\t\n\treturn re.sub(r'(xmlns=(\\w+"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\ttext = content.replace('xmlns=\"\"', 'xmlns=\"')\n\ttext = text.replace(\"urn:schemas-microsoft-com:office:word\", 'office:word')\n\n\treturn text\n\n# -----------------------------------------------------------------------------\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\t\n\tpos = 2\n\tout = ''\n\t\n\twhile True:\n\t\tif content[pos] in ['\\n','']:\n\t\t\tout += content[pos+1]\n\t\t\tpos += 2\n\t\telse:\n\t\t\tpos += 1\n\t\t\tbreak\n\t\n\twhile True:\n\t\tif content[pos] in ['b', 'i']:\n\t\t\tout += 'w-{0}'.format(content[pos:pos+2])\n\t\t\tpos += 3\n\t\telse:\n\t\t\tbreak\n\n\treturn out[:-1]\n\n\ndef get_element"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tif content =='st=\"':\n\t\treturn 'w-' + content\n\telse:\n\t\treturn content\n\ndef fix_namespace_prefix_r(content):\n\t\"\"\"\n\tConvert text that defaults to 'r:rt=\"' to 'w-rt=\"'\n\t\"\"\"\n\tif content == 'rt=\"':\n\t\treturn 'r-rt=\"'\n\telse:\n\t\treturn content\n"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tif content.startswith('w:'):\n\t\tcontent = '{%s}' % content[2:]\n\tif content.startswith('w-:'):\n\t\tcontent = '{%s}' % content[3:]\n\treturn content\n\t\ndef fix_namespace_prefix(content):\n\treturn re.sub(\"(<w:p[^>]*>)\", re.compile(\"<w:p{0,4}[^>]{0,4}>\".format(re.sub('[^a-zA-Z0-"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tparts = content.split()\n\tfor i in range(len(parts)):\n\t\tif parts[i].lower() == \"st\":\n\t\t\tparts[i] = \"wst\"\n\treturn ''.join(parts)\n\ndef fix_namespace_prefix(content):\n\tparts = content.split()\n\tfor i in range(len(parts)):\n\t\tif parts[i].lower() == \"stype\":\n\t\t\tparts[i] = \"w-stype\"\n\treturn ''.join(parts)\n\n_marshalings = {"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tp = re.compile(r\":<w(?P<name>.*?)>\")\n\t\t\n\tfor match in p.finditer(content):\n\t\tname = match.group('name')\n\n\t\tif name =='st':\n\t\t\tcontent = content.replace(match.group(), 'w-st=\"')\n\n\treturn content\n\ndef fix_namespace_prefix_th(content):\n\tp = re.compile(r\":<th(?P<name>.*?)>\")\n\n\tfor match in p.finditer(content):\n\t\tname ="}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tcontent = re.sub('wst=', 'w-st=', content)\n\treturn content\n\n\n\nclass Document(XMLSyntax):\n\t\"\"\"\n\tA valid XML document.\n\t\"\"\"\n\n\tname = None\n\tbody = None\n\t\n\tdef find(self, tag):\n\t\t# search children by tags\n\t\tif self.body:\n\t\t\tfor b in self.body:\n\t\t\t\tif b.tag == tag:\n\t\t\t\t\treturn\n\t\t\t\n\t\t# search children with attributes\n\t\tfor b in self.body:\n\t\t\tif b.is_element"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\treturn re.sub(re.compile('w:st=\"', re.UNICODE), r'w-st=\"', content)"}, {"generated_text": "def fix_namespace_prefix_w(content):\n\"\"\"\n\tConvert text that defaults to 'w:st=\"' to 'w-st=\"'\n\t\"\"\"\n\tif '' in content:\n\t\treturn content\n\tnew_content = []\n\tlines_in = content.splitlines()\n\tfor line_in in lines_in:\n\t\tnew_line_in = ''\n\t\tif line_in:\n\t\t\tindex = line_in.find(\"\\\"\")\n\t\t\tif index > -1:\n\t\t\t\tline_in = line_in[index:].lstrip()\n\t\t\t\tline_in = re.sub(r\"^\\\"(.+)\\\",(.*)\",\n\t\t\t\t\tr\"\\1-st=\\\"\\2\\\",\\3\",\n"}]}
{"task_id": "62b463283879012d1948153d", "prompt": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.find(pubdate_xpath)\n\t\tif pubdate is not None:\n\t\t\treturn pubdate\n\t\t\n"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor px in pubdate_xpaths:\n\t\tfor node in node.xpath(px):\n\t\t\tif not node.text or node.text.strip() == EMPTY: continue\n\t\t\ttokens = node.text.split()\n\t\t\tdt_tokens = [ t for t in tokens\n\t\t\t\tif t in [ '', 'T', 't' ] + [ MONTHS_AND_DAYS[0] ] + MONTHS_AND_DAYS[1] ]\n\t\t\tif len(dt_tokens) == 1:\n\t\t\t\treturn dat"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor xpath in pubdate_xpaths:\n\t\txps = XPath('xhtml:p|xhtml:x|xhtml:div.p', xml=node)\n\t\tps = pubdate_xpaths[xpath]\n\t\tfor ix in range(2):\n\t\t\tps = next(ps)\n\t\tif len(ps) > 0 and ps[-1].text.strip() == pubdate_xpaths[xpath]:\n\t\t\treturn xps.xpath(xpath)\n"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_nodes = node.xpath(pubdate_xpath, namespaces=get_staging()['namespaces'])\n\t\tif pubdate_nodes:\n\t\t\tpubdate_node = first(pubdate_nodes).text\n\t\t\treturn datetime.strptime(pubdate_node, '%d %B %Y')\n\treturn None\n\ndef remove_pubdate(node):\n\t\"\"\"\n\t\"\"\"\n\tnode.clear('pubdate')\n\treturn node\n"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_matches = node.xpath(pubdate_xpath)\n\t\tif pubdate_matches:\n\t\t\treturn pubdate_matches[0] == pubdate_xpath\n\treturn False\n\ndef match_node_by_tagname(node, tag_name):\n\t\"\"\"\n\tSearches node for tags with the given name\n\t\"\"\"\n\tfor item in node.iter():\n\t\tif item.tag in ('h2', 'h1'):\n\t\t\treturn item"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tmatches = re.finditer(pubdate_re, node.text_content())\n\t\ttry:\n\t\t\treturn matches.next().span()\n\t\texcept StopIteration:\n\t\t\tpass\n\treturn None\n\ndef main(argv):\n\tdef write_tree(tree, filename, indent):\n\t\t\"\"\"\n\t\tWrites a list of lines to a csv file\n\t\t\"\"\"\n\t\tfor node in tree:\n\t\t\twrite(node, indent, filename)\n\t\treturn\n\n\tdef write_line"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.find(xpath)\n\t\tif pubdate is not None:\n\t\t\treturn pubdate\n\t\n\treturn None"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor node_pubdate in node.select(pubdate_xpaths):\n\t\treturn node_pubdate\n\treturn None\n\ndef parse_pubdate(node, pubdate_xpaths):\n\tif node and len(pubdate_xpaths) > 0:\n\t\tnode_pubdate = match_pubdate(node, pubdate_xpaths)\n\t\treturn node_pubdate.text\n\ndef to_date(text):\n\t\"\"\"\n\tConverts the date (string) to date object with current system timezone (the default).\n\t\n"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\t# Only use xpaths with matching pubdate if using a pubdate xpath\n\t\tif hasattr(node, 'pubdate') and node.pubdate:\n\t\t\t# Make the string available to xpath functions\n\t\t\t# We don't have to save the node object because the value is saved internally\n\t\t\tnode_pubdate = xml_text(node)\n\t\t\tmatch1 = node.match((None, True), pubdate_xpath)\n\t\t\tmatch2 = node.match((None, False), pubdate_"}, {"generated_text": "def match_pubdate(node, pubdate_xpaths):\n\"\"\"\n\tReturns the first match in the pubdate_xpaths list\n\t\"\"\"\n\tmatches = (node.xpath(p) for p in pubdate_xpaths)\n\t\ttry:\n\t\t\treturn next(matches)  # only works if there is a single match\n\t\texcept StopIteration:\n\t\t\treturn None\n\n\tdef no_matches(msg):\n\t\treturn 'No matching pubdates in {}'.format(msg)\n\n\tdef format_elem(el):\n\t\t# todo: handle dates with no pubdates?\n\t\tpub_datetime = None\n\t\tdate_elem = Match.find_xpath(el.iter,"}]}
{"task_id": "62b463303879012d19481579", "prompt": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber = set_of_string.parse_number(_remove_punctuation_from_string(issue.Contents))\n\tfor n in set_of_string.parse_words(issue.Contents):\n\t\t# Ignore the \"and\" in the \"and\" word\n\t\tif len(set_of_string.parse_letters(n)) == 2:\n\t\t\tcontinue\n\t\t# Ignore the \"plus\" at the end\n\t\tif  set_of_string.parse_letters(n)[-1].lower() == \"plus\":\n\t\t\tcontinue\n\t\t# Ignore letter with a"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber = \"\"\n\tsupplement = \"\"\n\tif issue.has_attr(\"number\"):\n\t\tnumber = str(issue[\"number\"]).strip()\n\tif issue.has_attr(\"suppl_no\"):\n\t\tsupplement = \"(\" + str(issue[\"suppl_no\"]).strip() + \")\"\n\n\treturn (number, supplement)\n\ndef _get_supplied_issue_number(doc):\n\t\"\"\"\n\t\tThis function returns the number from the issue passed in as argument in the case that either a number is set or \n\t\teither a suppl"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber_value, value_type, _, _ = _issue_value(issue)\n\tvalue_label = _value_label(value_type, number_value)\n\treturn number_value, value_type, value_label\n\n\ndef _get_titles_from_articles(articles):\n\t\"\"\"\n\tExtract the titles from the article xml representation.\n\t\"\"\"\n\treturn (x.text or '') for x in _xml_iter(articles.find('articles/article'))\n\n\ndef _get_subjects_from_articles(articles):\n"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tsuppl_re = re.compile('[A-Z]+ (.+)')\n\tregex = re.compile('^(?:.*?)?([0-9]+) ')\n\tcount = ''\n\ttry:\n\t\tfor c in issue:\n\t\t\trep = re.search(suppl_re, c)\n\t\t\tmatch = regex.search(c)\n\t\t\tif len(rep.group()) >= 1 and len(match.group()) >= 1:\n\t\t\t\tcount += match.group(1)\n\t\t\t\tcount += ','+ rep.group()\n\t\t\t\tbreak"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber_index = issue.find('number', NS)\n\tsupplement_index = issue.find('supplementnumber', NS)\n\tnumber = None\n\tif number_index is not None:\n\t\tnumber = extractors.re_string_number.search(issue[number_index:].strip()).group(1)\n\tsupplement = None\n\tif supplement_index is not None:\n\t\tsupplement = extractors.re_string_supplement_number.search(issue[supplement_index:].strip()).group(1)\n\treturn [number"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tparts = list(item.group() for item in re.finditer(r\"Number\\s?=\\s([^;]+);\", issue))\n\tnumber = parts[0] if len(parts) > 0 else None\n\tsupplemental_elem = parts[1] if len(parts) > 1 else None\n\treturn number, supplemental_elem\n\nclass IssueElem(object):\n\t\"\"\"\n\tAn issue element containing number and supplemental.\n\t\"\"\"\n\n\tdef __init__(self, e_string):\n\t\tself.number_and_supp"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\t# extract value of the \"Number\" attribute\n\tattr_value = issue.attrib.get(\"Number\")\n\tif attr_value is not None:\n\t\t\t# remove the enclosing <> characters of the attrib and return value\n\t \t\t# return attr_value.replace('>','')\n\n\t\t\tresult = json.loads(attr_value)\n\n\t\t\t# if the result has a key \"Number\"\n\t\t\tif \"Number\" in result:\n\t\t\t\tif isinstance(result[\"Number\"], list):\n\t\t\t\t\tresult[\"Number\"] = int(result[\"Number\"][0"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber_regex = \".+?[\\d]*.+?\"\n\tsuppl_regex = \".+?(x|i).+?\"\n\n\t# Replace any instance of non-alphanumeric characters with a space.\n\tnumber_regex = re.sub('[^0-9a-zA-Z\\']','', number_regex)\n\n\t# Get all numbers and non alphanumeric characters.\n\tnumber_regex = re.compile(number_regex)\n\tsuppl_regex = re.compile(suppl_"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber_from_suppl = issue.get_supplementary_data(['number'])\n\tnumber = re.search(r'(\\d+)',number_from_suppl['number_formatted'])\n\tif not number:\n\t\treturn ''\n\telse:\n\t\tnumber = int(number[0])\n\treturn number\n\n\n\n\ndef _iterate_issues(dir_name):\n\t\"\"\"\n\tIterator used to loop through all the issues.\n\tdir_name: The root directory for all the issues\n\t\"\"\"\n\tfor issue in os.list"}, {"generated_text": "def _extract_number_and_supplment_from_issue_element(issue):\n\"\"\"\n\tExtract the possible values of number and suppl from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsuppl = None\n\n\tnumbers = issue['number']\n\tnumbers = numbers.split(',')\n\n\tsupllist = issue['supplement']\n\tsupllist=supllist.split(',')\n\tfor s in supllist:\n\t\ts = s.strip()\n\t\tif s:\n\t\t\tnumber += s\n\tif number:\n\t\t# if a number was extracted before, its the only one available in the\n\t\t# suppl list, thus we don't need to extract any more, because the list\n\t\t# is not a"}]}
{"task_id": "62b46740d2f69a53b466171a", "prompt": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif debug:\n\t\treturn self._prettyPrint(indent=indent, debug=debug)\n\treturn self._stringify(indent=indent, debug=debug)\n\n\n\t\n\t\n"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tnested=lambda lst: lst[0]\n\tflatten=lambda lst: lst[0]\n\tdef print_dict(x, indent):\n\t\t\"\"\"recursively print a dictionary.\"\"\"\n\t\tpr=x if debug else ''\n\t\tpr+=' '*indent\n\t\tif isinstance(x, dict):\n\t\t\tpr+=\"{ \"\n\t\t\tfor k,v in nest_dict(x):\n\t\t\t\tpr+=\"{ \"\n\t\t\t\tpr+=print_dict(k,indent+4)+\":\"+print_dict(v,indent"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tn = self.size()\n\tp = []\n\tif n:\n\t\tp.append(\"%d items: \" % n)\n\tp.append('\\n')\n\t# p += indent*'  '\n\ts = self.__class__.__name__\n\tif self._key == None:\n\t\ts += self._fields\n\telse:\n\t\ts += self._key\n\tif n:\n\t\tp.append(\"%s:\\n\" % s)\n\t\tp.extend(indent*' '+p_key)\n\t\tp"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tassert type(indent) is int and indent >= 0\n\tstack = [self]\n\tline = list()\n\n\tnew_line = False\n\tfor level in xrange(0, indent):\n\t\tif isinstance(stack[-1], list): # we have a string list\n\t\t\tline.append('|'*level) # put a vertical line in the output\n\t\t\tnew_line = True\n\t\telse:\n\t\t\tline.append(' '*level) # not a string list, but this is ok\n\t\tstack.pop()\n\n\tif type(stack[-1"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tpprint=indent*' '\n\tpindent=pprint+'|'\n\n\tdef doit(t, indent=0):\n\t\ttry:\n\t\t\tif type(t)==tuple:\n\t\t\t\tp1=doit(t[0], indent-1)\n\t\t\t\tp2=doit(t[1], indent+1)\n\t\t\t\treturn \"%s [%s]\" %(p1[:-1], p2[1:])\n\t\t\telif type(t)==str:\n\t\t\t\treturn \"\\\"%s\\\"\" %(util.quote("}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\treturn json.dumps(self.to_dict(), sort_keys=True, indent=indent, separators=(',', ': '),\n\t\tdefault=lambda o: str(o))\n\nclass ModelBase(object):\n\t\"\"\"\n\tBase model class that gives some additional functionality.\n\n\t\"\"\"\n\tmodel_type = None\n\tmodel_type_name = None\n\tmodel_type_description = None\n\n\tis_deletable = False\n\n\tdef __init__(self, db=None):\n\t\tself.db = db if db is not None else"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tif debug:\n\t\tprint(\"debug: pretty printed {0}\".format(self))\n\n\tif self._parent is not None:\n\t\treturn self._parent.pretty(indent=indent + 2, debug=True)\n\n\toutput = [\"{0}\".format(self.name)]\n\n\tif self.size is not None:\n\t\toutput.append(\"(size {0} bytes)\".format(self.size))\n\n\tif self.addr:\n\t\toutput.append(\"0x{0:02X}:{0:02X}:{0:02X}"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tbuf = io.StringIO()\n\t\t\n\t# write the type of the value\n\tif self.type:\n\t\tbuf.write('{\\n')\n\t\tbuf.write('\\ttype: \"Value\",\\n')\n\t\tbuf.write('}\\n')\n\t\t\n\t# write the name of the value\n\tif self.name:\n\t\tvalue = self.value\n\t\tif self.substructures:\n\t\t\tvalue = value[0] if type(value) == tuple else value[0]\n\t\tbuf.write('{\\n')\n\n\t\tbuf.write('"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tlines = []\n\tfor col in range(0, self.ncols):\n\t\t\tlines.append('{}:{}'.format(str(col), str(self.at(col))))\n\tif indent == 0:\n\t\t\treturn ''.join(lines)\n\telse:\n\t\treturn '\\n'.join(''* indent + line for line in lines)\n\n\n######################################################################\n# Basic Operations on Table\n######################################################################\n\ndef isempty(table):\n\t\"\"\"\n\tReturn True if the provided Table is empty.\n\n\tINPUT\n\t"}, {"generated_text": "def pretty(self, indent=0, debug=False):\n\"\"\"\n\tReturn a pretty formatted representation of self.\n\t\"\"\"\n\tdef make_indent(n):\n\t\treturn \" \" * min(n, 20)\n\n\tdef make_indent_2(n):\n\t\treturn make_indent(n + 1)\n\n\tdef append_indent(s, indent_str):\n\t\ts.append(indent_str)\n\n\tdef append_linebreak(s):\n\t\ts.append(\"\\n\")\n\n\tdef add_indent_2(s, str):\n\t\ts.append(make_indent_2(2))\n\t\ts.append(str)\n\n\toutput"}]}
{"task_id": "62b46746d2f69a53b4661722", "prompt": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tabs_func = lambda e: e * -1\n\tabs_func_inv = lambda e: e * -1\n\t\n\t#\n\t# For all the positive absorption\n\t# A & (A | B) = A\n\t# A | (A & B) = A\n\t#\n\tres = []\n\tfor i in range(len(args)):\n\t\tres.append(args[i])\n\tret = []\n\tfor i in range(len(res)):\n\t\tif isinstance(res[i], Expression):\n\t\t\tif i == 0"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tresult_list = []\t\n\tfor x in args:\n\t\tif isinstance(x, Expr):\n\t\t\tif x.op == AND and (abs(x.args[0]) == abs(x.args[1])):\n\t\t\t\tresult_list.append(x.args[0])\n\t\t\telif x.op == OR and (abs(x.args[0]) == abs(x.args[1])):\t\n\t\t\t\tresult_list.append(x.args[0])\n\t\t\telse:\n\t\t\t\tresult_list.append(Expr(NOT, x"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\n\tif args:\n\t\treturn [self._exists(self.exists_list(args)),\n\t\t\tself._exists(self.exists_list(list(reversed(args))))]\n\treturn [self._exists(self.exists_list([]))]\n\ndef all(items):\n\t\"\"\"\n\tReturn a list containing all instances of the given items (strings, expressions, numbers, etc.).\n\n\tSee https://en.wikipedia.org/wiki/All_equivalent_property\n\t\n\tExamples:\n\t\n\t    all('"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t# TODO implement absorb\n\n\t# abs(A & (A | B)) =  A,    abs(A | (A & B)) =  A\n\t# abs(A & (~A | B)) = A & B, abs(A | (~A & B)) =  A | B\n\t\n\tif len(args) == 1:\n\t\treturn abs(args[0])\n\n\ta = args[0]\n\tfor b in args[0:len(args)-1]:\n\t\ta = abs(a & (a | b))\n\n\tif a.sy"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\treturn [ e.apply_absorption() for e in args ]\n\ndef reduce(args):\n\t\"\"\"\n\tGiven a list of expressions, return the expression resulting from reducing the\n\texpressions one by one until the expression is non-null.\n\t\n\tExamples::\n\t\t\n\t\texpr = \"1 & 2 | 3 & 4\"\n\t\tassert expr == reduce(expr.split())\n\t\tassert expr == reduce(reduce(expr).split())\n\t\"\"\"\n\treturn reduce_t(args[0] if args else None, args[1:])\n\n\ndef"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\ttry:\n\t\tif any(a.op == \"~\" for a in args):\n\t\t\treturn args + [\"1\"]\n\t\telse:\n\t\t\treturn args\n\texcept AttributeError:\n\t\treturn [\"(%s) & (%s)\" % (a, b) for a, b in args]\n\t\t\n\t"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\t\n\tif not isinstance(args[0], Expression):\n\t\traise TypeError(\"Expected an Expression list as argument for absorption\")\n\t\n\tresult = [ Expression(args[0][0] & args[1][0], args[0][1] | args[1][1]) ]\n\tfor i in range(2, len(args)):\n\t\tresult.append(Expression(args[i][0], -(args[i][1] & -args[i][0])))\n\t\n\treturn result\n"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tassert isinstance(args, (list, tuple)), \"ABSORB() requires a list\"\n\treturn (a for a in args if is_absorbable_subexpression(a))\n\n#-------------------------------------------------------------------------------\n# Substitution method\n#-------------------------------------------------------------------------------\n\ndef substitute(dictionary, s):\n\t\"\"\"Given a substitution `dictionary` and a string `s`\n\t\n\tsub(x, y) -> x(y)\n\t\"\"\"\n\tassert isinstance(dictionary, (dict, OrderedDict)), \"SUBST() requires a dictionary\"\n\t"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\tt = Transformer.applied(AppliedRule(Absorption, [], 1)(list(reversed(args)))).as_nodes()\n\tt = t[:-1], AppliedRule(NegativeAbsorption, ['__OR', t[0], t[1]])\n\tassert not len(t) - 2\n\treturn t\n\nclass AbsoluteOrder(Transformer):\n\tname = \".Order\"\n\tpattern = \"__ABS(\"\n\n\t\n\tdef absorb(self, args):\n\t\t\"\"\"\n\t\tAbsolute ordering::\n\t\t\n"}, {"generated_text": "def absorb(self, args):\n\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression\n\tapplying absorption and negative absorption.\n\t\n\tSee https://en.wikipedia.org/wiki/Absorption_law\n\t\n\tAbsorption::\n\t\n\t    A & (A | B) = A, A | (A & B) = A\n\t\n\tNegative absorption::\n\t\n\t    A & (~A | B) = A & B, A | (~A & B) = A | B\n\t\"\"\"\n\treturn ([(neg_absorb(a) if isinstance(a, Expression) else Expression(a)) for a in args] if isinstance(args, list) else Expression(args))\n\t\n"}]}
{"task_id": "62b86707b4d922cb0e688c2a", "prompt": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef add_listener(callback):\n\t\t\"\"\"\n\t\tDecorator that adds a listener to a particular hook attribute \n\t\tfor the hook name specified in the call.\n\n\t\tArgs:\n\t\t\tcallback (function): Callback function passed on from other\n\t\t\t\thandler decorators to be called when a certain\n\t\t\t\ttype of hook is triggered.\n\n\t\tReturns:\n\t\t\tcallable: Decorator for registering listeners for the specified\n\t\t\thook.\n\t\t\"\"\"\n\t\tdef decorator(event_handler):\n\t\t\t\"\"\"\n\t\t\tDecorator that adds a listener for the specified hook. Whenever\n\t\t\t"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t@wraps(hook, (\"on_ \", \"\")[sys.version_info > (3,0,0)]) # noqa\n\tdef _on(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tDecorator for registering listeners for the specified hook.\n\t\t\"\"\"\n\t\tassert not self.get(\"active\", True)\n\t\tassert hook in HookType.attrs\n\t\t\n\t\tself.reg[hook] = (self.reg[hook], self)\n\t\tself.events[hook].append((self, hook))\n\t\t\n\t\treturn self\n\t"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t@functools.wraps(hook)\n\tdef hook_handler(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tFunction wrapper which registers the hook handler.\n\t\t\"\"\"\n\t\tself.registry[hook] = hook_handler\n\t\treturn hook_handler\n\treturn hook_handler\n\nHooks = HookRegistry(\"Hook\",\n\t\"\"\"A collection of handler functions which will be executed when a new chatmessage\n\tis received.\n\n\tHandlers are identified by the ``Hook`` class argument passed in to the __init__ method of\n\tthis class"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef wrapper(dispatch):\n\t\thandler = lambda *args: dispatch.dispatch(hook, *args)\n\t\tself.register_handler(hook, handler)\n\t\treturn handler\n\treturn wrapper\n\nclass DispatchRegistry(object):\n\t\"\"\"\n\tRegistry for storing hooks for the dispatcher (for easier maintenance).\n\t\"\"\"\n\tdef __init__(self, handlers):\n\t\tself.handlers = handlers\n\t\n\tdef dispatch(self, hook, *args):\n\t\tif hook not in self.handlers:\n\t\t\traise UnknownEventError()\n\t\treturn"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\t\n\tdef register_handler(f):\n\t\tself._handlers[hook] = f\n\t\treturn f\n\tregister_handler.__name__ = hook\n\treturn register_handler\n\ndef on_(hook):\n\t\"\"\"Alias for on(hook).\"\"\"\n\treturn on(hook)\n\n\nhooks = HookRegistry()\n\"\"\"Hook registry that can be used to register callable that handle\ncertain events. The hooks can then be used as decorators.\n\"\"\"\n\n\n\ndef call(hook, payload, target, *args, **kwargs):\n\t\"\"\"Internal"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself._registry[hook] = func\n\t\treturn func\n\treturn decorator\n\nclass HtmlRegistry(Registry):\n\tTYPE ='string'\n\t\n\t@on('hook:on')\n\tdef on(self, hook, *args, **kwargs):\n\t\treturn HtmlHandler.on(self, hook, *args, **kwargs)\n\t\n\t@typeCheck('string', int, float, str, bool, list, tuple, dict)\n\t@on('hook:off')\n\tdef off"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef _decorate(func): \n\t\tself.__registry.add(hook, func)\n\t\treturn func\n\treturn _decorate\n\n#------------------------------------------------------------------------------\nclass HookHandler(object):\n\tdef __init__(self, instance):\n\t\tself._handler_type = None\n\t\tself._handler_data = None\n\t\tself._instance = instance\n\n\tdef on(self, hook):\n\t\t\"\"\"\n\t\tRegister a listener for the specified hook.\n\n\t\tArgs:\n\t\t    hook (HookAttribute): Hook attribute for which to register the handler.\n\t\t\n\t\t"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef decorator(func):\n\t\t_registry[hook].append(func)\n\t\treturn func\n\treturn decorator\n\n\n# Set default settings if none exist.\nif not HAS_LOG:\n\tHAS_LOG = False\nif not HAS_CRYPTO:\n\tHAS_CRYPTO = False\n\n\n# List of registered listeners.\n_registry = {}"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\tdef method_add(func):\n\t\tself._type_to_listenser[hook].append(func)\n\t\tself._listenser_to_method[func] = hook\n\t\treturn func\n\treturn method_add\n\n\t\nclass HandlerGroup(object):\n\tdef __init__(self, listener_func):\n\t\tself._listener_func = listener_func\n\t\n\tdef __call__(self, *args, **kwargs):\n\t\tself._listener_func(*args, **kwargs)\n\t\n\t\nclass EventManager(object):\n"}, {"generated_text": "def on(self, hook):\n\"\"\"\n\tDecorator function to add a new handler to the registry.\n\t\n\tArgs:\n\t    hook (HookType): Hook attribute for which to register the handler.\n\t\n\tReturns:\n\t    callable: Decorator for registering listeners for the specified\n\t    hook.\n\t\"\"\"\n\treturn self.registry.register(hook)\n\t\n\t\nclass HookRegistry(object):\n\t\"\"\":class:`HookRegistry` is the main object that is used to find the\n\tregistered events. You can only register hooks using this object.\n\t\n\t.. note::\n\t\tThe registration is stored in a Python dictionary.\n\t\"\"\"\n\t\n\t\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tConstructor, initializes the registry and sets the event handler\n\t\tmap to None.\n\t\t\"\"\"\n\t\tself._items = {}\n\t\n"}]}
{"task_id": "62b86729b4d922cb0e688c2f", "prompt": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\treturn {\n\t\t\"authentication\": {\n\t\t\t\"db_type\": \"etcd\",\n\t\t\t\"etcd_host\": etcd_host,\n\t\t\t\"etcd_port\": etcd_port,\n\t\t},\n\t\t\"static\": {\n\t\t\t\"authentication\": {\n\t\t\t\t\"user\": user,\n\t\t\t},\n\t\t},\n\t}"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\td = {\n\t\t\"version\": 1,\n\t\t\"type\": \"static\",\n\t\t\"static_id\": base_id(),\n\t\t\"static_user\": user,\n\t\t\"static_enabled\": \"true\",\n\t}\n\tif etcd_host:\n\t\td[\"static_host\"] = etcd_host\n\tif etcd_port:\n\t\td[\"static_port\"] = etcd_port\n\treturn d\n\n\ndef run_cmd(cmd):\n\t\"\"\"\n\tRuns a simple command on a terminal.\n\t\n\tArgs:\n\t"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\t\n\tconfiguration = {\n\t\t'etcd_host':etcd_host,\n\t\t'user':user}\n\treturn configuration\n\ndef database_manager(database, host=\"localhost\", port=2378):\n\t\"\"\"\n\tCreates a connection to the database.\n\tTakes as input the name of the database, and a tuple with parameters to bind\n\tto the connection.\n\t\n\tArgs:\n\t    database (str): a string with the name of the database.\n\t    host (str): the host to bind the connection to.\n\t    port ("}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconfig = {\n\t\t\"User\": user,\n\t}\n\tlogging_conf = {\"Level\": \"INFO\"}\n\tclient_conf = {\"Etcd\": {\n\t\t\"Endpoints\": [\n\t\t\t\"localhost:\" + str(etcd_port)\n\t\t],\n\t\t\"TLS\": {\n\t\t\t\"Verify\": {\n\t\t\t\t\"CACert\": \"/etc/ssl/cacert.pem\",\n\t\t\t}\n\t\t},\n\t\t\"Prefix\": \"/config\",\n\t}}\n\tredis_conf = {\"Host\": \"127.0.0"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\treturn {'user_static': user,\n\t\t   'user_static_secret': randomString(15),\n\t\t   'gossip_leader_key': 'gossip_leader',\n\t\t   'gossip_server_key': 'gossip_server',\n\t\t   'gossip_client_key': 'gossip_client',\n\t\t   'election_leader_key': 'election_leader',\n\t\t   'heartbeat_leader_key': 'heartbeat_leader',\n\t\t   'heartbeat_server_key': 'heartbeat_server',\n\t\t   '"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tcfg = {\n\t\t\"etcd_host\": etcd_host,\n\t\t\"etcd_port\": str(etcd_port),\n\t\t\"user\": user,\n\t\t\"base\": BASE\n\t}\n \treturn cfg"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tfrom etcdctl.utils.baseconfig.baseconfig import base_config as _base_config\n\t\n\tetcdb = os.path.join(user, \"etcdb\")\n\tconfig = _base_config(user, etcdb, host=etcd_host, port=etcd_port)\n\t\n\treturn config"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\treturn {\"user\":user,\"etcd_host\":etcd_host, \"etcd_port\":etcd_port}\n\nclass DynamicConf:\n\tdef __init__(self, user, client):\n\t\t\"\"\"\n\t\tInits DynamicConf with a user and client.\n\t\t\n\t\tArgs:\n\t\t    user (str): the name of the user for the static authentication\n\t\t    client (client): the client for the Dynamic configuration.\n\t\t\n\t\tReturns:\n\t\t    None\n\t\t\"\"\"\n\t\tself.user = user\n\t\tself.client = client"}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tconf = {}\n\tconf[\"etcd_host\"] = etcd_host\n\tconf[\"etcd_port\"] = etcd_port\n\tconf[\"static_url_prefix\"] = \"\"\n\tconf[\"mongo\"] = {\"database_uri\": \"mongodb://test:test@localhost/tests1\",\n\t\t\t\t\t\"sessionstore_type\": \"ram\"}\n\tconf[\"auth\"] = {\"static_tokens\": {\n\t\t\t\t\t\"admin\": {\"email\": \"admin@localhost\", \"password\": \"admin\"},\n\t\t\t\t\t\"me\": {\"email\": \"admin@localhost\","}, {"generated_text": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\"\"\"\n\tCreates a configuration with some simple parameters, which have a default value\n\tthat can be set.\n\t\n\tArgs:\n\t    user (str): the name of the user for the static authentication\n\t    etcd_host (str): the host for the database.\n\t    etcd_port (int): the port for the database.\n\t\n\tReturns:\n\t    dict: the created configuration.\n\t\"\"\"\n\tauth = users.AuthDriver(\n\t\tname   = \"static\",\n\t\tprefix = \"s\",\n\t\turl = \"127.0.0.1:8080/auth\",\n\t\tuser = user,\n\t\tpassword = \"userpw\",\n\t\tcacert = ca_cert_path\n\t)\n\t\n\tetcd = auth.etcd.etcd(host=etcd_host,\n\t\tport=etcd_port)\n\t\n\ttls = auth.ca.ca_server_certs({\n\t\t'host':'mockhost',"}]}
{"task_id": "62b8a4a4755ee91dce50a3d2", "prompt": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\treturn dt if self.fold else self.tz.fromutc(dt)\n\nclass FoldMixin(object):\n\t\"\"\"\n\tA mix-in for :class:`datetimetree.TimeZoneFold`.\n\t\"\"\"\n\tdef _fold(self, dt):\n\t\t\"\"\"\n\t\tGiven a timezone-aware datetime in a given timezone, calculates a\n\t\ttimezone-aware datetime in the same timezone as the instance object.\n\t\t\n\t\t:param dt:\n\t\t    A timezone-aware :class:`datetime."}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tassert dt.tzinfo is not None\n\n\t# Time zone conversion with the local time zone\n\t# (no DST) can be performed without problems,\n\t# since the hour rolls over at 1 AM.\n\t#\n\t# NOTE: Some day and places on the internet\n\t# say that doing this can or can't be an issue,\n\t# but it can be pretty useful.\n\tnewdt = dt.astimezone(self.timezone)\n\n\t# For ambiguous moments, we need to take a look more closely at the\n\t# moment that we are working"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif self._fold:\n\t\treturn dt.replace(fold=False)\n\telif dt.fold:\n\t\treturn dt\n\telse:\n\t\ttry:\n\t\t\treturn dt.astimezone(self.tzinfo).replace(fold=True)\n\t\texcept AmbiguousTimeError:\n\t\t\treturn dt\n\n\ndef _from_aware(self, dt):\n\t\"\"\"\n\tGiven an aware :class:`datetime.datetime` or :class:`datetime.date`\n\tinstances, returns a naive (and possibly tz-"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\toffset = _tz_offset(self.tz)\n\t\n\treturn dt + offset\n\n\ndef _tz_offset(tz):\n\t\"\"\"\n\tGiven a :class:`pytz.tzinfo.DstTzInfo` object of a time zone,\n\treturns the offset in seconds of that time zone, as an integer.\n\t\"\"\"\n\treturn tz.utcoffset(datetime.datetime.max)"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# NB: timezone-aware datetimes are not actually immutable.\n\t#     I've chosen to have this function immutable, to prevent\n\t#     immunity to the datetime object being modifed in the course\n\t#     of processing.\n\t\n\tif not self.is_ambiguous(dt):\n\t\treturn dt\n\t\n\tif dt.tzinfo is None:\n\t\t# if the date is not aware of a timezone, assume local timezone\n\t\tdt = dt.replace(tzinfo=self._tzstrep)\n\telse:"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\timport pytz\n\t\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=pytz.utc).astimezone(self)\n\telif dt.tzinfo == pytz.utc:\n\t\treturn dt.astimezone(self)\n\telse:\n\t\t# the \"ambiguous/fold\" state is defined\n\t\t# in the constructor of an AmbiguousTimeZone.\n\t\treturn self._fromutc(dt)\n\n\n\nclass FixedTimeZone(TimeZone):\n\t\"\"\"\n\tA simple"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tself._utc = dt.utcoffset()\n\t\n\t# If the self._utc is None, then self.timezone is the default\n\t# that we'll use.\n\tif self._utc is not None:\n\t\tself.timezone += self._utc  # Add in the 'total' value\n\t\t\n\t\t# Only set the internal timezone if this isn't a\n\t\t# default timezone.\n\t\tif self.timezone!= _TIMEZONE_DEFAULT:\n\t\t\tself._tz = timezone(self.timezone"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdtnow = datetime.datetime.now(tzutc())\n\tdtnow_fold = dt.replace(tzinfo=tzutc())\n\tdtnow_fold_ambig = self.__tz_db.is_ambiguous_fold(dtnow_fold)\n\tif dtnow_fold and time_difference(dtnow, dtnow_fold) == 1 and self.__tz_db.is_ambiguous(dtnow):\n\t\timport sys\n\t\tfrom ambcache import Ambcache\n\t\tambcache = Ambcache('/var/sp"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# A few lines of context to remember:\n\t#\n\t# * At this point we know that the given datetime refers to a standard\n\t#   timedelta in some timezone we know the datetime belongs to.\n\t#\n\t# * datetime objects in Gedcom are internally stored in timezone-aware\n\t#   :class:`datetime.datetime` objects, so this method does not use\n\t#   timezone awareness to calculate the timezone-aware datetime\n\t#   correctly.\n\n\tif not dt.tzinfo:\n\t\t"}, {"generated_text": "def _fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is None and dt.tzinfo_offset == 0:\n\t\tif dt.time() == utcnanotime():\n\t\t\t# the datetime is the \"wrong\" way around\n\t\t\t# so the ambiguous datetime is this one!\n\t\t\treturn self\n\treturn self.datetime_with_fold(*self.datetime_from_unambiguous(*self.datetime_from_unambiguous(dt)))\n\t"}]}
{"task_id": "62b8982f755ee91dce50a241", "prompt": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tdays = relativedelta.normalize(self.__days)\n\thours = days.hours + days.days * 24\n\treturn relativedelta(days=days.days, hours=hours)\n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn _normalize(self._datetime,self._freq)\nif __name__ == '__main__':\n\tr = relativedelta(hours=1)\n\t\"\"\">>> relativedelta(hours=1)\n\trelativedelta(hours=1)\n\t\"\"\"\n\tprint(r.normalized())\n\n\tr = relativedelta(months=1)\n\t\"\"\">>> relativedelta(hours=1)\n\trelativedelta(hours=1)\n\t\"\"\"\n\tprint(r.normalized())"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n\treturn Relativedelta(years=self.years,\n\t\t\t\t\t\tmonths=self.months,\n\t\t\t\t\t\tdays=self.days,\n\t\t\t\t\t\thours=self.hours,\n\t\t\t\t\t\tminutes=self.minutes,\n\t\t\t\t\t\tseconds=self.seconds)\n\n\nclass D(object):\n\t\"\"\"\n\tDictionary-like data structure for defining the difference between two dates.\n\t\n\t>>> d = D(days=+1, hours=12)\n\t>>> d.days\n\t+1\n\t>>> d.hours\n\t12\n\t>>> d['days']\n\t"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n\trd = relativedelta()\n\treturn relative_delta(self.__class__, rd, +1)\n\t\n\n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\treturn Relativedelta(\n\t\tyears=int(math.floor(self.years)),\n\t\tmonths=int(math.floor(abs(self.months))),\n\t\tweeks=int(self.weeks),\n\t\tdays=abs(self.days),\n\t\thours=int(math.floor(abs(self.hours))),\n\t\tminutes=int(math.floor(abs(self.minutes))),\n\t\tseconds=int(math.floor(abs(self.seconds))),\n\t\tmicroseconds=int(self.microseconds)]\n\t\n\t"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tresult = relativedelta()\n\tfor name, value in self.items():\n\t\tresult[name] = value\n\treturn result\n\n\t# days = relativedelta(self.days)\n\n\t# if self.seconds < 0:\n\t#     days = -days\n\t# self.days += days\n\n\t# return days\n\nclass datetimecomparison:\n\t\"\"\"\n\tThis class provides a comparision operator between two datetime objects\n\twhich returns either:\n\n\t- 0, if ``self`` is equal to ``other``\n\t- 1, if ``self"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\ttotal = daysAdd(self.days.value, float(hoursAdd(self.hours.value)))\n\tmodDay = mod(self.days.value, total)\n\tdelta = timedelta(days=0)\n\tfor day in range(0, modDay):\n\t\tdelta += relativedelta.relativedelta(days=1)\n\treturn relativedelta.relativedelta(hours=float(self.hours.value),\n\t\tminutes=float(self.minutes.value), seconds=0,\n\t\tseconds=0, microseconds="}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tday_attribute_name_map = (\n\t\t('years', 'years'),\n\t\t('months','months'),\n\t\t('weeks', 'days_in_month')\n\t)\n\tdays_and_hours_attribute_name_map = day_attribute_name_map + (\n\t\t('hours', 'hours'),\n\t\t('minutes','minutes'),\n\t\t('seconds','seconds'),\n\t\t('microseconds','microseconds')\n\t)\n\treturn self.__class__(**{attribute_name : attribute_value for attribute_name, attribute_"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\t\n\treturn super(RelativeDelta, self).normalized\n"}, {"generated_text": "def normalized(self):\n\"\"\"\n\tReturn a version of this object represented entirely using integer\n\tvalues for the relative attributes.\n\t\n\t>>> relativedelta(days=1.5, hours=2).normalized()\n\trelativedelta(days=+1, hours=+14)\n\t\n\t:return:\n\t    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n\t\"\"\"\n\tdays = int(self.days)\n\thours = int(self.hours)\t\t\n\treturn relativedelta(days=days,hours=hours)\n"}]}
{"task_id": "62b89640755ee91dce50a114", "prompt": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif isinstance(namefunc, unicode):\n\t\treturn namefunc.encode('utf-8')\n\telse:\n\t\treturn namefunc\n\t \ndef is_tzurl(s):\n    \"\"\" Test if s is a tzurl \"\"\"\n    valid_URL_re = r'^https?:\\/\\/([\\da-z\\.-]+).*'\n    return bool(re.match(valid_URL_re, s))\n \ndef make_tzurl(tzurl):\n\t\"\"\" Make tzurl a UTC compatible string \"\"\"\n\n\treturn t"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif sys.version_info[0] < 3:\t\t\n\t\tdef py2to3_tzname_in_python2(tzname):\n\t\t\tif isinstance(tzname, bytes):\n\t\t\t\treturn tzname.decode('utf8')\n\t\t\treturn tzname\t\t\t\n\t\treturn py2to3_tzname_in_python2\n\telse:\n\t\tdef py3to3_tzname_in_python2(tzname):\n\t\t\tif isinstance(tzname, str):\n\t\t\t\treturn tzname\n\t\t\treturn tzname.decode('"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif sys.version_info[0] < 3:\n\t\tnamefunc2 = lambda x: namefunc(x).encode(\"ascii\", \"ignore\")\n\telse:\n\t\tnamefunc2 = lambda x: \"%s.tzinfo%s\" % (x, namefunc(x))\n\n\treturn namefunc2\n\n\ndef tzname_fix(namefunc):\n\t\"\"\"\n\ttzname() can be a Unicode-callable or bytestring. In Python 3, and also\n\tfor some versions of Python 2 (e.g. >=2.7."}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\treturn tzname_in_python2(lambda self, name: namefunc(self.strftime('%Z')))\n\n#-------------------------------------------------------------------------------\n\ntzname_in_python2 = staticmethod(partial(tzname_in_python2, namefunc = tzn_wday))\n\n#-------------------------------------------------------------------------------\n\n# This function was copied from Python 2.5 source code\n# Copyright (c) 2001-2015 Python Software Foundation; All Rights Reserved\ndef _format_offset(offset):\n    if offset < 0:\n        sign = '-'\n        offset = -offset\n"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif isinstance(namefunc,bytes):\n\t\tnamefunc = namefunc.decode('utf-8')\n\tname = name(namefunc)\n\treturn str(name.encode('ascii','replace').decode('ascii','replace'))\n\n\nif sys.version_info >= (2,6):\n\tfrom pytz.zoneinfo import gettz as get_tz\nelse:\n\tfrom pytz.tzinfo import gettz\n\tdef gettz():\n\t\traise NotImplementedError\n\n\nclass TimeZoneInPython2(object"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\treturn namefunc.encode(\"mbcs\")\n\n#\n#   Tests\n#\n\ndef normalize_tzname_output():\n\tprint 'Tests...'\n\tfor (i, name) in enumerate(zones_table):\n\t\t# For debugging, we can turn off all checks\n\t\tuse_tests_locale = 1\n\n\t\tif not use_tests_locale:\n\t\t\tcontinue\n\n\t\tprint '-' * 37\n\t\tprint 'Testing tzname() for {}'.format(name)\n\n\t\t# Check each combination of standard/daylight time,"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tif HAVE_2_TO_3:\n\t\treturn tzname\n\tfunc = namefunc\n\tif namefunc not in _TZNAMES:\n\t\tif isinstance(namefunc, type):\n\t\t\tfunc = lambda x: namefunc(x).decode('ISO-8859-1')\n\t\telse:\n\t\t\tfunc = namefunc\n\t\t# XXX: check what happens in the other threads\n\t\t_TZNAMES[func] = True\n\treturn func\n\ndef _check_tzname():\n\t# For now, we only check that the standard Python string"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef unicode_or_bytestring(thing):\n\t\tif type(thing) is unicode:\n\t\t\treturn thing.encode(\"utf-8\")\n\t\treturn thing\n\treturn unicode_or_bytestring\n\nif sys.version_info[0] < 3:\n\tpy_tzname_func = tzname_in_python2(getattr(py_datetime, 'tzname', py_datetime.tzname))\n\tpy_timezone_module = sys.modules['pytz']\nelse:\n\tpy_tzname_"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef wrap_tzname(self):\n\t\t\"\"\"returns a unicode string\"\"\"\n\t\t\n\t\tself.tzname\n\treturn wrap_tzname\n"}, {"generated_text": "def tzname_in_python2(namefunc):\n\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\n\ttzname() API changed in Python 3. It used to return bytes, but was changed\n\tto unicode strings\n\t\"\"\"\n\tdef tzname(self, dt):\n\t\t# Python3 has a slightly changed str/bytes/unicode interface\n\t\tif sys.version_info[0] >= 3:\n\t\t\tif isinstance(namefunc(dt), unicode):\n\t\t\t\treturn namefunc(dt).encode(default_encoding)\n\t\t# Python2\n\t\treturn namefunc(dt)\n\treturn tzname\n\ndef change_file_encoding(handler, encoding):\n\t\"\"\"\n\tChange the file encoding of responses\n\tThis was a workaround for some issues with the encoding handling in"}]}
{"task_id": "62b87d24d292efb640a55670", "prompt": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\tfrom fortpyversion import FortpyVersion\n\t\tversion=FortpyVersion()\n\texcept ImportError:\n\t\tversion='No version information available. Please copy fortpy/LICENSE to src, and set up __findoutfile__ as a file in src/site-packages, using the version number information returned.  This should also be done from the test framework.  The easiest way to do this is by running the following python command:\\n\\nimport sys; import fortpy; fortpy._findoutfile = sys.argv[0]\\n\\nor equivalent;"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\treturn g.version\n\texcept NameError:\n\t\treturn default_version\n\treturn None"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tdefault = {\"error_str\": None, \"error\": None, \"version\": None, \"full_version\": None}\n\tv = get_git_tag()\n\tif v:\n\t\tdefault.update(v)\n\n\tGIT_COMMIT_ID = get_git_commit_id()\n\tif GIT_COMMIT_ID:\n\t\tdefault.update({\"commit_id\": GIT_COMMIT_ID})\n\n\tGIT_DIR = dirname(__file__)\n\ttry:\n\t\t# Check for a git folder\n\t\tGIT_"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tdefault = {'version': 'unknown', 'full-revisionid': 'unknown', 'dirty': None,\n 'error': 'no',\n 'date': 'Thu Jan 09 09:22:01 1970 -0500'}\n\trev = which_symbolic_revision()\n\tif rev:\n\t\tp = os.popen('hg identify -R %s log | '\n                    'sed -e \"s/^Revision: //\" | tr -d \"\\r\"'\n                    ,'s')\n\t\ttry:\n\t\t\tfor line in p:\n\t\t\t\t"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tif hasattr(os, 'uname'):\n\t\timport platform\n\t\tif os.uname()[0] == \"Linux\":\n\t\t\t# Linux\n\t\t\ttry:\n\t\t\t\tver = open(\"/etc/debian_version\").readline().strip()\n\t\t\t\tif not (ver == \"<unavailable>\" or ver == \"unknown\"):\n\t\t\t\t\tver = ver.split(\".\")[0]\n\t\t\t\treturn str(ver), 0\n\t\t\texcept:\n\t\t\t\t# Maybe Debian not enabled?\n\t\t\t\tprogname = os.path.splitext(os.path.basename(sys"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttry:\n\t\t# Try command line argument -v, -vv, --version, --patchlevel\n\t\t# Order is important; --patchlevel must come before --version\n\t\treturn {'version': __version__, 'full-revisionid': str(REVISION)}\n\texcept (ValueError, NameError):\n\t\treturn {'version': '0+unknown', 'full-revisionid': None}\n\ndef get_versions():\n\t\"\"\"Get version information or return default if unable to do so.\n\n\tThis function provides version information using Python's\n\t\"development/"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\tversions = {}\t\n\t\n\ttry:\n\t\tf = open('/etc/version.info','r')\n\t\tin_str = f.read()\n\t\tf.close()\n\n\t\tin_str_arr = in_str.split('\\n')\n\n\t\tf = open('/etc/version.info','r')\n\t\tout_str = f.read()\n\t\tf.close()\n\n\t\tout_arr = out_str.split('\\n')\n\t\t\n\t\tfor i in range(0,min(len(in_str_arr),len(out"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\t# Try importing the install_logging module from the install_logging\n\t# library\n\ttry:\n\t\tfrom install_logging import get_version\n\t\treturn get_version\n\texcept ImportError:\n\t\t# Not available, return a reasonable default. This is what happens\n\t\t# when running \"setup.py install\", due to lack of logging.\n\t\treturn ( '<unknown>', '<unknown>', 0 )\n\n# Get some initial values.\nversion_info = get_versions()\n\nVERSION = version_info[0]\nCOMMIT_HASH ="}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\ttypes = dict(\n\t\tmodule_version=[],\n\t\tbot_version=[],\n\t\tnumpy_lib_version=[],\n\t\tnumpy_api_version=[],\n\t\tscipy_version=[],\n\t\tsklearn_version=[],\n\t\tmatplotlib_version=[],\n\t\tpylab_version=[],\n\t\t)\n\tfor type in types:\n\t\tthis_ver_info = types[type]\n\t\tthis_ver_info = this_ver_info if this_ver_info is not None else []\n\t\ttry:\n\t\t\tthis_ver"}, {"generated_text": "def get_versions():\n\"\"\"\n\tGet version information or return default if unable to do so.\n\t\"\"\"\n\timport yaml\n\ttry:\n\t\tf = open(os.path.join(os.path.dirname(__file__),\n\t\t\t\t\t'../version.yml'), 'r')\n\t\tver = yaml.safe_load(f.read())\n\t\tf.close()\n\t\treturn ver\n\texcept:\n\t\tprint(\"Unable to determine version from file version.yml\")\n\t\tversion = \"unknown\"\n\treturn version"}]}
{"task_id": "62b87d24d292efb640a5566f", "prompt": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif len(pieces) > 1:\n\t\treturn _format_error(\"too many arguments\")\n\tcache = getattr(settings, \"PINAX_PERMALINK_CACHE\", None)\n\tif cache and cache.get(pieces[0], False):\n\t\treturn \"\"\n\tbody = pieces[0]\n\turl_prefix = getattr(settings, \"PINAX_PERMALINK_URLPREFIX\", None)\n\tif url_prefix:\n\t\tbody = re.sub(r'(.*)://(?!https?://)', url_prefix,"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif style == \"pep440\":\n\t\t\treturn render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\t\treturn render_pep440_pre(pieces)\n\telif style == \"pep440-post\":\n\t\t\treturn render_pep440_post(pieces)\n\telif style == \"pep440-old\":\n\t\t\treturn render_pep440_old(pieces)\n\telif style == \"git-describe\" :\n\t\t\treturn render_git_describe(pieces)\n\telif style =="}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces:\n\t\tif style == 'perl':\n\t\t\treturn render_perl(pieces)\n\t\telif style == 'python':\n\t\t\treturn render_python(pieces)\n\t\telif style == 'npm':\n\t\t\treturn render_npm(pieces)\n\t\telif style == 'yaml':\n\t\t\treturn render_yaml(pieces)\n\ndef update(versionfile_abs, ipydir, fname):\n\t\"\"\"\n\tUpdate an ipython package's MANIFEST file.\n\t\"\"\"\n\tipydir = ipydir + os."}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tpass\n\n"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tpass\n\n_version_template = \"{my_version} - {my_commit_id}\"\n\nif __name__ == '__main__':\n\ttry:\n\t\timport json\n\texcept ImportError:\n\t\tprint( \"Could not load json module:  No version info will be available.\" )\n\n\tsys.exit(main())\n"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif not style or style == \"default\":\n\t\tstyle = \"sphinx\"\n\tif style == \"pyspark\":\n\t\tfor p in [\"org.apache.spark\"]:\n\t\t\tyield \".. include:: /{}/s{}/_meta.yaml\".format(prefix[p], sub_dir)\n\telif style in (\"rst\", \"sphinx\"):\n\t\textensions.append(\".py\")\n\n\tfor m in [\"meta.yaml\", \"package.md\", \"overview.md\", \"readme.md\"]:"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn  (style * 0) + ('!!'+ style * 0)\n\tif style == \"default\":\n\t\treturn render_version_pieces(pieces)\n\telif style == \"full\":\n\t\treturn render_full_version_pieces(parsed)\n\telif style == \"short\":\n\t\treturn render_short_version_pieces(parsed)\n\telif style == \"version_info\":\n\t\treturn render_version_info(parsed)\n\telse:\n\t\traise ValueError(\"unknown style '"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif None in pieces.values():\n\t\tsys.stderr.write('Warning: empty version files are not copied.')\n\t\treturn\n\topen(style, 'w').write(render_pieces(pieces).encode('utf-8'))\n\ndef render_pieces(pieces):\n\t\"\"\"Render the given version pieces into the requested style.\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\n\t\t\t\"version\":  \"unknown\",\n\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\"dirty\": None,\n\t\t\t\"error\": pieces[\""}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces.get('error'):\n\t\treturn style()\n\n\tfor key, value in style.changes.items():\n\t\tif value:\n\t\t\tstyle.start_insertion()\n\t\t\tstyle.insert_value()\n\tret = ''.join(pieces.text)\n\tfor key, value in style.changes.items():\n\t\tif value:\n\t\t\tstyle.end_insertion(key)\n\treturn ret\n"}, {"generated_text": "def render(pieces, style):\n\"\"\"\n\tRender the given version pieces into the requested style.\n\t\"\"\"\n\tif pieces[\"error\"]:\n\t\treturn {\"version\": None,\n\t\t\t\t\"full-revisionid\": pieces.get(\"long\"),\n\t\t\t\t\"dirty\": None,\n\t\t\t\t\"error\": \"%s\" % (pieces[\"error\"])}\n\tif not style or style == \"default\":\n\t\tstyle = \"pep440\"  # the default\n\tif style == \"pep440\":\n\t\trendered = render_pep440(pieces)\n\telif style == \"pep440-pre\":\n\t\trendered = render_pep440_pre(pieces)\n\telif"}]}
{"task_id": "62b87d24d292efb640a5566d", "prompt": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif pieces[1].startswith('g') or pieces[1] in ['gp', 'gyp']:\n\t\tif not 'g' in pieces[1]:\n\t\t\tpieces[1] += 'g'\n\telif not '.' in pieces[1]:\n\t\tpieces[1] += '.'\n\treturn pieces\n\ndef sha_hash(commit):\n\t\"\"\"\n\t\t\tReturn the sha-1 hash of a commit\n\t\t\t\"\"\"\n\tcommit_sha = git_ref_commit_id(commit)\n\t# We really want to hash the file\n\t"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif len(pieces) < 2:\n\t\treturn ''\n\n\tif pieces[-1] == '.':\n\t\t# We have existing dot\n\t\tdel pieces[-1]\n\t\treturn '.'\n\telse:\n\t\t# No existing dot, so just return the plus we got\n\t\treturn '+'\n\ndef reverse_from_dots(path):\n\t\"\"\"\n\tTakes a path +,., or.+ and reverse it\n\t\"\"\"\n\tif len(path) < 2:\n\t\t# There aren't any dots\n\t\treturn path\n\tpath ="}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif not u'+' in pieces:\n\t\treturn u'+'\n\telif not u'.' in pieces:\n\t\treturn u'.'\n\ndef prepare():\n\t\"\"\"\n\tPrepare for outputting lines of output\n\t\"\"\"\n\tglobal lines\n\tlines = []\n\n\tline_list = []\n\n\twith open(options.filename, 'r') as o_file:\n\t\tfor o_line in o_file:\n\t\t\tif \": \" not in o_line or o_line[0] == '#':\n\t\t\t\tcontinue\n\t\t\to_line"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif not \".+\" in pieces:\n\t\treturn \".\" + pieces.pop(0).replace('\\\\.', '.')\n\telse:\n\t\treturn \"+\"\n\ndef is_descriptor(filename):\n\t\"\"\"Returns True if the given filename corresponds to a directory or a file\n\tthat is a git-descriptor.\"\"\"\n\treturn gitConfig(\"git-dir\") == gitConfig(\"git-describe-dir\")\n\ndef _abbreviate(version, abbrev_len=7):\n\tversion_numbers = version.split('.')"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif is_plus_or_dot(pieces):\n\t\treturn '.'\n\telse:\n\t\treturn '+'\n\ndef strip_plus_or_dot(pieces):\n\t\"\"\"\n\tCleanup plus/dot characters.\n\t\"\"\"\n\tfor i in range(0, len(pieces)):\n\t\tpieces[i] = plus_or_dot.sub('', pieces[i])\n\treturn pieces\n\n\ndef to_uft8(data):\n\tuft8 = []\n\tfor chunk in data:\n\t\tchunk_bytes = chunk\n\t\tu"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif not isinstance(pieces, list):\n\t\traise ValueError(\"Expected a list of files, but got:\" + type(pieces))\n\tif len(pieces) == 1 and ( isinstance(pieces[0], dict) or isinstance(pieces[0], OrderedDict) ):\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n\ndef get_relative_from_dir(start_dir, relative_dir):\n\tstart_dir = normpath(start_dir)\n\trelative_dir = normpath(relative_dir)\n\treturn file(\""}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif not re.search('^[a-zA-Z]',\n\t\tpieces\n\t):\n\t\treturn {'plus' : '+'}\n\telse:\n\t\treturn {'dot' : '.' }\n\n\ndef split(piece):\n\t\"\"\"\n\tSplit a file at the first occurrence of a separator\n\n\t@param piece: String or string-like to be splitted\n\t@return: First separator found, or None if no separator found\n\t@raise Exception: If piece is not string-like\n\t\"\"\"\n\tif isinstance(piece,"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif '+' in pieces:\n\t\treturn '+'\n\telif pieces[0] == '.':\n\t\treturn '.'\n\telse:\n\t\treturn '+'\n\n\ndef validate_email(email):\n\tif email.find('@')!= 2 or not re.match(r'[a-zA-Z0-9]+$', email):\n\t\traise ValidationError('invalid email')\n\telse:\n\t\treturn True\n\n\ndef create_email(password):\n\t\"\"\"\n\tValidated, returns the 'lowercase' email address\n"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif '+' in pieces:\n\t\treturn '+'\n\telse:\n\t\treturn '.'\n\n\ndef safe_version(version):\n\t\"\"\"Return a string that should be used for a version number.\"\"\"\n\t# For strings that we do not want to parse.\n\tif not version:\n\t\treturn None\n\n\t# Convert to array of regexps\n\tmatch_groups = ()\n\tno_prereleases = False # Don't allow prerelease numbers\n\tif re.search('+[^.]+$', version): # Strip off anything with a '+'\n"}, {"generated_text": "def plus_or_dot(pieces):\n\"\"\"\n\tReturn a + if we don't already have one, else return a .\n\t\"\"\"\n\tif \"+\" in pieces:\n\t\treturn \"\"\n\telse:\n\t\treturn \".\"\n\"\"\"\n\t\"\"\"\n\n\t\"\"\"\n\t# Just one period; strip it and output.\n\tif pieces[-1] in \"xX\":\n\t\treturn pieces[:-1]\n\t# Not a period; break out of this state.\n\telif pieces[-1] in \"pP\":\n\t\traise ValueError(\"Invalid suffix\")\n\t# Two periods one after the other - strip one, output the other.\n\telif (\".\" in pieces) and (\".\" not in minus"}]}
{"task_id": "62b87d23d292efb640a5566b", "prompt": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\ttry:\n\t\tif verbose:\n\t\t\tlogging.debug(\"Run the command %s with args %s. Outputting to %s and logging to %s with verbose level %d\" % (str(commands), str(args), str(output_file), str(debug_log), verbose,))\n\t\telse:\n\t\t\tlogging.debug(\"Run the command %s with args %s.\" % (str(commands), str(args),))\n\t\tif not os.path.isabs(args[0]):\n\t\t\tcwd = cwd or os."}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcmd = subprocess.Popen(commands,\n\t    stdout=subprocess.PIPE,\n\t    stderr=subprocess.STDOUT if hide_stderr and args else subprocess.PIPE,\n\t    cwd=cwd,\n\t    env=env,\n\t    text=True,\n\t    shell=True\n\t)\n\tproc = None\n\n\tfor line in iter(cmd.stdout.readline, \"\"):\n\t\tif isinstance(line, bytes):\n\t\t\tline = line.decode(\"utf-8\")"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif isinstance(commands, str):\n\t\tcommands = (commands,)\n\tif isinstance(args, str):\n\t\targs = (args,)\n\treturn call(command(commands), args=args, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr, env=env)\n\ndef add_arg_to_commands(commands, arg, verbose=False):\n\t\"\"\"\n\tAdd a command (as string) to the given commands.\n\t\"\"\"\n\tif isinstance(commands,"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\trc = []\n\tfor command in commands:\n\t\tproc = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n\t\tfor line in iter(proc.stdout.readline, \"\"):\n\t\t\trc.append((command, line.strip()))\n\t\tif hide_stderr: continue\n\t\tproc.communicate()\n\t\tif proc.returncode:\n\t\t\tif verbose: print >>sys.stderr, \"command: \" + str(command),\n\t\t\tif verbose: print >>sys.stderr, \"return"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcall_args = [cmd for cmd in commands if commands[cmd]]\n\tprint \"Running\",''.join(call_args)\n\tpipe = subprocess.Popen(call_args, cwd=cwd, env=env, stdout=subprocess.PIPE,\n\t\t\t\tstderr=(subprocess.PIPE if not hide_stderr else None))\n\tstdout, stderr = pipe.communicate()\n\tif pipe.returncode!= 0 and not hide_stderr:\n\t\tprint 'ERROR: {}'.format(stder"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tproc = subprocess.Popen(commands, stdout=subprocess.PIPE if verbose else None,\n\t\tstderr=subprocess.PIPE if hide_stderr else None,\n\t\tstdin=subprocess.PIPE,\n\t\tcwd=cwd,\n\t\tenv=env)\n\n\t# Wait for all the subprocesses to join\n\ttry:\n\t\tproc_out, proc_err = proc.wait(int(TIMEOUT))\n\texcept subprocess.TimeoutExpired as e:\n\t\tproc_out, proc_"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif type(commands)!= list:\n\t\tcommands = [commands]\n\tproc = subprocess.Popen(commands, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None))\n\n\tstdout, stderr = proc.communicate()\n\n\tresult = {}\n\tif verbose:\n\t\tresult['cmd'] = commands\n\t\tresult['output'] = {\n\t\t\t'stdout': (verbose and stdout or stdout.strip"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\texit_code = True\n\tfor command in commands:\n\t\ttry:\n\t\t\tcall_shell(command, args, cwd=cwd, verbose=verbose,\n\t\t\t\t\thide_stderr=hide_stderr, env=env)\n\t\texcept Exception as ex:\n\t\t\tprint('Command \"%s\" failed: '%command, ex)\n\t\t\tif verbose:\n\t\t\t\ttraceback.print_exc()\n\t\t\texit_code = False\n\n\treturn exit_code\n\ndef run_shell(cmd):\n\t\"\"\"\n\tRun a shell command.\n\t"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcommands = [ str(x) for x in commands ]\n\tif verbose:\n\t    print '%s\\n%s' % (' '.join(commands),''.join(['$'+ x for x in args]))\n\tif os.access(cwd, os.F_OK):\n\t\tos.chdir(cwd)\n\tp = Popen(commands, stdout=open(os.devnull), stderr=STDOUT if hide_stderr else open(os.devnull), \\\n\t\t stdin=PIPE"}, {"generated_text": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t# Create tmpfile in temp directory\n\ttmpfile = os.path.join(tempfile.gettempdir(), 'pootle-nfs-pootle-admin.json')\n\t# Save dictionary\n\tf = open(tmpfile, 'w')\n\tf.write(json.dumps(env))\n\tf.close()\n\t# Run command in tmpdir\n\targs = [x for x in cmds if x]\n\tif cwd:\n\t\tcwd = os.getcwd()\n\telse:\n\t\tcwd = tempdir\n"}]}
{"task_id": "62b87d23d292efb640a55668", "prompt": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tthis_file = __file__\n\tgit_dir = os.path.abspath(os.path.join(this_file, os.pardir, 'gitchangelog'))\n\tif not os.path.exists(git_dir):\n\t\tgit_dir = os.path.abspath(os.path.join(this_file, os.pardir, '.git'))\n\t\tif not os.path.exists(git_dir):\n\t\t\tgit_dir = None\n\n\tmaster_file = configparser.ConfigParser()\n\twith"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tif not VCS:\n\t\treturn None\n\n\tconfig = Config()\n\tconfig.VCS = VCS\n\tconfig.PROJECT_MODULE = PROJECT_NAME\n\tif version is not None:\n\t\tconfig.version = version\n\n\tif os.getenv('VCS_VERSION_STRING') is not None:\n\t\tconfig.version_string = os.getenv('VCS_VERSION_STRING')\n\telse:\n\t\tconfig.version_string = VCS_VERSION_STRING\n\n\treturn config\n\n\nvcs_config = get_config()"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.set_options(command_options)\n\treturn config\n"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\ttry:\n\t\t\tconfig = VersioneerConfig()\n\t\t\texecfile(versioneer_local_file, config)\n\t\texcept (IOError, SyntaxError):\n\t\t\traise ImportError('Unable to read %s' % versioneer_local_file)\n\t\t\tsys.stderr.write(\"Error in %s\\n\" % versioneer_local_file)\n\t\t\ttraceback.print_exc()\n\treturn config\n\ndef get_cmdclass():\n\t\tcmds =\t{'version_cmd':VersionCommand,\n\t\t\t\t'version_plural_cmd':"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tfrom distutils.cmd import Command\n\tconfig = VersioneerConfig()\n\tconfig.set_options(\n\t\tmetadata = { \"url\": \"git@github.com:djangocore/django-versioneer.git\",\n\t\t\t\t\t\"version\": \"$version_short\",\n\t\t\t\t\t\"download_url\": \"https://github.com/djangocore/django-versioneer/tarball/$version\",\n\t\t\t\t\t\"doc_url\": \"http://packages.python.org/django-versioneer\",\n\t\t\t\t\t\"doc_download_url\": \"https://"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# creates empty VersioneerConfig instance\n\tversioneer_config = VersioneerConfig()\n\n\t# get settings\n\tfrom distutils.dist import Distribution\n\ttry:\n\t\tdist = Distribution()\n\texcept:\n\t\tdist = None\n\tneed_save = False\n\tif dist is not None:\n\t\tversioneer_config.VCS = dist.get_name()\n\t\tif dist.has_ext_modules():\n\t\t\tversioneer_config.src_dir = dist.ext_modules[0].get_source_dir()\n\n\t\tversioneer_config.user_dir"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tif config.versionfile_source is None:\n\t\t# Compute version from git:\n\t\tshow_response = main(\n\t\t\t['git', 'describe', '--tags', '--dirty', '--always'],\n\t\t\tcwd=config.checkout_root,\n\t\t\tstderr=subprocess.STDOUT)\n\t\t(ver, _) = show_response.split('-') # just use the first line\n\t\tshow_response.replace(ver, '')\n\t\tmatch = re.search(r'[^\\d]*\\d[\\.\\d"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# this method should always be called from here, right after\n\t# getcommandoptions.parser is set up\n\tif not os.getenv('VPACK_VERFILE'):\n\t\t# if no Versioneer config is set (by default it puts it there),\n\t\t# create config.\n\t\twith open(os.path.join(here, \"version.cfg\"), \"w\") as f:\n\t\t\tf.write(\"[versionfile_source]\\n\"\n\t\t\t\t\t\"VCS=git\\n\"\n\t\t\t\t\t\"tag_prefix = ddp_\\n\"\n\t\t\t\t\t\""}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\tif ENVIRONMENT.CODENAME is None:\n\t  raise Exception(\"VersioneerConfig not found.\")\n\t# Read config from vc_config.py if it exists\n\tif os.path.exists(VersioneerConfig):\n\t  config = VersioneerConfig()\n\t  exec(open(VersioneerConfig).read(),\n\t    {'__file__': VersioneerConfig})\n\telse:\n\t  config = VersioneerConfig()\n\t  config.update(\n\t    version=ENVIRONMENT.VERSION,\n\t    strict"}, {"generated_text": "def get_config():\n\"\"\"\n\tCreate, populate and return the VersioneerConfig() object.\n\t\"\"\"\n\t# this code is copied from versioneer.py, used to be included in\n\t# the versioneer.py file\n\tc = ConfigParser.ConfigParser()\n\tc.readfp(open('../../../setup.py'))\n\tassert c.has_option('versioneer','version') and \\\n\t\t(bool(c.get('versioneer','version')) or \\\n\t\t bool(c.get('version','short-version-only')))\n\tconfig = V(c.get('versioneer','version'),\n\t\t\t\tc.get('version"}]}
{"task_id": "62b87d23d292efb640a55667", "prompt": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(f):\n\t\tf._vcs = vcs\n\t\tf._method = method\n\t\treturn f\n\n\treturn decorator\n\ndef vcs_get_ref(ref, vcs):\n\t\"\"\"\n\tReturns vcs_ref and commit_id from a reference (a branch / tag / commit ID)\n\t\"\"\"\n\tif type(ref) == dict:\n\t\treturn None\n\n\tif vcs_regex.match(ref):\n\t\treturn ref, None\n\n\tm = vcs_ref_pattern.search(ref)\n\tif m:"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t# TODO: rename into'register_vcs_handler_method_with_repository'\n\t# TODO: merge with'register_vcs_handler'\n\t# TODO: merge with'register_vcs_handler_with_repository'\n\t# TODO: merge 'is_vcs_handler' with 'is_vcs_handler', and rename to 'is_vcs_handler_method'\n\tdef decorator(cls):\n\t\t\"\"\"\n\t\tCreate vcs base class with its VCS methods.\n\n\t\t\"\"\"\n\t\t#"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tregex = '^' + re.escape(vcs) + '$'\n\tdef decorator(handler):\n\t\tsetattr(handler, '_vcs', vcs)\n\t\tsetattr(handler, '_regex', regex)\n\t\tsetattr(handler, '_method', method)\n\t\treturn handler\n\treturn decorator\n\ndef unregister_vcs_handler(vcs):  # decorator\n\"\"\"\n\tCreate decorator to unmark a method as the handler of a VCS.\n\t\"\"\"\n\tdef dec(handler):\n\t\t"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\t# Method must be marked `@staticmethod` or `@classmethod`\n\t# on the decorated handler when it is called without an instance `vcs.\n\t# Otherwise:\n\t# 1. decorated handler may never work,\n\t# 2. decorated handler may throw exception when called on its own with argv.\n\tif not method.__module__.startswith('vcs'):\n\t\t# decorator should be applied on the `vcs` instance\n\t\traise ValueError('cannot apply decorator to method that is not vcs instance method')\n\t_vcs"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tif vcs in registry.handlers:\n\t\tregistry.handlers[vcs].add_handler(method)\n\t\treturn\n\n\tregistry.handlers[vcs] = set()\n\tregistry.handlers[vcs].add_handler(method)\n\tregistry.handlers[vcs].update(\n\t\t\trepo_handlers.handlers\n\t\t\tfor repo_handlers in repo_handlers\n\t) # remove common repo_handlers there, to avoid conflict\n\n\ndef register_repo_handler(name, methods):\n\t\"\"\"\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(f):\n\t\t\"\"\"\n\t\tCreate decorator with parameters.\n\t\t\"\"\"\n\t\tf.svn_methods = vcs\n\t\tf.svn_method = method\n\t\treturn f\n\n\treturn decorator\n\ndef unregister_vcs_handler(handler):\n\t\"\"\"\n\tUnregister handler from svn_unregister_command_handlers.\n\t\"\"\"\n\tif handler in handlers():\n\t\thandlers().remove(handler)\n\ndef handlers():\n\t\"\"\"\n\tReturn a list of registered VCS handlers.\n\t\"\"\"\n\treg"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tassert vcs == 'git'\n\tassert method.__is_vcs_method()\n\n\tdef decorator(f):\n\t\tf.__vcs_method_name__ = method.__name__\n\t\tf.__vcs_method__ = method\n\t\tf = _vcs_handler(method, f)\n\t\tsetattr(f, '_meth', f)\n\t\tf.vcs_key = f.__vcs_method_name__\n\t\treturn f\n\n\treturn decorator\n\n\ndef _vcs_handler(method, f):\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(f):\n\t\tf.VCS_method = vcs\n\t\tf.VCS_method_name = method\n\t\tf.VCS_method_kwargs = dict()\n\t\tf.VCS_method_kwargs['refresh'] = False\n\t\treturn f\n\treturn decorator\n\n\ndef VCS_method(**kwargs):\n\t\"\"\"\n\t\tDecorator for a method that handles the VCS\n\t\"\"\"\n\tdef decorator(f):\n\t\tkwargs['VCS_method'] = f\n\t\treturn f\n\treturn decor"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\torig_method_name = vcs + '_' + method\n\n\t@functools.wraps(orig_method_name)\n\tdef method_decorator(func):\n\t\tglobals()[orig_method_name] = func\n\t\treturn method_decorator\n\n\treturn method_decorator\n\n\ndef register_vcs_command(vcs, command):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS command.\n\t\"\"\"\n\torig_method_name = vcs + '_' + command\n\n"}, {"generated_text": "def register_vcs_handler(vcs, method):  # decorator\n\"\"\"\n\tCreate decorator to mark a method as the handler of a VCS.\n\t\"\"\"\n\tdef decorator(func):\n\t\tif vcs is not None and method is not None:\n\t\t\tvcs.register_handler(method, func)\n\t\treturn func\n\treturn decorator\n\n\n@register_vcs_handler\ndef vcs_update(vcs):\n\t\"\"\"\n\tUpdate a VCS, the VCS object is passed in as an argument after the method\n\tis decorated.\n\n\tNote that the function is expected to raise an :class:`UIPatchException` if\n\tthe VCS operation fails.\n\t\"\"\"\n\ttry:\n\t\tif"}]}
{"task_id": "62b45e945108cfac7f210a4a", "prompt": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\t# validate objects in hierarchy\n\tnum_objects = 0\n\tgood_objects = 0\n\t\n\tif not os.path.exists(self.root):\n\t\t\traise ValidationException(\"root path does not exist\")\n\telif not os.path.isdir(self.root):\n\t\t\traise ValidationException(\"root isn't a directory\")\n\telse:\n\t\thierarchy_objects = fs.listFiles(self.root)\n\t\t\n\t\tfor root in hierarchy_objects:\n\t\t\t# iterate through child objects\n\t\t\tnum_objects += 1;"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tif not self.validate_hierarchy_tree(show=False):\n\t\tlogger.error(\"hierarchy validation (tree) failed.\")\n\t\treturn False\n\t\n\tchecked_objects = [self.root_obj_uid]\n\tnum_objects = 0\n\tgood_objects = 0\n\t\n\twhile len(checked_objects) > 0:\n\t\tobj_uid = checked_objects.pop(0)\n\t\tcheck_obj = self.get_object(uid=obj_uid)\n\t\tif not check_obj:  # may be due to"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor cur_name, cur_type in self.get_all_names(validate_objects, check_digests):\n\t\tif cur_name =='smb.contents':\n\t\t\tcontinue\n\t\tcur_type_obj = None\n\t\ttry:\n\t\t\tcur_type_obj = cur_type.Object(self)\n\t\texcept:\n\t\t\texceptionType, exceptionValue, exceptionTraceback = sys.exc_info()\n\t\t\tsys_msg = \"%d - Invalid object type %s at"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\tnum_objects = 0\n\tgood_objects = 0\n\t\n\tself.logger.debug(\"checking %s\" % self.path)\n\t\t\n\tvalid = True\n\tdir_name = self.path\n\tfor i in xrange(10):\n\t\tnew_parent = self.get_parent_folder_path(dir_name)\n\t\tif new_parent is None:\n\t\t\tif self.version == 1:\n\t\t\t\tnew_parent = self.parent.path\n\t\t\t\tif new_parent and new_parent.endswith(\"/\"):\n\t\t\t\t\t"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\thmac_verify_all(self.store_dir, is_dir=True) # Make sure all files are valid\n\tnum_objects = 0\n\tgood_objects = 0\n\t\n\tfor root, dirs, files in walk(self.root_dir, topdown=False):\n\t\tdirs.sort() # Make sure subdirs are reported after their parents\n\t\tfor f in files:\n\t\t\tif f.find('data') == -1:\n\t\t\t\t# Skip files that aren't raw data\n\t\t\t\tcontinue\n\t\t\tself.store_path = u'{"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\t\n\t# Iterate through the hierarchy objects and check if we find a valid object\n\tfor o in os.listdir(self.root):\n\t\tif(not valid):\n\t\t\treturn None\n\t\telse:\n\t\t\tnum_objects += 1\n\t\t\to = os.path.join(self.root, o)\n\t\t\t\n\t\t\tif(os.path.isdir(o)):\n\t\t\t\tdirs.append(o)\n\t\t\telif(o.endswith('.o')):\n\t\t\t\tnum_objects"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tobjects = self.items()\n\ttotal_objects = len(objects)\n\tgood_objects = 0\n\tnum_objects = 0\n\t\n\tfor o in objects:\n\t\tnum_objects += 1\n\n\t\tif not o.path.is_dir():\n\t\t\tcontinue\n\n\t\tif check_digests:\n\t\t\tif not hasattr(o.stat, 'digest'):\n\t\t\t\t# Not digest - it must be a regular file or a symlink\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tif not o.digest_exists():\n\t\t\t\tself.warning(\"Incomplete digest"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\terrors = []\n\t\n\tif validate_objects:\n\t\tres = self.validate_object_ids(show_warnings, check_digests)\n\t\tfor i, obj in res:\n\t\t\tnum_objects += 1\n\t\t\tgood_objects += 1\n\t\t\tif i == 0:\n\t\t\t\terrors.extend(obj.storages)\n\t\t\telse:\n\t\t\t\tfor storage in obj.storages:\n\t\t\t\t\terrors.append(storage)\n\t\n\treturn (num_objects, good_objects"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects_checked  = 0\n\tnum_object_hierarchy = 0\n\t\n\tif validate_objects:\n\t\tnum_objects_checked += self.validate_object_hierarchy(check_digests=check_digests, show_warnings=show_warnings)\n\t\tnum_object_hierarchy += self.get_number_of_object_hierarchy_members(check_digests=check_digests, show_warnings=show_warnings)\n\t\n\treturn (num_objects_checked, num_object"}, {"generated_text": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\"\"\"\n\tValidate storage root hierarchy.\n\t\n\tReturns:\n\t    num_objects - number of objects checked\n\t    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\t\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tif self.path:\n\t\tself.root_collection = self.path\n\t\tself.root_name = self.root_collection.split(\"/\")[-1]\n\n\t\tstorage_hierarchy.validate(self.path, store_objects=self.store_objects, validate_objects=self.validate_objects, \n\t\t\tcheck_digests=check_digests, parent=self, verbose=self.verbose, show_warnings=show_warnings)\n\n#"}]}
{"task_id": "62b45e515108cfac7f210a3c", "prompt": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tpath = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"s3db\")\n\tself.s3dbpath = path if not path.endswith('/')  else path[:-1]\n\tself.options = {\n\t\t's3_config': 'default',\n\t\t}\n\tself._load_s3db()\n\tself.db = self._db\n\t\n\tsuper(StorageRoot, self).initialize()\n\t\n\t# If we weren't given a bucket name on the command line"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n\tself.path = \"/tmp/ocfl/storage\"\n\tself.storages = OrderedDict()\n\"\"\"\n\tThis function creates a base storage object and initializes the storage.\n\"\"\"\ndef createBaseStorage(path):\n\t\"\"\" \n\t\tParameters:\n\t\t\tpath = string\n\t\tReturns:\n\t\t\tbase storage object \n\t\"\"\"\n\n\treturn Storage(path=path)\n\n\"\"\"\n\tThis function creates a set_storage object.\n\t\n\tFor internal use. Call storage.setStorage() to switch storages.\n\t\"\"\"\ndef"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t# self.root = Directory(self.root_name, self.storage)\n\t# self.root.create()\n\t# self.root.close()\n\tself.root = OCFLStorageObject(self.root_name)\n\tself.root.open()\n\n"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t# Check arguments\n\tif not self._arguments.root_directory:\n\t\t\tself._arguments.root_directory = \\\n\t\t\t\tself._arguments.local_directory + \"_ocfl_root\"\n\telse:\n\t\tself._arguments.root_directory = \\\n\t\t\tos.path.normcase(self._arguments.root_directory)\n\t\tif not os.path.isdir(self._arguments.root_directory):\n\t\t\tself._print_error(\"Root directory does not exist: '%s'\" % (self._arguments.root_directory))"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tprint \"OCFL: Initalize...\"\n\n\t# get a list of storage roots to use.\n\toptions = gparray.getDbList(None)\n\t# OCFDB is for now the only supported storage type.\n\tself.dbRoot = get_object(options.GetListOfDbs(\"OCFDB\"), \"ocf_root\")\n\tself.storageRoot = gparray.getDbRootDir(self.dbRoot)\n\n\t# initialize storage paths\n\tself.configPath = get_object(self.storageRoot, \"oci_config\")\n"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tLog.debug(\"ocfl:initialize\", root=self.path)\n\tself.store = OCFLStorage(root=self.path)\n\nclass OCFLBackend(StorageBackend):\n\n\tdef exists(self, path=None):\n\t\treturn self.cached_mtime(path) is not None\n\n\tdef get_info(self, path=None):\n\t\ttry:\n\t\t\tmtime = self.cached_mtime(path)\n\t\texcept OSError:\n\t\t\traise RequestError(404)\n\n\t\treturn {\n\t\t\t\"mtime"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t# OCFL storage root id and root path\n\tself.id = uuid.uuid1().hex\n\tself.path = pathlib.Path(self.id)\n\n\t# OCFL metadata\n\tself.metadata = OCFLMetadata()\n\n\t# OCFL OCFLDataDirectory cache\n\tself.datafile_cache = OCFLDirectoryCache(self.id)\n\n\t# OCFL OCFLDirectory cache\n\tself.directory_cache = {}\n\n\t# OCFL OCFLTree cache\n\tself.tree_cache = {}\n\n\t# OCFL OCFL"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\t\n\tassert(self.repositoryID!= None)\n\t\t\n\tself.initializeFileStorage(self.repositoryID.name)\n\tself.initializeDagInfo(self.repositoryID)\n\tself.initializeDagObjectList(self.repositoryID)\n\n\treturn\n\n\"\"\"\n\tCalled by OCFLDataStore.save_repository_information() to save\n\trepository information. Returns status.\n\n\"\"\"\ndef save_repository_information():\n\t\n\tassert(self.repository"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tnewRoot = self._provider._newStorageRoot()\n\t\n\treturn newRoot;\n\t\ndef remove(self, path):\n\tself.getConnection().remove(path);\n"}, {"generated_text": "def initialize(self):\n\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.name = time.strftime(\"%.8s:%m-%d-%H%M%S\")\n\tself.path = tempfile.mkdtemp(prefix=self.name)\n\ndef finalize(self):\n\t\"\"\"\n\tDelete the OCFL storage root, including all content.\n\t\n\tThe function returns a list of (error, info) entries with errors.\n\t\"\"\"\n\terrors = []\n\n\ttry:\n\t\tos.rmdir(self.path);\n\t\terrors.append(None)\n\texcept OSError:"}]}
{"task_id": "62b45e2eb89c9fd354170232", "prompt": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif version:\n\t\tmajor, minor, micro = version.split('.')\n\t\tnewversion = '.'.join((str(int(major) + 1), minor, micro))\n\telse:\n\t\tnewversion = version or '0'\n\treturn newversion\n\t\n\ndef main_class() :\n\t\"\"\"\n\tReturn the class that implements the'main' command, ie. including main() into the module.\n\t\"\"\"\n\t# Get the current module\n\tcmdclass = vars(globals())['cmdclass']\n\t# Build up the list of"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif not version:\n\t\treturn version\n\tif not re.match(r'v?\\d+\\.\\d+\\.(\\d+)-', version):\n\t\traise ParseError('Invalid version. {} does not match the pattern: v?\\\\d+\\\\.\\\\d+\\\\.(\\\\d+)-'.format(version))\n\t\n\t# if version is only digits, we're okay\n\tif len(version) == 4:\n\t\treturn version[0] + version[3] + version[1] + '-' + version[2]\n\telse:\n"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\t# non-zero prefixed\n\tif version[0]!= '-':\n\t\tversion = '-%s' % version\n\t\n\t# non-zero postfixed\n\tif version[-1] == '-':\n\t\tversion = '%s%s' % (version[:-1], str(int(version[:-1]) + 1))\n\t\n\treturn version\n\ndef version_string(version):\n\t\"\"\"\n\tThe canonical version string for a library.\n\t\"\"\"\n\treturn '-'.join([str(x) for x in re"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tnum_digits = len(version)\n\tif num_digits >= 9:\n\t\treturn '' + chr(ord(version[0])+1) + version[1:]\n\telif num_digits >= 7:\n\t\treturn version[:-2] + chr(ord(version[-4])+1)\n\telse:\n\t\treturn version + chr(ord(version[-1])+1)\n\ndef previous_version(version):\n\t\"\"\"\n\tPrevious version identifier following existing pattern.\n\t\n\tMust deal with both zero-"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tversion_obj = Version(version)\n\tversion_obj.sort()\n\tnext_version = str(version_obj[-1].version)\n\tif len(next_version) == 1: # not zero-prefixed version?\n\t\tnext_version += \"0\"\n\treturn next_version\nversion_pattern = re.compile(\"[\\d.a-zA-Z]{32}\")\n\ndef parse_version(version):\n\t\"\"\"\n\t\tReturns the Version object for version.\n\t\tExample: parse_version(\"0.0.2\")"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif version[0] in ('0','') and len(version)>1:\n\t\tv = version[1:]\n\telse:\n\t\tv = version\n\tv_nums = [int(d) for d in re.split(r'(\\D+)', v)]\n\t# The version numbering scheme follows that used by SVN and CVS.\n\t# See http://subversion.apache.org/docs/release-notes/release_notes.html#version-numbers\n\t# e.g. 1.5-4 is SVN revision number four and"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\t\n\treturn _prefix(version) if version[0] in ('0', '') else '%s%d' % (_prefix(version), version[0])\n\t\n\n\ndef last_version(version):\n\t\n\tversion = ''.join(version) # for 0.1.x compatibility\n\ttry: \n\t\tversion = str(int(version))\n\texcept ValueError: \n\t\treturn '99.99.99'\n\t\n\treturn '%s.%s' % (version[1:-1], version[-1:])\n\n\ndef"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\tif version is None:\n\t\treturn version\n\tif not version.startswith('v'):\n\t\tversion = 'v'+version\n\tnewver = version[2:]\n\tversion_list = [(int(x) if x.isdigit() else x) for x in newver.split('.')]\n\tfor idx, val in enumerate(version.split('.')):\n\t\tval_list = version_list[idx:]\n\t\tif val:\n\t\t\tversion_list[idx] = val\n\t\telse:\n\t\t\ttry:\n\t\t\t\t"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\ts = \"\"\n\twhile True:\n\t\tv = str(version)\n\t\ti = 0\n\t\twhile (i < len(v)) and v[i] in (\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"):\n\t\t\ts += v[i]\n\t\t\ti += 1\n\t\tif len(v) > i+5:\n\t\t\ts += v[i]\n\t\telse:\n\t\t\tif len(v) < 10:\n\t\t\t\ts += v[0]\n\t\t\t\ti = 1"}, {"generated_text": "def next_version(version):\n\"\"\"\n\tNext version identifier following existing pattern.\n\t\n\tMust deal with both zero-prefixed and non-zero prefixed versions.\n\t\"\"\"\n\timport re\n\t\n\tre_pattern = '(%s)' % '|'.join(versions)\n\n\t# Try to build a regular expression:\n\ttry:\n\t\treturn re.sub(re_pattern, lambda s: next(versions), version)\n\texcept AttributeError:\n\t\traise ValueError('no patterns in \"%s\"' % version)\n\ndef is_valid_version_number(version):\n\t\"\"\"\n\tIf the given version number fits the format as defined by :func:`next_version`,\n\tit's considered valid for the purpose of generating identifiers."}]}
{"task_id": "62b45e23e0d4551b0392c90a", "prompt": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t# Make sure all versions exists\n\tif not os.path.isdir(version_dirs):\n\t\tmsg = 'Version(s) \"%s\" does not exist or is not a directory.' % version_dirs\n\t\treturn msg\n\tnum_versions = len(version_dirs)\n\t\n\tfor version_dir in version_dirs:\n\t\t# Get the versions's root inventory\n\t\tcurrent_root_inventory = self.version(version_dir).get_root_inventory()\n\t\t\n\t\t# The version directories under a version should all have the same directory\n"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t# we don't need to worry about version files we don't care about.\n\tignored_files = ['__builtin__.pyc', '.svn']\n\t\n\tvalid_files = list_files(version_dirs, ignored_files)\n\tversion_inventories = []\n\tfound_version_inventories = []\n\t\n\tfor path in valid_files:\n\t\tversion_inventories.append(get_inventories(path))\n\t\n\tfor path in version_inventories:\n\t\tfor i in range(len("}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\tself.assert_version_inventories(version_dirs)\n\t\n\tall_digests = set()\n\t\n\t# Loop through version directories and accumulate digests from children, root inventory\n\tfor version_dir in version_dirs:\n\t\tdigests = []\n\t\t\n\t\tdir_to_check = os.path.join(self.version_dir, version_dir)\n\t\t\n\t\troot_inventory = os.path.join(dir_to_check, ROOT_INVENTORY_FILE)\n\t\tif not os.path."}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tdef _inventory_file(version_dir, inventory_dir, fname):\n\t\tresult = {\n\t\t\t'filename' : os.path.join(inventory_dir, fname),\n\t\t\t'checksum' : None,\n\t\t\t'checksum_type' : None,\n\t\t}\n\t\tif os.path.exists(result['filename']):\n\t\t\tlines = open(result['filename']).read().split('\\n')\n\t\t\tfor line in lines:\n\t\t\t\tif line.startswith(''* 3 +'' * 19 +''): # checksum"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\ttry:\n\t\ttry:\n\t\t\tf = open(ROOT_REPO_INVENTORY_PATH)\n\t\t\troot_inventory = json.loads(f.read())\n\t\t\tf.close()\n\t\texcept IOError:\n\t\t\traise ConfigError(\"Cannot find repo inventory file '{0}'\".format(ROOT_REPO_INVENTORY_PATH))\n\t\texcept ValueError:\n\t\t\traise ConfigError(\"Invalid JSON in repo inventory file '{0}'\".format(ROOT_REPO_INVENTORY_PATH))\n\t\t\n\t\tfor"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\tfiles_in_version = [d for d in version_dirs if d in self.file_digest_hash]\n\t\n\troot_dirs = [d for d in version_dirs if d not in files_in_version]\n\t\n\tfile_to_hash = []\n\tfor rootdir in root_dirs:\n\t\tfor rootdir_file in os.listdir(rootdir):\n\t\t\tif os.path.splitext(rootdir_file)[1] == '.py':\n\t\t\t\tfile_to_hash.append(root"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_inventories = []\n\tself.assert_is_nonempty_array(\"version_dirs\", version_dirs)\n\t\n\tfor version_dir in version_dirs:\n\t\tversion_inventory_dir = os.path.join(version_dir, \"inventories\")\n\t\t\n\t\twith self.context() as ctx:\n\t\t\tversion_inventory_path = ctx.find_path(version_inventory_dir)\n\t\t\n\t\t\tif (not os.path.exists(version_inventory_path)):\n\t\t\t\tctx.outl"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\tfor version_dir in version_dirs:\n\t\t\n\t\twith self.open(os.path.join(version_dir, 'content_digest')) as vhd:\n\t\t\t\n\t\t\treturned = vhd.read(self._BLOCK_SIZE)\n\t\t\t\n\t\t\tdigests.validate_version_digest(returned, version_dir, self.size)\n\t\t\t\n\tfor version_dir in version_dirs:\n\t\t\n\t\tinventory_file = os.path.join(version_dir, 'root_inventory')\n\t\n\t\t"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor (i, version_dir) in enumerate(version_dirs):\n\t\ttree = etree.parse(version_dirs[i])\n\t\troot = tree.getroot()\n\t\tfor entry in root.findall('./dir/version'):\n\t\t\tversion_num = int(entry.get('num'))\n\t\t\tif version_num < 1:\n\t\t\t\traise Error('A version directory with a version number of <1 cannot be valid.')\n\t\t\t\n\t\t\tvalidkey = entry.xpath('./valid-key')[0]\n\t\t\tcert_"}, {"generated_text": "def validate_version_inventories(self, version_dirs):\n\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\t\n\tAlso keep a record of any content digests different from those in the root inventory\n\tso that we can also check them when validating the content.\n\t\n\tversion_dirs is an array of version directory names and is assumed to be in\n\tversion sequence (1, 2, 3...).\n\t\"\"\"\n\tfor version_dir in version_dirs:\n\t\tversion_dir_exists = os.path.exists(version_dir)\n\t\tif not version_dir_exists:\n\t\t\tprint('Version NOT found in {0}'.format(version_dir))\n\t\t\tif not os.path.exists(VERSION_BASE_INVENTORY):\n\t\t\t\traise IOError(\"{0} must be a version directory\".format(VERSION_BASE_INVENTORY))\n\t\t\treturn\n\t\t\n\t\tver_vars = os.listdir(version_dir)"}]}
{"task_id": "62b45e21e0d4551b0392c8ed", "prompt": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tp = path\n\tparts = os.path.split(p)\n\troot = parts[0].lower()\n\twhile root!= '':\n\t\troot = parts[0].lower()\n\t\tif (root == 'ocfl_root') or (root == 'root'):\n\t\t\treturn 'root'\n\t\tpieces = parts[0].split('=')\n\t\tif (len(pieces) > 1) and (pieces[0] == '0=*'):\n\t\t\treturn 'object'\n\t\tparts = parts[1:] \n\n#\n#\tCheck to see"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\trepoDir = os.path.join(path,'repo')\n\tif not os.path.exists(repoDir):\n\t\tif not os.access(repoDir, os.W_OK):\n\t\t\traise TypeError('Permission denied on'+ file_path)\n\t\tif os.access(file_path, os.W_OK) or os.access(file_path, os.F_OK):\n\t\t\treturn'repo'\n\t\traise TypeError('Not a repository folder')\n\tif os.path.isdir(file_path"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\t\n\t# return \"root\" if nothing at the given path -- e.g. the path is \"/\"\n\ttest_path = os.path.normcase(os.path.normpath(path))\n\ttest_path = test_path.split(\"\\\\\")\n\tnest_list = list()\n\tfor n in test_path:\n\t\tif n!= '': nest_list.append(n)\n\t#print nest_list\n\t# print nestlist[:2]\n\t# print nestlist[1]\n\treturn find_path_type"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t# is there a match?\n\tcmd = \"find '0=*' -maxdepth 2 -name '%s' \" % (path)\n\tretval = os.popen(cmd).readline().rstrip()\n\t\n\tif retval == '0' or retval == '':\n\t\treturn 'root'\n\t\n\tif retval == '1':\n\t\treturn 'dir'\n\t\n\tif retval == '2':\n\t\treturn 'file'\n\t\n\treturn retval\n\nif __name__ == '__main__':\n\t"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\tif len(path) < 2:\n\t\tif path == ['0=*']:\n\t\t\treturn 'root'\n\t\telif path == ['0/0/']:\n\t\t\treturn 'object'\n\t\telse:\n\t\t\treturn return_unknown_file_type_description(filename, path)\n\telif re.search(r'^.*\\.nam$', path[0]):\n\t\treturn 'root'\n\telif re.search(r'^[0-9]+/0/', path[0]):\n\t\treturn 'object'\n\telse"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\ttry:\n\t\tif path[0]==0:\n\t\t\traise ValueError(\"not a root container\")\n\t\telif path.name=='0=*' and path.isdir():\n\t\t\treturn 'file'\t# if empty directory?\n\t\telif path.isdir() and path.isfile():\n\t\t\treturn 'file' # is it a subpath?\n\t\telif path.isdir():\n\t\t\treturn 'root'\n\t\telse:\n\t\t\treturn 'object'\n\texcept Exception as e:\n\t\tprint \"Not a root container:\",e\n\t\treturn"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tretval = ''\n\ttry:\n\t\tname, namemask = os.path.split(path)\n\t\tif name[-1:].lower() in ('0=*'):\n\t\t\tfor x in [x.strip('0=').strip('*') for x in name[-1:] if (x and x not in '*0=').split(',')]:\n\t\t\t\tif os.path.isfile(os.path.join(name,x)):\n\t\t\t\t\tretval = 'file'\n\t\t\t\t\t# We could be doing better here...\n\t\t\t\t\tbreak\n"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\timport fnmatch\n\t\n\tret = ('root','fail')\n\tfor part in path.split('#'):\n\t\tif '=' not in part:\n\t\t\tbreak\n\t\tif part not in ('0=*','0=*/*','0=*/*/*','0=*/*/*/*'):\n\t\t\tbreak\n\t\tif part == '0=*':\n\t\t\tret = ('root','')\n\t\telif part == '0=*/*':\n\t\t\tret = ('root','normal')\n\t\telif part == '0=*/*/*':\n\t\t\tret ="}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\t\n\tpath_type = 'object'\n\tfor entry in path:\n\t\tif entry[0] == 0 and entry[1] == '=':\n\t\t\tif entry[2] == '/':\n\t\t\t\tpath_type = 'root'\n\t\t\telse:\n\t\t\t\tpath_type = 'object'\n\t\t\tbreak\n\tret = ''\n\tif path_type:\n\t\tret = path_type\n\telse:\n\t\tret = 'Invalid path type'\n\treturn ret\n\ndef find_path_name(path):\n\t\"\"\"\n\tReturn the name of the thing at"}, {"generated_text": "def find_path_type(path):\n\"\"\"\n\tReturn a string indicating the type of thing at the given path.\n\t\n\tReturn values:\n\t    'root' - looks like an OCFL Storage Root\n\t    'object' - looks like an OCFL Object\n\t    'file' - a file, might be an inventory\n\t    other string explains error description\n\t\n\tLooks only at \"0=*\" Namaste files to determine the directory type.\n\t\"\"\"\n\tresult=\"root\"\n\n\t# If the path ends in \"/\",\n\t# look for a.namast\" file instead of a \"0=\" file\n\tif len(path)>0 and path[-1]==\"/\":\n\t\tpath = path.strip(\"/\")\n\t\tpath += \".namast\"\n\t\tfiles = glob.glob(path)\n\t\tif files:\n\t\t\tresult = \"file\"\n\t\telse: \n\t\t\tprint \"No namaster file or object found: '%s'\" % path\n\t\t\treturn\n\n\t# If file path is \"/\","}]}
{"task_id": "62b45b396decaeff903e1001", "prompt": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparam_map = {'BzComputeSignature': self.options['cpr']}\n\tself.bz.amend_bug_with_attachment(bug_ids, params, **param_map)\n\n\ndef get_param(self, bug_id, param):\n\t\"\"\"\n\tGet the param and return the value\n\t\"\"\"\n\treturn self.bz.getbug(bug_id).attachment_by_filename(param)\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tchange = {\n\t\t'cmd': params['name'],\n\t\t'param': {\n\t\t\t'bug_ids': bug_ids,\n\t\t}\n\t}\n\tif 'bz_params' in params.keys():\n\t\tchange['param'].update(params['bz_params'])\n\telif 'fields' in params.keys():\n\t\tfields = {'bug_list': params['fields']}\n\t\tchange['cmd'] = 'create'\n\t\tchange['param'].update(fields)\n\treturn self.update(change)\n\n\ndef remove"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor bug in bug_ids:\n\t\tself.bugwatch_factory.get_bug_by_id(bug).openit(bzparams=params)\n\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\treturn params\n\n\t# TODO: Add this code for any other parameters we have.\n\t# params should already include all of those that are not overridden.  I think you can just change the default values instead of having to override them all.\n\n\t# Return\n\t# The params to add to the Bugzilla Bug.bugzilla_fields property of the\n\t# bug object.\n\n\t# The following line doesn't work b/c it doesn't include the bug_ids.\n\treturn params\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tif bug_ids:\n\t\tparams.append(\"fixed-in:\")\n\t\tparams.append('  - %s' % ','.join(bug_ids))\n\t\tfor bug_id in bug_ids:\n\t\t\tself.log_with_bug_id(bug_id, params, \"amending\", \"Fixed in bug\",\n\t\t\t                     \"bug fixed by\", \":\")\n\treturn params\n\n\ndef log_with_bug_id(bug_id, params, kind, description, email=\"\"):\n\t\"\"\"\n\tLog a message with a given kind and description"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tif not params:\n\t\treturn\n\tparams_list = list()\n\tfor each in params:\n\t\t# Each params is in format \"1\", \"2\" and so on\n\t\tparams_list += map(str, each.split('__'))\n\tfor k, v in Bug.objects.filter(bug_id__in=bug_ids).values_list('bug_id', 'bug_change').distinct().order_by('bug_id'), bug_ids:\n\t\tparams_list = map(lambda i: u'{{%s:%s}}'"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor bugid in bug_ids:\n\t\tif bugid in params:\n\t\t\tcontinue\n\t\ttry:\n\t\t\tself._bugzilla.edit(self.BUGZILLA_BUGID, self.get_bug_status(bugid), 'bugs/set_bug_status.cgi?status='+self.BUGZILLA_STATUS, params={'assigned_to':str(bugid)}, comment='Automated change by BugManager')\n\t\t\treturn True\n\t\texcept xmlrpclib.Fault as err:\n\t\t\tprint(\"Problem with params: \", err"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tif not bug_ids:\n\t\t# Get the bug IDs\n\t\tself.bug_ids = set([x.split('/')[3] for x in self.bug_ids])\n\n\telse:  # Multiple bug IDs were passed\n\t\tself.bug_ids = set(bug_ids)\n\n\t# Get the params to pass to 'Bug.add_comment'\n\treturn Bugzilla.params_for_bug_ids(params, self.bug_ids)\n\n\ndef get_params(options):\n\t\"\"\"\n\tParse Options from command line with argparse\n"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfor bug_id in bug_ids:\n\t\t\n\t\tparam_filepath = os.path.join(\n\t\t\tself.bugzilla_path, '%s.params.xml' % str(bug_id))\n\t\ttry:\n\t\t\t# if not os.path.exists(param_filepath):\n\t\t\tself.logger.info('amend_bzparams: %s does not exist' %\n\t\t\t\tparam_filepath) \n\t\texcept (AttributeError, IOError):\n\t\t\traise Exception('amend_bzparams: Error during accessing params_file"}, {"generated_text": "def amend_bzparams(self, params, bug_ids):\n\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tfrom bzcleanup import clean\n\tfor bug, info in bug_ids.iteritems():\n\t\tif info['fixed2'][0]:\n\t\t\tclean(['--fixed2', bug, info['filepath']])\n\n\tclean(['--ignore_valid_bug_id', '--debug_mode'])\n\n\treturn bug_ids\n\n"}]}
{"task_id": "62b4567ed7d32e5b55cc83d9", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 22.16 GiB total capacity; 20.53 GiB already allocated; 15.88 MiB free; 21.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b4567ad7d32e5b55cc83af", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t# ArgumentParser instance holds arguments parsed so far\n\targparser = argparse.ArgumentParser(description=\"Script to manipulate the configuration files of the LIFETIME system via the HTTP API\")\n\targparser.add_argument(\"--help\", \"--usage\", help=\"show usage instructions\", default=False, action=\"store_true\")\n\targparser.add_argument(\"--version\", help=\"show software version\", default=False, action=\"store_true\")\n\targparser.add_argument(\"--server\", help=\"set a server address for requests\", default=\"\")\n\targ"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tfrom sys import argv\n\n\tdef help():\n\t\t\"\"\"Print help for this script.\"\"\"\n\n\t\targuments['<filename>'] = \"File to be read.\"\n\t\targuments['<filename>'] = \"Treatment of input files.\"\n\t\targuments['<filename>'] = \"Input files must use Python 3.3+ format.\"\n\n\t\tfor element in arguments.keys():\n\t\t\tprint(element, arguments[element])\n\t\treturn\n\n\tdef parse():\n\t\t\"\"\"Parse the argument string and return a dictionary for this option.\n\t\t\"\"\"\n\t\tparser = ArgumentParser("}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=__file__)\n\tparser.add_argument('data_format')\n\tparser.add_argument('output_path')\n\tparser.add_argument('data_file')\n\tparser.add_argument('vocabulary_file')\n\tparser.add_argument('--size', type=int, default=1000)\n\tparser.add_argument('--top_k', type=int, default=5)\n\tparser.add_argument('--max_positions', type=int, default=20)\n"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--test', help='Run tests', action='store_true')\n\tparser.add_argument('--dev', help='Build a test dev build for all tests', action='store_true')\n\tparser.add_argument('--doc', help='Build documentation. [deprecated]', action='store_true')\n\tparser.add_argument(\n\t\t'--dev-html-only-include',\n\t\tmetavar='DIRECTORY',\n\t\thelp=\\\n\t\t\t'DEPRECATED:"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(description=main_description)\n\t# Note: The parser won't accept positional arguments as arguments anymore.\n\t# Instead you need to use a positional command line option (which is fine for us here...)\n\tparser.add_argument(\"--hostname\", \"-n\", action=\"store\", required=True,\n\t\t\t\t\t\thelp=\"The name or an ID of the OpenStack host to check\")\n\tparser.add_argument(\"--keystone_username\", \"-u\", action=\"store\", required=True,\n\t\t\t\t\t\thelp=\"The user used to authenticate with keystone\")\n"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\treturn ArgumentParser(*arguments)\n\n\ndef parse_arguments(argv):\n\t\"\"\"\n\tGiven command-line arguments for the script, parse them and return both the parsed arguments and\n\tparsed `.ArgumentParser` instance as a tuple.\n\t\"\"\"\n\tparser = parse_arguments(*argv)\n\treturn parser.parse_args()\n\ndef start(command, *args, **kwargs):\n\t\"\"\"\n\tStart this script with the given command, which must be a valid Python module name and file path.\n\tUse `options` to pass"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ap.ArgumentParser()\n\tparser.add_argument(\"-f\", dest=\"filepath\", type=str,\n\t\t\t\t\t\thelp=\"Path to the data set to be loaded into the workspace.\", required=True)\n\tparser.add_argument(\"-e\", dest=\"expdir\", type=str,\n\t\t\t\t\t\thelp=\"The Experiment directory containing the experiments in the data set.\\n\"\n\t\t\t\t\t\t\"If not given, the Experiment will attempt to be inferred by using the\\n\"\n\t\t\t\t\t\t\"default setting for the experiment's directory. Note that if the default\\n\"\n\t\t\t\t\t\t\""}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(description=\"\"\"Generate a C++ header file from a database of the PDB entries from the PDB.json file.\"\"\", epilog=\"\"\"Generate all of the PDB entries that are in PDB.json and write them out to a list in a c++ header file.  It will also create a separate file of that list which is only required for the tests.  These tests are currently coded to work with an online database of the entries, they can be improved when the PDB database can be found offline.\n\nThe script takes the following arguments:\n\t--verb"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(description=(\n\t\t'Extracts and optionally converts a Python source file using Google's Clang-based '\n\t\t\"Code Style Converter. Code Style Converter can automatically suggest the best fix to \"\n\t\t\"address issues such as syntax errors, missing docstrings, and name overlaps with previous \"\n\t\t'calls as well as provides the ability to make code styles consistent.\"))\n\tparser.add_argument(dest='src', metavar='path', required=True,\n\t\t\t\t\t   help='path to the file to be converted')\n\tparser.add_"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(description=__doc__)\n\t\t\n\tparser.add_argument(\"file\",\n\t\tmetavar=\"FILE\",\n\t\ttype=str,\n\t\thelp=\"File to process\")\n\t\t\n\tparser.add_argument(\"-v\", \"--verbose\",\n\t\taction=\"store_true\",\n\t\tdefault=False,\n\t\thelp=(\"Print all subcommands\"))\n\t\t\n\t# Arguments for parsing CSV rows\n\tparser.add_argument(\"--nfields\",\n\t\tdest=\"nfields\",\n\t\taction=\"store\",\n\t\tdefault="}]}
{"task_id": "62b45679d7d32e5b55cc83a9", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \"\"\n\n\tfor param in parser.parse_args().__dict__.keys():\n\t\tif not param.startswith(\"-\"):\n\t\t\tflags += \"%s \" % param\n\n\treturn flags.strip()\n\n\n#\n# Function for printing and verifying all examples that exist in the documentation.\n#\n@pytest.mark.cmd\ndef pytest_runtest_examples(item):\n\ttry:\n\t\twith open(os.path.join(config.BASE_PATH, \"tests\", \"examples\",\n\t\t\t\t\t\t\t\titem.name)):\n\t\t\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = list()\n\tfor flag in parser._optionals._option_string_actions.keys()+parser._optionals._option_strings:\n\t\tflags.append(\"{} {}\".format(flag.ljust(20), parser._option_string_actions[flag]))\n\n\tflags.sort()\n\tflags = \" \".join(flags)\n\n\treturn flags\n\ndef _get_program_fullname(s):\n\t\"\"\"\n\tArguments:\n\t\ts: the source file name without file extension\n\tReturn:\n\t\ta tuple\n\t\"\"\"\n\treturn"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tif hasattr(parser, '_action_groups'):\n\t\tflags = \"\"\n\t\tfor grp in parser._action_groups:\n\t\t\tassert type(grp) == _ArgumentGroup\n\t\t\t# A bit ugly, but we can't use the get_group().flags() method as the\n\t\t\t# _ArgumentGroup instances are not compatible.\n\t\t\tassert type(grp.choices) == _ChoicesType\n\t\t\tflags = flags + grp.flags\n\t\treturn flags\n\telse:\n\t\treturn \"\"\n\n\n@contextlib.contextmanager\ndef _args_"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \"\"\n\tfor flag in parser.argument_flags:\n\t\tflags +='' +''.join(flag)\n\treturn flags\n\ndef parser_has(parser, args_flags):\n\t\"\"\"\n\tCheck that given the names in 'args_flags', parse the command line arguments from 'argv' into the parser 'parser',\n\tthrow a parse error if not.\n\n\tReturns boolean\n\t\"\"\"\n\tfor arg_flag in args_flags:\n\t\tfound = False\n\t\tfor arg in parser.prog_args_re.findall(parser.pro"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn''.join(['--' + k for k in vars(parser.parse_args([]))])\n\ndef get_output(path, force=False, delete=False):\n\t\"\"\"\n\t\tGiven a path to a file, returns a string containing the content of the file.  Can\n\t\toptionally force re-reading the file.\n\t\t\"\"\"\n\tif path in sys.path and not force: # force=False means read file, but don't add it to $PYTHONPATH.\n\t\treturn open(path).read()\n\telif path not"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn \", \".join(parser.argument_flags)\n\t\n\t\n#------------------------------------------------------------------------------\n#                                   HELPERS\n#------------------------------------------------------------------------------\t\n\t\n\t\ndef parse_arguments(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance with optional arguments, parse the\n\targuments, return argument map and argparser.\n\t\n\tParameters\n\t----------\n\t\n\tparser : argparse.ArgumentParser with optional arguments\n\t\n\tReturns\n\t-------\n\tstr : argument flags\n\tdict : argument map\n\targparse.ArgumentParser\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = parser.format_args\n\treturn ', '.join(sorted(flags))\n\n\nclass App:\n\t\"\"\"\n\tLauncher class to run test scripts.\n\t\"\"\"\n\n\tdef __init__(self, script_file, config_file):\n\t\tsuper().__init__()\n\t\tself.script_file = script_file\n\t\tself.config_file = config_file\n\n\tdef run(self):\n\t\twith open(self.script_file) as file_handle:\n\t\t\tscript = file_handle.read()\n\n\t\toptions = (\n\t\t\t['"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = []\n\tfor k, v in parser.parse_args()._get_kwargs().iteritems():\n\t\tif isinstance(v, (list, tuple)):\n\t\t\tv = \", \".join(str(i) for i in v)\n\t\tflags.append('-{0}=({1})'.format(k, v) if v is not None else v)\n\treturn \", \".join(flags)\n\n\ndef setup_args(args, description):\n\t\"\"\"\n\tParse and process arguments.\n\t\"\"\"\n\tparser = argparse.Argument"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tparser_args = []\n\tfor param in parser.argument_list:\n\t\tparser_args.append('%s=\"%s\"' % (param.dest, param.default))\n\treturn''.join(parser_args)\n\n\n\ndef command_list(commands):\n\t\"\"\"\n\tGiven a list of command classes, return a space-separated string containing the commands\n\tand their names.\n\t\"\"\"\n\treturn''.join([command.get_command_name() for command in commands])\n\n\n\n@hook_api\ndef api_register(api,"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \"\"\n\tfor flag in parser.flag_values:\n\t\tflags += flag.name.upper() + \"=\" + str(getattr(parser.values, flag.name)) + \" \"\n\treturn flags\n\n\"\"\"\n\tGiven the parser and its argument flags, return an array of arguments and flags.\n\"\"\"\ndef parse_args_and_flags(parser, args, flags):\n\t\"\"\"\n\t\tGiven an argparse.ArgumentParser instance and an array of arguments and flags, return the\n\t\targuments and flags, if any, or None if the parser's arguments"}]}
{"task_id": "62b45665d7d32e5b55cc8365", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tif len(unparsed_arguments) == 0:\n\t\treturn None\n\tif len(unparsed_arguments) > 1:\n\t\traise CommandLineParser.ArgumentError ('more than one argument found for \"%s\"' %''.join(unparsed_arguments))\n\treturn CommandLineParser.get_arguments(unparsed_arguments[0])\n\ndef main (global_arguments, *unparsed_arguments):\n\t\"\"\"\n\tGet the parsed arguments (parsed into a Namespace instance), and"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# Parse the command-line arguments and merge them into the dictionary that will be returned\n\tparsed_arguments = {} # Dictionary of arguments keyed by argument name, not the parser name\n\tfor a, v in unparsed_arguments:\n\t\tif isinstance(v, dict):\n\t\t\tparsed_arguments.update(*parse_arguments(*v))\n\t\telse:\n\t\t\tparsed_arguments[a] = v # Otherwise, copy the argument directly into the parsed_arguments dictionary\n\treturn parsed_arguments\n\ndef main():\n"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparsed_arguments = {}\n\n\tfor argument_string in unparsed_arguments:\n\t\targs_iter = iter(argument_string.split(\",\"))\n\t\ttry:\n\t\t\tname, unparsed_args = next(args_iter)\n\t\texcept StopIteration:\n\t\t\t# The arguments string was parsed successfully and that's all we care about.\n\t\t\t# No unparsed arguments remain, so break out.\n\t\t\tbreak\n\t\texcept ValueError:\n\t\t\t# The arguments string was not parsed successfully. This is an error that we can\n\t\t\t# recover from"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser( *unparsed_arguments )\n\tparser.add_argument('--root', default='web', type=str)\n\tparser.add_argument('--db', default='web_db')\n\tparser.add_argument('--server-name', default='default_server')\n\tparser.add_argument('--host', default='localhost')\n\tparser.add_argument('--port', default='8081')\n\tparser.add_argument('--debug-port', default=5000)\n\tparser.add_argument"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# We need to parse arguments early, to see whether --list arguments --or- arguments were given by the user.\n\t# This allows for default values to be set. Otherwise they'll have to be passed to subprocesses via stdin, e.g.:\n\t# argparse.Namespace(name=\"first\", value=0, help=\"My first arg\", metavar=\"VALUE\")\n\n\targs = {\n\t\t\"unparsed\": unparsed_arguments\n\t}\n\n\t# We may want to move here the argument parsing stuff (from subparsers/set_arg"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tdescription = \"\"\"\n\"\"\"\n\tparser = argparse.ArgumentParser(epilog = description, formatter_class=argparse.RawDescriptionHelpFormatter)\n\tparser.add_argument(\"--help\", \"-h\", action = \"store_true\", default=False, help=\"display this help message and exit.\")\n\tparser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", default=False, help=\"show verbose logging.\")\n\tparser.add_argument(\"--config\", -1, action=\"store\", help=\"location of config file. If"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\toptions_arguments = command_line.add_arguments(parser,\n\t\t\t\t\t\t\t\t\t\t\t\t   argparse.SUPPRESS)\n\toptions = vars(parser.parse_args(options_arguments))\n\treturn options\n\ndef get_command_line_parameters(parser, unparsed_arguments):\n\t\"\"\"\n\tGiven a command-line parser (see parse_arguments) and an unparsed list of args (the arguments with which\n\tthis script was invoked), return a dict mapping from subparser name to a list of argument names\n\tfor that subparser,"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\timport argparse\n\timport sys\n\tfrom. import arg_validators\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\tThis script does the following:\n\n\t\t  1) Writes a new report from the command-line arguments\n\t\t  2) Validates the command-line parameters against the command-line usage and exit on\n\t\t\t  errors\n\n\t\tThis can be used for reporting issues with Tritone's command-line interface.\n\t\t\"\"\")\n\tparser.add_argument(\"--version\", action=\"version\", version=(\"Tritone \" + tr"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targument_parser = create_argument_parser()\n\targuments = argument_parser.parse_args(unparsed_arguments)\n\treturn arguments.__dict__\n\n\nclass Command:\n\t\"\"\"\n\t\tInherit from this class to support overriding the defaults for a particular command,\n\t\twith respect to its input and parameters.\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tself.kwargs = kwargs\n\t\tif \"name\" not in self.kwargs:\n\t\t\tself.kwargs[\"name\"] = self"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparsed_arguments = dict()\n\tnamespace = parser.parse_args()\n\tif namespace.func == 'test':\n\t\tparsed_arguments[namespace.func] = namespace\n\n\tif namespace.func not in parsed_arguments:\n\t\tparsed_arguments[namespace.func] = parser.parse_args(namespace.args)\n\n\tif namespace.func == 'test':\n\t\t# Remove leading /, if any.\n\t\tif parsed_arguments[namespace.func].args[0] == '/':\n\t\t\tp"}]}
{"task_id": "62b45665d7d32e5b55cc8364", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_map = { }\n\tunclaimed_args = []\n\tfor subparser_name, subparser in subparsers.items():\n\t\tfor current_argument in subparser._group_actions:\n\t\t\tfor subargument_name, subargument in current_argument.kwargs.items():\n\t\t\t\tif subargument_name in unparsed_arguments:\n\t\t\t\t\tunparsed_arguments.pop(subargument_name)\n\t\t\t\telse:\n\t\t\t\t\tunclaimed_args.append(subargument_name)\n\t\t\t\tparser_map[subparser_name] = sub"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\trval = parse_subparser(u(unparsed_arguments)), []\n\n\tfor name in subparsers:\n\t\taction_parser = subparsers[name]\n\t\tfor a in action_parser._actions:\n\t\t\tif hasattr(a, 'add_subparser_arguments'):\n\t\t\t\tassert(isinstance(a, sub_parsers.SubparsersAction))\n\t\t\t\ta.add_subparser_arguments(action_parser)\n\n\treturn rval\n\ndef argv_exists(argv, a):\n\t\"\"\""}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser = subparsers.choices['command_parsers'][0].add_parser(unparsed_arguments[0], description=_DESCRIPTION)\n\t\n\t# Use named argument parsing to accept common arguments like \"--repository\"\n\tfor common_argument in argparse.ArgumentParser(add_help=False)._option_registration:\n\t\tparser.add_argument(common_argument.metavar or common_argument.dest, **common_argument.kwargs)\n\t\t\n\treturn parser.parse_args(unpars"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# Grab the action objects from the namespace. This will be the dictionary mapping\n\t# from subparser name to the parser returned.\n\tactions = vars(subparsers.choices['main'].parse_args(unparsed_arguments))\n\n\t# We're done with the arguments if the action was handled.\n\treturn actions, unparsed_arguments\n\n\ndef run(unparsed_arguments):\n\t\"\"\"\n\tTake unparsed arguments and run them through the correct subcommands.\n\t\"\"\"\n\t# Make an ArgumentParser for this"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tresult = (dict(), [] for action in subparsers.choices())\n\tfor actions, keys in zip(subparsers.choices(), unparsed_arguments):\n\t\tfor argument in unroll_key_choices(actions.choices):\n\t\t\tfor name, action in actions.names_and_defaults.items(): # each subparser\n\t\t\t\tif argument in keys and keys[argument] is not None:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tresult[name] = action(keys[argument])\n\t\t\t\t\texcept CmdLineError as e:\n\t\t\t\t\t\tsys.stder"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tassert all(isinstance(x, unicode) for x in unparsed_arguments)\n\t\n\t_ns_from_args = defaultdict(dict)\n\t_ns_from_repo = defaultdict(dict)\n\t\n\t# Populate from main parser and repository args\n\tnamespace = parse_args(unparsed_arguments)\n\tfor subparser, args_parsing in subparsers.items():\n\t\t_ns_from_repo[subparser] = namespace\n\t\t_ns_from_args[subparser] ="}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\ttry:\n\t\t(parsed_namespace, remaining_arguments, errors_from_subparsers) = getattr(subparsers, action.func_name)(parser, unparsed_arguments)\n\t\treturn (parsed_namespace, remaining_arguments, errors_from_subparsers)\n\texcept SystemExit as sysexit:\n\t\traise sysexit\n\texcept ErrorInSubparsers:\n\t\tprint(\"ERROR: Argument with help key '%s' could not be parsed\"%arguments[action.dest])"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_results = {}\n\tremaining_arguments = []\n\t\n\tfor subparser in subparsers:\n\t\t# first try to handle it as if it was fully qualified\n\t\tparsed_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\t\n\t\t# if this subparser wants to handle the remaining arguments\n\t\tif parsed_arguments and not parsed_arguments[0].remaining_args:\n\t\t\tsubparser_results[subparser.name] = parsed_arguments[0]\n\t\t\n\t\t#"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsub_namespace_map = {}\n\tunclaimed_arguments = list(unparsed_arguments)\n\tfor subparser in subparsers:\n\t\tnamespace, unparsed_arguments = subparser.parse_args(unclaimed_arguments)\n\t\tsub_namespace_map[subparser.name] = namespace\n\t\tunclaim(unclaimed_arguments, subparser)\n\treturn sub_namespace_map, unparsed_arguments\n\ndef get_parsed_arguments(subparsers, unparsed_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tns = {}\n\tnew_arguments = []\n\tfor subparser_name in subparsers:\n\t\tparser = subparsers[subparser_name]\n\t\tnew_ns, remaining_arguments = parser.parse_known_args(unparsed_arguments)\n\t\tns[subparser_name] = new_ns\n\t\tnew_arguments.extend(remaining_arguments)\n\t\n\treturn ns, new_arguments\n\nclass arg_parser(object):\n\t\"\"\"\n\tHelper for command-line parsing.\n\t\"\"\"\n"}]}
{"task_id": "62b45665d7d32e5b55cc8363", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"muparse - Command line Python MUParse utility\",\n\t\tdescription=PROGRAM_DESCRIPTION,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tepilog=\"\"\"\\\nCopyright (C) 2018\n  David L. Whitehurst and the associated organizations.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software  and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy,"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(add_help=False)\n\t\t\n\tparser.set_defaults(func=None, version=None)\n\t\n\tgroup = parser.add_argument_group(\"Command-line options\")\n\t\t\n\tgroup.add_argument('--version', action='version', version='%(prog)s'+ VERSION)\n\t\n\treturn parser, group\n\t\n\t\ndef execute(args):\n\t\"\"\"\n\tMain execution function. Run the specified command.\n\t\"\"\"\n\t\n\tparser, group = args."}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\n\tsubparsers = parser.add_subparsers()\n\t\n\t# Add a parser for the \"add-new-team\" command\n\tparser_add_new_team = subparsers.add_parser('add-new-team')\n\tparser_add_new_team.add_argument(\"team\", type=str, metavar=\"team\", help=\"Name of the new team\")\n\tparser_add_new_team.set_defaults(func=add_new_team)\n\tsubp"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# Build the top-level parser - this is just python\n\tparser = argparse.ArgumentParser(add_help = False)\n\t\n\t# Build a sub-parser to parse command-line params for the programm\n\thelp = parser.add_argument_group('optional help section')\n\thelp.add_argument('-h, --help', 'Show help text and exit',\n\t\taction = 'help',\n\t\tnargs = 0,\n\t\thelp = 'Show this text and exit')\n\n\n\t####### Commands for all operations #######\n\t# Add all supported"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# First call the parser class with dummy arguments to construct the parser:\n\tparser = get_parsers()()\n\n\t# Then store in a variable the result of the parser for future reference:\n\treturn parser.add_subparser,parser.register_actions\n\n\n\ndef register_actions(actions, parser):\n\t\"\"\"\n\tRegister actions with the parser given as argument.\n\t\n\tArguments:\n\tactions -- A list of ActionParser objects.\n\tparser -- The parser to add the actions. It's a subparser.\n\t\"\"\"\n\t# Call the parser to"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# the root parser takes precedence over shared actions; that makes\n\t# is_handler() results consistent between the root and the shared actions\n\troot_parser = optparse.OptionParser(prog='zmqpp')\n\n\tshared_group = root_parser.add_option_group('shared options')\n\n\toptions.make_option_group(shared_group, description='common options for all commands')\n\n\troot_parser.add_option('-l', '--log', metavar='LEVEL',\n\t\t\t\t\t\t   default='info',\n\t\t\t\t\t\t   help='Set the log"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tp = ArgumentParser(prog=\"parspec\",\n\t\t\tdescription=\"Parses the command line, or its history\"\n\t\t\t\" and emits output to stdout/stderr/stdin.\",\n\t\t\tepilog=\"This is an autogenerated, scripted version \"\n\t\t\t\"and should only be used as a last-ditch effort to avoid \"\n\t\t\t\"errors during the command line parsing process.\")\n\tp.add_argument(\"-d\", \"--debug\",\n\t\t\taction=\"storeTrue\",\n\t\t\tdefault=False,\n\t\t\thelp=\"Enable debugging.\")\n\n\tdefault"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# Create the parser and set up the top level option\n\t# parser and add subcommand's options, if any exist\n\tparser = argparse.ArgumentParser(add_help=None)\n\tparser.add_argument('--verbose', help='Verbose logging output.', action='store_true', dest='verbose')\n\tparser.add_argument('--quiet', help='Quiet logging output.', action='store_true', dest='quiet')\n\t# TODO(clogstaff) support logging to a file\n\tparser.add_argument('log',"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser( add_help = False )\n\tparser.add_argument( '-h', '--help' )\n\tparser.add_argument( '-l', '--list',dest=\"list\",\n\t\taction='store_const',const=True,default=False,\n\t\thelp='print listing of all valid modes')\n\t\n\t#parser.add_argument(\"infile\", nargs=1, help='input image file')\n\tparser.add_argument(\"infile\", nargs=1, type=str, help='input image"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tt =  argparse.ArgumentParser(prog=_prog,\n\t\t\t\t\t\t\t\t description=_desc,\n\t\t\t\t\t\t\t\t epilog=_epi)\n\tt.add_argument(\"-v\", \"--verbose\", action=\"store_true\",\n\t\thelp=\"display more messages\")\n\t\n\t# A subparser to parse the individual commands, e.g. [command]: [arguments]\n\ttp =  argparse.ArgumentParser(prog=\"c-l\",\n\t\t\t\t\t\t\t\t  description=\"Command line tool for interacting with the library\")\n\n\t# A group for"}]}
{"task_id": "62b438ba66fea644fe22cca2", "prompt": "def deep_merge_nodes(nodes):\n\"\"\"\n\tGiven a nested borgmatic configuration data structure as a list of tuples in the form of:\n\t\n\t    (\n\t        ruamel.yaml.nodes.ScalarNode as a key,\n\t        ruamel.yaml.nodes.MappingNode or other Node as a value,\n\t    ),\n\t\n\t... deep merge any node values corresponding to duplicate keys and return the result. If\n\tthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n\tof the values wins.\n\t\n\tFor instance, given node values of:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n\t                ),\n\t            ]),\n\t        ),\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\t... the returned result would be:\n\t\n\t    [\n\t        (\n\t            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n\t            MappingNode(tag='tag:yaml.org,2002:map', value=[\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n\t                ),\n\t                (\n\t                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n\t                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n\t                ),\n\t            ]),\n\t        ),\n\t    ]\n\t\n\tThe purpose of deep merging like this is to support, for instance, merging one borgmatic\n\tconfiguration file into another for reuse, such that a configuration section (\"retention\",\n\tetc.) does not completely replace the corresponding section in a merged file.\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 22.16 GiB total capacity; 20.60 GiB already allocated; 15.88 MiB free; 21.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b438b666fea644fe22cc78", "prompt": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\t\t\n\tparser.add_argument('-i','--inputFile', help=\"specify which input file to read the data from.\", required=True)\n\tparser.add_argument('-o','--outputPrefix', help=\"prefix of the output files\", default='geneSig_')\n\tparser.add_argument('-p','--plotFormat', help=\"format of the outputs\")\n\tparser.add_argument('-w','--width', help=\"width of the plot output in pixel\", default='640')\n\tparser"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t# Parse command-line arguments.\n\tap = ArgumentParser(prog='COSMO_LIMS_CALCULATE_CUTOFF_RATES')\n\t\t\n\t# Add the positional arguments.\n\tap.add_argument('-c', '--covariates',\n\t                nargs=argparse.REMAINDER,\n\t                metavar=('COVAR'),\n\t                help='Allowed covariates (e.g. list).',\n\t                required=False)\n\t\n\t# Add the optional arguments.\n"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser()\n\tparser.add_argument('name_of_executable', metavar='PATTERN', type=file)\n\tparser.add_argument('regexp_to_match', metavar='REGEXP')\n\n\treturn parser.parse_args()\n\nif __name__ == '__main__':\n\t# parse arguments\n\tparser = parse_arguments()\n\n\t# read content of file\n\tfile = parser.name_of_executable.read()\n\n\t# print result\n\tpattern = parser.regexp_to"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(\n\t\tprog='python-clam', \n\t\tdescription='Clam is a compiler.  This is a standalone program to aid in building source code using'+ \\\n\t\t\t\t'the built-in compiler tools.  This means there is no external dependency needed. '+ \\\n\t\t\t\t'It is built into distutils, Python's distutils module, or easy_install... '+ \\\n\t\t\t\t'(and it supports some command-line options).  This means that the program can be run'+ \\ \n\t\t\t\t'from the command line.  It is"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=os.path.basename(arguments[0])[:-len(os.path.basename(arguments[0]).split(\".\")[-1])])\n\tparser.add_argument(name=\"model_file\", help=\"a model file of a form of any kind for the model. The model file should have a '#include'line defining it as a child of the parent file\")\n\tparser.add_argument(\"file\", help=\"filename of the training file to be used.\", nargs=\"?\", default=\"data/train"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(*arguments)\n\tparser.add_argument('--in-directory', type=str, help=\"Directory which contains input files\")\n\tparser.add_argument('--out-file', type=str, help=\"File which will receive the result\")\n\treturn parser.parse_args(*arguments)\n\ndef main(*arguments):\n\t\"\"\"\n\tProgram entry point\n\t\"\"\"\n\tglobal VERBOSE\n\tVERBOSE = sys.argv[-1]==\"--verbose\"\n\t# Parse command line arguments\n\toptions ="}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tglobal default_cwd\n\treturn ArgumentParser(*arguments)\n\tpass\n\n# parse the command-line arguments (argv) and extract parameters and options\nglobal options\nglobal args\n\noptions = parse_arguments(sys.argv[1:]) # list(args, '--help')\nargs = sys.argv[1:] if len(sys.argv) == 1 else sys.argv[1:2]    # just the single positional argument\n\nprint(options.debug, args)\n\n\"\"\"\n\tNOTE: the code above is from the py"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog='py-fetch-gitbook',\n\t\t\t\t\t\t\t\t\t description='Downloads a gitbook site and generates the required files.',\n\t\t\t\t\t\t\t\t\t arguments=arguments,\n\t\t\t\t\t\t\t\t\t allow_abbrev=False)\n\tparser.add_argument('gitbook', help=\"Name of the gitbook.org repository from which to download.\")\n\tparser.add_argument('--destination', type=str, help=\"Name of the directory in which to create subdirectories to \"\n\t\t+ \"store webapp subroutines and files, including the site"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\t# Set up basic argument parser and parse command arguments, but don't store the results\n\t# as values of any of the parser attributes. This lets the user override the defaults\n\t# themselves, but keeps track of any problems and exits instead of reporting them.\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument('-o', '--output', required=True, type=argparse.FileType('w'),\n\t\t\t\t\taction='store', help=(\n\t\t\t\t\t\t\"Write output from each of the processes to this file.\"))\n\tparser.add_argument('-B',"}, {"generated_text": "def parse_arguments(*arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as an ArgumentParser instance.\n\t\"\"\"\n\tparser = ArgumentParser(description=__doc__)\n\tparser.add_argument('--verbose', '-v', action='count', default=0,\n\t\t\t\t\t\thelp=\"print additional messages; can be used multiple times\")\n\tparser.add_argument('-d', '--debug', action='store_const', const=2, dest='loglevel',\n\t\t\t\t\t\thelp=\"write debug messages to the console; default is error level [default]\")\n\tparser.add_argument('-V', '--version', action='store_const', const=1, dest='version',"}]}
{"task_id": "62b438b666fea644fe22cc72", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn \"\\n\".join(flag.names for flag in parser.argument_groups[0].get_flags() if flag.metavar)\n\nif __name__ == \"__main__\":\n\tmain(sys.argv[1:])"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = set()\n\tfor flag in parser.flags:\n\t\tflags.add(flag.name)\n\t\tflags.add(flag.help)\n\treturn \"\\n\".join(flags)"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\ts = \"\"\n\tfor flag in parser._actions:\n\t\t# FIXME: flags_to_string needs to work with action instances\n\t\tif not hasattr(flag, \"positional_arg_string\"):\n\t\t\tcontinue\n\t\ts += \"{}{}\".format(flag.positional_arg_string, flag.flags if hasattr(flag, \"flags\") else '')\n\treturn s\n\n\n\ndef argparse_flags(arg_handler):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\t"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn''.join(arg_flags(parser))\n\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn (parser.add_argument('--' + k, v) for k, v in parser.parse_args()._get_kwargs_dict().items())\n\n\n@contextmanager\ndef tmpdir(path=\"tmp\", chmod=None, skip_if_exists=None):\n\t\"\"\"\n\tAs with os.makedirs(), but yields a context-manager which creates the directory and deletes it when the context manager is closed.\n\n\tA path for the new directory can be specified using \"path\" and it can be created\n\tor deleted.\n\n\tIf \"ch"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \" \".join(sorted(v.lower() for v in parser.FLAGS(None).keys()))\n\treturn '--'+flags.replace('-','--')[2:]\n\n\nclass Commandline:\n\t\"\"\"\n\tThis wraps raw command-line arguments and allow to call them in a fluent way.\n\t\t\n\t\t>>> cmd = Commandline(parser_or_str='my_parser', arg1=1, arg2=2, arg3='optional', arg4=True)\n\t\t>>> cmd.parse()\n\t\t>>> print cmd.cmd_"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags =''.join('-{}'.format(k) for k in parser.flag_values())\n\tif parser.epilog:\n\t\tflags = '{}{}'.format(flags, parser.epilog)\n\treturn flags\n\n\n@functools.total_ordering\nclass CliOptions(namedtuple('CliOptions', [\n\t'schema_dir', 'hostname', 'domain', 'path', 'file_type', 'query_class',\n])(\n\t\"schema_dir hostname domain path file_type query_class"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn''.join(parser_flags_subparser(parser))\n\nclass ArgumentParser(object):\n\tsubparsers = {}\n\t\n\t\n\tclass AddSubparser(object):\n\t\tdef __init__(self, name, *args, **kwargs):\n\t\t\tself.add_to_parser = kwargs.pop('add_to_parser', False)\n\t\t\tself.name = name\n\n\t\tdef convert_type(self, type_func):\n\t\t\tdef handler(*args, **kwargs):\n\t\t\t\treturn type_func(self.converted_"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn \" \".join([\"--\" + flag.dest for flag in parser._actions if flag.dest])\n\n\n# ======================================================================================\n# Tests\n# ======================================================================================\nclass ArgumentParserTests(unittest.TestCase):\n\n\tdef test_argument_parser(self):\n\t\t\"\"\"\n\t\tMake sure argument parsing worked!\n\t\t\"\"\"\n\t\tparser = argparse.ArgumentParser(description=argparse.SUPPRESS)\n\t\tparser.add_argument('-f', '--first', required=True)\n\t\tparser.add_argument('-l', '"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = ''\n\tfor arg in parser._optionals._option_string_actions.keys():\n\t\tif arg == '--':\n\t\t\tcontinue\n\t\telif arg == '--help':\n\t\t\tcontinue\n\t\telse:\n\t\t\tflags += arg + '=[]'\n\tfor opt in parser._optionals._option_string_actions.keys():\n\t\tif opt.startswith('-'):\n\t\t\topt = opt[1:]\n\t\tif opt == '--':\n\t\t\tcontinue\n\t\telif opt == '-h' or opt == '--help':\n\t\t\tcontinue\n"}]}
{"task_id": "62b438b566fea644fe22cc70", "prompt": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\timport pprint\n\n\timport argparse\n\timport commands_loader\n\timport borgmatic.commands\n\n\tcompletion = commands_loader.load_all(commands=[], load_only=['borgmatic'])[0][1]\n\tprint_completion = lambda s, p: print(\"%s%s\" % (s, p))\n\n\tfor l in sorted([x for x in dir(commands_loader.commands) if x.lower()]):\n\t\tcombo = l[0:1].upper() + l[1:]\n\t\tsub"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tscript = \"borgmatic_completion()\\n\"\n\tscript += \"exports = {0}\\n\".format(env[\"BORG_COMPLETION_MAPPING\"].keys())\n\treturn script\n\n\ndef borg_main():\n\t\"\"\"\n\tThe entry point. This is not the function that is exported by borgmatic-completion.py.\n\t\"\"\"\n\tcommands = list(env[\"BORG_BASH_COMPLETION_COMMANDS\"].split())\n\tcompletion_script = __completion_script(commands)\n"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tcommands = []\n\tfor cls_name in Borgmatic.commands:\n\t\tfor method_name, method in inspect.getmembers(Borgmatic, lambda x: inspect.ismethod(x) and getattr(x,\"_cli_action\", False)):\n\t\t\tcommand = cls_name.split(\".\")[-1].replace(\"_\", \"-\")\n\t\t\tdoc = method.__doc__.splitlines()\n\t\t\tusage = getattr(method, \"cli_usage\", None)\n\t\t\tdescription = \"{0} {1}\".format(\n\t\t\t\tdoc"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tcompletion_script = \"\"\n\n\tcompletion_script += \"borgmatic\"\n\tcompletion_script += \" \" * (\" \" + getattr(sys.argv[2], \"_orig_len\", 0) + 4)\n\tcompletion_script += \" \" * (\" \" + getattr(sys.argv[3], \"_orig_len\", 0) + 6)\n\n\tif completion_script[-1]!='':\n\t\tcompletion_script += \" \"\n\n\tcompletion_script += \" \" * (\" \" + getattr(sys.argv[4"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(add_help=False)\n\n\tdefault_parsers = borgmatic.default_parsers\n\n\tfor cl_cmd, p_cl_cmd in list(default_parsers.items()):\n\t\tparser.add_argument(\"%s_cmd\" % cl_cmd, help=\"Borgmatic command to generate bash completion for\", action=\"store_true\")\n\n\tfor p_cl_cmd, p_parser in list(default_parsers.items()):\n\t\tif p_parser is"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tfor cmd in iter_commands(__package__):\n\t\targ_parser = getattr(cmd,'set_arg_parser', None)\n\t\tif not isinstance(arg_parser, ParserMetaclass):\n\t\t\tcontinue\n\t\t# Parse the parser and return the options as a list of string names\n\t\topts = arg_parser.parse_args(cmd.parser.parser.option_strings[1:]).args\n\t\t# Add the option names to the completion script\n\t\tif opts:\n\t\t\tp.stdout.write(\"%_[%s]\""}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tparsers = [\"help\", \"clone\", \"list\", \"config\" ]\n\n\tfor parser in parsers:\n\t\tcompletions = []\n\t\tfor t in globals().keys():\n\t\t\tif not t.startswith(parser):\n\t\t\t\tcontinue\n\n\t\t\tif globs(t) is not None:\n\t\t\t\tif t.endswith(\"_completer\"):\n\t\t\t\t\tc = parser + \"$\" + t.replace(\"_completer\", \"\").replace(\"_\", \".toCompleter\")\n\t\t\t\telse:\n\t\t\t\t\tc = t\n\t\t\t\tcompletions"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom borgmatic import commands\n\tcommand_names = sorted([name for name, cls in commands.registered_commands.items() if cls.command is not None])\n\n\treturn \"\"\"\n_borgmatic_complete()\n{\n\tlocal cur prev words cword\n\t_init_completion -n \"$cur\" \"$words[0]\"\n\tfor w in $words[1:]; do\n\t\tprev=${cur##*}\n\t\tcur=\"${cur:-${words[0]}}\"\n\t\tcompopt -o pipefail\n\t\tcompgen -A -- \"$"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\tcompletion_script = b\"\"\n\tcompletion_script += (\" \".join(_parse_args_and_execute(parse_args_and_exec_commands, complete=True).keys()))\n\treturn sarge.CompletionScript(completion_script)\n\n\ndef parse_args_and_exec_commands(args, stdin):\n\t\"\"\"\n\tConvert the parsed borgmatic arguments into a dictionary of arguments, and execute the\n\tspecified commands.\n\t\"\"\"\n\tfrom.commands import Commands\n\targuments = _parse_args_and_"}, {"generated_text": "def bash_completion():\n\"\"\"\n\tReturn a bash completion script for the borgmatic command. Produce this by introspecting\n\tborgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport imp\n\timport os\n\timport os.path\n\n\t_fh = None\n\tscript_path = os.path.realpath(__file__)\n\tif script_path.endswith('.py'):\n\t\tscript_path = script_path[:-3]\n\tscript_path = os.path.join(script_path, 'BORG_COMPLETE.py')\n\ttry:\n\t\t_fh = open(script_path, 'rb')\n\t\tscript = _fh.read()\n\texcept IO"}]}
{"task_id": "62b438a266fea644fe22cc2e", "prompt": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targuments = parser.parse_args(*unparsed_arguments);\n\treturn arguments;\n\n\"\"\"\n\tThis function is a placeholder for the command-line parser. It checks to see if the\n\tfirst unparsed argument is the name of a script to run. If so, it attempts to look up that\n\tscript and run it.\n\n\"\"\"\ndef process_arguments(*unparsed_arguments):\n\t\"\"\"\n\tFirst unparsed argument should be the name of a script, and this function will attempt to run\n\tthe script to which that name"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targ_parser = argparse.ArgumentParser(description=\"Run tests of Nested Decision Model using this software.\",\n\t\t\t\t\t     fromfile_prefix_chars='@')\n\n\targ_parser.add_argument('test_cases',\n\t\t\t\taction='store',\n\t\t\t\tdefault=None,\n\t\t\t\tnargs='*',\n\t\t\t\thelp='Test cases to run')\n\n\treturn vars(arg_parser.parse_args(unparsed_arguments))\n\n\ndef run_cmdlist(command_list, shell=True, env=None, check_status=True, suppress"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targuments = {}\n\tfor argname, argvalue in unparsed_arguments():\n\t\targuments[argname] = argvalue\n\n\treturn arguments\n\ndef parse_arguments(parsed_arguments, **unparsed_arguments):\n\t\"\"\"\n\t\tGiven parsed arguments from previous parsing as well as args and kwargs with which this script was invoked,\n\t\tparse those as well and override any missing options, then return it all as a dict.\n\t\"\"\"\n\targuments = {}\n\tfor parsed, to_override in [parsed"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparsed_arguments = argparser.parse_args()\n\t\n\tif not unparsed_arguments:\n\t\tparsed_arguments[\"subparsers\"][None][\"description\"] \\\n\t\t= (\"Parses configuration files given on the \"\n\t\t\"command line, returning a Python object which can be passed to the \"\n\t\t\"server subcommand.\")\n\telse:\n\t\tunparsed_arguments = unparsed_arguments[0]\n\t\n\tfor argument in unparsed_arguments:\n\t\tif isinstance("}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = ArgumentParser(\n\t\tformatter_class=RawDescriptionHelpFormatter,\n\t\tdescription=sys.modules[__name__].__doc__)\n\tparser.add_argument(\"-a\", \"--action\", help=\"action (run or fetch)\", choices=[\"run\", \"fetch\"], required=True)\n\tparser.add_argument(\"-i\", \"--in_channel\", help=\"input channel (e.g. '/r/dailyprogrammer_testing')\")\n\tparser.add_argument(\"-o\", \"--out_channel\", help=\"output channel ("}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targparser = argparse.ArgumentParser()\n\targparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \"+__version__)\n\tif unparsed_arguments:\n\t\targparser.parse_args(unparsed_arguments)\n\treturn {k:v.dest for k,v in argparser._actions.items() if k in [\"output-prefix\", \"output-suffix\"]}\n\nif __name__ == \"__main__\":\n\tglobal_namespace = parse_arguments(sys."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\targs = argparse.ArgumentParser().parse_args(*unparsed_arguments);\n\treturn(vars(args));\n\ndef load_requirements(requirements):\n\t\"\"\"\n\tGiven the requirements file for a single app,\n\treturn a set of requirement strings, and a list of requirements strings which may\n\tbe appended by ``setup.py fetch-requirements``.\n\t\"\"\"\n\trequirements_file = requirements;\n\tif not fileExists(requirements_file):\n\t\terrors.fatal(\"File {0} doesn't exist\"."}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t# The subparsers object here will take as its argument to the main method a list of command-line arguments (as returned by sys.argv).\n\t# The first command-line argument in this list can be:\n\t#   - the name of a subparser (a string)\n\t#   - a shorthand for more than one command, in the form'repo':[<arg>...]\n\t#   - a global ('global' name), in the form 'global:<arg>'\n\t# All other arguments will be treated as positional arguments\n\t# In case of an"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument('--config', dest='config', metavar='config_file', type=str, action='store',\n\t\t\t\t\t\tdefault=get_default_config_file_path())\n\tparser.add_argument('--debug', dest='debug', action='store_true', default=False)\n\tparser.add_argument('--verbose', dest='verbose', metavar='level')\n\tparser.add_argument('--version', dest='version', action='store_const"}, {"generated_text": "def parse_arguments(*unparsed_arguments):\n\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\n\tthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser() # Argument parser for specifying options, and for parsing command-line arguments\n\tparser.add_argument('-r','--results_dir',type=str,required=True,help='directory containing results (either for individual tests or for testing all')\n\tparser.add_argument('--test-class',type=str,help='test class to run - valid values are: default, test_with_all_params, test_with_all_params_unified_result, test_with_all_params_unified_result, test_"}]}
{"task_id": "62b438a266fea644fe22cc2d", "prompt": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\tresult = {}\n\t\n\t# Iterate in the order of subparser.arguments\n\tfor subparser_name in sorted(subparsers.keys()):\n\t\t# Find the parser\n\t\tsubparser = subparsers[subparser_name]\n\t\t\n\t\t# Get the arguments and parse them, including a final check at the end of each\n\t\targuments = subparser.arguments\n\t\tparsed_arguments = sub_parse_arguments(unparsed_arguments + ['--' + subparser.title], arguments)\n\t\tif"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tfrom re import findall, compile\n\t\n\tparser_dict = {}\n\tremaining_arguments = []\n\tfor k, v in subparsers.items():\n\t\tparser = v.add_parser(k, help=\"%(prog)s sub-command help\")\n\t\tparser.set_defaults(func=v._execute)\n\t\t\n\t\t# Give any remaining arguments to the subparser, so they get parsed\n\t\t# in subparser_repr_compiled.\n\t\tparser.set_defaults(all_remaining_arguments=remaining"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tfinal_results = {}\n\tfor sub_parser in subparsers.values():\n\t\tns = None\n\t\t# Use the parser of the subparser to process arguments, as the parser's help string can\n\t\t# override the result of get_parser().\n\t\tparser = sub_parser.parser\n\t\tns, unparsed_arguments = parser.parse_known_args(unparsed_arguments)\n\t\tfinal_results[sub_parser.name] = ns\n\treturn final_results, unparsed_arguments\n\nclass ArgumentParserSubstitutionPlugin("}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser = subparsers.choose(unparsed_arguments)\n\t# Remove any \"--\" prefix, and split out individual argument fields.\n\tparser_args = [arg for arg in unparsed_arguments if arg.startswith(\"--\")][1:]\n\t# Parse given arguments and add result to parser, after any defaults.\n\tresult = parse_namespace_options(parser, parser_args, parser.defaults, True)\n\treturn result, parser_args"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t# Convert to list from tuple for easier chaining.\n\tparsed_args = list(unparsed_arguments)\n\n\tparsed_options = None\n\tfor subparser in subparsers:\n\t\tsub_parser = subparser.add_parser(name=name, description=description, **kwargs)\n\t\tnew_kwargs = dict(**kwargs)\n\t\tnew_kwargs.pop('help', None)\n\t\tsub_options, remaining_args = sub_parser.parse_args(args=parsed_args, **new_"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed = dict()\n\tremaining_items = []\n\tfor parser_name, subparser in subparsers.iteritems():\n\t\tparser = subparser.parse_args(unparsed_arguments[parser_name])\n\t\tif parser is not None:\n\t\t\tparsed[parser_name] = parser.namespace\n\t\t\tremaining_items = [\n\t\t\t\titem for item in unparsed_arguments[parser_name]\n\t\t\t\tif item not in parsed[parser_name]\n\t\t\t]\n\treturn (parsed, remaining_items"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\tunclaimed_args = unparsed_arguments[:]\n\tremaining_args = []\n\tseen_subparsers = {}\n\t\n\tfor subparser_name, subparser in subparsers.items(): # Parse for each requested subparser in turn.\n\t\t# Store the list of remaining arguments.\n\t\tremaining_args.extend(subparser.dest.values())\n\t\t\n\t\t# Create a new namespace based on the parsed arguments passed to this subparser and discard\n\t\t# any other arguments that were not claimed by this handler.\n\t\tns"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\t# TODO - make sure arguments are actually in command's namespace \n\tnamespace_to_arguments = {}\n\tall_names = set() \n\tfor cmd in subparsers.keys(): \n\t\tnamespace = getattr(subparsers[cmd], \"_namespace\")\n\t\tif namespace is not None:\n\t\t\tnamespace_to_arguments.update(namespace._arguments)\n\t\t\tall_names |= set(namespace._names)\n\tall_arguments = parser.parse_args(unparsed_arguments"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_ns = {}    # namespace of unpacked subparser args\n\tother_args = []      # rest of arguments, not consumed by any subparser\n\t\n\tfor action in subparsers.choices.values():\n        \tfor unparsed_arg in unparsed_arguments:\n\t\t\tif len(unparsed_arg) < len(action.prefix_chars):  # don't bother to claim an argument if it doesn't match prefix chars\n\t\t\t\tcontinue\n\t\t\targ_name, arg_value = parse_subparser_arg(unpars"}, {"generated_text": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n\tinstance, give each requested action's subparser a shot at parsing all arguments. This allows\n\tcommon arguments like \"--repository\" to be shared across multiple subparsers.\n\t\n\tReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n\targuments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparsers_actions = dict()\n\tfor action_name, subparser in (subparser.actions for subparser in subparsers.choices.values()):\n\t\tsubparser_parser = subparser.choose_subparser(action_name, unparsed_arguments)\n\t\tif subparser_parser is None:\n\t\t\tcontinue\n\t\tsubparsers_actions[action_name] = subparser_parser.parse_args(unparsed_arguments)\n\tret = dict()\n\tfor name, action in actions.items():\n\t\t"}]}
{"task_id": "62b438a266fea644fe22cc2c", "prompt": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# the root parser\n\troot = argparse.ArgumentParser(prog='structureBuilder')\n\t# Add command line options\n\troot.add_argument('-t', '--structure', required=True, help='the name of the input files')\n\troot.add_argument('-o', '--outdir', required=True, help='the name of the output directory')\n\troot.add_argument('-b', '--batch', required=False, help='the number of processors to use for batching (defaults to system num_procs)')"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = p.arg_decl(\"name\", docopt.Parser)\n\n\tfiles_parser = p.sub_parser(\"files\", \"File selection options\")\n\t\t.help(\"File selection options\")\n\t\t.help(\"Read a single file or a range of files\")\n\t\t.argument(\"range\",\n\t\t\taction=\"store\",\n\t\t\ttype=int,\n\t\t\thelp=\"Single file or a range of files to be read\")\n\t\t.argument(\"-l\", \"--lines\",\n\t\t\taction=\"store\",\n\t\t\tdefault=None,\n\t\t\thelp=\"Read the line range between the start position"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# build the parser, top-level\n\tparser = MyParser(\n\t\t\tprog=\"jumpscale\",\n\t\t\tdescription=\"\"\"Jumpscale is a multi-platform open-source,\n\t\t\t\tself-progamming, integrated, desktop based development\n\t\t\t\tand user friendly application management software.\n\t\t\t\t\"\"\",\n\t\t\tepilog=\"\"\"Copyright \u00a9 2014 by JumpScale inc\n\n\t\t\t\tPermission is hereby granted, free of charge, to any person\n\t\t\t\tobtaining a copy of this software and associated\n\t\t\t\tdocumentation files (the \"Software\"), to deal in the\n\t\t\t\tSoftware without restriction,"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tprog = PROGRAM_NAME,\n\t\tdescription = \"A program to convert JSON files to Python classes\", \n\t\tepilog=\"See --help argument for full list of sub-commands\"\n\t)\n\tclass ParserHelper:\n\t\tgroup = parser.add_mutually_exclusive_group(required=True)\n\t\tgroup.add_argument(\n\t\t\t\"-s\", \"--script\",\n\t\t\thelp = \"Generate a Python class from the information in a JSON file (generate_py.py)\",\n\t\t\taction ="}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\ttop_parser = argparse.ArgumentParser(usage=USAGE,\n\t\t\t\t\t\t\t\t\t\t summary=SUMMARY,\n\t\t\t\t\t\t\t\t\t\t description=DESCRIPTION,\n\t\t\t\t\t\t\t\t\t\t formatter_class=CustomHelpFormatter)\n\n\ttop_parser.add_argument(\"-d\", \"--delay\", help=\"delay between runs\",\n\t\t\t\t\t\t\ttype=int, default=1, required=False)\n\ttop_parser.add_argument(\"-v\", \"--vector-length\", help=\"length of the vectors\",\n\t\t\t\t\t\t\ttype=int, required=False)\n\ttop_parser.add_"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\ttop_parser = argparse.ArgumentParser(add_help=False)\n\n\ttop_parser.add_argument(\"--logic-level\", help=\"set the current level of logging as a string (e.g. 'warn|error' for all)\", default=\"info\")\n\n\ttop_parser.add_argument(\"-p\", \"--params\", help=\"specify input parameters for the experiment\", action=\"append\", type=str)\n\ttop_parser.add_argument(\"-v\", \"--variable\", help=\"specify variable used to construct the experiments' names\", action=\""}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"\"\"A library for building\n\tdynamically growing languages and programs on Linux.\"\"\",\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tepilog=\"\"\"This is free software: you are welcome to redistribute it\n\tand/or modify it under the terms of the GNU General Public License as\n\tpublished by the Free Software Foundation, either version 3 of the\n\tLicense, or (at your option) any later version.\n\n\"\"\")\n\tparser.add_argument('--version',\n\t\thelp='"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgParser(\n\t\tprog=\"%s [options]\" % name,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tusage=get_usage(),\n\t\tdescription=\"\"\"\\\n\t{} [options] [files...]\n\n\tRun the %s, parsing command line arguments to construct a\n\t:py:class:`sibis.cli.Cmd` instance which can be used to run different test cases\n\tconcurrently. These are currently the basic commands:\n\t- `test_dir` - Run a test dir.\n\n"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# The help option needs to know about all the subcommands in order\n\t# to display them in the help text.\n\tparser = optparse.OptionParser(usage=\"%prog COMMAND [options]\")\n\tparser.add_option(\"-l\", \"--list-plugins\", action=\"store_true\",\n                help=\"list the available plugins from which to load a \" +\n                \"plugin (can be chained)\")\n\n\tparser.add_option(\"-w\", \"--wipe\", action=\"store_true\",\n                help=\"clear the database of loaded plugins\")"}, {"generated_text": "def make_parsers():\n\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t# Load parser definitions.\n\t# The loader doesn't have to be set here!\n\t# If None is used for the loader, setuptools will figure it out\n\t# using the value in sys.path.\n\tloader = None\n\t# setuptools uses the loader to look into standard library. The default\n\t# loader loads the module from the current directory, which is probably\n\t# not what you want. Use the empty loader instance instead.\n\tfor loader in [getdlopenutil(), get_file_loader(), get_module_loader(),\n\t\t\t\t\tget_path_"}]}
{"task_id": "62ece4982e6aefcf4aabbd5f", "prompt": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tmax_results_int = int(max_results)\n\ttotal = int(response[0][\"meta\"][\"total\"])\n\tmax_page = math.ceil(total / max_results_int)\n\tfor page in range(1, max_page + 1):\n\t\tif page!= 1:\n\t\t\tresponse = response[max_results_int::page][0]\n\t\telse:\n\t\t\tresponse = response[0]\n\t\tyield response\n\n\ndef pagination(response, page, max_results):\n\t\"\"\"\n\tReturns WAPI response split"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage_nr = 1\n\tpaging_size = 100\n\twhile page_nr <= int(math.ceil(len(response) / float(max_results))):\n\t\tfor obj in response[page_nr * paging_size:(page_nr + 1) * paging_size]:\n\t\t\tyield obj\n\t\tpage_nr += 1\n\ndef parse_wapi_response(response):\n\t\"\"\"parses WAPI response and returns list of dicts\"\"\"\n\treturn convert_response_to_obj(responses.Pagination(paging("}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage_size = max_results\n\tcurrent_index = 0\n\t\n\twhile True:\n\t\tyield response\n\t\t\n\t\tif len(response) < max_results:\n\t\t\tbreak\n\t\t\n\t\tcurrent_index += 100\n\t\tresponse = response[current_index:]"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage = response\n\n\tdef yield_page():\n\t\ttry:\n\t\t\twhile True:\n\t\t\t\tstart = 0\n\t\t\t\twhile len(page) >= start:\n\t\t\t\t\tend = start + max_results\n\t\t\t\t\tif end > len(page):\n\t\t\t\t\t\tend = len(page)\n\t\t\t\t\tyield page[start:end]\n\t\t\t\t\tstart = end\n\t\t\t\telse:\n\t\t\t\t\treturn\n\t\texcept (AttributeError,StopIteration):\n\t\t\traise StopIteration\n\t\treturn\n\t\t\n\treturn yield_page()"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tcount = 0\n\ttotal = 0\n\tfor pager in paging_iter(page_size=max_results, response_obj=response):\n\t\t\tyield pager\n\t\t\tif pager['count']:\n\t\t\t\ttotal    += pager['count']\n\t\tif ((total+page_size-1)/ page_size) < max_results: # if less results than the last page\n\t\t\tyield {'count':total, 'next' : urljoin('/',get_resource_base('WAPI'),get_resource_key('WAPI'))+'/?"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tif response is None:\n\t\treturn\n\tnext_page_list = response\n\tpage = 0\n\twhile next_page_list is not None:\n\t\tpage += 1\n\t\tyield next_page_list\n\t\tif page >= max_results:\n\t\t\tbreak\t\n\t\tnext_page_list = next_page_list.get('next')\n\tnext_page_list = None\n"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tcurrent_page = 1\n\twhile len(response) > max_results:\n\t\tyield response[:max_results]\n\t\tresponse = response[max_results:]\n\t\tcurrent_page += 1\n\n# def get_data():\n\t\"\"\"\n\t\tReturns generator with all pages of WAPI response\n\t\tReturns:\n\t\t    Generator object with all data pages.\n\t\"\"\"\n\tresponse = wapi.search()\n# \tresponse2 = wapi.search_filters()\n\tif not isinstance(response, (tuple, _basestring, bytes"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tfor i, j in enumerate(response):\n\t\tyield j, max_results\n\t\tmax_results-=1\n\ndef search_paging(text, count):\n\t\"\"\"\n\tReturns list of objects by search and paging\n\tUsed with search or similar\n\t\n\tArgs:\n\t    text (str): search text\n\t    count (int): total number of search results\n\tReturns:\n\t    List of objects\n\t\"\"\"\n\tresponse = WAPIRequest(\"https://developer.riotgames.com/api/lol/platform/summoner/"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tyield response[:max_results]\n\tfor result in response[max_results:]:\n\t\tyield result\n\n\n\"\"\"=========================================================================\n    Main function\n=========================================================================\"\"\"    \n\ndef _create_request(command, params_list = [], method_url = '/', timeout = 3, headers = None, data = None):\n        \"\"\"creates WAPI request.\n\t\n\tArgs:\n\t    command (string): WAPI command to be performed.\n\t    params_list (list): parameter list to be passed to WAPI command, if any.\n"}, {"generated_text": "def paging(response, max_results):\n\"\"\"\n\tReturns WAPI response page by page\n\t\n\tArgs:\n\t    response (list): WAPI response.\n\t    max_results (int): Maximum number of objects to be returned in one page.\n\tReturns:\n\t    Generator object with WAPI response split page by page.\n\t\"\"\"\n\tif max_results > MAX_RESULTS_IN_PAGE:\n\t\tmax_results = MAX_RESULTS_IN_PAGE\n\t\tprint(\"Reached maximum number of results.\")\n\t\n\tmax_results = min(max_results, response[total_records-1]['total_count'] - 1)\n\ttotal_pages = int((response[total_records-1]['total_count']-1) / max_results) + 1\n\t\n\tfor pagination in pagination(start=offset, page_size=max_results, collection_name"}]}
{"task_id": "62ece4982e6aefcf4aabbd60", "prompt": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\tsuffixes = {\n\t    'K': 1024,\n\t    'M': 1024 ** 2,\n\t    'G': 1024 ** 3,\n\t    'T': 1024 ** 1.7,\n\t    'P': 1024 ** 1.5\n\t}\n\n\ttry:\n\t\tsuffix = suffixes[size[size.rfind(\" \"):]]\n\texcept KeyError:\n\t\t# Invalid sizes - let it fail\n\t\treturn size\n\t\n\tdecimal = round(float(size[:-1]) / suffix, 3)\n\treturn decimal\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\tunits = [\n\t\t('K', 1_000),\n\t\t('M', 1_000_000),\n\t\t('G', 1_000_000_000),\n\t\t('T', 1_000_0000_000),\n\t\t('P', 1_000_000_000_000),\n\t\t('E', 1_000_000_000_000_000)\n\t]\n\t\n\tfor unit, decimals in units:\n\t\tif size.endswith(unit):\n\t\t\tsize = float(size[:-1]) * decimals\n\t\t\t#"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\tsize_tables = {'K': pow(1024, 1), 'M': pow(1024, 2), 'G': pow(1024, 3)}\n\t\n\tif not size.strip().isnumeric():\n\t\treturn size\n\t\n\tt = size.strip()\n\tif t.endswith('B') or t.endswith('b'):\n\t\treturn int(t[:-1])\n\t\n\tres = 0\n\ttry:\n\t\twhile t:\n\t\t\tres = res * pow(size_tables[t[-1]"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tunits = [('bytes', 1), ('KB', 1024), ('M', 1024 * 1024), ('MB', 1024 * 1024)]\n\tif size.isdigit():\n\t\trval = int(size)\n\telif size[-1].isdigit() and size.isdigit():\n\t\trval = int(float(size[:-1])*units[int(size[-1])][1] * 100)\n\telse:\n\t\traise ValueError(\"Not a valid input size\")\n\t\n\tfor k, v in units:\n\t\trval = r"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\tresult = 0\n\tif size is None:\n\t\treturn result\n\tif isinstance(size, int):\n\t\treturn size\n\tsize = size.replace(\" \", \"\")\n\tmultiplier = 1024\n\tfor suffix in [\"B\", \"K\", \"M\", \"G\", \"T\", \"P\", \"E\"]:\n\t\tif size.endswith(suffix):\n\t\t\tsize = size[:-len(suffix)]\n\t\t\tmultiplier = 1024 ** len(suffix)\n\t\telse:\n\t\t\tbreak\n\ttry:\n\t\tif size.startswith(\""}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t# 1K-1M\n\tif 'K' in size:\n\t\treturn int(float(size[:len(size)-1]) * 1000)\n\t# 1M-10G\n\telif 'M' in size:\n\t\treturn int(float(size[:len(size)-1]) * 1e6)\n\t# 10G-1T\n\telif 'G' in size:\n\t\treturn int(float(size[:len(size)-1]) * 1e9)\n\t# 1T-1P\n\telif 'T' in"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\t# TODO: Is there a better way to handle decimal?\n\t\n\t# TODO: This might not work for unicode input or if user specified \n\t# bytes instead of KiB, MiB etc\n\t\n\treturn int(float(size.replace(' ','')) * 1024)\n\t\nif __name__ == \"__main__\":\n\timport doctest\n\tdoctest.testmod()\n\t\n\t\n"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\tsize = re.sub(\"[tKMkH]?B\", \"\", size).upper()\n\tfor value, unit in RE_SIZES:\n\t\tif size.endswith(unit):\n\t\t\tsize = float(size.rstrip(unit))\n\t\t\tbreak\n\treturn round(size)**(1/3.)\n\t\n\t\ndef convert_size(size: str, suffix: str) -> str:\n\t\"\"\"\n\t    Convert file size to human readable string.\n\t    \n\t    Accepted suffixes for output ``format``"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\tif not size:\n\t\treturn 0\n\t\n\tsize = float(size.upper().replace(\"B\", \"\"))\n\tsuffix = 'B'\n\t\n\tunits = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\"]\n\tmultipliers = [1, 1000, 1000000, 1000000000, 1000000000000, 100000000000,\n                    1000000000, 10000000000000, 10000000000000000]\n\tsize = 0\n\t\n\tfor i in range(len(units)-1"}, {"generated_text": "def size_to_bytes(size: str) -> int:\n\"\"\"\n\tConvert human readable file size to bytes.\n\t\n\tResulting value is an approximation as input value is in most case rounded.\n\t\n\tArgs:\n\t    size: A string representing a human readable file size (eg: '500K')\n\t\n\tReturns:\n\t    A decimal representation of file size\n\t\n\t    Examples::\n\t\n\t        >>> size_to_bytes(\"500\")\n\t        500\n\t        >>> size_to_bytes(\"1K\")\n\t        1000\n\t\"\"\"\n\t\n\tdecimal = Decimal(size) / 1000\n\t\n\treturn int(decimal) if int(decimal)!= decimal else decimal"}]}
{"task_id": "62ece4982e6aefcf4aabbd61", "prompt": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn {k.lower(): sum(v) for k, v in itertools.chain(*(d.items() for d in dicts))}\n\n\ndef _dictdict(*dicts):\n\t\"\"\"\n\tCombine dictionaries sorted by keys from the first.\n\t\n\t>>> _dictdict({'a': [1, 2, 3], 2: ['a', 'b', 'c']})\n\t[(1, {'2': 'a'}), (2, {'2': 'b'}), (2, {'2': 'c'}"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\t\n\treturn sum(dict(dicts, **({}} ** x)) for x in itertools.product(*dicts)) \n\ndef dictcombine(dicts):\n\t\"\"\"Concatenate all the dictionaries in dicts into\n\ta single dictionary with all the items in dictionary from\n\tall the dicts combined.\n\t\n\t>>> dictcombine([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])['c']\n\t6\n\t\"\"\"\n\t\n\treturn _dictsum(dictcombine(["}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tresult = {}\n\tfor iterdict in dicts:\n\t\tfor (k,v) in iterdict.items():\n\t\t\tif k in result:\n\t\t\t\tresult[k] = result[k] + v\n\t\t\telse:\n\t\t\t\tresult[k] = v\n\treturn result\n\nclass Dictionary:\n\t\"\"\"\n\tA Dictionary is a simple and fast implementation of a persistent dictionary.\n\tIts items are stored in a single list, and all operations return a reference\n\tto this single list.\n\t\n\t>>> Dictionary([(id(o),o) for o in range("}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tresult = {}\n\tfor d in dicts:\n\t\tfor k,v in d.items():\n\t\t\tif k not in result:\n\t\t\t\tresult[k] = v\n\t\t\telif k == \"b\":\n\t\t\t\tresult[k] += d[k]\n\t\t\telse:\n\t\t\t\tresult[k] += v\n\treturn result\n\ndef _listsum(seq, **kwargs):\n\t\"\"\"\n\tReturns a copy of a list, with each item added as a separate entry.\n\n\t>>> _listsum([1,2,3])\n\t[1, 2,"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\tdsum = dict()\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tdsum[k] = dsum.get(k, 0) + v\n\treturn dsum\n\ndef _dictproduct(dicts):\n\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t>>> _dictproduct([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t[(1, 5), (1, 2)]\n\t\"\"\"\n\trset ="}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\timport collections\n\tresult = collections.defaultdict(int)\n\tfor d in dicts:\n\t\tfor k, v in d.iteritems():\n\t\t\tresult[k] += v\n\treturn result\n\ndef _dictunion(dicts):\n\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> print _dictunion([{'a': 10, 'b': 20}, {'a': 5, 'b': 0}])\n\t{'a': 15, 'b': 20}\n\t\"\"\"\n\timport collections\n"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\td = dictsum(dicts)\n\tif d: return d\n\traise ValueError\n\ndef _zip(t1, t2):\n\t\"\"\"\n\tzip(pairs of arrays, indices of the 2nd array)\n\n\t>>> t1 = [['a', 'b'], [1, 2]]\n\t>>> t2 = [['c', 'd'], [3, 4]]\n\t>>> [_x[0] + _x[1] for _x in zip(t1, t2)]\n\t['a1', 'b2', 'c"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\td = {}\n\t\n\tfor d_ in dicts:\n\t\tfor key in d_:\n\t\t\td[key] = d_[key]\n\treturn d\n\ndef _dictmin(dicts):\n\t\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictmin([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 1, 'b': 0}\n\t\"\"\"\n\td = {}\n\t\n\tfor d_ in"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn dict(itertools.chain.from_iterable(values for values in dicts))\n\t\ndef test():\n\t\"\"\"Test script.\"\"\"\n\t\n\timport doctest\n\treturn doctest.testmod()\n\n\n## MAIN ##\n\nif __name__ == \"__main__\":\n\tprint(_stringsum('bogus', 2))\n\tprint(test())\n"}, {"generated_text": "def _dictsum(dicts):\n\"\"\"\n\tCombine values of the dictionaries supplied by iterable dicts.\n\t\n\t>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n\t{'a': 6, 'b': 2}\n\t\"\"\"\n\treturn dict((key, sum([dicts[x][key] for x in dicts])) for key in dicts[0].iterkeys())\n\ndef _sumdict(d):\n\t\"\"\" Compute sum over keys of dictionary (d) \"\"\"\n\treturn sum([d[k] for k in d.iterkeys()])\n\ndef _sortedlist(l):\n\t\"\"\" Return a list of the elements of list (l) with the elements\n\tsorted by increasing order.\n\t\n\t>>> l = [3, 6, 2, 1, 4]\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd62", "prompt": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\turl_args_regex = re.compile(r'''\\$(?:(\\w+)\\@)?(?P<key>[^\\$\\@]+)''')\n\treturn url_args_regex.sub(lambda x: x.group(\"key\") in url_args and url_args[x.group(\"key\")] or x.group(\"key\")[1:-1], url)\n\n\nclass UrlRewrite(object):\n\t\"\"\"\n\tBase rewrite class, should not be used directly\n\n\tAttributes sets the default values of the parameters\n\n\tRew"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tnew_url = _unicode(url)[1:-1]\n\tnew_url = re.sub(\"\\{\\{\\s*url_args\\s*\\}\\}\", u\".\".join(map(unicode,url_args)), new_url)\n\treturn _unicode(new_url)[1:-1]\n\n\ndef _query_items_to_dict(iterable):\n\t\"\"\"\n\tBuilds a dictionary from a sequence of querystring items\n\n\t\"\"\"\n\treturn dict(map(lambda x: x.split(\"=\"),filter(lambda x:"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tparsed = urllib2.urlparse(url)\n\tnewurl = parsed[2] + \"?\"\n\t\n\tplist = list(parsed[4])\n\tplist += list(url_args)\n\t\n\tparsed[4] = tuple(plist)\n\tnewurl += urllib.urlencode(parsed[4])\n\treturn newurl\n\ndef _make_request(url, http_method):\n\t\"\"\"\n\tIssue HTTP requests\n\t\"\"\"\n\timport urllib2\n\timport logging"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\turl_tmp = url\n\tfor param, arg in url_args.iteritems():\n\t\ttry:\n\t\t\turl_tmp = url_tmp.replace(\"{{ \" + param + \" }}\", arg)\n\t\texcept:\n\t\t\t#print  param, arg, \"not found in: \", url_tmp\n\t\t\tcontinue\n\treturn url_tmp\n\ndef get_url(url, referer=None, force_basic_auth=False, basic_auth_string=None, decode_param=True, remove_basic_auth=False, post_data=None):\n\t"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\turl_pattern = url.replace(\"/{}/\".format(settings.LANGUAGE_CODE), \"/\")\n\tfor url_key, url_value in url_args.iteritems():\n\t\t#print \">>> %s = %s\"%(url_key, url_value)\n\t\tif isinstance(url_value, str) and \"%s\" in url_value:\n\t\t\t#print \">>> %s: Replacing %s with %s\"%(url_key, url_value, url_value.replace(\"{0}\".format(settings"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\turl = url.split('?')\n\tif len(url) < 2:\n\t\treturn (url[0])\n\telse:\n\t\tqueryargs = url[1].split('&')\n\t\tfor queryarg in queryargs:\n\t\t\ttry:\n\t\t\t\turl_arg = queryarg.split('=')\n\t\t\t\tif url_arg[0] in url_args.keys():\n\t\t\t\t\t# replace the key with its value\n\t\t\t\t\turl.append(('%s=%s' % (url_arg[0], url_args[url_arg[0]])))\n\t\t\t\t\t"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tdef replace(m):\n\t\tv = m.group(1)\n\t\tif url_args and getattr(url_args, v, None):\n\t\t\treturn \"%s\" % url_args[getattr(url_args, v, None)]\n\t\treturn v\n\treturn re.sub(\"\\{\\{([^}]*)}}\", replace, url)\n\nclass Page(models.Model):\n\ttitle = models.CharField(max_length=255)\n\turl = models.CharField(max_length=255, help_text=\"Optional. Defaults to"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tscheme = urlparse(url)[0]\n\tif scheme in ['http', 'https']:\n\t\tfixed_url = scheme + '://'\n\n\t\tif ':' in url:\n\t\t\tif url.find('/') > -1:\n\t\t\t\turl_split = url.split(':', 1)\n\t\t\t\tfixed_url += url_split[0]\n\t\t\telse:\n\t\t\t\tfixed_url += url\n\n\t\telse:\n\t\t\tfixed_url = fixed_url + url\n\n\t\tif len(url_args) > 0:\n\t\t\tfixed_url = fixed_url +"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tif '{' in url:\n\t\turl, url_args = re.subn( '{[^}]+}', str(url_args), url )\n\treturn url\n\n# This will cause a lot of problems: should we still support this?\n#if __main__ == __name__:\ndef main():\n\t'''\n\tPrints (and optionally stores) input options for each service type.\n\t'''\n\tfor service_type in SERVICE_TYPES:\n\t\tservice = SERVICE_TYPES[service_type]()\n\n\t\tif service"}, {"generated_text": "def _replace_url_args(url, url_args):\n\"\"\"\n\tReplace any custom string URL items with values in args\n\t\"\"\"\n\tstr_url = url.replace('{}', '').replace('{0}', '%s')\n\tstr_url = str_url % url_args\n\treturn str_url\n\ndef _replace_url_args2(url, url_args):\n    d =  dict((urllib.quote(k, safe=''),v) for (k,v) in url_args.items())\n    return urllib.unquote(url.format(**d))\n\nclass _WebCred(object):\n\tdef __init__(self,"}]}
{"task_id": "62ece4982e6aefcf4aabbd63", "prompt": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif val is not None:\n\t\treturn False\n\treturn True\n\ndef str_to_bool(val: str) -> bool:\n\tif val.lower() == '1':\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef bool_to_str(val: bool) -> str:\n\treturn '1' if val else '0'\n\ndef str_to_float(val: str) -> float:\n\ttry:\n\t\treturn float(val)\n\texcept (ValueError, TypeError):\n\t\treturn 0\n\ndef float_to"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn isinstance(val, str) and val == \"\"\n\ndef is_bool_value(val: any) -> bool:\n\t\"\"\"\n\tCheck if a value represents a boolean value.\n\t\"\"\"\n\treturn isinstance(val, bool)\n\n#\n#\n\ndef is_not_none_number_value(val: any) -> bool:\n\t\"\"\"\n\tCheck if a value represents a integer or floating number. Negative numbers aren't supported\n\t\"\"\"\n\treturn (isinstance(val, int) or isinstance(val, float)) and val >= 0\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\tif val === '':\n\t\treturn True\n\ttry:\n\t\treturn isinstance(val, six.text_type)\n\texcept NameError:\n\t\ttry:\n\t\t\treturn isinstance(val, six.string_types)\n\t\texcept NameError:\n\t\t\treturn False\n\ndef string_to_obj(val: str) -> Any[Any]:\n\t\"\"\"\n\tConvert a value from a string (string, None, int) to the desired object\n\t\"\"\"\n\tif val is None:\n\t\treturn None\n\telif isinstance(val, six.text"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val is None\n\t\n# -----------------------------------------------------------------------------\n#  String representation of the type\n# -----------------------------------------------------------------------------\n\ndef get_type_name(val: any) -> str:\n\t\"\"\"\n\tGet string representation of a type.\n\t\"\"\"\n\tif isinstance(val, (int, float, str) ):\n\t\treturn val.__repr__()\n\telif isinstance(val, (list, tuple)):\n\t\treturn \"[\" + \"],\".join(map(get_type_name, val))\n\telif isinstance(val, dict):\n\t\t"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn NoneStringPattern.match(val)!= None\n\n################################################################################\n# STRING TO INT RANGE IMPLEMENTATION\n################################################################################\nclass StringToRange(object):\n\t\"\"\"\n\tString to range converter.\n\t\"\"\"\n\tdef __init__(self, *args: Any) -> None:\n\t\t\"\"\"\n\t\tInitialize with string range specification (min,max).\n\t\t\"\"\"\n\t\tif len(args) > 2 or len(args) == 0:\n\t\t\traise TypeError(\"String to range converter takes between 1 and 2 arguments\")\n\n\t\tmin_str"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val == NULL_VALUE\n\ndef get_json_object(data: Mapping[str, Any], key: Str):\n    \"\"\"Get the value inside a json object.\"\"\"\n\n    if data is None:\n        return None\n\n    try:\n        return data[key]\n    except KeyError:\n        return get_json_object(data.get(key, None), key)\n\ndef get_list_obj(obj, key):\n    \"\"\"Get the values outside a list.\"\"\"\n    if obj is None:\n        return None\n\n    return [get_"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val and val == \"None\"\n\n@dataclass\nclass FieldData:\n\t\"\"\"Data from a single field.\"\"\"\n\tfield_id: Dict[str, ColumnData]\n\tfield_type: str\n\ndef get_field_for_table(table: Table, field_id: str) -> FieldData:\n\t\"\"\"Get the `FieldData` of one field from the dataset.\"\"\"\n\tfield_id = parse_field_id(field_id)\n\tresult = {field_id.id: {\n                field_id.datat"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn (isinstance(val, (str, unicode)) and val.lower() == \"none\")\n\ndef is_empty_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string representation is empty or None\n\t\"\"\"\n\treturn (isinstance(val, str) and val == \"\")\n"}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val is None\n\ndef is_true(val: bool) -> bool:\n\t\"\"\"\n\tCheck if a bool value represents a value.\n\t\"\"\"\n\treturn val\n\ndef is_false(val: bool) -> bool:\n\t\"\"\"\n\tCheck if a bool value represents False.\n\t\"\"\"\n\treturn not is_true(val)\n\n# Some function definitions\n\ndef get_value_or_function(k: str, v: (str, Any)) -> str:\n\t\"\"\"\n\tReturn a value and a function, given a key and value."}, {"generated_text": "def is_none_string(val: any) -> bool:\n\"\"\"\n\tCheck if a string represents a None value.\n\t\"\"\"\n\treturn val.lower() == 'none'  # nocoverage -- This should not be used in production code.\n\n\n@dataclass(frozen=True)\nclass Task(TaskMixin, NailgunTask):\n    \"\"\"A task that may be executed by :py:class:`pants.build_graph.BuildGraphMixin`.\"\"\"\n\n    options_scope: type[str]\n    _options_scope_warning: bool = True\n\n    @classmethod\n    def register_options(cls, register) -> None:\n        super().register_options("}]}
{"task_id": "62ece4982e6aefcf4aabbd65", "prompt": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn \" \".join(\"-\" + flag for flag in parser.prog.kwargs.keys())\n\n\nif __name__ == \"__main__\":\n\timport argparse\n\n\tclass ParserWithFlags(ArgumentParser):\n\t\tdef __init__(self):\n\t\t\tsuper().__init__(prog='test', description=\"test program\")\n\t\t\tself.add_argument('--one', type=str, required=True, help=\"test1\")\n\t\t\tself.add_argument('--two', type=str, required=True, help=\"test2\")\n\n\targ"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\ttry:\n\t\treturn''.join(parser.flags['dest'] + list(parser.flags['help']))\n\texcept KeyError:\n\t\treturn ''\n\nif __name__ == '__main__':\n\tmain()"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tresult=\"\"\n\n\tfor opt in parser.option_list:\n\t\tresult=opt.flag\n\t\tif opt.short_flag is not None: result+= '-' + opt.short_flag\n\t\tif opt.long_flag is not None: result+= '--' + opt.long_flag\n\n\treturn result\n\nclass CommandParser(object):\n\n\tdef __init__(self, option_string_list=[]):\n\n\t\t# Defaults.\n\t\tself.option_string_list=option_string_list\n\t\tself.parser=ap.ArgumentParser(usage"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn parser.format_options()\n\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\treturn \" \".join(f.flags for f in parser._actions)\n\ndef parser_argument_string(parser):\n\treturn \" \".join(v for _, v in (parser.parse_args(), vars(parser.parse_args())) if v is not None)\n\ndef parser_error_string(parser, message):\n\treturn \"{} {}\".format(parser.prog, message)\n\ndef iterable_error_string(item):\n\treturn repr(item)\n\ndef iterable_error_string_to_iterable(item):\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = ''\n\tfor param_data in parser._action_groups:\n\t\tflags += param_data.title\n\tfor param_data in parser._group_actions:\n\t\tflags += param_data.title\n\treturn flags\n\n@memoize\ndef parser_version(parser):\n\treturn parser.format_version\n\n@memoize\ndef parser_description(parser):\n\treturn parser.description\n\n@memoize\ndef parser_epilog(parser):\n\treturn parser.epilog\n"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tif not parser._action_groups:\n\t\tparser.add_argument('--debug', default=False, action=\"store_true\", help=\"output debugging messages and informational messages\")\n\t\tparser._parser.add_argument('--verbose', default=False, action=\"store_true\", help=\"output detailed informational messages and progress updates\")\n\t\tparser._parser.add_argument('--no-color', default=False, action=\"store_true\", help=\"don't use colored output\")\n\tparser_flags = []\n\tfor flag in parser._parser._get_kwargs.get(\""}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = \" \".join(argspec.flags)\n\tdefault_flags = \" \" + \" \".join(argspec.default)\n\treturn default_flags if argspec.default in (\"\", [], (1,)) else flags\n\ndef argspec_from_dict(opts, parser=None):\n    \"\"\"\n       Given a dictionary containing the options, populate an argparse.ArgumentParser instance.\n    \"\"\"\n    default_flags = dict_to_default_flags(opts)\n    argspec = help_argspec(parser=parser or argparse.Arg"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tflags = []\n\n\tfor x,y in parser.FLAGS.items():\n\t\t\tif x!= parser.dest:\n\t\t\t\t\tflags.append(\"--%s\" % x)\n\t\t\t\t\ttry:\n\t\t\t\t\t\tflags.append(\" %s\" % y.__getattr__(\"__doc__\"))\n\t\t\t\t\texcept AttributeError:\n\t\t\t\t\t\tpass\n\n\tif hasattr(parser.get_default_values(), \"__getitem__\"):\n\t\t# pylint:disable=unsubscriptable-object\n\t\tfor x in parser.get_default_values():\n\t\t\tflags.append"}, {"generated_text": "def parser_flags(parser):\n\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\n\tstring.\n\t\"\"\"\n\tif not isinstance(parser, ArgumentParser):\n\t\traise TypeError(\"Expected an ArgumentParser instance, got %r\" % parser)\n\treturn \" \".join(parser.flags)\n\n\nclass ParsedInstance(object):\n\t\"\"\"\n\tEncapsulated a single parsed command-line option instance, allowing it to be\n\ttreated like an option while calling it (most of the time).\n\n\tTo access this, create an instance of this object for a given parsed option\n\tinstance.\n\n\tWhen parsing subcommands, this is used to identify them, which makes it easy\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd66", "prompt": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif not processed.has_key(path_name):\n\t\tif verbose:\n\t\t\tprint \"Processing %s\" % path_name\n\t\t\n\t\t# Expand the path to get the absolute name...\n\t\tabspath = os.path.abspath(path_name)\n\t\ttry:\n\t\t\t# Attempt to open the file...\n\t\t\t#print \"Trying: %s\" % abspath\n\t\t\thandler = open(abspath, 'r')\n\t\texcept IOError:\n\t\t\tif verbose:\n\t\t\t\tprint \"IOError: %s\" % path_"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\tif isinstance(path_name, str):\n\t\tpath_name = realpath(path_name)\n\telif not isinstance(path_name, Path):\n\t\tpath_name = Path(path_name)\n\t\n\tif verbose:\n\t\tprint('[cacare.core.utils.was_processed] Checking for processed '\n\t\t\t  'directories: {}'.format(path.relpath(path_name)))\n\t\n\tpossible_paths = [path.dirname(path_name)]\n\twhile possible_paths"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tdirpath = Path(path_name[:path_name.rfind(os.sep) + 1])\n\t\n\t# Is the entry in the set?\n\tif dirpath.compare_recursive(processed, verbose):\n\t\treturn True\n\t\n\t# Is the entry already in the set? Ignore it\n\tif processed.has_key(dirpath):\n\t\treturn False\n\t\n\t# Entry does not exist in processed set, add it\n\tprocessed[dirpath] = path_name\n\t\n\treturn False\n\ndef get_"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose: print 'WARNING: Already processed: %s' % path_name;\n\t\treturn True\n\tprocessed.add(path_name)\n\tif os.path.isdir(path_name):\n\t\tfor filename in os.listdir(path_name):\n\t\t\tif was_processed(processed, os.path.join(path_name, filename), verbose):\n\t\t\t\tif verbose: print 'WARNING: Already processed ("}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tentry = os.path.abspath(path_name)\n\tif entry in processed:\n\t\treturn True\n\ttry:\n\t\tverbosity = None\n\t\tif verbose:\n\t\t\tverbosity = 1\n\t\tprocessed.add(entry)\n\t\tyield verbosity\n\texcept:\n\t\tsys.stderr.write(str_err_invalid)\n\t\tfailed = True\n\t\tsys.stdout.write(str(entry) + \": \" + str_err_process)\n\tfinally:\n\t\ttry:\n\t\t\tprocessed.remove("}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif len(expanded_path) == 1:\n\t\t(expanded_path, path_name) = expanded_path[0], \"\"\n\telse:\n\t\texpanded_path += \"0\"\n\texpanded_path += \"/\"\n\tif verbose:\n\t\tprint str(path_name) + \"   \" + expanded_path\n\t\t\n\t(parent, filename) = os.path.split(path_name)\n\tif parent:\n\t\tparent_dir = process_path(processed, parent, verbose)\n\telse:\n\t\tparent_"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif verbose:\n\t\tlogger.info(\"Checking to see if {path} has been visited with the \"\n\t\t\t\"following parameters: {args}\"\n\t\t\t.format(path=path_name, args=str(locals().items()))\n\t\t)\n\treturn path_name not in processed\n\n\n\t\n#\n# For each file or directory that passes the check\n#\ndef process_file(path_name):\n\t\"\"\"\n\tExpand the path to an absolution path and call the specific file processor.\n\t\n\tArgs:\n\t    path_"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t\n\tfrom pathos.PathResult import PathResult;\n\t\n\twith PathResult(path_name, verbose=verbose) as pathname:\n\t\tif pathname.is_file():\n\t\t\treturn processed.intersection.add(pathname.name)\n\t\t\n\t\telse:\n\t\t\twas_processed(processed, pathname.dir, verbose)\n\t\t\n\t\treturn False\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\t# expand the path name since it's already absolute\n\tpath_name = os.path.expanduser(path_name)\n\t# check the set for the path_name\n\tif path_name not in processed:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\telse:\n\t\t# Already in set, report the error\n\t\tif verbose:\n\t\t\tprint(\"Error: {} already processed\".format(path_name))\n\t\treturn True\n\n\ndef process(in_set, out_set, input_file, verbose):\n\t\"\"\"\n"}, {"generated_text": "def was_processed(processed, path_name, verbose):\n\"\"\"\n\tCheck if a file or directory has already been processed.\n\t\n\tTo prevent recursion, expand the path name to an absolution path\n\tcall this function with a set that will store all the entries and\n\tthe entry to test. If the entry is already in the set, report the issue\n\tand return ``True``. Otherwise, add the entry to the set and return\n\t``False`` to allow the path to be processed.\n\t\n\tArgs:\n\t    processed: Set to store processed pathnames\n\t    path_name: Path to a directory or file\n\t    verbose: True if verbose output is requested\n\t\n\tReturns:\n\t    True if it's already in the set. False if not.\n\t\"\"\"\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint \"Already processed \\\"{0}\\\"\".format(path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\tif verbose:\n\t\t\tprint \"Processed \\\"{0}\\\"\".format(path_name)\n\t\treturn False\n\ndef delete_dir_contents(dir_name, verbose = True):\n\t\"\"\"\n\tDeletes all the files and subdirectories under the given directory.\n\t\n\tArgs:\n\t    dir_name"}]}
{"task_id": "62ece4982e6aefcf4aabbd67", "prompt": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tpts = []\n\tfor i in range(0,len(vertices)):\n\t\tp1 = vertices[i]\n\t\tp2 = vertices[(i + 1)%len(vertices)]\n\t\tp3 = vertices[(i + 2)%len(vertices)]\n\t\tp4 = vertices[(i + 3)%len(vertices)]\n\t\tpts.append((p2,p3,p1))  # order of points matters!\n\treturn pts\n\ndef area(a,b,c,d):\n\t"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tverts = []\n\tfor verts in vertices:\n\t\tverts.append(tuple(verts))\n\treturn verts\n\ndef line2poly(lines):\n\t\"\"\"\n\tline2poly(lines) -> will return the lines as a polygon. The lines must be as long as the longest line between the two vertices::\n\t\n\tpolygon with vertices DAB, ECB, ABD\n\tWill return\n\tDAB, BCB, ACB, CDA -> returns a list of lines\n\t#A, B, C, D  -> of vert"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tpverts = []\n\tfor r in range(len(vertices)-1):\n\t\tpverts = pverts + [(vertices[r], vertices[r+1])]\n\treturn pverts \n\t\t"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tif not vertices:\n\t\treturn \n\tx = vertices.pop(0)\n\ty = vertices.pop(0)\n\tw = vertices.pop(0) - x\n\tz = vertices.pop(0) - y\n\n\tvertices = tuple(vertices)\n\tx0,y0 = vertices[0]\n\tx1,y1 = vertices[1]\n\tx2,y2 = vertices[2]\n\n\tn = abs(y2 - y0)\n\n\tdef get_point_2(v"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\treturn range(len(vertices))\n\n\ndef vertex2tuple(vertices):\n\t\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\n\treturn [c[0] for c in vertices], [c[1] for c in vertices],list("}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\ta = tuple(x for x in vertices if x!= None)\n\tb = tuple(x for x in vertices[:vertices.index(a)+ 1] if x!= None)\n\tc = tuple(x for x in vertices[vertices.index(a)+1:] if x!= None)\n\treturn ((a,b,c), (c,b,a))\t\t\t#tuple of 3 tuple\n\n\t\ndef point2polygon_tuple(point, polygon):\n\t\"\"\"\n\tReturn tuple (x,y) of the vertices of"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\treturn [vertices[0], vertices[1], vertices[2], vertices[3]]\n\ndef get_vertex_points(points):\n    \"\"\" Takes a list of vectors to determine where vertices will have these coordinates to get it's geometry. \"\"\"\n    return Vertex(numpy.array(points))\n\ndef get_vertex_xy(points):\n    \"\"\" Takes a list of vectors to determine where vertices will have these coordinates to get it's geometry. \"\"\"\n    # TODO: This function needs to work on vertex types other than Vertex\n    return"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tv=[]\n\tfor i in xrange(len(vertices)):\n\t\tv.append(Point3D(vertices[i][0],vertices[i][1],vertices[i][2]))\n\treturn tuple(v)\n\t\ndef intersect_segments(segment1, segment2):\n\t\"\"\"\n\treturn a list of intersection points between segment1 and segment2. This method accepts tuples.\n\t\"\"\"\n\tintersections=[]\n\tif (segment1[0]>=segment2[0] and segment1[1"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\tpoints = []\n\tpoints.append(points)\n\tfor i in range(len(vertices)):\n\t\tpoints.append(points)\n\tfor i in range(len(vertices)):\n\t\t#print(vertices[i],vertices[(i+1) % len(vertices)],(vertices[(i+2) % len(vertices)-1]))\n\t\tx1,y1=vertices[i]\n\t\tx2,y2=vertices[(i+1) % len(vertices)]\n\t\tx3,"}, {"generated_text": "def vertex3tuple(vertices):\n\"\"\"\n\treturn 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\t\n\tpolygon with vertices ABCD\n\tWill return\n\tDAB, ABC, BCD, CDA -> returns 3tuples\n\t#A    B    C    D  -> of vertices\n\t\"\"\"\n\t\n\tresult = []\n\n\tfor polyline in vertices:\n\n\t\tresult.append(polyline[0][:2])\n\t\tresult.append(polyline[-1][:2])\n\n\t\tresult.append(polyline[1][:2])\n\t\tresult.append(polyline[1][:2])\n\n\t\tresult.append(polyline[2][:2])\n\t\tresult.append(polyline[-1][:2])\n\n\t\tresult.append(polyline[-2][:2])\n\t\tresult.append(polyline[1][:2"}]}
{"task_id": "62ece4982e6aefcf4aabbd68", "prompt": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif padding is None: padding = 3\n\n\t# if number is negative, we want the least significant digit\n\tif number < 0:\n\t\tif padding == 3:\n\t\t\treturn \"-\" + int_to_string(number, alphabet, 3)\n\t\telif padding is None:\n\t\t\treturn \"-\" + abs(number)\n\n\t# if number is zero, we want to print empty string\n\tif number == 0: return \"\"\n\n\tout = \"\"\n\tnegative = False\n\tif number < 0:\n\t\tnegative_flag = \"-\"\n\t\tnegative = True\n\t\tnumber"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif padding is None:\n\t\tpadding = 0\n\t\n\treturn ''.join(map(alphabet.__getitem__, \n\t\t[str(number - padding)\n\t\t\tfor number in\n\t\t\t[i * alphabet[i % len(alphabet) + i // len(alphabet)] \n\t\t\t\tfor i in range(len(alphabet))]))\n\t\t)\n\t\n\n## The following is taken from the documentation: \n# https://www.tensorflow.org/blog/2016/one_liners\ndef add_padding(number: int, width"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\tif padding is None:\n\t\tpadding = (len(alphabet) - 1) // 2\n\t\n\tpadded_number = number + padding\n\tstart_padding = padding - 1  # index in alphabet of first integer\n\t\n\tif padded_number == 1:\n\t\treturn alphabet[start_padding]\n\t\n\tremainder = padded_number % 2\n\t\n\tif remainder:\n\t\treturn int_to_string(padded_number - remainder, alphabet, padding)\n\t\n\treturn int_to_string(padded_number - 1"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif padding is None:\n\t\tif number == 0:\n\t\t\treturn alphabet[0]\n\t\telse:\n\t\t\treturn int_to_string(number * (len(alphabet) - 1), alphabet, padding + 1)\n\treturn int_to_string(number, alphabet, padding)\n\ndef string_to_hex(string: str) -> str:\n\tout = []\n\tfor i, c in enumerate(string):\n\t\tif i % 2 == 1:\n\t\t\tout.append(\"0x%02x\" % int(c, 16))\n\t\telse"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tpadding_ = \"\" if padding is None else ((\"0\"*padding) if padding>16 else \"\")\n\t\n\toutput=[]\n\tif number == 0:\n\t\treturn \"0\"\n\t\n\twhile number!= 0:\n\t\tremainder = number % len(alphabet)\n\t\toutput.insert(0, alphabet[remainder])\n\t\tnumber = number // len(alphabet)\n\t\n\treturn padding_ + \"\".join(reversed(output))\n\nprint(int_to_string(1234567812, [0,1,"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\tif padding is None:\n\t\tpadding = 1\n\ts = []\n\ti = 0\n\twhile i < padding:\n\t\ts.append(0)\n\t\ti += 1\n\twhile number:\n\t\ts.append(ord(alphabet[number%26])) # Python 3.4-3.3 \n\t\tnumber //= 26\n\ts.reverse()\n\treturn str.join('',''.join(str(c) for c in s),'')\n\ndef string_to_num(s: str, alphabet: List[str]) ->"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tresult = ''\n\tsign = False\n\t\n\twhile number!= 0:\n\t\tremainder = number % 10\n\t\tnumber /= 10\n\t\t\n\t\tif remainder == 1 and padding:\n\t\t\tremainder = 0\n\t\t\n\t\tif padding:\n\t\t\tresult += alphabet[padding - remainder]\n\t\telse:\n\t\t\tresult += alphabet[remainder]\n\t\t\t\n\t\tif padding is not None:\n\t\t\tpadding -= 1\n\t\telse:\n\t\t\tsign = not sign\n\t\t\n\tif not sign:\n\t\treturn result[::-1] +''\n\t"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tresult = []\n\t\n\twhile True:\n\t\tif number % 3 == 0:\n\t\t\tif number == 0:\n\t\t\t\tresult += alphabet[0]\n\t\t\telse:\n\t\t\t\tresult += alphabet[number % 3]\n\t\t\tnumber = int((number / 3) * 3)\n\t\t\tpadding = padding or 2\n\t\t\n\t\telif number % 9 == 0:\n\t\t\tresult += alphabet[number % 9]\n\t\t\tnumber = (number / 9) * 9\n\t\t\n\t\telse:\n\t\t\tresult.append(alphabet[number % 10])\n\t\t\tnumber = number / 10\n"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\t\n\treturn list(reversed(string_from_int(number, alphabet, padding)))[::-1]\n\t\n\t\ndef string_from_digit(digit: str, alphabet: List[str]) -> str:\n\t\"\"\"\n\tConvert a digit to a string, without use of alphabet, padding.\n\t\n\tExamples:\n\t>>> string_from_digit('c', alphabet = [])[0]\n\t'c'\n\t>>> string_from_digit('2', alphabet = [])[0]\n\t'2'\n\t>>> string_"}, {"generated_text": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\"\"\"\n\tConvert a number to a string, using the given alphabet.\n\t\n\tThe output has the most significant digit first.\n\t\"\"\"\n\tif padding is not None:\n\t\tresult = []\n\t\tfor digit in str(number):\n\t\t\tresult.append(alphabet[digit])\n\t\t\tif len(result) == padding:\n\t\t\t\tbreak\n\t\treturn \"\".join(result)\n\n\telse:\n\t\tresult = str(number)\n\t\tresult = [alphabet[i] for i in result]\n\t\treturn \"\".join(result[::-1]) # Reverse the string\n\ndef read_data(filename: str, alphabet: List[str], use_input: bool) -> str:\n\t\"\"\"\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd69", "prompt": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tif register_value in flow_params:\n\t\treturn flow_params[register_value]\n\telse:\n\t\tprint('\\nInvalid value: %s' % register_value)\n\t\t\n\treturn None\n\t\ndef _replace_register_flow_params(flow_params, register_number, register_values_dict):\n\t\"\"\"\n\tReplace values from given dict to specified register number\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_values_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t# get value to be replaced\n\tvalue = flow_params.pop(register_value, None)\n\t# if value not found, then set to 0\n\tvalue = value if value else 0\n\t# write register value \n\tself._write_register(register_number, value)\n\n\n\t\nif __name__ == \"__main__\":\n    \"\"\"\n    test main:\n    \"\"\"\n    \n    flow_params = {\n        # this is a constant for flow 5 (output to pin 16)\n        'flow5_src': 5,\n        'time"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\t\n\tfor flow_name, register_values in flow_params.iteritems():\n\t\tregister_values[register_name] = register_value\n\t\ndef _get_flow_values(flow_name, flow_params):\n\t\"\"\"\n\tRetrieve flow parameters\n\t\"\"\"\n\t\n\tflow_values = []\n\t\n\tfor key in flow_params.keys():\n\t\tvalue = flow_params[key]\n\t\tflow_value = dict()\n\t\t\n\t\tif key == 'values':\n\t\t\tfor flow_value_key in value:\n\t\t\t\tflow_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tval = flow_params[register_value]\n\tfor flow in val.keys():\n\t\tif 'target' in val[flow]:\n\t\t\tval['target'] = val['target'][register_number]\n\tflow_params[register_value] = val\n\t\t\ndef _create_flow_params(params):\n\t\"\"\"\n\tReturns string representing the value of dictionary passed in in 'params'\n\t\n\t:param params: Dict with 'target' and 'data' key(s), value of which will be replaced\n\t\"\"\"\n\tflow_str = \"\"\n"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor (key, value) in flow_params.iteritems():\n\t\tif key == \"REPLACE_TO_\" + register_value:\n\t\t\tflow_params.update({key: register_number})\n\treturn flow_params\n\n\n\ndef get_device_flows_params(device_parameters):\n\t\"\"\"\n\tReturn a dictionary containing flows to perform to given device \n\t\n\tdevice_parameters, will contain information as input parameters \n\tavailable : [\n\t\t\t* uid\n\t\t\t* ip_address\n\t\t\t* ip_mask\n\t\t\t* mac_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor key in flow_params:\n\t\tif key == register_value:\n\t\t\tflow_params[key] = register_number\n\t\n\treturn flow_params\n\ndef _set_flows_params(packet_name, flow_params):\n\t\"\"\"\n\tSet default parameters to flows defined in YAML-file\n\t\n\t:param packet_name: Packet name, e.g. ttl\n\t:param flow_params: Dictionary with default parameters\n\t\"\"\"\n\ttry:\n\t\tdefault_packet_parameters = json.loads(\n\t\t\topen"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfor flow in flow_params['filters']:\n\t\tif stream_name in flow:\n\t\t\tfor name, parameters in flow.items():\n\t\t\t\tif'register' in parameters and parameters['register'] == register_number:\n\t\t\t\t\tflow[name][register_value] = register_value\n"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\timport copy\n\tfor flow, flow_params in flow_params.items():\n\t\t# Replace value from all the flows\n\t\tfor flow_path, flow_params in flow_params.items():\n\t\t\t# For flows related the same object, replace value\n\t\t\tif flow_path:\n\t\t\t\tif isinstance(flow_params, dict):\n\t\t\t\t\t_replace_register(flow_params, register_number, register_value)\n\n\t\t\t\t# If it is a list or tuple\n\t\t\t\tif isinstance(flow_params, (list, tuple)):\n\t\t\t\t\tfor flow_param in flow_"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tif register_value in flow_params:\n\t\tflow_params[register_value] = register_number\n\telse:\n\t\tprint \"Register value \\\"{}\\\" wasn't defined in params\" \\\n\t\t\t.format(register_value)\n\n'''\n\tFunction used to replace register name keys and values\n\t\n\t1. Inflows flows to target device - flows dictionary is used as parameter\n\t2. Iterate through all flows and for each flow\n\t3. If flow have'registers' key - replace register key(s) keys by values\n\t3. Else if"}, {"generated_text": "def _replace_register(flow_params, register_number, register_value):\n\"\"\"\n\tReplace value from flows to given register number\n\t\n\t'register_value' key in dictionary will be replaced by register number\n\tgiven by 'register_number'\n\t\n\t:param flow_params: Dictionary containing defined flows\n\t:param register_number: The number of register where value will be stored\n\t:param register_value: Key to be replaced by register number\n\t\"\"\"\n\tfound = False\n\tfor i in list(flow_params.keys()):\n\t\tfound_dict = False\n\t\tif flow_params[i].get('flow_type') == 'REG_RD':\n\t\t\tif flow_params[i].get('flow_params')[0] == register_number:\n\t\t\t\tfound_dict = True\n\t\t\t\tfor sub_elem in flow_params[i]['flow_params']:\n\t\t\t\t\tflow_params[i]['flow_params'][sub_elem] = register_value\n\t\tif found_dict is True:\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd6a", "prompt": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\treturn value.replace(\".\", arg)\ndef replace_dashes(value, arg):\n\t\"\"\"\n\t\tReplaces all values of '-' to arg from the given string\n\t\"\"\"\n\treturn value.replace(\"-\", arg)\ndef replace_undefined_dashes(value, arg):\n\t\"\"\"\n\t\tReplaces all values of '-' to arg from the given string, even for undefined variables\n\t\"\"\"\n\treturn str(value).replace(\"-\", arg)\n\ndef replace_none_or_empty(value):\n\t\"\"\"\n\t\tReturns the value of the given"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\targ_re = re.compile(arg)\n\treturn arg_re.sub('', value)\n\n\ndef replace_whitespaces(value, arg):\n\t\"\"\"\n\tReplaces all strings in the given string from the given arg with the given value\n\t\"\"\"\n\treturn re.sub(' ', arg, value)"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif value is None and arg == None:\n\t\treturn None\n\tif arg is None:\n\t\treturn value\n\tif len(value)>=2 and value[0] == '.' and value[-1] == '.':\n\t\treturn value[1:-1]\n\treturn  value\n\t\ndef replace_slashes(value, arg):\n\t\"\"\"\n\tReplaces values of '\\' to arg from the given string\n\t\"\"\"\n\tif value ==\"\":\n\t\treturn value\n\tif arg == None:\n\t\treturn value\n\tif len(value)"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tvalue = value.replace(\" \", \"%20\")\n\tvalue = value.replace(\".\", arg)\n\treturn value\n\n\t\ndef load_config_file(name=\"config.ini\"):\n\t\"\"\"\n\tLoads a config file on the form:\n\t    [section=value]\n\t    \n\tFor convenience, if config.ini is in the working directory it is loaded\n\t\n\tAny configuration that is found in 'os.environ' is parsed in the same order and \n\tnot overwrited\n\t\"\"\"\n\tif os.path.dirname(os"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tfor i, val in enumerate(value):\n\t\tif val == \".\":\n\t\t\tvalue = value[:i] + arg + value[i+1:]\n\t\t\tbreak\n\treturn value\n\ndef find_dims(value):\n\t\"\"\"\n\tReturns list of dimension names for value\n\t(e.g. ['.1.1', '1.1', '.'])\n\t\"\"\"\n\tval = value\n\tdims = []\n\twhile 1:\n\t\tdims.insert(0,val)\n\t\tif '.' not in val:\n\t\t\tbreak"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tstring = value.replace(\".\", str(arg))\n\n\treturn string\n\n\n# Function to replace the names of the values in the arguments list\n# in the message\ndef replace(string, arg, args):\n\t\"\"\"\n\tReplaces all occurances of '%s' to 'arg' in the given string to the value\n\tcorresponding to 'arg' in the arg list\n\t\"\"\"\n\tfor i in range(len(args)):\n\t\tstring = string.replace(\"%s\", args[i])\n\n\treturn string\n\n\n# Function to check and see if the"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\t# This function is used in order to have consistent behaviour for\n\t# replace_dots\n\tvalue = str(value).strip()\n\treturn value.replace(\".\", arg)\n\n\ndef main():\n\t\"\"\"\n\tThe main function that does all the necessary logic for the script\n\t\"\"\"\n\ttry:\n\t\tparser = OptionParser(usage=\"usage: [options] <filename.json>\")\n\t\tparser.add_option(\"-p\", \"--pretty\", metavar=\"PRETTY-COLORS\", action=\"store_true\", dest=\"use_pretty_colors"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tvalue = value.replace('.', arg)\n\tvalue = value.replace('..', arg)\n\tresult = \"\"\n\tfor w in value.split('\\\\'):\n\t\tif w == \"\":\n\t\t\tbreak\n\t\tif w[0] == '\\\\' and w[1] == '.':\n\t\t\tresult = result + arg\n\t\telse:\n\t\t\tresult = result + w\n\treturn result\n\n\"\"\"\n\tRemoves all leading spaces and adds an underscore,\n\tto prevent further processing\n\"\"\"\ndef remove_comments(line):\n\tresult = ''\n"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tprint(re.sub('\\\\.([^.]+\\\\.)', lambda m: arg, value))\n\treturn\n#\t\tif the first character is '.', then replace the whole string as it\n#\t\tcould be an attribute, so we will treat it as a complex attribute.\n\narg = \"s_id\"\nreplace_attribute(arg)\nprint(\"Attribute s_id\")\n\n# The first character of the value will be the attribute name. Then, the value from the\n# attributes file will be the value of the attribute.\nargs_list = ['s_id',"}, {"generated_text": "def replace_dots(value, arg):\n\"\"\"\n\tReplaces all values of '.' to arg from the given string\n\t\"\"\"\n\tif value.find(\".__\") >= 0:\n\t\treturn arg\n\telse:\n\t\treturn value\n\ndots_arg_map = {\n\t'pwd': '..', 'cwd': '.', 'user': os.getlogin(), 'user.name': os.getlogin(), 'user.home': os.environ['HOME', '~']\n}\n\n@app.route('/data', methods=['GET'])\ndef get_data():\n\tvalues = request.args['values']\n\tvalues = [values] if not isinstance("}]}
{"task_id": "62ece4982e6aefcf4aabbd6b", "prompt": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tcurrent_cls = cls\n\tsubclasses = {}\n\tfor base_cls in current_cls.__bases__:\n\t\tnew_cls = base_cls.__new__(base_cls, *base_cls.__dict__.values())\n\t\tif new_cls.__class__ is not class:\n\t\t\tsubclasses.update(new_cls.subclasses())\n\treturn subclasses"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tfor sub in cls.__subclasses__():\n\t\tyield sub\n\t\tfor sub in sub.satellites()\n\nsubclasses = staticmethod(subclasses)"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tresult = []\n\tif cls in _subclasses:\n\t\treturn cls, result\n\tfor subclass in cls.__subclasses__():\n\t\tresult.append(subclass)\n\t\tresult.extend(subclasses(subclass))\n\tresult.append(cls)\n\treturn cls, result\n\n@register.simple_tag\ndef all_subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, non-recursive form\n\n    >>> all_subclasses(Duck())\n    ['Donald', 'DuckCall']"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tfor c in cls.__subclasses__():\n\t\tyield c\n\t\tfor i in subclasses(c):\n\t\t\tyield i\n\n\nclass PluginBase(object):\n\tdef __init__(self, plugin_name):\n\t\t\"\"\"\n\t\t@type  plugin_name: string\n\t\t@param plugin_name: The name of the plugin or any prefix of the plugin.\n\t\t\"\"\"\n\n\t\tself.name = plugin_name\n\n\t\t\"\"\"\n\t\tThis is set to true on the first pass (at init time) when\n\t\tPluginManager.scan_plugins is called."}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tall_classes = []\n\tfor a in [a for a in list_subclasses(cls.__bases__[0]) if '_class_info' in a.__dict__]:\n        if a.__dict__['_class_info']._class_level!='subclasses': all_classes.append(a)\n        elif a not in all_classes: all_classes.append(a)\n\treturn all_classes\n\n\ndef classes_of_same_superclass(theclass):\n\t\"\"\"\n\tReturn all the classes that inherit the"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tclasses = set()\n\tfor sc in cls.__subclasses__():\n\t\tclasses.add(sc)\n\t\tclasses.update(subclasses(sc))\n\treturn classes\n\nclass TestMeta_is_a(unittest.TestCase):\n\t\"\"\"\n\tDefined meta-class is-a tests\n\t\"\"\"\n\tdef test_meta_class(self):\n\t\t\"\"\"\n\t\tTest meta-class is-a\n\t\"\"\"\n\t\tclasses = get_subclasses(SubclassOfSubclassOfSubclassOfSubclassOfClass)\n\t\tself.assert"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tfor name, obj in inspect.getmembers(cls):\n\t\tif inspect.isclass(obj):\n\t\t\tyield obj\n\t\telif inspect.isfunction(obj) or inspect.ismethod(obj):\n\t\t\tfor subs in inspect.getmro(obj):\n\t\t\t\tif subs.__name__ == cls.__name__:\n\t\t\t\t\tcontinue\n\t\t\t\tyield subs\n\n@with_metaclass(namedtuple('_ClassInfo', 'name'))\nclass _ClassInfo(object):\n\t\"\"\"Annotated class information\"\"\"\n\tdef attr"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tall_subclasses = []\n\tfor subclass in cls.__subclasses__():\n\t\tall_subclasses.append(subclass)\n\t\tall_subclasses.extend(subclasses(subclass))\n\treturn all_subclasses\n\nclass SubModel(type):\n\t\"\"\"\n\tThis metaclass automatically creates all subclasses in a \n\tgiven model and prevents class-level initialization code \n\tfrom executing.\n\t\"\"\"\n\tdef __init__(cls,name,bases,attrs):\n\t\tsuper().__init__(name,b"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\treturn [c for c in cls.__subclasses__()] + [c for c in cls.__subclasses__()]\n\nclass SubClassChecker():\n\t@classmethod\n\tdef check_inheritance(cls, child, parent):\n\t\tassert child.Meta.name == parent.Meta.name, \"Parent and Child do not share the same meta\"\n\tdef test_default_subclasses(self):\n\t\tinherit = self._get_inherit_subclasses(BaseInherit)\n\t\tsuper_sub = self._get"}, {"generated_text": "def subclasses(cls):\n\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tresults = []\n\n\tfor s in cls.__subclasses__():\n\t    results.append(s)\n\t    results.extend(subclasses(s))\n\n\treturn results\n\ndef main():\n\t\"\"\"\n\tEntry point for demo\n\t\"\"\"\n\ta = Employee()\n\ta.salary = 3000\n\ta.name = 'Gustavo R. S\u00e1nchez'\n\ta.work_place = \"Brazlano\"\n\t# a.get_bonus()  # Error - not possible\n\ta.give_bon"}]}
{"task_id": "62ece4982e6aefcf4aabbd6d", "prompt": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n        num = sum([digit * size for digit,size in zip(string, alphabet)])\n\t# Strip off the extra digit, and add in the value\n\treturn int(num) + int(string[-1])\n\ndef string_to_int_and_print(string: str, alphabet: List[str]) -> bool:\n\t\"\"\"\n\t\tConvert a string to a number, using the given alphabet.\n\t\t\n\t\tThe input is assumed to have the most significant digit first.\n\t\tThis code will then print out the number by concatenating the entire string"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\tdigits = dict()\n\tfor c in string:\n\t\tdigits[c] = digits.get(c,0) + 1\n\t\n\tsum = 0\n\tfor i in digits:\n\t\tif int(i) in digits:\n\t\t\tsum += digits[int(i)]\n\t\telse:\n\t\t\tsum += 1\n\treturn sum/len(alphabet)\n\t\ndef int_to_string(n: int, alphabet: List[str]):\n\t\"\"\"\n\tConvert a number to a string using the given alphabet.\n\tIt has the"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tif string == '':\n\t\treturn 0\n\t\t\n\tdigits = []\n\tif string[-1] not in alphabet:\n\t\tdigits.append(ord(string[-1]) - alphabet.index((string[-1])))\n\tdigits.append(len(string)-1 - number_of_right_to_left_digits(string[:-1]+string[-1], alphabet))\n\t\n\treturn int(''.join(str(digit) for digit in digits), 10)\n\t\n\t\ndef reverse_string(string: str) ->"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tvalue = 0\n\tif string!= '':\n\t\tfor position, letter in enumerate(string):\n\t\t\tletter = letter.lower()\n\t\t\tif letter in alphabet:\n\t\t\t\tvalue *= 63\n\t\t\t\tvalue += alphabet.index(letter)\n\t\t\n\t\t\telif letter == '.':\n\t\t\t\tvalue += 1\n\t\t\t\n\t\t\telif letter == ',':\n\t\t\t\tvalue += 62\n\t\t\t\n\t\t\telif letter!= ':':\n\t\t\t\traise ValueError(string + \"  Contains invalid character:\", letter, \n\t\t\t\t\" (only digits, : decimal point, and.)"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn sum([value*ind for value, ind in zip(map(int, string), alphabet)])\n\ndef string_to_float(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a floating point number, using the given alphabet.\n\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\treturn int(string_to_int(string, alphabet) / 10.0)\n\ndef int_to_string(number: int, alphabet_size: int, num_digits: int):\n"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\toutput = 0\n\t\n\tfor digit, character in enumerate(reversed(string)):\n\t\tnumber = alphabet.index(character)\n\t\toutput += number * (10**(1-digit))\n\t\n\treturn output\n\nclass NumericSortHelper:\n\tdef __init__(self, input_strings: List[str], alphabet: List[str]):\n\t\tself.input_strings = input_strings\n\t\tself.alphabet = alphabet\n\t\t\n\t\tself.input_array = sorted(self.input_strings, key=self.string"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\tdigit = 0\n\texp    = 0\n\tfor c in string:\n\t\texp = exp << 8  # multiply by 256\n\t\texp += get_char_freq(c, alphabet)\n\t\tdigit = (digit << 8) + int(c)\n\n\t# check to make sure that we're not going from less to less significant digits\n\twhile exp < (0 << 8):\n\t\tdigit >>= 8\n\n\t\tif digit == 0:\n\t\t\tbreak\n\t\texp += 1\n\t\n\treturn digit\n\ndef get_char_freq(c: str, alphabet:"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\talphabet = sorted(alphabet) + ['#']\n\tinteger = 0\n\tnegative = False\n\tfor char in string:\n\t\tif char not in alphabet:\n\t\t\talphabet = ['#'] + alphabet\n\t\t\tbreak\n\t\ti = alphabet.index(char)\n\t\tinteger += i * (1 - 2 * (i < len(alphabet) - 1))\n\treturn integer if negative else integer * 10\n\t\n\t\ndef input_reader(input_string: str) -> Generator[int, None, None]:\n\t\"\"\"\n\t\tAn iterator to read"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\tsum = 0\n\tfor i in range(0, len(string)):\n\t\ttry:\n\t\t\tsum = sum + int(i + string[i]) * alphabet[i]\n\t\texcept:\n\t\t\tprint(\"string[\", i, \"] is not recognized as a number.\")\n\tprint(\"The sum of the first \", len(string), \" integers is\", sum)\n\t\ndef string_to_binary(string: str, number: int) -> str:\n\t\"\"\"\n\tConvert a string from a number to a binary string, using the\n\tgiven alphabet"}, {"generated_text": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\"\"\"\n\tConvert a string to a number, using the given alphabet.\n\t\n\tThe input is assumed to have the most significant digit first.\n\t\"\"\"\n\t\n\tfor ch in string:\n\t\tif ch not in alphabet:\n\t\t\t# All alphabets should at least have '2' and '3'.\n\t\t\traise ValueError(\"Invalid character '%s' in %s\" % (ch, string))\n\t\t\t\n\treturn int(string)\n\t\n\t\ndef print_tree_node(node: Any, depth: int) -> None:\n\t\"\"\"\n\tPrint node and all subtrees.\n\t\"\"\"\n\t\n\tprint (\"%s%s\" % (\n\t\t\" \" * depth,\n\t\tnode,\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd6f", "prompt": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tdestination_path = None\n\ts = requests.Session()\n\t\n\turls = [(\"https://repo.archlinux.org/api/v1/pkgs/\", \"get_pacakges\" if url.endswith('tar.gz') else \"get_packages\"),\n\t    (\"https://repo.archlinux.org/api/v1/search/\", \"fetch_packages\")]\n\t\n\tfor url, func_name in urls:\n\t\t# Prepare call the API\n\t\tprepare_rpc(url, s)"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t\n\t# download archive\n\tprint(\"Downloading %s\" % (destination))\n\twith tarfile.open(destination) as tf:\n\t\t\t # this is the filename inside the.tar.gz archive (tar-ed package)\n\t\t\t # (excluding the.tar.gz itself)\n\t\t\t tar_extracted_filename_inside_archive = os.path.basename(\n\t\t\t\t\t\t\t\t os.path.splitext(destination)[0]) \n\t\t\t\n\t\t\t # the extracted archive should contain the same exact file as the\n\t\t\t # \"url\" which we're using."}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\turl = \"{}/{}\".format(url, 'desc')\n\n\ttry:\n\t\tresp = requests.get(url)\n\t\twith open(destination_path, 'w') as f_out:\n\t\t\tf_out.write(resp.content)\n\texcept:\n\t\tpass\n\t\n\treturn (destination_path)\n        \n"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\turl = \"http://mirrors.x86-64.archives.canonical.com/archive.ubuntu/dists/trusty/main/\" + url\n\tif url.split('/')[-1] == 'ubuntu-14.04.4-desktop-amd64.iso':\n\t\turl = \"http://mirrors.x86-64.archives.canonical.com/archive.ubuntu/dists/trusty/pool/\" + url\n\n\tdestination_filename = tarball\n\tdestination = Path(destination_path)\n\tlogging.info(\""}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tif not os.path.exists(destination_path):\n\t\tos.mkdir(destination_path)\n\t\n\t# download and extract archives\n\t# url: http://packages6.archlinux.org/latest/repodata/12-release.xml.gz\n\t# url = \"http://packages6.archlinux.org/latest/repodata/12-release.xml.gz\"\n\tlogger.debug('Extracting %s' % url)\n\textracted = False\n\twhile not extracted:\n\t\tfile_path = destination"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\t# Note: can't print download progress when download url is a file\n\tresponse = requests.get(url, stream=True)\n\t# check HTTP response\n\tif response.status_code!= requests.codes.ok:\n\t\traise Exception(response.text)\n\n\t# Retrieve destination path\n\tdirpath = os.path.join(destination_path, response.url.split('/')[-2])\n\n\t# TODO: create zip when downloading archive to disk\n\t# download the.zip archive\n\tif os.path.isdir(destination_path"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\ts = requests.Session()\n\treq = requests.get(url)\n\n\tname, ext = splitext(abspath(req.url))\n\tdest = name\n\tdest = f'{dest}/{dest.lower().replace(\"_\",\"/\")}{ext}'\n\tdest = dest.rstrip('.')\n\n\tos.mkdir(dest)\n\twith open(dest+'/desc', 'w') as f:\n\t\tf.write(req.text)\n\n\tos.chdir(dest)\n\tshutil.unpack_archive"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tarchive_url = url\n\n\ttar_path = Path(destination_path)\n\ttar_path.mkdir()\n\n\t# Download archive\n\t# print('Downloading', tar_path.name.rstrip(ext))\n\t# print(archive_url)\n\tr = requests.get(archive_url, stream=True)\n\textracted_path = Path(tar_path.name + \".extracted\")\n\twith open(extracted_path, 'wb') as dest:\n\t\tshutil.copyfileobj(r.raw, dest)\n\t\t"}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tpath = pathlib.Path(destination_path)\n\tlogging.info(\"download '{}' to '{}'\".format(url, path))\n\tresponse = requests.get(url)\n\tresponse.raise_for_status()\n\tpath.mkdir(parents=False, exist_ok=True)\n\tout_name = \"{}.tar.gz\".format(path.name)\n\twith open(out_name, 'wb') as file:\n\t\tfile.write(response.content)\n\ttar_name = get_filename(response."}, {"generated_text": "import requests\n\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive\n\twhich contains 'desc' file for each package.\n\tEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\t\n\tArgs:\n\t    url: url of the .tar.gz archive to download\n\t    destination_path: the path on disk where to extract archive\n\t\n\tReturns:\n\t    a directory Path where the archive has been extracted to.\n\t\"\"\"\n\tdownload_filename = ''\n\n\turl_parts = list(urlparse(url))\n\tpath, filename = url_parts[2], url_parts[4]\n\t\n\twhile not path.endswith('/'):\n\t\tpath = path.rstrip('/')\n\t\tdownload_filename = '{0}/{1}'.format(path, filename)\n\n\textracted_path = Path(destination_path) / download_filename\n\tr = requests.get(url, stream=True, verify=False)\n\n\tif not r.ok:\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd70", "prompt": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\t# Use the platform for macOS, otherwise fallback to platform for windows!\n\ttry:\n\t\treturn platform.system() == 'Darwin'\n\texcept:\n\t\treturn platform.system() in ['Windows', 'Microsoft']\n\ndef _read_file_as_text(filename, encoding):\n\tfh = open(filename, 'rb')\n\ttext = fh.read().decode(encoding)\n\tfh.close()\n\treturn text\n\n"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tarch = platform.system()\n\tif 'Darwin' in arch:\n\t\treturn True\n\treturn False\n"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tif not platform.system() == \"Darwin\":\n\t\traise Exception(\"System not iOS or Mac\")\n\telse:\n\t\treturn True\n\nif __name__ == '__main__':\n\tsys.exit(Main().main())\n"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\t\n\treturn platform.system() == \"Darwin\"\n"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tif os.name!= \"nt\":\n\t\treturn True\n\telse:\n\t\treturn False\n\t\t\ndef is_mac():\n\t\n\t# Checks if the os is macOS\n\tif not is_windows():\n\t\treturn os.name == \"darwin\"\n\telse:\n\t\treturn False\n\t\t\ndef is_linux():\n\t\n\t# Checks if the os is linux\n\treturn os.name == 'posix' or os.name == 'posixgeneric'\n\t\ndef is_windows():\n\t\n\t# Checks if the os is windows\n\treturn"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\tif platform.uname()[0] == \"Darwin\":\n\t\treturn True\n\treturn False\n\ndef get_local_ipv4_address():\n\t\"\"\"\n\tChecks if the current interface has a default IPv4 address\n\n\t:return: Return an IPV4 address\n\t:rtype: str\n\t\"\"\"\n\ttry:\n\t\tip = socket.gethostbyname(socket.gethostname())\n\t\tif \":\" in ip:\n\t\t\treturn ip[:ip.find(\":\")]\n\texcept:\n\t\tpass\n\treturn \"\"\n\ndef"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn os.path.exists('/System/Applications/AppleScript.app/Versions/Current/Main')\n\n\"\"\"\n\tCheck if the file exists\n\t:param path: path of the file to check\n\t:type path: str\n\t\n\t:return: True if the file exists\n\t:rtype: bool\n\t\"\"\"\n\treturn os.path.exists(path)\n\n\"\"\"\n\tThis checks if the path exists as link\n\t:param path: path of the file to check\n\t:param symlink: check if path is a symlink"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn __os.platform() == 'darwin'\n\t\n\t\ndef darwin_version():\n\t\"\"\"\n\tReturns the version of darwin \n\t\n\t:return: A tuple with the OS version\n\t:rtype: tuple\n\t\"\"\"\n\n\tif __os_macos:\n\t\tcmd = '/usr/bin/sw_vers'\n\t\ttry:\n\t\t\tp = Popen(cmd,shell=True,stderr=sys.stderr,stdout=sys.stdout)\n\t\t\tstdout,stderr = p.commun"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn platform.system() == 'Darwin'\n\ndef _macos_distribution(): \n\t\"\"\"\n\tReturns the version of macOS the script is running on\n\t\n\tExamples of macOS distros: 10.12, 10.13, 10.14\n\t\"\"\"\n\t# Get the release version of the running\n\t# OS. Get this using platform.platform()\n\trelease = platform.mac_ver()[0]\n\n\tif release == '': \n\t\treturn None\n\n\tp = None\n\ttry:\n\t\tp = tuple([int(i) for i"}, {"generated_text": "import os\n\"\"\"\n\tChecks if the os is macOS\n\t\n\t:return: True is macOS\n\t:rtype: bool\n\t\"\"\"\n\treturn platform_name() == \"osx\";\n\nif __name__ == '__main__':\n\t\n\tif is_macos():\n\t\tprint \"Mac OS is detected\"\n\telse:\n\t\tprint \"Mac OS is NOT detected\"\n\n\"\"\"\n\tRun tests if in verbose mode\t\n\t\n\t:return: None\n\t:rtype: None\n\t\"\"\"\n\tif os.environ.get(\"DEBUG\", None) == 'on':\n\t\trunTests();\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd71", "prompt": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tnew_regex_dict = {}\n\tfor key, value in item.copy().iteritems():\n\t\tnew_regex_dict[re.compile(re_from_string(key))] = value\n\treturn new_regex_dict\n\ndef is_regex(key):\n\t\"\"\"\n\tReturn whether a key is a regex or not\n\tReturns:\n\t    A boolean\n\t\"\"\"\n\ttry:\n\t\tkey = re.compile(key)\n\texcept re.error, e:\n\t\tif e.message!= \"Could not compile regular"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_keys_to_update = new_regex_dict(item)\n\t\n\tfor key in item:\n\t\tkey_regex = key\n\t\tfor key_part in key_regex.split('.'):\n\t\t\tkey_part_regex = None\n\t\t\tif not key_part.startswith('^'):\n\t\t\t\tkey_part_regex = \"^\" + key_part\n\t\t\tif not key_part.endswith('$'):\n\t\t\t\tkey_part_regex += \"$\"\n\t\t\tkey_part_regex = key"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tret = {}\n\tfor key, val in item['all'][].items():\n\t\tret[re.compile('^' + key + '$')] = val\n\treturn ret\n\ndef main():\n\t\"\"\"\n\t\tCommandline program\n\t\tGet options\n\t\tGet filename\n\t\tGet list of *.cpp.* if wildcard else just filename\n\t\tConvert to regex\n\t\tValidate against regex list\n\t\tGet list of matches\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-f', '--file', type=str"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tdef clean_line(l):\n\t    l = re.sub('/', '\\/', l)\n\t    l = l.replace(\"'\", \"''\")\n\t    l = l.replace(\";\", \";\")\n\t    l = l.strip()\n\t    l = l.lower()\n\t    l = l.split(',')\n\t    for i in range(0, len(l)):\n\t        if len(l[i]) == 0:\n\t            l.remove(l[i])\n\t    return l\n\t\n\t"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tfor key, value in item.items():\n\t\tnew_value = parse_regex_key(key)\n\t\tif new_value is not None:\n\t\t\titem[new_value] = value\n\treturn item\n\ndef regex_dict(items):\n\t\"\"\"\n\tTakes a dict with keys, where there is a regex entry, and converts any keys\n\tmatching the regex to keys that have the regex replaced if they exist otherwise\n\tto their original name.\n\t\n\tArgs:\n\t    items: dict to modify\n\t\t\n\tReturns:\n\t    dict"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\t\n\toutput = {}\n\t\n\tfor in_key, in_item in item.items():\n\t\toutput[re.escape(in_key)] = {}\n\t\tfor k in in_item:\n\t\t\tif not re.match('^\\s*/\\s*|^\\ */\\s*|^\\s*$', k):\n\t\t\t\toutput[re.escape(in_key)][re.escape(k)] = in_item[k]\n\t\t\telse:\n\t\t\t\tregex_key = re.escape(in_key) \n\t\t\t\tregex_value"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_items = dict()\n\tfor item in item_list:\n\t\tmatch = re.match(item['regex'], item['key'])\n\t\tif match:\n\t\t\tregex_items[item['key']] = match\n\t\t\t\n\treturn regex_items\n\t\ndef main(argv):\n\tif len(argv) == 1:\n\t\tshow_help();\n\t\treturn\n\t\n\tif argv[1] == \"list\":\n\t\tif len(argv)!= 2:\n\t\t\tlogging.error(\"usage: list regex_"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_dict = {re.compile(re.sub('\\\\W','_',i)): j for i, j in item.items()}\n\t\n\treturn regex_dict\n\n\"\"\"\n\tGiven a list of items to be matched a pattern and an index, return all keys matching the pattern\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tregex_keys = {}\n\tfor key, item in rules.items():\n\t\tregex_keys[key] = []\n\t\tfor regex in item:\n\t\t\tkey_re = \"(.*)(\" + regex + \")(.*)\"\n\t\t\tmatch_re = re.match(key_re, key)\n\t\t\tif match_re:\n\t\t\t\tre_string = match_re.group(1)\n\t\t\t\tre_string += match_re.group(2)\n\t\t\t\tre_string += match_re.group(3)\n\t\t\t\tre_string = regex_string(re"}, {"generated_text": "import re\n\"\"\"\n\tConvert *.cpp keys to regex keys\n\t\n\tGiven a dict where the keys are all filenames with wildcards, convert only\n\tthe keys into equivalent regexes and leave the values intact.\n\t\n\tExample:\n\t\n\trules = {\n\t    '*.cpp':\n\t        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n\t    '*.h':\n\t        {'h': 'help'}\n\t}\n\tregex_keys = regex_dict(rules)\n\t\n\tArgs:\n\t    item: dict to convert\n\tReturns:\n\t    dict with keys converted to regexes\n\t\"\"\"\n\tnew_item = {}\n\tfor key in item:\n\t\tnew_item[re.compile(key)] = item[key]\n\treturn new_item\n\ndef filter_empty_items(item):\n\t\"\"\"\n\tFilters items which contain a key with only whitespace and returns this item. \n\n\tNote:\n\t       This must be used after regex_dict(), which doesn't have a filter method. \n\n\tArgs:\n\t    item: dict to filter\n\tReturns:\n\t    dict with empty keys if empty\n\t\"\"\"\n\treturn {key:"}]}
{"task_id": "62ece4982e6aefcf4aabbd72", "prompt": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tname = \"\"\n\tfor character in name:\n\t\tif character in \"\\\"'\":\n\t\t\tname = name.replace(character, \"\")\n\treturn name \n\ndef removeQuotes(name):\n\t\"\"\"\n\tRemove first and last characters from the given name.\n\t\"\"\"\n\t# Remove first character\n\tname = name[1:]\n\t# Remove last character\n\tname = name[:-1]\n\treturn name\n\n# Get the given name without the quotes\nif __name__ == \"__main__\":\n\tname = getName()\n\tprint(strip"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\treturn re.sub(r'\"|\\'', '', name) \ndef read_data(file):\n\t\"\"\" \n\t\tRead the data from the given file.\n\n\t\t:type file: str\n\t\t:param file: file name with the data\n\t\t:rtype: dict\n\t\t:returns: a dictionary with the file read\n\t\"\"\"\n\twith open(file, 'r') as f:\n\t\t\n\t\tcontent = f.readlines() \n\t\tres = {}\n\t\t\n\t\tfor i in range(0, len(content)):\n\t\t\tline = content"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tdef strip_quote(self, name):\n\t\treturn _remove_from_list(name,\n\t\t\tre.compile('\"')\n\t\t)\n\n\t\"\"\"\n\t\tCheck whether the given name is an upper-case name.\n\t\t\"\"\"\n\tdef is_upper(self, name):\n\t\t#return re.compile(\"^[A-Z]\\w*\")\n\t\ts = _remove_from_list(name, re.compile(\"^[a-z]\\w*\"))\n\t\treturn (s[0].islower())\n\nif __name__"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tname = re.sub(\"'(.*?)'.*?(#&|$)\", '\\\\1', name)\nreturn str(name)\n\n\ndef load():\n\t\"\"\"\n\tLoad the configuration from the file given by the\n\tcommand line argument. If no argument was given, try\n\tto load the default configuration file. If both fail,\n\tuse the default configuration. \n\t\"\"\"\n\ttry:\n\t\tconfig = read(configfile)\n\t\tconfig = load_config(config)\n\n\texcept IOError:\n\t\tsys.stderr.write('Cannot"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tname = name.strip(\">\")\n\tname = name.strip('<')\n\n\t# Replace quotation.\n\tif name[0] == '\"':\n\t\tname = name[1:len(name)]\n\t\tname = name.strip(\"\\\"\")\n\tif name[0] == \"'\":\n\t\tname = name[1:len(name)]\n\t\tname = name.strip(\"'\")\n\tif name[-1] == '\"':\n\t\tname = name[:-1]\n\t\tname = name.strip(\"\\\"\")\n\tif name[-1]"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tp = re.compile(r\"\"\"\\s|'[^']*[']\"\"\")\n\treturn p.sub('', name).strip()\n\t\t\ndef get_key(f):\n\tm = re.match(\"[^[]+\\[(.+)\\]\", f)\n\tif m:\n\t\treturn m.groups()[0]\n\treturn f\n\ndef is_in_list(list, item):\n\treturn len([x for x in list if x == item])!= 0\n\ndef get_list(string, regex=re.comp"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tpattern = re.compile(\"\\\"([^\\\"]*)\\\"\")\n\tresult = pattern.sub(r\"\\1\", s)\n\t\n\tif not result.endswith(\"\\\"\") or result.startswith(\"\\\"\"):\n\t\treturn None\n\n\treturn result\n\n\"\"\"\n\tRemove number from the given name.\n\t\"\"\"\n\tdef removeNumber(s):\n\t\tn = 3\n\t\tif re.search(\"-\", s):\n\t\t\tn = 2\n\t\tm = re.search(\"\\d+\", s)\n\t\treturn re.sub(\"^"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tif len(name) > 2 and name[0] == \"a\" and name[2] == \"a\" :\n\t\tname = name[1:]\n\treturn( name )\n\n\ndef to_upper_case(text):\n\t\"\"\"\n\t\tconverts the given text to upper case.\n\t\t\"\"\"\n\treturn string.upper(text)\n\n\n\t\n\t\nif __name__ == \"__main__\":\n\t\"\"\"\n\t\ttest\n\t\t\"\"\"\n\tif (not os.path.exists(out_dir)): \n\t\tos.makedirs("}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tname = name.strip('\"\\'')\n\tname = name.lower()\n\tres = name.split(',')\n\n\tif len(name)>1:\n\t\tres.clear()\n\t\tres.add( name )\n\telse:\n\t\tres.add( name )\n\t\n\treturn res\n\ndef is_phone(text):\n\t# Check if the text is a phone number.\n\tpattern = re.compile('^\\+?[\\d\\D-]*[\\d\\D-]*$', re.IGNORECASE"}, {"generated_text": "import re\n\"\"\"\n\tRemove quote from the given name.\n\t\"\"\"\n\tfor i in range(len(name)-1, -1, -1):\n\t\tif name[i] == \"\\\"\":\n\t\t\tname = name[:i]\n\t\t\tbreak\n\n\treturn name\n\n\"\"\"\n\tSplit given text in a list according to first character\n\t\"\"\"\ndef splitter(text):\n\treturn [chg[0] for chg in split(re.compile(r'.(\\s\\([^\\)]+\\)\\s*|\\s\\([^\\)]+\\)\\s*|(.*))'),text)]\n\n\"\"\"\n\t"}]}
{"task_id": "62ece4982e6aefcf4aabbd73", "prompt": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\tdef shl_split(self,string_in=''):\n\n\t\t# If no string to split, or list is empty, return just one tuple with [''].\n\t\tif(string_in=='' or string_in==None):\n\t\t\treturn ['']\n\t\telif(type(string_in) is not str):\n\t\t\ttry:\n\t\t\t\tif sys.platform == \"linux2\":\n\t\t\t\t\tsplit_func = globals().get(\"shell_posix_split\")\n\t\t\t\telif sys.platform == \"darwin\":\n\t\t\t\t\tsplit_func = globals"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t__slots__ = ('platform',)\n\t\n\t# RE pattern can take any non-empty string, even more complex regexes may be better\n\t# Note, regexes have the following limitations:\n\t#\t1. Does not handle escaped escapes\n\t#\t2. Does not match space character\n\t#\t3. Does not handle unicode and other possible character encodings\n\t#\t4. Does not support unicode group matches\n\t#\n\t#  This is used by shlex.split('abc\\'def', posix = 1, space_split = 0)"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tv = shell\n\ttry:\n\t\tv = _popen_shlex_split\n\texcept:\n\t\tpass\n\t\n\treturn v(value, 1)\n\n#####################################\n#   PIPELINE\n#####################################\nfrom pipeline import *\n"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t__version__ = \"0.3.1\"\n\t__version_info__ = tuple([1,0,0]) \n\t__author__ = \"Ziggy Estela Viana\"\n\t\n\t# -----------------------------------------------------------------------------\n\n\tDEFAULT_SYNTAX = 'Windows'\n\n\t# -----------------------------------------------------------------------------\n\t\n\tdef __new__(cls,*args,**kwargs):\n\t\treturn super(shlex, cls).__new__(cls, *args, **kwargs)\n\n\t# -----------------------------------------------------------------------------\n\n\t@classmethod\n\tdef _split("}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\tdef __init__(self,platform='this'):\n\t\tsuper(split,self).__init__((s for s in _shell_split(platform)))\n\nclass findPipeArg(CommandLine,unittest.TestCase):\n\tif platform() == 0: # Windows\n\t\tpass\n\telse: # POSIX\n\t\tdef setUp(self):\n\t\t\tself._s=shlex\n\t\t\tshlex=self._s_posix\n"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t__slots__ = ['_platform']\n\t\n\tdef __init__(self, s='', input=None, platform=-1, strict=True):\n\t\tfrom posix import split as _posix\n\t\tfrom cmdwin32 import split as _cmdwin32\n\t\tfrom cscmdwin32 import split as _cscmdwin32\n\t\tfrom win32 import split as _win32\n\t\tfrom _win32 import _cscmdwin32_osx as _win32_osx\n\t\t\n\t\tself._platform = platform or _win32(input or _win"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\t\n\tdef __init__(self):\n\t\t\n\t\tPlatform.__init__(self)\n\t\tself.split = shlex.split\n\t\tself.find_terminator()\n\t\n\tdef find_terminator(self):\n\t\t\"\"\"Find a platform-specific character used to end the command.\n\t\tFor example, for a Windows command, a linefeed is used, for a POSIX\n\t\tcommand some form of newline. For shell, it is semicolon, as in\n\t\tPOSIX shell.\n\t\t\n\t\tReturns a value (a POSIX termint"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\t\n\tdef __init__(self, instring, **kw):\n\t\tself.instring = instring\n\t\tself.kw=kw\n\t\t\n\t\t#self.instring = instring\n\t\tinp=instring\n\t\t\n\t\t#if self.platform() == 0: # Windows / CMD\n\t\t\t#inp=instring.replace(\"\\\"\",\"\").replace(\"\\n\",\"\") # Windows / CMD\n\t\t\t#inp=instring.encode(\"ascii\",\"replace\")\n\t\t\t#inp = inp.decode('ascii', '"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\tplatform = _get_platform()\n\tif platform == 0:\n\t\treturn []\n\telif platform == 1:\n\t\treturn shlex.split(sys.argv[0])\n\t## Windows only stuff\n\telif platform == 2:\t# This is CMD on both.\n\t\tcmd = []\n\t\tif sys.stdin.isatty():\n\t\t\tcmd_split = sys_cmd(_get_stdin(), _syscmd_stdout(), _syscmd_stderr())\n\t\t\tcmd = cmd_split[0]\n\t\tcmd_split = sys"}, {"generated_text": "import re\n\"\"\"\n\tMulti-platform variant of shlex.split() for command-line splitting.\n\tFor use with subprocess, for argv injection etc. Using fast REGEX.\n\t\n\tplatform: 'this' = auto from current platform;\n\t          1 = POSIX;\n\t          0 = Windows/CMD\n\t          (other values reserved)\n\t\"\"\"\n\timport os, sys, string\n\t\n\tif 0 == platform:\n\t\t# detect current platform.\n\t\tif 0 == len(sys.argv):\n\t\t\t__syscmd(sys.argv)\n\t\tif isinstance(sys.argv, list):\t\t# platform-detect from substrings in sys.argv\n\t\t\tplatform = __syscmd(sys.argv)[0]\n\t\telif isinstance(sys.argv, basestring):\n\t\t\tif string.lower(sys.argv).find(\"cmd\") >= 0:\n\t\t\t\tplatform = 0\n"}]}
{"task_id": "62ece4982e6aefcf4aabbd74", "prompt": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\tprocname = \"unpack\"\n\targs  = [ \"-f\", archive_path ]\n\t\n\tdir_path, name_or_relpath, _ = urlparse(origin_url)\n\t# TODO get the correct repo url when the repo is specified.\n\t# The issue arises in an alternative setup. \n\t# If we use a repo url, it should be in the form of https://<site>.tidepool.io/repo/someuuid/some\n\t\n\treturn(run_ex(procname, args, dir_path"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\texts = [ 'tar', 'gz', 'bz2', 'xz', '7z', '7zip', 'lzma', 'xz' ]\n\t\t#'zip' ]\n\texts.reverse()\n\tfor ext in exts:\n\t\ttry:\n\t\t\tprocess = subprocess.Popen(['file','-i', archive_path+'.'+ext], stdout=subprocess.PIPE)\n\t\t\tstdout, stderr = process.communicate()\n\t\t\tif \"Linux\" in stdout.lower():\n\t\t\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\treturn subprocess.check_output(['/usr/bin/curl', '--retry', '10', '--show-error', '-o', archive_path, '-L', '--data-urlencode', 'DEST_URI'])\n\ndef git_get_archive_url(origin_url, archive_path):\n\t\"\"\"\n\treturns a repo url which can be used to fetch the archive\n\t\"\"\"\n\treturn '+'.join([origin_url, archive_path])\n\ndef find_file_in_folder(file_name"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\treturn _archiver.unarchive_archive( archive_path, output_path )\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t(status, output) = self.check_repo_exists(self.archive_path)\n\tif status == 0:\n\t\tself.repourl = \"file://\" + self.archive_path\n\t\treturn self.repourl\t\n\n\tp = subprocess.Popen([\"unzip\", '-q', '-d', self.archive_path], stdin=subprocess.PIPE, stdout=subprocess.PIPE);\n\t(status, output) = p.communicate()\n\n\tif status == 0:\n\t\tself."}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tif not archive_path:\n\t    raise Exception(\"There is no archive file to pull from.\")\n\twith tempfile.NamedTemporaryFile() as temp_archive:\n\t    temp_archive.write(archive_path)\n\t    temp_archive.flush()\n\n\t    # This makes the command line always print a new line char\n\t     \n\tsubprocess.call([\"gunzip\", \"-c\",temp_archive.name])\n\tshutil.move(temp_archive.name + \".tar.gz\", archive_path)\n\treturn\n\ndef unpack"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tsubprocess.call([ \"tar\", \"-x\", archive_path ])\n\tfile(output_repo_path, \"w+\")\n\tsubprocess.call([ \"w+\", output_repo_path ])\n\toutput_repo_path.writelines(archive_path_tokens) \n\ndef main():\n\toptions, remainder = parse_args()\n\t\n\tif options.checkout_path:\n\t\tprint checkouts(options.checkout_path, options.repository, options.tag)\n\telse:\n\t\tprint make_"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\t\n\t# TODO: deal with all the possible archives\n\t# TODO: add testsuite for invalid archives\n\n\t# For testsuite\n\tprint(\"TODO: call with archive\")\n\t\n\tarchive_path = os.path.realpath(archive_path)\n\n\tif archive_path is None:\n\t\t\t \traise\n\tf = os.path.basename(archive_path)\t\t# Extracts just the name.\n\tif not is_tar_or_zip(f):\n\t\t# TODO: deal with other archives too.\n\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tfor archive_extension in ARCHIVES_EXTENSIONS:\n\t\tarchive_name, _, _ = archive_path.partition('_')\n\t\tarchive_path = os.path.join(archive_path, archive_name + archive_extension)\n\n\tarchive_extension = archive_path.split('.')[-1]\n\tif archive_extension in ARCHIVES_EXTENSIONS:\n\n\t\tcmd = subprocess.Popen('/bin/tar xvf'+ archive_path, shell=True, stdout=subprocess.PIPE"}, {"generated_text": "import subprocess\n\"\"\"\n\tGiven an existing archive_path, uncompress it.\n\tReturns a file repo url which can be used as origin url.\n\t\n\tThis does not deal with the case where the archive passed along does not exist.\n\t\"\"\"\n\tr = git(['show'])\n\thash, name = r.split()[:2]\n\t\n\t# read in the archive\n\tcwd = os.path.join(os.path.abspath(os.path.curdir), name)\n\tif not os.path.exists(cwd):\n\t\tos.makedirs(cwd)\n\t\tprint \"Making directory:\" + cwd\n\t\t\n\t\tos.system('tar xf archive.tar > tmp.txt ')\n\t\t\n\t# untar\n\tprint hash+\""}]}
{"task_id": "62ece4982e6aefcf4aabbd75", "prompt": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tp = subprocess.Popen('git ls-files --exclude-standard-forms.gitignore  | sort -rn', shell=True, stdout=subprocess.PIPE)\n\n\n\treturnValue = p.stdout.read().decode('utf-8')\n\treturnValue = returnString.replace(\"\\n\", \",\")\n\treturn returnValue\n\ndef set_gitiles():\n\t\"\"\"\n\tFind the gitiles.json and use the subprocess call to fetch the JSON data and return it as a string for parsing.\n\t\"\"\"\n\tsub"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t#call git with args -C -C to get all files recursively, then split and sort the list by the last component (i.e. the file names)\n\tfiles = subprocess.check_output(\"git log --raw --reverse --pretty=format:'%h' --name-only -C -C\", shell=True)\n\t#print files\n\t#files=files.split(',')\n\tfiles = sorted(set(files.split())) #set is built-in in python. This removes the redundant duplicates from the list.\n\t#files=[f"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tfiles = subprocess.check_output([\"git\", \"ls-files\"])\n\tfiles = files.split()\n\tfiles = [f for f in files if f.endswith(\".pyc\") or f.endswith(\".pyo\")]\n\tfiles = [f for f in files if not f.startswith(\".\")]\n\treturn files\n\ndef check(command:str) -> bool:\n\ttry:\n\t\targs = command.split()\n\t\toutput=subprocess.check_output(args,stderr=subprocess.STDOUT"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tgit_command = \"git --no-pager ls-files *.#*|head -100|sort -u|uniq|xargs git diff-index --ignore-submodules=all --name-only --diff-filter=ACM= -- %s\".replace('\\n','\\\\n').replace('\\%', '%')\n\tp = subprocess.Popen(git_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd='/home/sean')\n\n\tp.wait()"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tdef getIgnoredFiles(theDir) :\n\t\tcmd = [ \"git\", 'ls-files', \"-z\",theDir, \"--others\"]\n\t\tfileDict = {}\n\t\t#print(cmd)\n\t\tls_files = check_output(cmd, universal_newlines=True).splitlines()\n\t\t#print(ls_files)\n\t\tfor filename in ls_files:\n\t\t\tfileDict[filename] = filename\n\t\tprint(fileDict)\n\t\treturn fileDict\n\nif __name__ == \"__main__\":\n\ttry:"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tretval = None\n\tsubprocess.run([\"git\", \"ls-files\", \"--no-color\", \"--exclude-standard\", '\"' + str(ignored_file_name) + '\"'],stdout=subprocess.PIPE)\n\tfile_names = str(subprocess.check_output([\"git\", \"ls-files\", \"--no-color\", \"--exclude-standard\"], stdout=subprocess.PIPE), encoding='ascii', errors='replace')\n\tfile_names = str.split(file_names"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\treturn_string = \"\"\n\tignored_files = subprocess.check_output([\"git\",\"ls-files\", \"--others\", \"--exclude-standard\", \"--exclude-other\", \"--exclude-lock\", \"--exclude=CVS\", \"--exclude=\\\".gitignore\\\"\", \"../\"])\n\tignored_files = ignored_files.decode(\"utf-8\")\n\tignored_files = ignored_files.split(\"\\n\")\n\tignored_files.sort()\n\tfor file in ignored_files:\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t#if not args:\n\t#\targs = None\n\tret = subprocess.check_output([\"./setup_data/\" + data_dir + \"/git_ignore\", \"./\" + data_dir], universal_newlines=True).split(\", \")\n\treturn ret\n\ndef get_files_in(location = \"./\"):\n\treturn [x for x in os.listdir(location) if os.path.isdir(os.path.join(location, x)) and not x.startswith(\".\")]\n\t\ndef main():\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\toutput = subprocess.check_output(['git', 'diff'])\n\tfilelist = output.splitlines()\n\tfilelist = filter(lambda line: line.startswith('?'), filelist)\n\tsortlist = sorted(filelist)\n\treturn ', '.join(filelist)\n\ndef get_only_ignored_file_names():\n\t\tignore_list = get_ignore_file_list()\n\t\tonly_ignored_list = reduce(lambda x, y: x+y, ignore_list.split(), [])\n\t\treturn only_"}, {"generated_text": "import subprocess\n\"\"\"\n\tUse the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcommand = 'git ls-files *.gitignore | sort -u | grep -i \"^\\.|^/proc/\" ; while read filename ; do echo \"$filename\" | sort -u -k1,1 > gitignores.tmp ; done'\n\tp1 = subprocess.Popen(command,shell=True,stdout=subprocess.PIPE)\n\tstdout = p1.communicate()[0]\n\tstdout = stdout.split('\\n')\n\tlist = []\n\tfor file in stdout:\n\t\tif  os.path"}]}
{"task_id": "62ece4982e6aefcf4aabbd76", "prompt": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\textension = os.path.splitext(filename)[1].lower()\n    \n\t# Match on all files\n\tif extension.lower() in extension_list:\n\t    return True\n\t\n\t# Match on files for a particular language\n\tfile_to_check = os.listdir(lang_path + \"\\\\\")\n\tfor f in file_to_check:\n\t    # The filename needs to end with our extension\n\t    extension = os.path.splitext(f)[1].lower()\n\t    if extension == extension_list["}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\n\ttry:\n\t\text = os.path.splitext(filename)[1]\n\texcept NameError:\n\t\t# filename has no splitext method\n\t\t# or splitext is not defined\n\t\treturn False\n\telse:\n\t\t# splitext() successful; filename has a splitext method\n\t\treturn ext in _SUPPORTED_FILE_TYPES\n\nclass FileSize(object):\n\t\"\"\"\n\tSize of a file in bytes\n\t\"\"\"\n\t_UNITS = (\n\t\t(1, \"B\"),\n\t\t(1024, \""}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\t# If extension, return true\n\tif os.path.splitext(filename)[-1].lower() in ext_set:\n\t\treturn True\n\t\n\t# Otherwise return false\n\treturn False\n\ndef check_input(input_data, name):\n\t\"\"\"\n\tchecks the file size of the file\n\t\n\tArgs:\n\t    file_path: input file path\n\tReturns:\n\t    True if the file does not exist\n\t\"\"\"\n\t\n\tif not os.path.exists(input_data):\n\t\tmessage_file_"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\t\n\tmatch_list = []\n\t\n\tif re.match(r\".*\\.rlc$\", filename, re.IGNORECASE) is not None:\n\t\tmatch_list.append(\".rlc\")\n\telse:\n\t\tmatch_list.append(0)\n\t\n\tif re.match(r\".*\\.xml$\", filename, re.IGNORECASE) is not None:\n\t\tmatch_list.append(\".xml\")\n\telse:\n\t\tmatch_list.append(0)\n\t\n\tif re.match(r\"."}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\treturn os.path.splitext(filename)[1][1:] == type\n\n\nclass DATASOURCE:\n\t\"\"\"\n\t\tDefinition of a dataset source type\n\t\t\n\t\tArgs:\n\t\t    ttype:\tDataset source Type\n\t\"\"\"\n\tdef __init__ (self, ttype, source):\n\t\tself.ttype = ttype\n\t\tself.source = source\n\n\"\"\"\n\tDataset class definitions\n\t\"\"\"\n\tdef __init__ (self, directoryName, type, fileN):\n\t\tself.directoryName = directoryName\n\t\t"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tfor typ in supported_types:\n\t\tif fnmatch.fnmatch(filename,typ):\n\t\t\treturn True\n\treturn False\n\n\ndef open_file(filename, mode='wb'):\n\t\"\"\"\n\tOpen a file in binary mode\n\t\n\tArgs:\n\t    filename: Filename to open\n\t    mode: Open mode, 'rb' for reading or 'wb' for writing\n\tReturns: \n\t    File object\n\t\"\"\"\n\tif not is_supported(filename):\n\t\traise Exception(\"File %s is not supported by this module.\" % filename)"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tcheck = False\n\tif os.path.isdir(filename):\n\t\tcheck = False\n\tif os.path.isfile(filename):\n\t\tcheck = True\n\t\tfor filter in self._filters:\n\t\t\tif filter in filename.lower():\n\t\t\t\tcheck = True\n\t\t\t\tbreak\n\n\t\tif not check or (self._force and not self._force_list.count(filename)):\n\t\t\tlogger.debug(3, \"not checked: %s\", filter)\n\t\telse:\n\t\t\tlogger.debug(3, \"%s has an unsupported filetype\", filename)"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tfor ext in [\"c\", \"h\", \"cxx\", \"cpp\", \"c#\", \"h#\", \"hh\", \"hpp\", \"hxx\", \"inc\", \"moc\", \"inl\", \"gfx\", \"py\", \"php\", \"pyx\", \"sh\", \"S\"]:\n\t\tif filename.endswith(\".\"+ext):\n\t\t\treturn True\n\treturn False\n"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif ('txt' in filename) or ('nfo' in filename) or (\"csv\" in filename):\n\t\treturn True\n\treturn False\n\t\n\n# Check if the file is not in a zip and is present and load\ndef OpenFile(filename):\n\tlogging.debug (u\"Loading \" + filename)\n\tfilen = open(filename, 'r')\n\tlines = filen.readlines()\n\tfilen.close()\n\treturn parse_string(lines)\n\t\n\n# Create parser from loaded file\ndef parse_string(lines):\n\t"}, {"generated_text": "import os\n\"\"\"\n\tCheck if the filename is a type that this module supports\n\t\n\tArgs:\n\t    filename: Filename to match\n\tReturns:\n\t    False if not a match, True if supported\n\t\"\"\"\n\tif filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".gif\") or filename.endswith(\".bmp\") or filename.endswith(\".png\") or filename.endswith(\".tif\") or filename.endswith(\".cr2\") or filename.endswith(\".crw\") or filename.endswith(\".orf\") or filename.endswith(\".arw\") or filename.endswith(\".3fr\") or filename.endswith(\".dcr\"):"}]}
{"task_id": "62ece4982e6aefcf4aabbd77", "prompt": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\t# convert the frequency into a timedelta instance with seconds\n\tseconds = 0\n\tif not frequency:\n\t\tseconds = -1\n\telse:\n\t\ttmp = frequency.split(\" \")\n\t\t\n\t\tdt = None\n\t\ttry:\n\t\t\t# try without seconds\n\t\t\tdt = parse(tmp[0])\n\t\texcept:\n\t\t\t# try with seconds\n\t\t\tdt = parse(tmp[0] + \"s\")\n\t\t\t\n\t\tseconds = dt.seconds\n\t\tif len(tmp)==2:\n\t\t\ttmp[1] = tmp[1].lower()"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\tdef _convert_unit(unit):\n\t\t# Return a function that converts a unit string (e.g. \"hours\", \"days\", \"in\")\n\t\t# to a function with same return signature\n\t\tif unit in (\"hours\", \"days\"):\n\t\t\treturn lambda v: datetime.timedelta(hours=v)\n\t\telif unit in (\"in\",):\n\t\t\treturn lambda v: datetime.timedelta(days=v*5.3)\n\t\telse:\n\t\t\treturn lambda v: None\n\t\n\t\n\tdef _convert_"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\treturn_value = None\n\n\tif _is_time_unit(_get_unit(frequency)):\n\t\tif isinstance(dt, str):\n\t\t\tdt = datetime.datetime.strptime(dt, \"h\")\n\t\t\t\n\t\tdelta = dt - datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n\t\t\n\t\treturn_value = timedelta(**dict([(k,int(v)) for k,v in delta.__dict__.items() if str(v)!="}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\tfrequencies = [ \"once\", \"twice\", \"triple\", \"every\",\"week\", \"month\", \"quarter\",\n\t\t\t\t\t\"year\", \"day\", \"hour\", \"minute\", \"day_of_year\", \"hour\", \"minute\" ]\n\t\t\n\t# We have a unit, but we don't know the frequency: it's only a guess.\n\t# Let's see if we can find a frequency to which the unit applies.\n\ttry:\n\t\tfrequ = frequencies.index(unit)\n\texcept ValueError:\n\t\treturn None\t\t# No match"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif not frequency:\n\t\treturn None\n\tif not isinstance(frequency, basestring):\n\t\treturn None\n\t\n\tif not frequency.startswith('#'): \n\t\traise ValueError('Frequency must start with \"#\"')\n\tfreq = frequency[1:]\n\t\n\tdef normalize(x): return int(x) if x[-1].isdigit() else x\n\tfreqs = filter(lambda p: p.isdigit(), freq.split('-'))\n\tunit, count = normalize(freq[:-1"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\t# Get string:\n\ttry:\n\t\tfrequency = parse_frequency(frequencies_str)\n\texcept ValueError:\n\t\treturn None\n\n\tif frequency == None or frequency == 'always':\n\t\treturn None\n\t\n\treturn timedelta_from_frequencies[frequency]\n\ndef datetime_from_frequencies(frequencies_str, start_time=datetime.min):\n\t\"\"\"\n\tGiven a string specifying the date at which event should begin and and\n\tthe unit of each event, determines the offset from start_time to\n"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tdef parse(self,frequency):\n\t\tfrequency = float(frequency[:-1])\n\t\tunit = None\n\t\tif frequency!= 0:\n\t\t\tunit = frequency_units[frequency]\n\t\t\tif unit == \"always\" or unit == \"days\":\n\t\t\t\tunit = None\n\t\treturn self.getDTD(unit) \n\ndtd_to_frequency = {\t\"quarter\": QuarterFrequency,\n\t\t\t\t\t\t\"months\": MonthFrequency,\n\t\t\t\t\t\t\"weeks\": WeekFrequency,\n\t\t\t\t\t\t\"hours\": HourFrequency,\n\t\t\t\t\t\t\"day(s)\": DayFrequency"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tm = FACTOR_REGEX.match(frequency)\n\tif m:\n\t\tif m.group('factor') in ('day', 'week'):\n\t\t\t# Convert days and weeks to timedeltas\n\t\t\treturn datetime.timedelta(days=int(m.group('factor-number')),\n\t\t\t                          seconds=FACTOR_SECONDS[m.group('factor')])\n\t\telse:\n\t\t\treturn None\n\t\n\traise ValueError('Invalid frequency string')\n\ndef addDays(date, value):\n\t\"\"\"\n\tAdd a days worth"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\t\n\t# Get unit name\n\tfor unit in units:\n\t\tif unit in unit_names:\n\t\t\tname = unit\n\t\telse:\n\t\t\t# Assume it's a float, not a unit\n\t\t\tname = None\n\t\t\n\t\tif name:\n\t\t\tbreak\n\t\t\n\t# No unit found\n\tif not name:\n\t\traise ValueError(\"Could not determine unit, got %r\" % unit_names)\n\t\t\n\t# Parse the float/unit pair\n\tr = re.match(\"^([0-9]+)([a-zA-Z]"}, {"generated_text": "import datetime\n\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n\tdatetime.timedelta instance or None if the frequency is None or \"always\".\n\t\n\tFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\t\n\tRaise ValueError if the given frequency cannot be parsed.\n\t\"\"\"\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\tfreq = re.split(\"[*]\", frequency)\n\tif len(freq) == 1:\n\t\tnum, unit = None, freq\n\telif len(freq) == 2:\n\t\tnum, unit = re.split(\"[s]*\", freq[1])\n\t\tunit = unit.upper()\n\t\tif unit not in unit_registry:\n\t\t\traise ValueError(\"Unsupported timing unit: '%s'\" % unit)\n\telse:\n\t\traise ValueError"}]}
{"task_id": "62ece4992e6aefcf4aabbd78", "prompt": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif host == '':\n\t\treturn True\n\ttry:\n\t\tsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\tsock.settimeout(0.00005); # 0.005\n\t\tsock.connect((host, 22))\n\t\tsock.close()\n\t\treturn True\n\texcept Exception:\n\t\treturn False\n\ndef start_shell():\n\t\"\"\"Start a local shell\"\"\"\n\ttry:\n\t\tsp = subprocess.Popen(['/bin/bash'])\n\texcept Exception:\n\t\t"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\ttry:\n\t\ts=socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n\t\tresult = s.sendto(host, (\"localhost\", 4647))\n\texcept:\n\t\tprint(\"Not the local hub\")\n\telse:\n\t\tprint(\"Is the local hub\")\n\t\ts.close()\n\t\n\t"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\thost = host.split(':')\n\tif (len(host) >= 2) and host[1].lower() == \"0.0.0.0\":\n\t\treturn (True)\n\telse:\n\t\treturn (False)\n\t\t\ndef main(host, ip, port, num_threads):\n\tglobal verbose_debug\n\t\n\t#print(host, host)\n\tif (len(host) < 3):\n\t\tprint(usage)\n\t\tprint(sys.exit())\n\telse:\n\t\thostname = get_hostname(host)"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\ttry :\n\t\tprint 'Trying to connect to localhost:%s'%(host)\n\t\t(is_localhost, is_localhost_name, host_ip, host_name) = socket.gethostbyaddr(host)\n\t\tif is_localhost == 127 and is_localhost_name!= '' and host_name!= '' and host_ip!= '':\n\t\t\tprint 'Looks like you are locally accessing the WebSocket on localhost. '\n\t\t\treturn True\n\t\telse:\n\t\t\tprint 'Your host is not localhost'\n\t\t\treturn False\n\texcept:\n\t\t"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif 'localhost' in host:\n\t\treturn True\n"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\ttry:\n\t\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\tconnected = False\n\t\ttry:\n\t\t\ts.connect((\"\", PORT))\n\t\t\tconnected = True\n\t\texcept socket.error as e_:\n\t\t\tconnected = False\n\t\tfinally:\n\t\t\ts.close()\n\t\treturn connected\n\texcept socket.error as e_:\n\t\treturn False\n\t\t\nclass Command(object):\n\t\"\"\"\n\tThis class represents a command executed and its value\n\t\"\"\"\n\t\n\tdef __init__("}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\ttry:\n\t\t#create the socket\n\t\ts = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n\t\t#connect to the remote host\n\t\ts.connect((host, int(p)))\n\n\t\t#close the socket\n\t\ts.close()\n\t\t\n\t\treturn True\n\n\texcept:\n\t\treturn False\n"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\t\n\t# Make sure we can get our hostname\n\tif not os.access('/etc/hostname', os.R_OK):\n\t\treturn False\n\t\n\ttry:\n\t\t# Read the hostname\n\t\thostname = open('/etc/hostname').read()\n\t\t\n\t\t# Compare with the hostname read from configuration file\n\t\tif not hostname == host:\n\t\t\treturn False\n\t\t\n\t\treturn True\n\t\n\t# We get here when the hostname does not match\n\texcept (OSError, IOError):\n\t\t\n\t\t# We return"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tif ((host.find(\"localhost\") >= 0) or (host.find(\"127.0.0.1\") >= 0) or \n\t\t(host.find(\"0.0.0.0\") >= 0) or (host.find(\"127.0.0.1\") >= 0)):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\"\"\"\n\tReturns the port of the ip if the IP is not the localhost\n\t\n\t:param ip: The ip\n\t:return: The port if the IP is not the localhost\n\t\t:return: The"}, {"generated_text": "import socket\n\"\"\"\n\tChecks if the host is the localhost\n\t\n\t:param host: The hostname or ip\n\t:return: True if the host is the localhost\n\t\"\"\"\n\tlocalhost_str = host\n\n\tif localhost_str == \"127.0.0.1\":\n\t\tlocalhost = True\n\telse:\n\t\tlocalhost = False\n\n\treturn localhost\n\nclass TCPClient():\n\tdef __init__(self, addr=\"0.0.0.0\", port=3000):\n\t\tself.addr = addr\n\t\tself.port = port\n\n\tdef connect(self):\n\t\tsocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\ttry:\n\t\t\tsocket.connect((self.addr,"}]}
{"task_id": "62ece4992e6aefcf4aabbd79", "prompt": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tret = {}\n\tfor a in find_paths:\n\t\tret[a] = expand_globs(a, patglob)\n\treturn ret\n"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tout = []\n\tfor frag in search_paths:\n\t\tif isinstance(frag, list):\n\t\t\tout += frag\n\t\telse:\n\t\t\tout.append(re.sub(r'(\\/+)', r'sh:.*\\**/', frag))\n\treturn set(out)\n\ndef find_find_patterns_to_cache(dirs, cache_dir, include_paths=False):\n\t\"\"\"\n\tGiven a sequence of file or directory path fragments, return the list\n\tof the \"find\" globs to use to look for files"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\tglobal glob_flags\n\tglobal PATTERN_START_RE\n\tglobal PATH_END_RE\n\t\n\tpattern_parts = []\n\t\n\tassert isinstance(find_paths, list)\n\t\n\tfor fragment in find_paths:\n\t\tparts = os.path.normpath(fragment).split(os.sep)\n\t\tnew_string = PATTERN_START_RE.sub(r\"\\1:**\", fragment)\n\t\tfor part in parts:\n\t\t\tif ':' in part:\n\t\t\t\tnew_string ="}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tfind_dirs = [item for list_of_str in find_paths for item in list_of_str.split('|')]\n\tfind_paths = [item for list_of_str in find_paths for item in list_of_str.split('|')]\n\t\n\tdef translate(path):\n\t    if path.endswith(':'):\n\t        return os.path.isdir(path) and os.path.join('**', path) or path\n\t    else:\n\t        return re.sub(r"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\tpatterns = []\n\tfor path_fragment in find_paths:\n\t\t# Use re.sub to convert the path fragment into a regex. This means we can't use a dot\n\t\t# separator because we are using ** in the glob pattern, so use an underscore instead.\n\t\tfragment_regex = re.sub(r'(\\w+)(?:(\\.(\\w+)))?\\*', r'([^]*[^\\^])?^((?:.*[/])(.*))?$', path_fragment)\n\t\t\n"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tfind_matcher = matcher.matcher_for(\n\t\tfind_patterns=patterns,\n\t\tfind_opts=opts,\n\t)\n\tfor f in find_patterns:\n\t\tfind_matcher.add(\n\t\t\tpath=os.path.expanduser(os.path.expandvars(f)),\n\t\t\tpattern='sh:**/*'.format(f)\n\t\t)\n\n\ndef read_git_file(file_name):\n\tfile_lines = []\n\tfile_found = False\n\tfile_path = os"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tdef __init__(self, find_paths):\n        self.find_paths = find_paths\n\tdef __call__(self):\n        return [self.globs_for_path(p) for p in self.find_paths]\n\n    def globs_for_path(self, p):\n\t\tif '/' in p:\n\t\t\tglob_patterns = self.translate(p)\n\t\t\tglobbed = glob.glob(p)\n\t\telse:\n\t\t\tglob_patterns = [p]\n\t\t\tglob"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\tpatterns = []\n\t# Split paths into 2 pieces: prefix and rest\n\tfind_patterns = find_paths[0].split(\":\", 1)\n\t\n\t# Recursively replace ':' with '**' pattern\n\tmatch_count = 0\n\tfor find_pattern_in_find_pattern in find_patterns:\n\t\t# Matches any input\n\t\tfind_pattern_in_find_pattern = find_pattern_in_find_pattern.replace(\":\",\"^:*\\\\^*$\")\n\t\t\n\t\t# Match every path-se"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\tfor root, dirs, files in os.walk(find_path):\n\t\tprint \"Walk to {}\".format(root)\n\t\tfor filename in files:\n\t\t\tfilename_path = root+\"/\"+filename\n\t\t\tprint \"Walk to {}\".format(filename_path)\n\t\t\tyield filename_path\n\t# if it passes by this stage, we're done\n"}, {"generated_text": "import re\n\"\"\"\n\tGiven a sequence of path fragments or patterns as passed to `--find`, transform all path\n\tfragments into glob patterns. Pass through existing patterns untouched.\n\t\n\tFor example, given find_paths of:\n\t\n\t  ['foo.txt', 'pp:root/somedir']\n\t\n\t... transform that into:\n\t\n\t  ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n\t\"\"\"\n\t\n\tpattern_parts = [ ]\n\tfor fragment in find_paths:\n\t\tif path_part := path.join(dir,'',fragment):\n\t\t\tpattern_parts.append(re.sub(pattern_path_pattern,path_part,re.escape(path_part))+'/**')\n\t\telse:\n\t\t\tpattern_parts.append(fragment)\n\t\n\tif pattern_parts[0] =='sh:**':\n\t\tpattern_parts = pattern_parts[1:]\n\t\n\treturn pattern_parts\n\ndef main("}]}
{"task_id": "62ece4992e6aefcf4aabbd7a", "prompt": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif not is_windows():\n\t\treturn False\n\n\tgit_bash = os.environ.get(\"GIT_BASH\", None)\n\tif not git_bash:\n\t\treturn False\n\n\treturn git_bash == 'git bash' or git_bash == '.git bash'\n\n\ndef get_windows_git_exec_env(windows_git_exec=False):\n\t\"\"\"\n\t\tGets git command env\n\t\t\n\t\t:return: True if git\n\t\t\"\"\"\t\n\tif windows_git_exec:\n\t\treturn os.environ\n\telse:"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\ttry:\n\t\tp = Popen(['git', '--git-dir'], stdout=PIPE, stderr=STDOUT)\n\t\tout, err = p.communicate()\n\t\tlines = out.split('\\n')\n\t\twin = True\n\t\tfor line in lines:\n\t\t\tline = line.rstrip()\n\t\t\tif line == 'git':\n\t\t\t\twin = False\n\t\t#print(out)\n\t\treturn win\n\texcept:\n\t\treturn False\n\t\t\nif isWindows():\n\tsys.path.append('..')\n\t\n\t"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif os.path.exists(\"__git-bash__\"):\n\t\treturn True\n\treturn False\n\n\ndef get_user_name():\n\t\"\"\"\n\tReturns user name of the current user in windows\n\t\n\t:return: user name\n\t\"\"\"\n\tif os.sys.platform.startswith(\"win32\"):\n\t\tif os.path.isfile(\"__git-bash__\"):\n\t\t\treturn get(\"__git-bash__\")\n\t\treturn get(\"USERNAME\")\n\ndef get_user_name_from_shell():\n\t#get"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t# get version and compare with gitbash\n\tversion_git = check_output(['git', 'describe', '--abbrev=0']).strip()\n\tversion_git_file = gitbash_version.replace(\"v\",\"\").split(\".\")[0]\n\tversion_git_int = version_git[1:]\n\t\n\t# strip extra information\n\tversion_git_file = version_git_file.split(\"-\")[0]\n\tversion_git_file = version_git_file.split(\".\")[0]\n\t\n\t#"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn os.path.isdir('gitbash') is True\n\ndef get_gitbash_name():\n\t\"\"\"\n\t:return: gitbash name as string\n\t\"\"\"\n\thome = '%USERPROFILE%'\n\tname = 'Git Bash'\n\tif os.path.isdir(home):\n\t\tif not os.path.isdir(os.path.join(home,'.gitbash')):\n\t\t\treturn False\n\t\telse:\n\t\t\tfiles = [f for f in os.listdir(os.path.join(home,'.gitbash"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tif os.name == 'nt':\n\t\treturn True\n\treturn False\n\n\n\"\"\"\n\tFunction is not used yet. It's to check if you are on windows\n\t:return: True if windox\n\t\"\"\"\ndef isWindows():\n\tif os.name == 'nt':\n\t\treturn True\n\treturn False\n"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\n\trepo = str(__file__.rsplit(\"\\\\\", 1)[0].rsplit(\"/\", 1)[0])\n\t\t\n\tif (os.system(\"git config --get remote.origin.url\")==1) and (os.system(\"git config --get remote.origin.url\")==0) and (os.system(\"git config --get git.gitBash\")==1) and (os.system(\"git config --get git.gitBash\")==0) and (os.system(\"git config --get git.gitbash\")==1"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\tos.sys.path=[\"C:\\\\Program Files (x86)\\\\7-Zip-7.9.1/7z.exe\"]\n\tos.sys.path.append(\"C:\\\\Program Files (x86)\\\\7-Zip-7.9.1\\\\7z.mui\")\n\tos.sys.path.append(\"C:\\\\Program Files (x86)\\\\7-Zip-7.9.1\\\\7z.exe\")\n\tos.sys.path.append(\"C:\\\\Program Files (x86)\\\\7-Zip-7"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\t\n\t#if not gitbash:\n\t#\treturn False\t\t\t\t\n\t\n\tgitpath = [\"C:/Program Files/SubWii\", \"C:/Program Files (x86)/SubWii\"]\n\n\tgitbash = False\n\t\n\tfor directory in gitpath:\n\t\tif os.path.isdir(directory):\n\t\t\tgitbash = True\n\t\t\n\tif not gitbash:\n\t\tprint(\"\\n\\n!!! We failed this test\\n\\n\")\n\treturn gitbash\n\n\"\"\"\n\tcheck if gitpath is in gitpath\n\t:return"}, {"generated_text": "import os\n\"\"\"\n\treturns True if you run in a Windows gitbash\n\t\n\t:return: True if gitbash\n\t\"\"\"\n\treturn os.path.isfile( r\"C:\\gitbash.exe\" )\n\n\n# return True if gitbash\ndef in_git_bash( ):\n\treturn os.path.isfile( \"gitbash.exe\" )\n\n\n# get absolute name from a relative path\ndef get_absolutepath():\n\tfullpath = os.path.realpath(__file__)\n\tname = os.path.basename(fullpath)\n\treturn os.path.join(os.path.dirname(fullpath), name)\n\n# get absolute"}]}
{"task_id": "62ece4992e6aefcf4aabbd7b", "prompt": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tif not os.path.isdir( os.path.dirname( path_to_write_target_to ) ) :\n\t\tos.makedirs( os.path.dirname( path_to_write_target_to  ) )\n\n\tif not overwrite : \n\t\twith open( path_to_write_target_to, \"r\" ) as f :\n\t\t\tcontents = f.read()\n\t\t\tif contents!= \"\" :\n\t\t\t\t# Already existing file, need to abort \n\t\t\t\treturn\n\t\t\n\twith open( path_to_write_target"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tlogging.debug('write_target_config(\"%s\", \"%s\", \"%s\", \"%s\")' % (target_config_file,rendered_config,overwrite,os.path.basename(target_config_file)))\n\t\n\tif overwrite and os.path.isfile(target_config_file):\n\t\t#logging.error('target config file already exists, will not overwrite - target_config_file=%s' % target_config_file)\n\t\treturn\n\telif not overwrite:\n\t\t#logging.error('target config file already"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tconfig_file = config_path + '/' + config_file\n\tif os.path.exists(config_file) and overwrite is False:\n\t\tprint(\"Target config %s already exists!\" % config_file)\n\t\treturn\n\tout_file = open(config_file, 'w')\n\tout_file.write(rendered_config);\n\tout_file.close()\n\tprint(\"%s created\" % (config_file))\n\nclass Render(dict):\n\t\"\"\"\n\tA Render class that makes it easy and painless to create YAML"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tfrom cobbler import api\n\n\ttarget_config_filename = os.path.join(config_dir,\"cobbler_config.yaml\")\n\ttry:\n\t\told_config = open(target_config_filename).read()\n\t\ttarget_config = old_config\n\t\tif cfg.has_key(\"autoinstall_meta\"):\n\t\t\ttarget_config = cfg[\"autoinstall_meta\"]\n\t\trender_func_name = \"RenderConfig\"\n\t\tif old_config!= target_config:\n\t\t\tprint \"Overwriting \", target_config_filename\n"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tabs_config_dir = os.path.dirname(abs_config_file_path)\n\tconfig_file_name = os.path.basename(abs_config_file_path)\n\t# Check if file is in sync with current target config\n\tstored_config = Config(abs_config_file_path)\n\tif stored_config.config_changed(rendered_config):\n\t\tabort(\"Configuration was already changed. Aborting\")\n\t# Write changes to file.\n\twith open(abs_config_file_path, \"w\") as abs"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\ttarget_file = os.path.join(CONFIG_OUTPUT_DIR, target_name + '.yaml')\n\tif os.path.isfile(target_file) and not overwrite:\n\t\traise Exception('Target file {0} already exists, must be different to avoid stomping on it'.format(target_file))\n\tconfig_data = yaml.load(config_content)\n\tif config_data is None:\n\t\traise Exception('Unable to load target config YAML')\n\ttarget_file_dir = os.path.dirname("}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\t\n\tif not overwrite:\n\t\tif os.path.exists(destination):\n\t\t\terror = \"Configuration file \\\"\" + os.path.basename(destination) + \"\\\" already exists.\"\n\t\t\tprint(error)\n\t\t\treturn False\n\t\n\t#create the target directory before writing the file out\n\tif os.path.exists(os.path.dirname(destination)):\n\t\tos.makedirs(os.path.dirname(destination))\n\t\n\t#write the file contents to the destination file\n\tif overwrite:\n\t\t"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tif os.path.isfile(filename) and overwrite == False:\n\t\treturn\n\n\tif not os.path.exists(os.path.dirname(filename)):\n\t\tos.makedirs(os.path.dirname(filename))\n\n\twith open(filename, \"w\") as f:\n\t\tf.write(yaml.dump(config))\n\ndef main(argv):\n\tdef set_default(val, default_value):\n\t\tif (val == 'True' or val == True or val == 'false' or val == False or\n"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\tif os.path.exists(targetConfigFile):\n\t\tif overwrite == False:\n\t\t\tprint('[config] {} already exists. Not writing.'.format(targetConfigFile))\n\t\t\treturn\n\n\trenderedConfig = renderYaml(configYamlFile)\n\trenderedConfigFile = targetConfigFile.replace('.yml', '').replace('.', '_') + '.yml'\n\n\tconfigFile = open(targetConfigFile,'w+')\n\tconfigFile.write(renderedConfig)\n\tconfigFile.close()"}, {"generated_text": "import os\n\"\"\"\n\tGiven a target config filename and rendered config YAML, write it out to file. Create any\n\tcontaining directories as needed. But if the file already exists and overwrite is False,\n\tabort before writing anything.\n\t\"\"\"\n\twith open(output_filename, 'w') as txtfh:\n\t\ttxtfh.write(rendered_config)\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd7c", "prompt": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tlines = []\n\tlines += ['sudo sh -s']\n\tindent=len(script.__name__)\n\tfor line in script._script.splitlines():\n\t\tif len(line) > indent:\n\t\t\tlines += [ \"    %s\" % (line) ]\n\t\telse:\n\t\t\tlines += [ line ]\n\tlines += []\n\treturn \"\\n\".join(lines)\n\ndef main():\n\timport argparse\n\tparser =  argparse.ArgumentParser(parser_name=\"ssh_command_converter\")\n\tparser.add"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t\n\tif not os.path.isfile(script):\n\t\tprint(\"File '\"+script+\"' is not exist\")\n\telif os.access(script,os.R_OK):\n\t\twith open(script,'r') as f:\n\t\t\tlines = f.read().splitlines()\n\t\t\tcmd = lines[0]\n\t\t\tfor line in lines[1:]:\n\t\t\t\tprint(\"\\t\" + textwrap.fill(line.rstrip(),50))\n\t\tprint(\"conversed to '\"+cmd+\"'\")\n\telse:\n\t\tprint(\""}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\treturn \"\\n\".join( map(str, script[0]) + script[1:] )\n\nif __name__ == \"__main__\":\n\n\tfrom subprocess import call\n\n\tscript = textwrap.dedent(\"\"\"\n\t\t# This is my super awesome script.\n\t\tssh ${IP} 'echo \"hello\"'\n\t\ttest \"there\"\n\t\trm -f ${TARGET}\n\t\texit 100\n\t\"\"\")\n\n\tcall( [\"./convert_to_command.py\", script] )\n"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tlines = split_lines(script)\n\tcommand = \" \".join([\"/bin/sh -s\", lines[0]])\n\tfor l in lines[1:]:\n\t\tcommand = \" \".join([command, l[:-1]])\n\treturn command\n\t##\n#}\n"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\t# Split commands and variables.\n\t# The variables should be enclosed in {}\n\tscript = textwrap.dedent(script).expandtabs(4).replace(\"'\", r\"'\")\n\t\n\t# Fix variables.\n\t# The script should be enclosed with \\\n\tscript = re.sub(r\"\\n\", \"\\n\\\\\",\n\t\t\t\t\t\tscript, count=1)\n\t\n\tscript2cmd(script)\n\t\ndef parse():\n\t\"\"\"\n\t\tParse the arguments passed.\n\t\t:return: dict \n\t\tdict = {\n\t\t\t\t\"key"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tlines = textwrap.wrap(script, 100)\n\tout = ''.join(lines).replace('\\n', '$ ').replace(' ', '\\\\$ ')\n\treturn ''.join(line.replace('\\n', '$ ') for line in out.split('$ '))\n\ndef get_key_value_of_key(keys):\n\tvalues = keys.split(': ')\n\tif len(values) > 1:\n\t\treturn values[0]+':'+values[1]\n\telse:\n\t\treturn values[0]\n"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\twith open(script, 'r') as infile:\n\t\treturn \" \".join([line.strip() for line in infile])\n#######################################################\n\n\"\"\"\n\tconverts a single line of output to a list of lines.\n\tOnly useful to run if the system returns multiple lines.\n\n\tTHis is useful for generating a list of output lines and a list of errors.\n\t\n\t:param output:\n\t:param output_structure:\n\t:return: [output_lines, errors]\n\t\"\"\"\n\toutput_lines = []\n\terror_lines"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tcmd = \"cat \\\"{script}\\\" | \".format(script=script).\\\n\t\treplace(\"\\n\",\";\").\\\n\t\tsplit(\" \")\n\treturn cmd"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\tlines = script.split('\\n')\n\tret = 'exec ssh %s \"%s\"' % (hostname, lines[0])\n\tfor line in lines[1 : ]:\n\t\tret = ret +'&&'* len(script)\n\t\tret = ret + line\n\treturn ret\n"}, {"generated_text": "import textwrap\n\"\"\"\n\tconverts a script to one line command.\n\tTHis is useful to run a single ssh command and pass a one line script.\n\t\n\t:param script:\n\t:return:\n\t\"\"\"\n\treturn \"\\n\".join([str(r) for r in script])\n\n\ndef to_multiline_script(script):\n\t\"\"\"\n\t\tconverts a script to multi-line commands.\n\t\t\n\t\t:param script:\n\t\t:return:\n\t\"\"\"\t\n\tscript.insert(0, '#! /bin/bash\\n\\n')\n\treturn [ \" \".join(str(r)) for r in script ]\n\n\ndef ssh_client_args(scp_remote_file=\"\", use_shell=False):\n\t\"\"\"\n\t\treturn a list"}]}
{"task_id": "62ece4992e6aefcf4aabbd7d", "prompt": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tif not hasattr(func,'module') or func.module is None:\n\t\traise TypeError(\"First argument must be a function.\")\n\tmodname = func.__module__\n\tclassname = func.__name__\n\tprint(modname)\n\tif extra_env is None:\n\t\textra_env = {}\n\tkwargs = {'stderr': subprocess.STDOUT,'stdout': subprocess.PIPE}\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\n\tif hasattr(sub"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\t# For Python3\n\t# args = subprocess.list2cmdline(args)\n\tkwargs['args'] = (args,)\n\tkwargs['stderr'] = subprocess.PIPE \n\tkwargs['shell'] = True\n\tcmd = subprocess.run(**kwargs)\n\tcmd_str = (str(cmd) if six.PY3 else unicode(cmd).encode('utf-8'))\n\treturn cmd_str.decode()\n\ndef start_subproc(func, *args, **kwargs):\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tfrom jupyter.core.application import Application\n\tfrom tornado.options import options\n\ttornado_options = Application.initialized()\n\tif tornado_options is not None:\n\t\ttornado_options.instance_path = \"tmp\"\n\n\tsubprocess.run(r\"{} {}\".format(func.__module__.split(\".\")[0], func.__name__),\n\t\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\t\tstderr=subprocess.STDOUT,\n\t\t\t\t\tcheck=True,\n\t\t\t\t\t**dict(list(getattr(options,"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\targs = kwargs.get(\"args\")\n\textra_env = kwargs.get(\"extra_env\")\n\tif extra_env is None:\n\t\textra_env = {}\n\tenv = dict(os.environ.copy(), **extra_env)\n\tif kwargs.get('detach'):\n\t\treturn subprocess.run(func, args, stdout=subprocess.PIPE, text=False, **kwargs).returncode\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tsubprocess_args = ['node_executable']\n\tsubprocess_args.append(func.__module__)\n\tsubprocess_args.append(func.__name__)\n\tsubprocess_args.extend(extra_env.items())\n\tsubprocess_args.extend(args)\n\tsubprocess.run(subprocess_args)\n\n\ndef check_process_cwd(func, testfunc, cwd=None):\n    \"\"\"\n    Test a function with the possibility of failing on Windows.\n    First run a test function in the current directory"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tcall = func.__name__ + \" \" + str(args)\n\tretcode = subprocess.run(\n\t\t[sys.executable] + sys.argv,\n\t\tenv=extra_env,\n\t\tshell=True,\n\t\tcheck=True,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tencoding='utf-8',\n\t\tdata=call,\n\t\ttext=True,\n\t)[1]\n\treturn retcode\n\n\ndef call(*args, **kwargs):\n\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tdef __init__(self, func, *args, **kwargs):\n\t\tself.func = func\n\t\tself.args = args\n\t\tself.kwargs = kwargs\n\t\t\n\tdef __call__(self, extra_env=None):\n\t\tenv = os.environ.copy()\n\t\tif extra_env:\n\t\t\tenv.update(extra_env)\n\t\t\n\t\tcmd = ['tensorflow/tools/graph_rewriter/run_graph_rewriter.py'] #'--all_variables'] + self.args\n\t\t\n\t\t"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\targs = [func.__name__] + list(args)\n\tsubprocess.run(args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n\t\t\t\t  shell=True, universal_newlines=True,\n\t\t\t\t  stdin=subprocess.PIPE,\n\t\t\t\t  env=extra_env)\n\ndef run_program(args, return_code=None, stdout=None, stderr=None,\n\t  universal_newlines=None, return_error=False):\n\t\"\"\"Run a program, returning"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tlogger.info('Execute as subprocess: '+ func.__name__)\n\treturn (subprocess.run(['python', func.__module__, func.__name__] + list(args),\n\t\t#env = env_dict,\n\t\tstdout=True, stderr=subprocess.STDOUT,\n\t\tcheck=True,\n\t\t#universal_newlines=True,\n\t\t#encoding='utf8',\n\t\t)\n\t)\n\ndef run_module(fn, args, env, extra_env=None):\n\t\"\"\"Run module"}, {"generated_text": "import subprocess\n\"\"\"\n\tRun a function in a sub-process.\n\t\n\tParameters\n\t----------\n\tfunc : function\n\t    The function to be run.  It must be in a module that is importable.\n\t*args : str\n\t    Any additional command line arguments to be passed in\n\t    the first argument to ``subprocess.run``.\n\textra_env : dict[str, str]\n\t    Any additional environment variables to be set for the subprocess.\n\t\"\"\"\n\tp = subprocess.run(\n\t\targs=args,\n\t\treturncode=0,\n\t\tstdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n\t\tcapture_output=True,\n\t\tenv=extra_env,\n\t)\n\tif p.returncode!= 0:\n\t\tmsg = p.stdout\n\t\traise subprocess.CalledProcessError(p.returncode,''.join(args), msg)\n\t\ndef check_python_version(version):\n\t\"\"\"\n\tCheck that the given version of Python is"}]}
{"task_id": "62ece4992e6aefcf4aabbd7e", "prompt": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tenv_value = os.environ.get(matcher[0], match_default_value if ':' in matcher else default_value)\n\treturn env_value if ':' in matcher else match_default_value_for_matcher(matcher, env_value)\n\n\ndef match_default_value_for_matcher(matcher, default_value):\n\tif ':' in matcher: # It's a matcher without a colon (e.g: TEMPLATE)\n\t\treturn default_value\n\telse: # It's a mat"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\t# if not os.environ and not default is None:\n\t# \traise ValueError(\n\t# \t\t\"Required variable \" + name + \" is not defined and no default was specified!\")\n\tif not default is None and default is not None:\n\t\treturn default\n\telse:\n\t\treturn os.getenv(name)\n\ndef split_into_path_tokens(path):\n\tsplit_path_tokens = path.split('/')\n\tif split_path_tokens == ['']:\n\t\treturn os.getc"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvalue = os.environ.get( variable, default )\n\tif value is None:\n\t\traise RuntimeError( 'Variable {} not defined'.format( variable ) )\n\treturn value\n\n\n\n\n\ndef toFloat( val: str ) -> float:\n\treturn float( val )\n\n\n\ndef toString( val: float ) -> str:\n\treturn str( val )\n\treturn val\n\n\n\n\n\ndef toBoolean( value ) -> BoolType:\n\treturn bool( value )\n"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tdefault = []\n\treturn os.environ.get(match, default)\n\n\"\"\"\n\tEnsure the path in which the binary is located exists\n\t\"\"\"\n\tp = os.path.realpath(sys.argv[0]).split(os.path.sep)[:-1]\n\tp = os.path.join(os.path.sep.join(p), \"..\")\n\tp = os.path.normpath(p)\n\t#print(\"P\",p)\n\twhile not os.path.isfile(p):\n\t\tprev"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\ttry :\n\t\tif isinstance(matcher, str) :\n\t\t\tvalue = os.environ[matcher]\n\t\telse :\n\t\t\tvalue = os.environ[matcher['name']]\n\texcept :\n\t\tif 'name' not in matcher or 'value' not in matcher :\n\t\t\traise Exception('Cannot read environment variable with type %s' % type(matcher))\n\t\tvalue = matcher['value']\n\treturn value\n"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tresult = os.environ.get(name)\n\tif result is None:\n\t\treturn defValue\n\telse:\n\t\treturn result\n\ndef debug(s):\n\tif debugFlag:\n\t\tprint(s)\n\ndef formatTime(t):\n\treturn strftime('%M:%S', localtime(t))\n\ndef main():\n\t\"\"\" Main function - parse the command line args and run the selected demo. \"\"\"\n\timport sys\n\timport optparse\n\timport gettext\n\timport time\n\timport re\n\tglobal debugFlag\n\n\tdebug"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvarNameEnv = matcher[1:].split(\"=\", 1) if len(matcher) > 1 else None\n\ttry:\n\t\treturn varNameEnv[0] in os.environ[varNameEnv[1]] and os.environ[varNameEnv[0]] or varNameEnv[1]\n\texcept KeyError:\n\t\traise KeyError(\"Unknown variable '%s'\" % varNameEnv[0]) if len(varNameEnv) > 2 else None\n\n\ndef get_version_number(path):\n\tversion ="}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tdef __init__(self, name, default_value = None):\n\t\t\"\"\"\n\t\tName and default_value of variable to get\n\t\t\"\"\"\n\t\tself.name = name\n\t\tself.default_value = default_value\n\n\tdef __call__(self, config_data):\n\t\t\"\"\"\n\t\tThis needs to be overrided.\n\t\t\"\"\"\n\t\tpossible_name = os.environ.get(self.name, None)\n\t\tif possible_name is None and self.default_value is None:\n\t\t\traise ValueError(\"'%s' is not defined"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tif var in os.environ:\n\t\tif default is None:\n\t\t\treturn os.environ.get(var)\n\t\treturn os.environ.get(var, default)\n\n\tif default is None:\n\t\traise Error(\"Could not resolve variable {}\".format(var))\n\n\treturn default\n\ndef expandvars_raw(s):\n\t\"\"\"Expands environment variables using shlex.split() and os.environ\"\"\"\n\treturn \"\".join([\"${%s[%s]}\" % (k, v) for k, v in SHS.get"}, {"generated_text": "import os\n\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value.\n\tIf the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tenv_key_value_result = dict()\n\t\t\n\tfor env_key_value in matcher_iterable:\n\t\tif env_key_value:\n\t\t\t\n\t\t\tenv_key_value_split = env_key_value.split(\"=\", 1)\n\t\t\tif len(env_key_value_split) == 2:\n\t\t\t\tenv_key_value_result[env_key_value_split[0]] = env_key_value_split[1]\n\t\t\telse:\n\t\t\t\traise EnvironmentVariableException(\"The name in the matcher must be of the"}]}
{"task_id": "62ece4992e6aefcf4aabbd7f", "prompt": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t\n\tmatch = re.match(\".+\\/\\/(.+)\\/(.+)\\./.*\", image_href.strip())\n\ttry:\n\t\timage_id = map(int, match.groups()[0].split(\".\"))\n\texcept IndexError:\n\t\traise ValueError(\"Can't parse image href\")\n\t\n\tnetwork = match.groups()[1]\n\tuse_ssl = parse_bool(match.groups()[2])\n\treturn image_id, network, use_ssl\n\n\ndef parse_link_href(link_href):\n\t"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\ttry:\n\t\timage, server, use_ssl = image_href.split('/')\n\texcept ValueError:\n\t\traise ValueError(\n\t\t\t_('invalid image href: \"%s\"') % image_href\n\t\t)\n\tp = parse_uri(server)\n\tif len(p)!= 3:\n\t\traise ValueError(\n\t\t\t_('invalid image href: \"%s\"') % image_href\n\t\t)\n\treturn (\n\t\tp[0], \n\t\tp[1], \n\t\tbool(p[2])\n\t)\n"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\turl = image_href\n\tnetloc = ''\n\tuse_ssl = False\n\t\n\tm = re.match(\"^https?://\", image_href)\n\tif(m is None):\n\t\tm = re.match(\"^http://\", image_href)\n\tif(m is not None):\n\t\turl = m.group(1)\n\tm = re.match(\"^(http|https)://([^/]*)(.*)\", url)\n\tif(m is not None):\n\t\tnetloc = m.group(1)\n\t\turl"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\ttry:\n\t\tbase, host, path, params, query, fragment = urlsplit(image_href)\n\texcept ValueError:\n\t\tif os.path.isfile(image_href):\n\t\t\timg = Image.open(image_href)\n\t\t\timg.verify()\n\t\t\treturn (None, host, False)\n\t\telse:\n\t\t\t# assume it's a file name, return (None, host, False)\n\t\t\treturn (None, host, False)\n\t\n\telse:\n\t\tuse_ssl = None\n\t\n\tif base==':'"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\ti = image_href.rfind('/')\n\tif i == -1:\n\t\traise ValueError('invalid image href')\n\n\treturn (image_href[i + 1:], image_href[:image_href.find('/', i)], bool(image_href.endswith('/')))\n\ndef use_ssl(image_href):\n\t\"\"\"\n\tWhether an image is on a secure or insecure network.\n\n\t:param image_href: href of an image\n\t:returns: True if secure, False otherwise\n\t\"\"\"\n\t"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\timage_ref = urllib.parse.urlsplit(image_href)\n\t\n\t(image_id, netloc, use_ssl) = (image_ref.netloc, image_ref.netloc, image_ref.netloc == 'triton.com')\n\tnetloc = netloc.lower().strip('.')\n\t\n\tif not '.' in image_ref.path:\n\t\treturn (None, None, None)\n\t\n\timage_path = image_ref.path.split('.', 1)[0]\n\t\n\t"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tmatch = re.match(r'(([^/]+)/)?(?:\\.(\\w+))/?'\n\t\t\t\t\t  '([^?&#]*)'\n#\t\t\t\t  '((?:http|https|ftp|file|dav)://)'\n#\t\t\t\t\t  '([^?&#]+)(?:[?&#]*[\\w\\/%].*)?')\n\t\t, image_href[1:]) # strip off protocol\n\tif not match:\n\t\traise ValueError('No image_href found in href %s' % image_href)\n\n"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\tuse_ssl = False\n\tif image_href.startswith('ht'):\n#\t\timage_id, netloc, use_ssl = base.parse_image_href(image_href, True)\n\t\timage_id, netloc, use_ssl = base.parse_image_href(image_href, False)\n\telse:\n\t\timage_id, netloc, use_ssl = base.parse_image_href(image_href, False)\n\t\traise ValueError('''parse_image_href: href must start with 'ht' or '"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\th = urlparse(image_href)[-1]\n\tif not h.startswith('/'):\n\t\traise ValueError\n\tparts = h.split('/')\n\tif len(parts) < 2 or len(parts) > 3:\n\t\traise ValueError\n\n\treturn (parts[0], parts[-2], (parts[-1] =='secure'))\n\ndef expand_image_href(image_href):\n\timage_id, netloc, use_ssl = parse_image_href(image_href)\n\t\n\tif not"}, {"generated_text": "import urllib\n\"\"\"\n\tParse an image href into composite parts.\n\t\n\t:param image_href: href of an image\n\t:returns: a tuple of the form (image_id, netloc, use_ssl)\n\t:raises ValueError:\n\t\"\"\"\n\t# the href is of the form: (image_id, netloc)\n\t# image_id is the id specified on the image (0-based)\n\t# netloc is either http or https depending on the location of the image\n\t\t#   being parsed (may be empty if image is provided through the API)\n\t# use_ssl is either 'https' or '' depending whether the connection is to the\n\t\t#   SSL server or the http server\n\tif re.match(ur'http[s]://', image_href):\n\t\timage_id = 0\n\t\tuse"}]}
{"task_id": "62ece4992e6aefcf4aabbd80", "prompt": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tfor i in input_list:\n\t\tlen_i = len(i)\n\t\tif 1 < len_i:\n\t\t\tlast_char = i[len_i - 1]\n\t\t\tif (last_char == os.path.sep) or (last_char == os.sep):\n\t\t\t\ti = i[:-1]\n\treturn(i)\n"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tif not(type(input_list) == list):\n\t\traise TypeError\n\tfiltered_list = [filtered_string for filtered_string in input_list]\n\tfiltered_list = [filtered_string for filtered_string in input_list\n\t\tif hasattr(filtered_string, \"length\") and filtered_string.length > 1]\n\tfiltered_list = [filtered_string for filtered_string in input_list\n\t\tif filtered_string[-1] == os.sep]\n\tfiltered_list = [filtered_"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tif not isinstance(input_list, list):\n\t\traise TypeError\n\tfor elem in input_list:\n\t\tif len(elem) > 1 and os.path.sep in elem[-1]:\n\t\t\telem.pop()\n\treturn input_list\n\ndef get_pathnames(file_path, pathsep):\n\t\"\"\"\n\tGet the list of file names (pathnames) in a given path.\n\t\n\tEach pathname (file name) is tested if its length is greater than two\n\tand if the last 4 characters are"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tif not type(input_list) is list:\n\t\traise TypeError('Expecting a list of strings')\n\t\n\tfor str in input_list:\n\t\tif len(str) > 1 and str[-1] == os.sep:\n\t\t\tstr = str[:-1]\n\t\t\n\t\treturn str\n\t\n\treturn input_list\n"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\tif type(input_list) is not list:\n\t\traise TypeError(\"Expected a list of strings!\")\n\t\n\tfor i in range(len(input_list)):\n\t\tif input_list[i][-1] is os.sep:\n\t\t\tinput_list[i] = input_list[i][:-1]\n\t\n\treturn input_list\n"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tprocessed_list = []\n\n\tfor item in iter(input_list):\n\t\t# check if it is a pathname\n\t\tif len(item) > 1 and item[-1] in _FS_SEPARATORS.split(os.path.sep):\n\t\t\tprocessed_list.append(item[0:-1])\n\t\telse:\n\t\t\tprocessed_list.append(item)\n\n\treturn processed_list\n\n"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tproc_list = []\n\tfor item in input_list:\n\t\titem = item.rstrip(os.sep)\n\t\tproc_list.append(item)\n\treturn proc_list\n\t\ndef remove_duplicates(string_list):\n\t\"\"\"\n\tRemoves duplicate items in a list and returns a set of unique elements.\n\t\n\tRemoves duplicates as well as repeating names.\n\t\n\tArgs:\n\t\t\tinput: str\n\t\t\t\n\tReturns:\n\t    List of tuples consisting of two items.\n\t\"\"\"\n\tdict ="}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tfor item in input_list:\n\t\t# This part will be expanded on the future\n\t\tif len(item) > 1 and item[-1:] == os.path.sep:\n\t\t\titem = item[:-1]\n\t\tyield item\n\nif __name__ == \"__main__\":\n\t# TODO: more example usages\n\t# TEST:\n\t# >>> list(process_list(\"/home/titan/foo\", \"/usr/bin/gpp\"))\n\t# >>> ['/home/titan/foo', '/usr/bin/gpp"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\tret = []\n\tfor e in input_list:\n\t\tif len(e) > 1 and e[-1] == os.sep:\n\t\t\tret.append(e[:-1])\n\t\telse:\n\t\t\tret.append(e)\n\treturn ret\n\t\n"}, {"generated_text": "import os\n\"\"\"\n\tIterate over a string list and remove trailing os seperator characters.\n\t\n\tEach string is tested if its length is greater than one and if the last\n\tcharacter is the pathname seperator. If so, the pathname seperator character\n\tis removed.\n\t\n\tArgs:\n\t    input_list: list of strings\n\t\n\tReturns:\n\t    Processed list of strings\n\t\n\tRaises:\n\t    TypeError\n\t\"\"\"\n\t\n\tfor item in input_list:\n\t\tif len(item) > 1 and item[-1] == os.path.sep:\n\t\t\titem = item[:-1]\n\t\toutput.append(item)\n\t\n\treturn output\n\t\t"}]}
{"task_id": "62ece4992e6aefcf4aabbd82", "prompt": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef __init__(self, text):\n\t\tself._regex = re.compile(text, re.IGNORECASE)\n\nclass CommandNotSupportedException(Exception):\n\tpass\n\nclass Command:\n\ttext = None\n\tregex = None\n\thandler = None\n\tdef __init__(self, text = None):\n\t\tif not text:\n\t\t\ttext = '([CAT]*)' + ''.join(random.sample(string.ascii_printable, random.randint(6, 10)))\n\t\tself._text = text"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef __init__(self, source = None):\n\t\tif source:\t\t\t\n\t\t\tself.source = source\n\t\t\t\n\t\t\tself.__init_pattern()\n\t\telse:\n\t\t\tself.source = \"\"\n\t\t\t\n\tdef get_pattern(self):\n\t\treturn self.__pattern\n\t\t\n\tdef get_pattern_str(self):\n\t\treturn self.source\n\t\t\t\t\n\tdef get_source(self):\n\t\treturn self.source\n\t\t\n\tdef get_source_str(self):\n\t\treturn self.source\n\t\t\n\tdef set_pattern("}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t# remove all space and sign\n\t# remove all capital letters except alphabets\n\tconvert = re.sub(\"\\s\", \" |\", convert)\n\tconvert = re.sub(\"[^\\u002A][^\\u0021-\\u007E]\", \" \", convert)\n\n\t# remove all non-alphabets\n\tif (len(convert) == 0):\n\t\treturn None\n\telse:\n\t\t# check for no pattern\n\t\tpattern = re.sub(\"[\\W_]\", \"\", convert)\n\t\tif (len("}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tnew_pattern = self.__regex_string_to_pattern(self.__pattern)\n\tlog.debug(\"Converted %s pattern to %s\", self.__pattern, new_pattern)\n\treturn new_pattern\n\n\n\tdef __regex_string_to_pattern(self, pattern):\n\t\"\"\"\n\t@pattern: The string to convert to regex pattern\n\t\"\"\"\n\tif len(pattern) == 0:\n\t\treturn \"%\"\n\t\n\tlog.critical(len(pattern))\n\t\n\tlog.critical(pattern)\n\t\n\t"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef __init__(self, str):\n\t\tif isinstance(str, str):\n\t\t\tself.p = str\n\t\telse:\n\t\t\traise Exception('\\n Wrong type of arguments. str must be string ')\n\t\"\"\"\n\t\tThis method returns the reg-ex obj with the converted string\n\t\t\"\"\"\n\tdef __call__(self):\n\t\tpat = re.compile(self.p)\n\t\treturn pat\n\t\"\"\"\n\t\tIn case you want check the given reg-ex obj against the given string, set the pattern as str param\n\t\t\"\"\"\n\tdef set"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\t\ndef string_to_regex(string):\n\t\"\"\"\n\t\tThis method converts the given string to regex pattern\n\t\t\n\t\t@param string: String\n\t\t@return regex: String\n\t\"\"\"\n\t\ndef replace_all(text, pattern, replacement):\n\t\"\"\"\n\tThis method converts the given string to regex pattern\n\t\n\t@param text: String\n\t@param pattern: String\n\t@param replacement: String\n\t\"\"\"\n\treturn re.sub(pattern, replacement, text)"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef regexFromString(self, string):\n\t\tstring = string.replace(' ', '')\n\t\tstring = string.replace('?', '')\n\t\tpattern = re.compile('^(.*)' + re.escape('|'.join(string)) + '$', re.DOTALL)\n\t\treturn pattern\n\"\"\"\n\n\nclass TextLine(object):\n\tdef __init__(self):\n\t\tself.matches = {}\n\t\tself.index = 0\n\n\tdef insert(self, key, value):\n\t\tif re.match(self.matches[key"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\treturn re.sub(\"[\\*\\ ]\", \"\", str)\n\ndef get_word_count(str) :\n\t\"\"\"\n\t   This method converts the given string to regex pattern and\n\t   obtains word count of the given text\n\t\"\"\"\n\treturn [int(c) for c in re.findall(\"[A-Za-z]+\", str)]"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\tdef toRegex(self, string){\n\t\tstring = string.strip()\n\t\tstring = re.sub(' /+','', string)\n\t\tstring = re.sub('([a-z]+).+', '', string)\n\t\tstring = \"^\" + string + \"$\"\n\t\treturn re.compile(string)\n\"\"\"\n\n\n\ndef readFile(fileName){\n\tvar result = []\n\n\tfileName = fileName.replace('\\\\','/');\n\tif(typeof(fileName)!= 'number')\n\t\treturn \"Invalid fileName\";"}, {"generated_text": "import re\n\"\"\"\n\tThis method converts the given string to regex pattern\n\t\"\"\"\n\treturn re.sub(\"\\(\\S+ \", \"()\", str)\n\"\"\"\nThe method returns the first character of the given character sequence and \nremoves the trailing space\n\"\"\"\ndef split_first(str):\n\treturn str.split(\" \")[0]\n\"\"\"\n\tThis method returns the last item of a given list (of strings)\n\t\"\"\"\ndef last(lst):\n    return lst[-1]\n\"\"\"\n\tThis method returns the item at given index of a given list \n\t\"\"\"\ndef item(lst, index):\n\treturn l"}]}
{"task_id": "62ece4992e6aefcf4aabbd83", "prompt": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif isinstance(command, (tuple, list)):\n\t\tcommands = command\n\telse:\n\t\tcommands = [command]\n\tfor command in commands:\n\t\ttry:\n\t\t\tres = proc.communicate()\n\t\t\tres = res[0]\n\t\t\tif res:\n\t\t\t\tout = out or ''\n\t\t\t\tlogger.debug(out)\n\t\texcept Exception as e:\n\t\t\tout = out or ''\n\t\t\tlogger.error('%s %s' % (str(e), out))\n\t\telse:\n\n\n\n\n#def cmd_stdout(*"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\ttry:\n\t\texecutable = os.path.join(os.path.dirname(os.path.abspath(__file__)), command)\n\t\t#print 'command', command, 'is', executable\n\t\treturn subprocess.call(executable, shell=True)\n\texcept:\n\t\tprint 'command', command, 'is', executable, 'already executed'\n\t\traise\n\n# for some reason, in order to use this command from the interpreter, at least we need to write that file \n# otherwise it will cause an exception and fail the system tests\nexec"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\t#if not os.path.exists(self.outdir):\n\t#\tos.makedirs(self.outdir)\n\n\tlogFile = os.path.join(self.outdir, self.logFile % self.current_dataset)\n\tlogFile_fp\t= open(logFile, \"w\")\n\tlogFile_fp.write(\"Call: %d\\n\" % len(self.commands))\n\n\t#logging\n\tlogFile_fp.write( \"PID\tCPU time\tMax mem\\n\");\n\tlog"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tfor cmd in cmdList:\n\t\tsubprocess.call(cmd, shell=True)\n\ndef main():\n\t# get list of commands\n\tfor line in open(pathToFile):\n\t\tcmd = line.split(\"/\")[-1].rstrip('\\n')\n\t\tcmdList.append(cmd)\n\n\tprint cmdList\n\t# call all given commands\n\ttry:\n\t\tcallCommands(cmdList)\n\texcept subprocess.CalledProcessError as exc:\n\t\tprint 'the command %s failed with error %d:' % (cmd, exc"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tif not options.replay_mode:\n\t\treturn \n\t\n\tif hasattr(options, \"output_file\"):\n\t\toutput = open(options.output_file)\n\t\t\n\t\tprint output.read()\n\t\treturn\n\t\n\t# Get the test file\n\tif self.filename:\n\t\tf = open(self.filename)\n\telse:\n\t\tf = None\n\n\tif self.cmd:\n\t\tcmd = self.cmd\n\telse:\n\t\tcmd = [os.path.join(self.test_dir, self.name + \""}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tdef call(self, command):\n\t\tp = subprocess.Popen(command, shell=True)\n\t\treturn p.wait()\n\n\n\tdef runscriptcmd(self, args):\n\t\t# TODO : remove the call script when we use self.execute\n\t\tif type(args) is list:\n\t\t\tcommand =''.join(['\"%s\"' % x for x in args])\n\t\telse:\n\t\t\tcommand = args\n\n\t\tcommand  = 'python -c \"%s\" >/dev/null 2>&1;{} -c'% (command,"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcmd = \"\"\n\tif isinstance(cmd_str, (list, tuple)):\n\t\t\tcmd_list = cmd_str\n\telse:\n\t\t\tcmd_list = cmd_str.split()\n\treturn_cnt = 0\n\tfor cmd_str in cmd_list:\n\t\tcmd_str = cmd_str.strip()\n\t\t\"\"\"\n\t\tif the command string is too long, the last two characters are ''\n\t\t\t\"\"\"\n\n\t\t#TODO make a more elegant way\n\t\tif len(cmd_str) > 128:\n\t\t\tcmd_str = cmd_"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcmds_in = \"\\n\".join(cmds)\n\n\t#print(\"Running '{}'\\n\".format(cmds_in))\n\tchild = subprocess.Popen(cmds_in, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n\tstdout = child.communicate()[0]\n\nif __name__ == '__main__':\n\tmain ()\n"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tdef __init__(self,command):\n\t\tsuper(MyCmd,self).__init__(command)\n\nclass MyCmds:\n\n\t\"\"\"\n\tBase Class of sub-commands.\n\t\"\"\"\n\n\tdef __init__(self):\n\n\t\tself.myCmds = {\n\n\t\t\t}\n\t\tpass\n\tdef runCmd(self,cmd_name,cmd_args=[]):\n\t\t# TODO [add]\n\n\tdef addCommand(self,cmd_name,cmd_func):\n\t\tself.myCmds.update({\n\t\t\tcmd_name"}, {"generated_text": "import subprocess\n\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcmd = ['nohup', \"python\", sys.argv[0], \"--mode\", \"shell\" ] + sys.argv[1:]\n\tprint 'Running command:', cmd\n\tsubprocess.call (cmd)\n\n\n\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd84", "prompt": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\treturn isIPv4(ipaddress)\n\n\n\ndef isIPv6(ipaddress):\n\t\"\"\"\n\tParses string ipaddress to IPv6 and then determines boolean value\n\t    \n\t\"\"\"\n\ttry:\n        \t#convert IPv6 to IPv4 address\n        \tipaddress = ipv6toIPv4IPaddress(ipaddress)\n\texcept ValueError:\n\t\t#if error throw KeyError\n\t\traise KeyError\n\t#return boolean value\n\treturn isIPv4(ipaddress)\n\n\n\ndef ipv6toIPv4"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\treturn check_ip_address(ip_address)\n\ndef main():\n    \"\"\"\n    \t Main entry \n    \"\"\"\n    ip = input(\"Please enter the IP Address (Ex: 192.168.0.1):\")\n    print(\"Checking IP...\")\n    check_ip_addr = ip_checker(ip)\n    if check_ip_addr:\n        print(\"I have access to this IP Address..\")\n    else:\n        print(\"I can't access this IP address\")\n    \nif __name__ == '__main__':\n    "}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t# IPv4 address pattern:\n\t#\tx.x.x.x\n\t#\n\t# Example of the test string:\n\ttestStr = \"8.8.8.200\"\n\t\n\ttry:\n\t\tIPAddress(testStr)\n\t\taddrNum = 1\n\texcept ValueError:\n\t\taddrNum = 0\n\n\tlog.printInfo(\"\\nTest passed for address %d.\"%addrNum)\nif __name__ == \"__main__\":\n\t#print \"test...\"\n\ttest()\n\n\"\"\"\n\n\"\"\"\n    \tclass Test"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tif not isIPv4address( ipaddress ):\n\t\t    print \"%s is NOT IPv4\" % ipaddress\n\t\t    return False\n\telse:\n\t\t    print \"%s is IPv4\" % ipaddress    \n\t\t    return True\n\t\t    \n'''\n\tipv6 regex\n'''\n    def getIPaddressViaRegex(self, ipv6addr):\n\t\"\"\"\n\t    Takes an ipv6 address as string\n\t    and returns the corresponding ipaddress\n\t\"\"\"\n        try:\n                (ipv6addr,port)=ipv"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\treturn \"IPv4\" in ipaddress\n\t\n\t##################################################\n\t\"\"\"\n\t\tTest if IPv6 address or not\n\t\t    \n\t\"\"\"\n\treturn \"IPv6\" in ipaddress"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n\t#test for ipv4 format only ip4 address\n\ttry:\n\t\tip = str(ipaddress.ip_address(ip))\n\t\treturn True\n\texcept:\n\t\treturn False\n\n\n#\n#\n#\n#\nclass IpFormat:\n    def __init__(self, arg):\n        #parse given argument\n        self.arg = arg\n        self.format()\n     \n    def format(self):\n        if self.is_ipv4(self.arg):\n            self.ipv4 = self.arg\n\n        \n        "}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tdef test_is_ip4_valid(self):\n\t\tself.assertTrue(self.ip4.__class__.__name__=='IPv4')\n\n\"\"\"\n\tTest for IPv6 address\n\t    \n\t\"\"\"\n\tdef test_is_ip6_valid(self):\n\t\tself.assertTrue(self.ip6.__class__.__name__=='IPv6')\n\t\tprint \"----------------------------------\\n\"\n\n\tdef test_is_ip8_valid(self):\n\t\t\"\"\"\n\t\tMake sure we've converted IPv8 to"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\t\n\tdef ifIPv4(input):\n\t\tip = input.split(\":\")\n\t\t#print(\", \".join(ip))\n                if (len(ip)!= 4 and len(ip)!= 7) and(ip[0].find(\".\") == -1):\n\t\t\t  return False\n\t\treturn True\n\t\t\n\t#print(\"test\", ifIPv4(\"192.168.0.1\"))\n\t\"\"\"\n\tTest if IPv6 address or not\n\t    \n\t\"\"\"\n\tdef ifIPv6(input):\n\t\tip = input.split"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\tIPv4 = IPv4Obj[1,2,3,4]  # 4 elements IPv4\n\tIPv6 = IPv4Obj[::]        # 0 elements IPv6\n\tprint \"IPv4 is OK\"\n\tprint \"IPv6 is OK\"\n\tprint (IPv4 in IPv6)\n\n\n\n\t\n\n#--------------------------------------------------------------------\ndef IPTest(IPv4):       # test \n\t\"\"\" Function is used to check if IPv4 is in a given address space\n\t\t \n\t\t    Parameters:\n\t\t    ----------\n\t\t\tIPv4 : IPv"}, {"generated_text": "import ipaddress\n\"\"\"\n\tTest if IPv4 address or not\n\t    \n\t\"\"\"\n\treturn\n\n#\tprint \"ipaddress: \", str(ipaddress)\n#\ta = \"127.0.0.1\"\n#\tprint \"ipaddress1: \", a.findall(ipaddress)\n    \n#-----------------------------------------------------------------------------\ndef getip(){\n\n#\ta = \"127.0.0.1\";\n#            print \"IP Address: \", a.findall(ipaddress)\n\n\ts = ipaddress.split('.');\n\tip = [int(s[0]), int(s[1]), int(s[2]), int(s"}]}
{"task_id": "62ece4992e6aefcf4aabbd85", "prompt": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\t# find roots\n    roots = []\n    for triple in graph.triples((None, prop, parent)):\n        roots.append(triple[1])\n\n    # remove self-loops\n    for root in roots:\n        if root in roots:\n            roots.remove(root)\n\t\n\treturn roots\n\nif __name__ == '__main__':\n\tprint find_roots(g=rdflib.Graph(), rdflib.RDFS.subClassOf)\n"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\troots = set()\n\t\n\t# for each of the roots, recursively do this\n\tfor root in graph.triples((None, rdflib.RDFS.subClassOf, None)):\n\t\troots.add(root)\n\t\troots &= find_roots(graph, rdflib.RDFS.subClassOf, root)\n\t\n\troots.add(None)\n\n\treturn roots\n\nif __name__ == \"__main__\":\n\t\n\timport sys\n\t\n\t# find the sub-class hierarchy of"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tif _has_rdf_type(graph, rdflib.RDFS.subClassOf, parent):\n\t\treturn {child for child, prop, parent in graph[child]}\n\telif _has_rdf_type(graph, skos.broader, parent):\n\t\treturn {child for child, prop, parent in graph[child]}\n\telse:\n\t\t# no subclasses at all, so there is no root there\n\t\treturn set()\n\n\ndef _normalise_value(value):\n\tif value is None:\n\t\t# R"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\tvisitedGraph=dict()\n\tresultRootDict=defaultdict(list)\n\tseenTuple={}\n\tfor triple in g:\n\t\tif triple not in seenTuple:\n\t\t\tseenTuple[triple]=True\n\t\t\t# only search forward links\n\t\t\tnextTriples=set()\n\t\t\tif triple[2]:\n\t\t\t\tnextTriples=g.objects(triple[0],triple[1],triple[2])\n\t\t\telif triple[0]:\n\t\t\t\tnextTriples=g.subjects(triple[0],triple[1])"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\tsubclassof_roots=set()\n\tbroader_roots=set()\n\tfor c in graph:\n\t\tif isinstance(c, BNode):\n\t\t\tcontinue\n\t\tif rdflib.is_empty_node(c):\n\t\t\tcontinue\n\t\tp=graph.predicate_visible_node(c)\n\t\tif rdflib.is_blank(p):\n\t\t\tcontinue\n\t\tif isinstance(p, rdflib.URIRef):\n\t\t\tprint(\"Found blank node {}\".format(p))\n\t\t\tcontinue\n\t\t\n\t\t"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\troots = set()\n\tfor subClass in graph.triples((None, prop, parent)):\n\t\tif subClass[2]!= parent:\n\t\t\troots.add(subClass[0])\n\t\t\troots.update(find_roots(graph, prop))\n\treturn roots\n\ndef generate_graph_from_dag(dag):\n    \"Given the dag, connect each parent node using a new node pointing to it\"\n    graph = Graph()\n    for vertex in dag.vertices:\n        if vertex == None:\n            graph.add((None, r"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\troots = set()\n\twhile graph:\n\t\ttmp = graph\n\t\tfor s, p, o in graph.triples((None, None, None)):\n\t\t\tif not p in roots:\n\t\t\t\troots.add(p)\n\t\t\t#if s not in tmp:\n\t\t\t\t#tmp[s] = {}\n\t\t\tif s not in [s, o]:\n\t\t\t\ttmp[s][p] = o\n\t\t\t\n\t\tgraph = tmp\n\n\treturn roots\n\n\ndef find_roots(triples):\n\tgraph = rdflib.ConjunctiveGraph()\n\tgraph"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\troots = set()\n\tfor s, p, o in graph:\n\t\tif s in roots:\n\t\t\tcontinue\n\t\tif p == rdfs.subClassOf or p==skos.broader: #skip some weird things\n\t\t\troots.add(s)\n\t\t\t\n\treturn roots\n\ndef is_sub_class_of(graph, child, parent):\n\t\"\"\"\n\tIs the child a sub-class of parent.\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.sub"}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\t\n\troots = set()\n\t\n\t# add root nodes to roots\n\tfor triple in graph.triples(subject, property, object):\n\t\tfor root in find_nearest_common_parent(graph, triple): break\n\t\troots.add(root)\n\t\tassert(triple[2] == root)\n\t\n\treturn roots\n\t\ndef find_nearest_common_parent(graph, triple):\n\t# Find the nearest common parent of the child and the parent\n\tchild = graph.value(subject, property, object)\n\tparent = graph."}, {"generated_text": "import rdflib\n\"\"\"\n\tFind the roots in some sort of transitive hierarchy.\n\t\n\tfind_roots(graph, rdflib.RDFS.subClassOf)\n\twill return a set of all roots of the sub-class hierarchy\n\t\n\tAssumes triple of the form (child, prop, parent), i.e. the direction of\n\tRDFS.subClassOf or SKOS.broader\n\t\"\"\"\n\troots = set()\n\tdef recursively_find_parents(child, prop, visited, visitedprop, context):\n\t\tvisitedprop = frozenset((prop, visitedprop))\n\t\tif not prop.is_URIRef():\n\t\t\treturn\n\t\tif prop not in visited:\n\t\t\tif (child, prop, context) not in visited:\n\t\t\t\tvisited.add(child)\n\t\t\t\tif direction =='subClassOf':\n\t\t\t\t\tfor sibling in graph.subjects(prop, context):\n\t\t\t\t\t\tif (sibling, prop2, context) not in visited:\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd86", "prompt": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t# TODO: Support all the data types\n\t# TODO: Allow custom serialization functions\n\t# TODO: Support different encodings\n\t# TODO: Handle other special characters\n\"\"\"\n\tLoad a string in to a yaml object\n\"\"\"\n\t# TODO: Support all the data types\n\t# TODO: Allow custom serialization functions\n\"\"\"\n\tGiven an object create a string from it\n\"\"\"\n\t# TODO: Handle special characters\n\"\"\"\n\tGiven a file-like object, convert the body to a py2-unicode or py3-string\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef process_text(self):\n\t\tif PY2:\n\t\t\tself.nodes_out[-1].text = unicode(self.nodes_out[-1].text)\n\t\telse:\n\t\t\tself.nodes_out[-1].text = str(self.nodes_out[-1].text)\n\t\"\"\"\n\tLoad from a py2-unicode or py3-string\n\t\"\"\"\n\tdef process_tag(self):\n\t\tif PY2:\n\t\t\tself.nodes_in[self.tag_start.value"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tval = value.str_\n\n\tif PY3:\n\t\tif val is None:\n\t\t\traise ValueError(\"Invalid value\")\n\t\t\n\t\treturn val\n\n\tval = unicode(val, \"utf-8\")\n\n\t# In the case of python 2.5 we need to do this conversion\n\t\n\tif not isinstance(val, unicode):\n\t\tval = val.decode(\"utf-8\")\n\n\treturn val\n\t\ndef is_list(value):\n\t\"\"\"Check if given value is a list\n\t\n\tArgs:\n\t\tvalue: value"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef dump(data):\n\t\tif PY2:\n\t\t\treturn to_unicode(yaml.dump(data, default_flow_style=False))\n\t\telse:\n\t\t\treturn yaml.dump(data, default_flow_style=False)\n\n\t#print dir(yaml)\n\treturn dump(data)\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tout = None\n\n\ttry:\n\t\tself.stream = StringIO(self.dump())\n\n\t\t# This is a hack since yaml isn't using `open-string-stream`\n\t\t# so `open-string-stream` tries to open using bytes.\n\t\tif py2:\n\t\t\tfrom yaml.constructor import Constructor\n\t\t\tconvert_stream = StringIO\n\n\t\telse:\n\t\t\tfrom yaml.constructor import SafeConstructor\n\n\t\t\tdef _convert(stream):\n\t\t\t\t\"\"\"SafeConstructor._convert(stream) -> unicode\"\"\"\n\t\t\t\ttry:"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tu='\\u2022\\u2022\\u2022\\u2022\\u2022\\u2022'\n\ty='\\u2022\\u2022\\u2022\\u2022\\u2022\\u2022'\n\tz = dump( u )\n\tz = dump(u) \n\t\"\"\"\n\tDump without the prefixes\n\t\"\"\"\n\ty = dump(y)\n\t# The u-dumping does not output the prefixes:\n\t# u=yaml.load(dump(u, prefix=yaml"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tif isinstance(value,yaml.nodes.ScalarNode):\n\t\treturn value.value.decode(encoding)\n\telse:\n\t\treturn encode_yml_node(value)\n\ndef decode_string(s,**kwargs):\n\t\"\"\"\n\tDecode a unicode/string to a python string. Incoming value must be a\n\tstring (unicode) or a string (bytes) encoded with ASCII\n\t\"\"\"\n\tif isinstance(s,str):\n\t\treturn s.decode(kwargs.get('encoding',DE"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\t\n\tif dataType == \"str\":\n\t\t# unicode or byte string\n\t\tif is_py3k:\n\t\t\treturn str(data)\n\t\telse:\n\t\t\treturn str(data).encode(\"utf-8\")\n\t\n\t# unicode and byte string\n\telif  dataType == \"dict\":\n\t\tassert isinstance(data, dict)\n\t\t\n\t\t# stringify\n\t\td = \"\"\n\t\tfor key,value in data.items():\n\t\t\td = d+ repr(key) + \": \" + repr(value) + \",\\n"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdoc = \"\"\"docstring\"\"\"\n\tdumps(yaml.dump(doc))\n\"\"\"\n---[ YAML DUMP ]-------------------------------------------------------------\n**Example**\n\n$ python3 dump.py --yaml --yaml-string\ndoc: yaml-test\n\"\"\"\n\n\n\"\"\"\n--json-only\ndump-module.json\n---[ JSON DUMP ]------------------------------------------------------------\n**Example**\n\n$ python3 dump.py --json --test-module dump_module_module_1\n\"\"\"\nyaml = yaml_read()\ndoc = {\n\t\"doc\":"}, {"generated_text": "import yaml\n\"\"\"\n\tDump to a py2-unicode or py3-string\n\t\"\"\"\n\tdef dump(value):\n\t\tif isinstance(value, unicode):\n\t\t\treturn encode_unicode(value)\n\t\telse:\n\t\t\treturn str(value)\n\n\treturn dump\n\ndef unicode2str(value, encoding='utf8'):\n\t\"\"\"\n\tConvert unicode string to unicode string and return string\n\t\"\"\"\n\tfrom cStringIO import StringIO\n\timport codecs\n\n\tif not isinstance(value, unicode):\n\t\tvalue = unicode(value)\n\n\tvalue_file = StringIO()\n\tcodecs.look"}]}
{"task_id": "62ece4992e6aefcf4aabbd87", "prompt": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tclass Logger(object):\n\t\tdef __init__(self, log_path,loglevel):\n\t\t\tself.loglevel = loglevel\n\t\t\tl = logging.StreamHandler()\n\t\t\tl.setLevel(logging.DEBUG)\n\t\t\tself.logger = logging.getLogger()\n\t\t\tself.logger.setLevel(logging.DEBUG)\n\t\t\tlog_format = \"%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - %(lineno)d - %(message)-"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.__logger = logging.getLogger(\"logger\")\n\t\tself.__logger.setLevel(logging.DEBUG)\n\t\t# Create a handler and add it to the logger\n\t\tfh = logging.FileHandler(\"log_loggerdebug.log\")\n\t\tfh.setLevel(logging.DEBUG)\n\t\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\t\tfh.setFormatter"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self):\n\t\tlogging.basicConfig(filename='%s/%s.log' % (os.path.join(os.getcwd()),app_name),\n\t\t\t\t\t\t\tformat='%(asctime)s.%(msecs).03d  %(name)s %(levelname)s:  %(message)s',\n\t\t\t\t\t\t\tdatefmt='%m/%d/%Y %I:%M:%S %p',\n\t\t\t\t\t\t\tlevel=logging.INFO,\n\t\t\t\t\t\t\tfilemode='w')\n\t\tself"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self):\n\t\tsuper(Logger, self).__init__()\n\t\tself._logger=logging.getLogger(__name__)\n\t\tself._formatter={'format':'%(asctime)s | %(levelname)s | %(message)s'}\n\t\tself._file=None\n\t\tself._level=logging.DEBUG\n\t\tself.SetLevel(self._level)\n\t\t\n\tdef OpenFileAppender(self, file_name):\n\t\tself._file = os.path.abspath"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self,loglevel=logging.INFO):\n\t\tlogger.setLevel(loglevel)\n\t\tlogger.propagate=False\n\t\t\n\t\t#\n\t\t# Log to a logfile\n\t\t#\n\t\tfh = logging.FileHandler(\n\t\t\t\tos.path.join(os.path.dirname(__file__),'log',\n\t\t\t\t\t\t\ttime.strftime('%Y%m%d-%H%M')+'.log'))\n\t\tfh.setLevel(loglevel)\n\t\tfh.setForm"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self,logLevel=0):\n\n\t\t#logging.basicConfig(level=logging.INFO)\n\n\t\tself.loglevel=logLevel\n\t\tself.filename=\"\"\n\t\tself.fmt=\"%(asctime)s: %(levelname)s - %(message)s\"\n\t\tself.logHandler = self.logWriter = log.logWriter(logFile=self.filename,loglevel=log.loglevel.L_ERROR)\n\n\tdef log(self,loglevel=logging.INFO,message=\""}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\t\nclass Application (object):\n\t\"\"\"\n\tApplication class\n\t\n\tAttributes\n\t----------\n\tlogger: log\n\t\ta generic logger based on an in memory file handler. \n\t\t\n\tNotes\n\t-----\n\tThis is used to have multiple handlers. For simple functions and\n\tbasic errors, simply use a regular logger.\n\t\"\"\"\n\t_lg = None\n\t_error_handler = NullHandler  # default logging. NullHandler handles basic console,\n\t# but this is useful for some systems without console support (e.g., CI)\n\t# _"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__( self,appName=\"log\"):\n\t\t\"\"\"\n\t\tClass constructor\n\t\t\"\"\"\n\t\tself._logger=logging.getLogger(appName)\n\t\tself._logger.setLevel(logging.INFO)\n\t\t#self._logFormatter=logging.Formatter('[%(levelname)1s][%(asctime)s] %(message)s')\n\t\tself._logFormatter=logging.Formatter('%(message)s')\n\t\tself._fileHandler=logging.handlers.Rotating"}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tdef __init__(self, fstring = \"%(module)s %(thread)d %(process)d - %(message)s\",\n\t\t\t\t\tlevel  = logging.DEBUG):\n\t\t\"\"\"\n\t\tLogging setup\n\t\t\"\"\"\n\t\tlogging.basicConfig(filename=\"logfile.log\", level=logging.DEBUG, format=fstring)\n\t\t\nclass LogApp():\n\t\"\"\"\n\tLog an object of type 'e.g. Thread(1).run'\n\t\"\"\"\n\tdef __init__(self, obj, **kwargs):\n\t\t\"\""}, {"generated_text": "import os,logging\n\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n\t\"\"\"\n\tname = \"\"\n\t\n\t# The log file\n\tlog_file = os.path.join(os.getcwd() + \"/log/\")\n\tif not os.path.exists(log_file):\n\t\tos.makedirs(log_file)\n\n\t# The logfile name\n\tlog_file_file = os.path.join(log_file, name + \".log\")\n\tlogging.basicConfig(filename=log_file_file, format='%(asctime)s - %(levelname)s - %(message)"}]}
{"task_id": "62ece4992e6aefcf4aabbd88", "prompt": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\treturn np.ndarray(shape = shape, dtype = dtype)\n\ndef ctype_np2ctype(ctype):\n\treturn ctype[1:]\n\n# ------------------------------------------------------------------\n# ctypes wrappers\n# ------------------------------------------------------------------\n_ndtype_arr = np.dtype([('c_contiguous',np.int8, 16), ('f_contiguous',np.int8, 16)])\n_np_ndtype_arr = np.dtype(np.dtype(_ndtype_arr).subtypes[0][1:])\n\n@context"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tdef __init__(self, *args, **kw):\n\t\tassert len(args) == 1\n\t\tself.shape = args[0]\n\t\tself.dtype = kw.get(\"dtype\", np.float64)\n\t\tself.data = np.empty(self.shape, dtype=self.dtype)\n\t\tself.offset = np.arange(self.shape[0])\n\t\t_ = self.__getattr__(\"set_values\")\n\t\tself.set_values()\n\t\n\tdef set_values(self):\n\t\t\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tdef __init__(self,shape,dtype):\n\t\n\t\tself.shape = tuple(shape)\n\t\tself.dtype = tuple(np.dtype(dt).type for dt in dtype)\n\t\n\tdef copy(self):\n\t\n\t\treturn array(np.array(self),order='F')\n\t\n\tdef _get_dtype(self):\n\t\n\t\treturn self.dtype\n\t\n\tdef __getitem__(self,index):\n\t\n\t\tindex = tuple(index)\n\t\tfor i in range(len"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\treturn np.tile(np.random.rand(*shape), repeats=shape).astype(dtype)"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t\n\treturn np.ones(shape, dtype=dtype)\n\ndef from_list(arr):\n\t\"\"\"\n\tFunction to create an array from list elements\n\t\n\tIt is used to create lists used inside the kernel.\n\t\n\tParameters\n\t----------\n\tarr : list\n\t    list of the length of `shape` parameter\n\t\n\tReturns\n\t-------\n\tnp.array(shape)\n\t    array filled with `arr` \n\t\"\"\"\n\t\n\treturn np.array([arr])\n\n# def from_file(fname):"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tn_dims = len(shape)\n\treturn _make_simple_type(dtype, shape, n_dims)\n\ndef _make_simple_type(dtype, shape, n_dims):\n\t\"\"\"\n\t   Function to wrap the given data-type into a numpy compatible type.\n\t   This function is used to wrap the given data-type into a numpy compatible type. \n\t   This function can be used for any type in Numpy. \n\t   For example. dtype = np.float64 and shape = (2,3"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\t# to allow the use of scalars, array of size (1, 1) is used which gives \"1d\" array\n\tif (shape == (1, 1)):\n\t\treturn np.array((0,), dtype=dtype)\n\n\telif(len(shape)>1):\n\t\tout = np.ones(shape)\n\t\treturn np.array( out, dtype=dtype)\n\n\t# default for size 1 array\n\telse:\n\t\tout = np.array( (0,), dtype=dtype)\n\t\treturn out\n\n\ndef"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\treturn np.zeros(shape).astype(dtype)\n\ndef random(shape, dtype=float):\n\t\"\"\"\n\tFunction to create an array with random data.\n\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tarr = np.zeros(shape)\n\tarr = np.random.chisquare(np.random.rand(len(arr)).astype(float), arr.shape"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tif dtype not in [np.float32,np.float64,np.complex64,np.complex128]:\n\t\traise TypeError(\"Data type not supported\")\n\tarray_type = dtype.type if dtype.type is np.dtype else dtype.str if dtype.type is np.str else dtype\n\t\n\tarray_dtype = np.dtype(array_type)\n\tdata = np.zeros(*shape,dtype=array_dtype)\n\treturn data\n\n\ndef create_tensor(shape,dtype,"}, {"generated_text": "import numpy as np\n\"\"\"\n\tFunction to create an array with shape and dtype.\n\t\n\tParameters\n\t----------\n\tshape : tuple\n\t    shape of the array to create\n\tdtype : `numpy.dtype`\n\t    data-type of the array to create\n\t\"\"\"\n\tarr = np.empty(shape)\n\tarr.dtype=dtype\n\treturn arr\n\ndef read_numpy_data(fpath):\n\t\"\"\"\n\tFunction to read numpy file.\n\n\tParameters\n\t----------\n\tfpath : string\n\t    filename from which to read numpy file.\n\t\"\"\"\n\n\treturn np.load(fpath)\n"}]}
{"task_id": "62ece4992e6aefcf4aabbd89", "prompt": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\n\tx_data = np.linspace(-5, 5, 512)\n\tk = np.exp(-(x_data-0.2) * (x_data - 0.2) / 0.1**2)\n\tk /= k.sum()\n\t\n\treturn k\n\n\ndef main():\n\n\tmf = np.array([[1]])\n\tmf_sigma = np.array([[1]])\n\tmf_data = nb.data.Data(mf, mf_sigma, xy_centers"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tg = np.random.randn(6, 1)\n\tg = (1.5 * g / np.sqrt(6)) + 0.2\n\treturn g\n\n\ndef make_samples(num_samples=1):\n\t\"\"\"Generate some sample data. This is a helper function for making data for\n\t\tthe neural network regression problem used in the regression tests.\n\t\t\"\"\"\n\t# Create the x and y arrays.\n\tx = make_data()\n\ty = np.sin(x[:, 0] ** 3)\n\tx = x."}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tx = np.array([.2 -.1*np.random.randn()])\n\tmock_g = 0.2 +.1*np.random.randn()\n\n\t# Generate a noise signal ~ 1 / (x - x.mean() + 1e-3)\n\ty = 1 / (x - x.mean() + 1e-3) * np.random.randn(100)\n\ty += mock_g\n\n\tplt.step(np.arange(0, y.shape[0]), y, c='red')\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t# Make sure we have 0 > sigma > 10 to avoid numerical problems\n\tsigma = np.zeros_like(theta)\n\tsigma[0] = 0.1\n\tsigma[1:-1] = 5\n\n\n\t# We are going to rotate a grid of points in a spherical coordinate system.\n\tphi = np.zeros_like(theta)\n\tphi[0] = np.pi\n\tphi[1:-1] = -np.pi/2\n\n\ttheta_grid = np.stack([theta, phi], axis"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t# Generate random normal noise\n\tnoise = np.random.normal(0.2, 0.1, (100, 2))\n\n\t# Normalize it\n\tnoise = noise / np.sqrt(noise.shape[0] * np.var(noise))\n\n\t# Reshape the data matrix into a vector\n\ttrain_t = np.reshape(train_data, (-1))\n\tnoise_t = np.reshape(noise, (-1))\n#\n#\t# Fit the model\n#\tm = pdnc"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\n\t#Initialize the array to store the mean and standard deviation\n\tx = np.zeros_like(y)\n\tsigma = np.zeros_like(y)\t\n\t\n\tx[2:] = 0.2\n\tsigma[2:] = 0.05**-1\n\t\t\n\tn = len(x)\n\tm = n/2.0\n\t\n\tg = np.zeros_like(x),1.0\n\tgamma = m**2 \n\t\n\ttau = np.log(np.e"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\t\nclass Data(object):\n\t\"\"\"\n    A data set to be used as a problem in a Bayesian\n    learning algorithm.\n\n    Attributes\n    ----------\n    X : ndarray, shape (n_samples, input_dim)\n        Samples.\n    y : ndarray, shape (n_samples, 1)\n        Outcome values.\n    metric : a metric object. Defines any necessary format of X\n        and y.\n    weights : array-like, shape (n_samples,)\n        Weights.\n    \"\"\"\n\t"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef func(x, a, b, c):\n\t\treturn a * np.exp(-0.1 * (x - b) ** 2)\n\n\ndef do_pca(data, k):\n\t#\n\t#\tperform PCA\n\t#\n\tprint('Performing PCA on data...')\n\t(u, d, v) = np.linalg.svd(data - np.mean(data))\n\tindices = range(k)\n\t#\n\t#\tpick the first k-principal components\n\t#\n\treturn"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tdef __init__(self, sigma=0.1):\n\t\tself.sigma = sigma\n\t\tself.mu = 0.2  # Gaussian by definition is centered around 0.2\n\t\tself.n = 100000\n\n\tdef sample(self):\n\t\tz = np.random.normal(size=self.n)\n\n\t\treturn self.mu + z * self.sigma\n\n\tdef sample_p(self, x):\n\t\treturn np.exp(-(x - self.sample()) ** 2 / (2. * self.sigma ** 2))\n\n"}, {"generated_text": "import numpy as np\n\"\"\"\n\tGaussian centered around 0.2 with a sigma of 0.1.\n\t\"\"\"\n\tmu = 0.2\n\tsigma = 0.1\n\tn = 1024\n\t\n\tX = np.random.randn(n, 1)*sigma+mu\n\tplt.plot(X)\n\t\t\n\"\"\"\n\tDraw n samples from a Gaussian with mean 0.1 and sigma 0.2, sample them and \n\tdisplay the mean and standard deviation of samples drawn.\n\t\"\"\"\n\tn = 100\n\tmu = 0.1\n\tsigma = 0.2\n\tX = np.empty(n)\n\tfor i in range"}]}
{"task_id": "62ece4992e6aefcf4aabbd8a", "prompt": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tout = []\n\tconfig_logger = logging.getLogger('config')\n\tconfig_logger.setLevel(logging.DEBUG)\n\tconfig_formatter = logging.Formatter(\"%(levelname)s:%(message)s\")\n\n\tfor fil, fd in files:\n\t\tcfg = load_config_file(fil)\n\t\tout.append(cfg)\n\t\ttry:\n\t\t\trds = validate_config(cfg)\n\t\t\tconfig_logger.debug(\"Parsed configuration in %s:%s\", fil, rds)"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\ttry:\n\t\tfor name in sequence:\n\t\t\tparser = ConfigParser.SafeConfigParser()\n\t\t\tparser.read(name)\n\t\t\tres = parse_config_with_parser(parser)\n\n\t\treturn res\n\texcept:\n\t\tlg.exception(\"Error processing config files:\")\n\t\traise\n\nclass ConfigFileParser:\n\t\"\"\"Given a parser, parse the configuration stored in that file. Return the results as a dict\n\tof configuration filename to corresponding parsed configuration.\"\"\"\n\tdef __init__(self, filename, debug_level):\n\t\tself.filename = filename"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlogger = logging.getLogger(__name__)\n\t\n\t# List of configuration filenames passed in to program\n\tfiles = []\n\ttry:\n\t\t# Get config file names with full paths from arguments\n\t\t# TODO: get command line arguments and add them to the list instead of parsing command line at every iteration.\n\t\tfiles = [line.strip()\n\t\t\t\tfor line in sys.argv[:]\n\t\t\t\tif (line.startswith('-c') or line.startswith('--config-file'))\n\t\t\t\tand not line.startsw"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlogger = logging.getLogger(__name__)\n\tloaders = OrderedDict()\n\t# First, load the base configurations, which are expected to exist\n\t# in the current working directory and all other configuration\n\t# paths\n\tfor path in config_paths:\n\t\tfilename = os.path.basename(path) if os.path.basename(path).endswith('.config') else path + '.config'\n\t\tif config_validator_class:\n\t\t\tloaders[filename] = config_validator_class(path)\n"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlog = logging.getLogger(\"config\")\n\tlog.addHandler(logging.NullHandler())\n\tresults = {}\n\terrors = []\n\tfor filename in filenames:\n\t\ttry:\n\t\t\tlog.debug(u\"parsing configuration file {}\".format(filename))\n\t\t\tresults[filename] = load_config_with_errors(filename)\n\t\texcept Exception as e:\n\t\t\tlog.debug(u\"configuration validation error: \" + str(e))\n\t\t\terrors.append(logging.LogRecord(\"validation error\", logging.ERROR,"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlog.info(\"loading config data... %s\", \", \".join(config_filenames))\n\n\tconf = {}\n\tconfig_records = []\n\terrors = []\n\n\tfor i, filename in enumerate(config_filenames):\n\t\tparsed = load_config(filename)\n\t\tconf[filename] = parsed\n\t\tif parsed!= None and isinstance(parsed, dict):\n\t\t\tconfig_records += [LOG_FORMAT % (i + 1, filename, log_level_to_text[parsed.get(\"log_level"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlogging.basicConfig()\n\tlog = logging.getLogger(\"config\")\n\tlog.setLevel(logging.INFO)\n\n\tconfigurations = []\n\terrors = []\n\tfor filename in files:\n\t\tconfig = loadConfiguration(filename, log)\n\t\tif config is not None:\n\t\t\tconfigurations.append(config)\n\n\tif len(configurations) == 0:\n\t\traise Exception(\"No configurations found\")\n\treturn configurations, errors\n\n\ndef loadConfiguration(filename, log):\n\t\"\"\"Given a filename string, attempt to parse and validate the configuration using"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlogging.basicConfig(level=logging.INFO, format='%(asctime)s <%(name)s:%(lineno)d> %(levelname)s %(message)s')\n\tparsed_configs = []\n\tfailed_parsed_files = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = Config(filename)\n\t\t\tConfig.check_validity(config)\n\t\t\tif filename not in parsed_configs:\n\t\t\t\tparsed_configs.append(filename)\n"}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tclass_logger = logging.getLogger('configuration')\n\t\n\tfilenames = map(filepaths.normalize_fn, filenames)\n\t\n\tall_configs = {}\n\tall_logs = []\n\tfor fn in filenames:\n\t\tdata = parse_config_file(fn)\n\t\tif data is None:\n\t\t\tcontinue\n\t\tif'version' in data:\n\t\t\tclass_logger.info('%s: loading from %s, version %s',\n\t\t\t\t\t\t\t  fn,\n\t\t\t\t\t\t\t  os.path."}, {"generated_text": "import logging\n\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. Return\n\tthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n\tand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tdef __init__(self, config_file_paths, validate=True):\n\t\tself.config_paths = [os.path.abspath(x) for x in config_file_paths]\n\t\tself.configs_map = defaultdict(lambda:None)\n\t\tself.validations = defaultdict(list)\n\t\tself.parse_errors = defaultdict(list)\n\t\tself.validate_files(validate)\n\n\t@staticmethod\n\tdef new_from_string(config_dict_str, validate=True, allow_new"}]}
{"task_id": "62ece4992e6aefcf4aabbd8b", "prompt": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t", "suggestions": [{"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\t# Not a string, assume it is a byte object\n\treturn obj\n\n\ndef to_binary(tokens, encoding='utf-8'):\n\t\"\"\"\n\tConverts a :class:`list` of values **tokens** to a bytearray.\n\n\tThe *tokens* parameter is converted into bytes using the \n\t:func:`utf_8_string` converter. For efficiency, if the\n\tconverter is the system's default"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif type(obj) == bytes:\n\t    return obj\n\n\tif type(obj) == type(str):\n\t\tobj = obj.encode('utf-8')\n\n\treturn bytes(obj, 'utf-8')\n\n\n\"\"\"\n\tThis function loads object ``obj``, an instance of pkgutil.get_data \n\tor pkgutil.get_data_path.\n\t\"\"\"\ndef loadFile(obj):\n\t\"\"\" \"\"\"\n\tpath = pkgutil.get_data_path(__name__, os.path.join(\"files\", obj"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\treturn bytes(obj, \"utf-8\")\n\ndef remove_dquote(s): \n\t\"\"\"\n\tGiven a string, remove any 'dquote' embedded in the bytes array.\n\n\t\"\"\"\n\t# TODO: Is there more efficient way to do this?  \n\tif s.startswith('\"'.encode()): return s[1:].decode()\n\treturn s\n\n# TODO: This was made into a generator at the end of the file. \ndef build(fname, t):\n\tfor line in fname:\n\t\t"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\ttry:\n\t\treturn obj\n\texcept:\n\t\treturn obj.encode('utf-8')\n\ndef safe_decode(encoded):\n\t\"\"\"\n\tThis function calls ``safe_decode`` on the single string ``encoded``\n\tand raises an exception unless everything goes well.\n\t\"\"\"\n\ttry:\n\t\treturn safe_decode(encoded)\n\texcept:\n\t\terr = exc_info()[1][0]\n\t\traise SafeDecodeError(err)\n\nclass SafeDecodeError(StandardError):\n\tpass"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tb = bytes(obj, 'utf-8')\n\tif isinstance(b, bytes):\n\t\treturn b\n\telse:\n\t\traise TypeError(\"Encoding is not supported: \" + \n\t\t\t\t\"(%s, %s)\" % \n\t\t\t\t(type(b), type(obj)))\n\ndef safe_str(obj):\n\t\"\"\"\n\tThis function returns the string object corresponding to ``obj``\n\tin case it is a string. \n\t\"\"\"\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode()\n\telse:\n\t\t"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif hasattr(obj, \"decode\"):\n\t\treturn bytes(obj.encode(\"utf-8\"))\n\treturn bytes(obj)"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif isstring(obj):\n\t    return bytes_to_str(obj).encode(\"utf-8\")\n\telif hasattr(obj, \"__getitem__\") and isstring(obj[0]):\n\t    if isstring(obj[1]):\n\t        return bytes_to_str(obj).encode(\"utf-8\")\n\t    else:\n\t        return bytes(obj)\n\telif hasattr(obj, \"__getitem__\") and isstring(obj[0]):\n\t    if isstring(obj[1]"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\tif not hasattr(obj, 'encode'):\n\t\tif isinstance(obj, str):\n\t\t\tobj = obj.encode('utf-8')\n\t\telse:\n\t\t\traise TypeError(\"%s is not a string type\" % obj)\n\treturn bytes(obj, \"utf-8\")\n\ndef stringlist2byteslist(s):\n\t\"\"\"\n\tReturns the bytes list corresponding to a string list\n\tor a string. The bytes objects will have unicode characters\n\ttranscoded if they're the first character is ASCII and the rest\n\tare not ASCII ("}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t# make sure we handle bytes object\n\tif not isinstance(obj, bytes):\n\t\treturn bytes(obj)\n\t# convert string to a byte object\n\tobj = str(obj, \"UTF-8\")\n\t# pad with the appropriate number of bytes \n\t# this is necesary due to Python being stupid around binary data \n\tobj += b\"\\0\"*(0x100-len(obj))\n\t# return bytes object\n\treturn obj\n\n\n\"\"\"This function returns a hex dump as a bytearray of ``obj``\"\"\"\ndef bh"}, {"generated_text": "import numpy\n\"\"\"\n\tThis function returns the bytes object corresponding to ``obj``\n\tin case it is a string using UTF-8. \n\t\"\"\"\n\t\nif type(obj) == str and not PY3:\t\n\ttry:\n\t\tutf = True\n\t\treturn str(obj, encoding=\"utf-8\")\n\texcept TypeError:\n\t\tutf = False\n\tif not utf: return bytes(obj)\n\n\ndef encode_str(s):\n\t\"\"\"Utility function to encode a string in byte stream\"\"\"\n\tif sys.version_info>=(3,0):\t\n\t\tret = bytes(s, encoding=\"utf-8\")\n\telse:\n\t\tret = bytes(s)\n\treturn ret"}]}
{"task_id": "62e60723d76274f8a4026b76", "prompt": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn cls(ticks, tz)"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn cls(round(ticks / 1000000000), tz)\n\ndef utc(ticks=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\n\t:return: a datetime.time\n\t\"\"\"\n\tif ticks is None:\n\t\tticks = time.time()\n\treturn Time(ticks)\n\ntz_utc = datetime.timezone.UTC\n\n\n@to_string\ndef to_string(time):"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tt = time()\n\tt.__ticks = ticks\n\tif tz is not None:\n\t\tdelta = tz.utcoffset(t.to_datetime().toordinal())\n\t\tt = Time(t.toordinal() + delta.days * 60 * 60 * 24 + delta.seconds + delta.microseconds / 1e6)\n\telse:\n\t\tt = t.toordinal()\n\treturn t\n\ndef to_ticks(t):\n\t\"\"\"\n\t\n\t\"\"\"\n\tif t is None:\n\t\treturn"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\treturn cls(ticks, from_timezone(tz))\n\t\n# TODO: rename to to_ticks?\ndef to_ticks_time(self):\n\tif not self.is_time:\n\t\traise ValueError(\"Given value wasn't a time!\")\n\treturn self.ticks_since_midnight\n\t\ndef to_ticks(self):\n\treturn self.to_time().ticks\n\ndef to_datetime(self):\n\tself.to_ticks_time()\n\treturn datetime.datetime.from"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t# TODO: ref: http://www.mired.com/Ticker/Time.aspx\n\t\n\tfrom datetime import timedelta\n\tif ticks > cls.MAX:\n\t\traise ValueError(\"ticks out of bounds: {0}\".format(ticks))\n\t\n\ttd = timedelta(0, ticks)\n\treturn cls.from_timedelta(td, tz)\n\n@class_method\ndef from_unix_time(cls, time):\n\t\"\"\"\n\tCreate a time from Unix time (seconds since epoch)."}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif ticks < (86400000000000 or 0):\n\t\traise ValueError(\"Invalid ticks value: {}; must be >= 86400000000000\"\n\t\t\t.format(ticks))\n\t\n\tif tz == None:\n\t\ttz = time.tzlocal()\n\t\n\ttt = cls.from_microsecond(ticks // 1000000, tz)\n\ttt.nanoseconds += ticks % 1000000\n\t\n\treturn tt\n\ndef to_ticks(delta):\n\t\"\"\"\n\tConvert a duration to ticks (nanoseconds)."}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t# normalise\n\tticks &= TICKS_FREQUENCY - 1\n\t\n\t# convert nanosecond to day\n\tticks = ticks // TICKS_PER_DAY\n\t\n\t# determine tz\n\tif tz is None:\n\t\tnow = datetime.datetime.now()\n\t\tnow_tz = datetime.datetime.fromtimestamp(now.second + now.minute * 60 + now.hour * 3600)\n\telse:\n\t\tnow_tz = datetime.datetime.fromtimestamp(time"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tif (not 0 <= ticks < 86400000000000):\n\t\traise ValueError('Ticks out of bounds.')\n\tif (tz is None):\n\t\treturn cls(ticks)\n\treturn cls(ticks, tz=tz)\n\t\n\t\nclass BaseTime(long):\n\tdef to_ticks(self, tz=None):\n\t\treturn self.val\n\nclass Midnight(BaseTime):\n\tdef __new__(cls, tz=None):\n\t\t# if (not 0 <= midnight_ns <= 86400000000000):"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\t\n\t# TODO: use tz.utcfromtimestamp(ticks / 1e7) instead of datetime.utcfromtimestamp()\n\t# The latter function will use the user zone instead of UTC.\n\t# datetime.utcfromtimestamp is deprecated.\n\t# See: https://docs.python.org/2/library/datetime.html#datetime-tz-and-tzinfo-objects\n\t\n\tif ticks < 0 or ticks >= cls.MAX_TICKS:\n\t\traise ValueError(\"ticks out of range:"}, {"generated_text": "def from_ticks(cls, ticks, tz=None):\n\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\t\n\t:param ticks: nanoseconds since midnight\n\t:type ticks: int\n\t:param tz: optional timezone\n\t:type tz: datetime.tzinfo\n\t\n\t:rtype: Time\n\t\n\t:raises ValueError: if ticks is out of bounds\n\t    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tcls.check(ticks)\n\t\n\t_min = int(((2 ** 6) * (1000000000 * 60 * 60)) - 1)\n\t_max = int(((2 ** 6) * (100000000000 * 60 * 60)) - 1)\n\t\n\treturn cls(int(ticks * 86400000000000.0 / 1000000000.0), tz)\n\t\nclass Clock(object):\n\t\"\"\"\n\tManages the time for this process.\n\t\n\t:param tz: optional timezone\n\t:type tz:"}]}
{"task_id": "62e60873d76274f8a4026bd8", "prompt": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thdlrs = dict()\n\tif any([isinstance(_, tuple) for _ in hdlrs]):\n\t\traise TypeError(\"Protocol version must not be a tuple instance!\")\n\tif protocol_version is None:\n\t\thdlrs = dict(hdlrs)\n\t\tprotocol_handler_by_version = hdlrs.values()\n\telse:\n\t\thdlrs = dict(hdlrs, **{protocol_version: handlers[protocol_version]})\n\t\tprotocol_handler_by_version = hdlrs.values()"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\th = {}\n\tfor handler_class in cls.__subclasses__():\n\t\th.update(handler_class.protocol_handlers(protocol_version))\n\treturn h\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\t\n\tdef add_handlers(tuple_version, hand):\n\t\t\"\"\"\n\t\tAdd a new protocol handler to the specified tuple\n\t\tif it is compatible with the current bolt version.\n\t\t\n\t\t:param tuple_version: tuple of protocol version\n\t\t:param hand: new hand to add\n\t\t:raise TypeError: if tuple_version is passed not in tuple format\n\t\t\"\"\"\n\t\tif tuple_version:\n\t\t\tif len(tuple_version)!= 2:\n\t\t\t\traise TypeError\n\t\t\tif tuple_version not in cls._all_versions:\n\t\t\t\tcl"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tret = {}\n\tif protocol_version is not None:\n\t\tassert(3 <= len(protocol_version) <= 3),\"Protocol version must be 3, 5, or None\"\n\t\tret.setdefault(protocol_version, None)\n\telse:\n\t\tfor v in cls.versions:\n\t\t\tret.setdefault(v, None)\n\t\t\n\treturn ret\n\ndef protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a list of handlers\n\t\"\"\"\n\thandlers = []\n\tfor handler_class"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_handlers = dict()\n\n\tfor version in [protocol_version] if protocol_version is not None else sorted(cls._versions):\n\t\tfor protocol_handler_cls in list(cls._protocol_handlers[version]):\n#\t\t\tcls._initialized = False\n\t\t\tprotocol_handlers[version] = protocol_handler_cls\n\t\n\treturn protocol_handlers\n\t\n\t\nclass BoltConnection(object):\n\t\"\"\"\n\tA connection object to open a Bolt connection to a remote server.\n\t\"\"\"\n\tdef"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandlers = {}\n\tfor handler in cls.__subclasses__():\n\t\tif not protocol_version or handler.PROTOCOL_VERSION <= protocol_version:\n\t\t\thandlers.update(handler.PROTOCOL_HANDLERS)\n\t\tif protocol_version:\n\t\t\thandlers.clear()\n\tif protocol_version:\n\t\thandlers[cls.PROTOCOL_VERSION] = cls\n\treturn handlers\n\t\t\t\n\nclass _IncomingConnection(object):\n\t\"\"\"\n\tHandle messages coming in from users.\n\t\"\"\"\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tif protocol_version is not None:\n\t\tversions = [protocol_version]\n\telse:\n\t\tversions = cls.supported_protocol_versions()\n\t\tif not versions:\n\t\t\traise TypeError(\"No version tuple specified\")\n\n\thandlers = dict()\n\tfor version in versions:\n\t\thandler_class = cls.get_protocol(version)\n\t\thandlers[version] = handler_class\n\treturn handlers\n"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandlers = {}\n\tsupported_versions = cls.protocol_versions()\n\n\tif protocol_version is not None and not isinstance(protocol_version, (tuple, s_version.VersionTuple)):\n\t\traise TypeError('version argument must be a tuple with the '\n\t\t\t'tuple representation of a Bolt protocol version (2, 3, 4,...); '\n\t\t\t'instead got {!r}'.format(protocol_version))\n\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple) or not"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandlers = {}\n\tfor version, handler_cls in cls.PROTOCOL_HANDLERS.items():\n\t\thandlers.setdefault(version, handler_cls)\n\tif protocol_version is not None:\n\t\tif protocol_version not in handlers:\n\t\t\traise TypeError(\"unsupported protocol version: %s\" % (version,))\n\t\thandlers[protocol_version.major] = handlers[protocol_version.minor]\n\treturn handlers"}, {"generated_text": "def protocol_handlers(cls, protocol_version=None):\n\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\n\tkeyed by version tuple. If an explicit protocol version is\n\tprovided, the dictionary will contain either zero or one items,\n\tdepending on whether that version is supported. If no protocol\n\tversion is provided, all available versions will be returned.\n\t\n\t:param protocol_version: tuple identifying a specific protocol\n\t    version (e.g. (3, 5)) or None\n\t:return: dictionary of version tuple to handler class for all\n\t    relevant and supported protocol versions\n\t:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tresult = {}\n\tif protocol_version is None:\n\t\tfor version in cls.PROTOCOL_VERSIONS:\n\t\t\tif version is not None:\n\t\t\t\tresult[version] = cls\n\t\t\telse:\n\t\t\t\tresult.update({\n\t\t\t\t\tversion: cls\n\t\t\t\t})\n\telif isinstance(protocol_version, (tuple, list)):\n\t\tif len(protocol_version) == 2 and all(isinstance(e, int) and e >= 0 for e in protocol_version):\n\t\t\tfor protocol_version in cls.PROT"}]}
{"task_id": "62e60e3bd76274f8a4026d1a", "prompt": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\tfor item in values:\n\t\tif len(item) > 0:\n\t\t\tyield cls.from_raw_string(item)\n\t\nBookmark.from_raw_values = from_raw_values\n\ndef from_raw_string(line):\n\t\"\"\"\n\tCreate a Bookmark object from a single string value.\n\t\n\tA Bookmark is just a URL.\n\t\n\t:param line: ASCII string values (raw bookmarks)\n\t:param type: Bookmark to create\n\t\"\"\"\n\t\n\treturn Bookmark(line)\n\n"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\tbookmarks = []\n\t\n\tfor string in values:\n\t\tkey, text = string.decode('ascii').split(':', 1)\n\t\tbookmarks.append(cls(key, text))\n\t\n\treturn bookmarks\n\t\n\t\nclass Bookmark(object):\n\t\"\"\"\n\tBookmark object represents a bookmark in the file system.\n\t\n\tBookmarks contain a label and a URL, and are considered immutable.\n\tTo update values or create a new bookmark, use the :func:Bookmarks.edit_one() and :func"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn cls(values)\n\n_BookmarksBuilder = collections.namedtuple('BookmarksBuilder','source')\n\nclass _BookmarksParser(object):\n\t\"\"\"\n\tHolds a parser object that is provided by the user and can extract values\n\tfrom the raw source string.\n\t\n\t:param parser: The parser object\n\t:type parser: Parser\n\t\"\"\"\n\tdef __init__(self, parser):\n\t\tself._parser = parser\n\t\treturn\n\t\n\t@classmethod\n\tdef from_ascii_values(cls,"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tif not values:\n\t\treturn None\n\t\t\n\tbookmarks = cls()\n\t\n\tif len(values) == 0:\n\t\treturn bookmarks\n\t\n\tfor value in values:\n\t\tbookmarks.add(Bookmark.from_raw_value(value))\n\t\n\treturn bookmarks\n\n\ndef get_values(bookmark):\n\t\"\"\"\n\tGenerate a list of raw string values from a Bookmark object.\n\n\t:param bookmark: Bookmark to serialize\n\t:type bookmark: Bookmark\n\t:returns: List of serialized bookmark"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\treturn cls()+cls(values)\n\n@reify\ndef _from_json(bookmarks):\n\t\"\"\"\n\t\tCreate an immutable Bookmarks object from JSON text.\n\t\"\"\"\n\t\n\tbookmarks = Bookmarks._from_raw_values(bookmarks['values'])\n\t\n\t# add bookmarks to a cache\n\timport pytoml\n\tdata = pytoml.load( bookmarks.encode('utf-8') )\n\tif data:\n\t\tBookmark.cache.update(data)\n\t\t\n\treturn bookmarks"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\t\n\tdef parse_bookmark_row(idx, row):\n\t\tidx += 1\n\t\treturn Bookmark(idx, row[4:idx], idx+1, row[idx:])\n\t\n\treturn cls(dict((parse_bookmark_row(idx, value) for value in values))).sort()\n\nif __name__ == '__main__':\n\t# test code\n\timport pytest\n\timport os\n\tfrom. import _read_line, write_line\n\tfrom. import save_bookmark_file"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn cls(values)\n\ndef get_bookmark(values, query=None):\n\t\"\"\"\n\tConvenience method to create and get a Bookmark object or a BookmarkInfo object.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t:param query: An optional Query object.\n\t:type query: Query\n\t:rtype: Bookmarks|BookmarkInfo\n\t\"\"\"\n\tfrom bookie.web.auth import BmarkAuth\n\tbmark = Bookmarks(values, query"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tbookmarks = []\n\t\n\tfor raw_value in values:\n\t\tbookmarks.append(Bookmark(*raw_value.split(\" \")))\n\t\t\n\treturn cls(*bookmarks)\n\n\nclass Bookmark(object):\n\t\"\"\"\n\tEncapsule a bookmark from the user\n\t\"\"\"\n\tdef __init__(self, uri, display, title, mime_type, thumbnail=None):\n\t\t\"\"\"\n\t\tInit a Bookmark object.\n\t\t\n\t\t:param uri: The URI associated with a bookmark\n\t\t:type uri: str\n"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\treturn cls(values)\n\t\nclass BookmarkItem(object):\n\t\"\"\"\n\tBookmarkItem contains the information on a bookmark.\n\t\"\"\"\n\t\n\tdef __init__(self, values):\n\t\t\"\"\"\n\t\t:param values: values from sqlite\n\t\t:type values: dict\n\t\t\"\"\"\n\t\tself.values = values\n\t\n\t@staticmethod\n\tdef from_raw_values(values):\n\t\t\"\"\"\n\t\tStatic method for creating BookmarkItem objects from values.\n\t\t\"\"\"\n\t\tbookmark = BookmarkItem(values)\n\t\treturn"}, {"generated_text": "def from_raw_values(cls, values):\n\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\n\tYou should not need to use this method unless you want to deserialize\n\tbookmarks.\n\t\n\t:param values: ASCII string values (raw bookmarks)\n\t:type values: Iterable[str]\n\t\"\"\"\n\tpvalues = list()\n\tfor value in values:\n\t\t\ttry:\n\t\t\t\tpvalues.append(cls.deserialize(value))\n\t\t\texcept TypeError as e:\n\t\t\t\traise BookmarksError('Invalid raw bookmark value %s: %s' % (value, e))\n\t\n\treturn cls(values=pvalues)\n\t\nclass BookmarksError(RuntimeError):\n\t\"\"\"\n\tRaised by bookmarks if there is an internal problem.\n\t\"\"\""}]}
{"task_id": "62b87b199a0c4fa8b80b354c", "prompt": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif type(seq)!= type([]):\n\t\t# if it's not a sequence\n\t\t# return it directly\n\t\treturn seq, type(seq)\n\n\telif seq and (isinstance(seq[0], int) or isinstance(seq[0], float)) and (bufsize is not None and seq.count >= bufsize):\n\t\t# sequence has only numbers and was big enough\n\t\t# => it's a numpy array\n\t\t# return it directly\n\t\treturn seq[:bufsize], type(seq[:bufsize])\n\n\telse:\n\t\t# first,"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, basestring):\n\t\tif not _get_codec():\n\t\t\tseq = seq.decode('ascii')\n\t\tseq = seq.split()\n\telif not isinstance(seq, Sequence):\n\t\traise TypeError('only a string object and an instance of\n\t\t\t\t\t\t\tSequence can be returned')\n\tif seq and isinstance(seq[0], (str, unicode)):\n\t\tseq = map(seqifier(seq[0]), seq[1:])\n\tif bufsize:\n\t\tseq = [s[:bufsize"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, collections.Sequence):\n\t\t# If a sequence type, use that\n\t\tseq = seq\n\telse:\n\t\tseq = convert_to_sequence(seq)\n\tif isinstance(seq, collections.Iterable) and bufsize:\n\t\treturn _get_seq_with_type(list(seq), bufsize)\n\telse:\n\t\treturn (_get_seq_with_type(collections.Iterable(seq), bufsize), type(seq))\n\n\ndef get_fmt_datetime(fmt=\"datetime\", time_"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif is_sequence(seq):\n\t\treturn seq, SEQ_SEQUENCE\n\tif hasattr(seq, '_getbuf'):\n\t\timport buffering\n\t\treturn seq.__class__(seq.getbuf()), SEQ_BUFFER\n\ndef _check_type(seq, args):\n\t\"\"\"\n\t\tReturn the arguments (or arguments[0] if sequence is given) and\n\t\tits type.\n\t\"\"\"\n\ttype_ = get_type(seq)\n\ttype_or_arg = type_\n\tif type_ == SEQ_BU"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, bytes):\n\t\tif bufsize:\n\t\t\ts, seq  = u(seq[:bufsize]), u(seq[bufsize:])\n\t\telse:\n\t\t\ts = u(seq)\n\telif isinstance(seq, str):\n\t\t#seq is a string.\n\t\t#if we were told to just get bufsize, return as is.\n\t\t#else, get the buffer size.\n\t\tif bufsize:\n\t\t\ts = seq\n\t\telse:\n\t\t\ts, seq  = u(seq), u(seq)\n\telse"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif bufsize is None:\n        \tif is_type(seq, SequenceTypes): bufsize = max(len(seq) * 2, 40)\n        \telse: bufsize = len(seq)\n\n        if len(seq)>0:\n                try:\n                \tif isinstance(seq, list) or isinstance(seq, tuple):\n                                seq = list(seq)\n                        \tbuffer = ctypes.create_string_buffer(bufsize)\n                        \tctypes.memset(buffer, 0xdd, bufsize)\n                        \toffset = 0\n"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttype = None\n\tif isinstance(seq, collections.Mapping):\n\t\t\treturn (Dict, type), seq\n\telif isinstance(seq, collections.Iterable) and not isinstance(seq, basestring):\n\t\t\tif isinstance(seq, unicode):\n\t\t\t\tfor s in seq:\n\t\t\t\t\tif type is None:\n\t\t\t\t\t\ttype = isinstance(s, int)\n\t\t\t\t\telse:\n\t\t\t\t\t\tif not isinstance(s, int) and type is None:\n\t\t\t\t\t\t\ttype = False\n\treturn (type, Dict), seq\n\n\t\ndef is_"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tcls = type(seq)\n\tif issubclass(cls, str):\n\t\treturn seq, int\n\tif issubclass(cls, int):\n\t\treturn seq, int\n\tif issubclass(cls, bool):\n\t\treturn seq, int\n\tif issubclass(cls, tuple):\n\t\traise TypeError(\n\t\t\t\"Don't know how to convert %s to a %s\"%(\n\t\t\t\tcls, get_seq_type_name(seq)))\n\traise TypeError(\"Don't know how to convert '%s'"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, self.array.types):\n\t\treturn seq, None\n\telif isinstance(seq, (tuple, list)):\n\t\treturn str(seq), list\n\telif isinstance(seq, (int, float, long)):\n\t\tif bufsize is not None:\n\t\t\treturn unicode(seq), unicode\n\t\telse:\n\t\t\treturn str(seq), str\n\telif issubclass(seq, self.array.types):\n\t\treturn self._get_seq_with_type(seq, bufsize)\n\telse:"}, {"generated_text": "def _get_seq_with_type(seq, bufsize=None):\n\"\"\"\n\tReturn a (sequence, type) pair.\n\tSequence is derived from *seq*\n\t(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tif isinstance(seq, types.GeneratorType):\n\t\tseq = list(seq)\n\n\ttry:\n\t\tnext_seq = next(seq)\n\texcept StopIteration:\n\t\tif isinstance(seq, types.GeneratorType):\n\t\t\tseq = (seq.send(), None)\n\t\telse:\n\t\t\traise StopIteration\n\ttry:\n\t\tnext_type = type(next_seq)\n\texcept TypeError:\n\t\tnext_type = type(seq)\n\telse:\n\t\tnext_type = getattr(next_seq, '__next"}]}
{"task_id": "62b87b4f9a0c4fa8b80b3581", "prompt": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\treturn self.__hist.__scale\n\telse:\n\t\tself.__hist.__scale = float(other)\n\t\tif reconfigure:\n\t\t\tself.__hist.recompute()\n\treturn self.__hist.__scale\n\t\ndef _get_scale(self):\n\tassert isinstance(self, IHist), \"get_scale called on non-histogram\"\n\tif self.__hist.scale == 0.:\n\t\traise LenaValueError(\"0 scale is ambiguous\")\n\telse:\n\t\treturn self.__"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\treturn (self.bins*(self.bin_width or 1)) + self.offset\n\tif other < 0:\n\t\traise TypeError(\"Given value '{}' is < 0\"\n\t\t\t\t\t\t.format(other))\n\tvalue = other*(self.bin_width or 1)\n\t# Check if bin_width was computed by cnt()\n\tif hasattr(self, 'cnt'):\n\t\tvalue = float(value)/(self.cnt or 1)\n\tif self.offset:\n\t\tvalue += self."}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other is None:\n\t\tself._hist.scale = self.hist_func(self._hist.bins)\n\t\tif self._hist.scale == 0.0:\n\t\t\traise ValueError(\"Can't rescale a histogram with 0 bins\")\n\t\t\n\t\treturn self._hist.scale\n\t\n\telif recompute:\n\t\tself.resize(n=0.0, recompute=False)\n\t\n\telse:\n\t\tpass\n\t\t\n\tself._hist.scale = other\n\treturn self._hist.scale\n\n\n# --------------------------------------------------------------------\n#"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\tself._make_histo()\n\t\n\t# get scale from xaxis, if not provided\n\tif not other:\n\t\t# store scale in memory in case needed again\n\t\tif not hasattr(self, '_scales'): self._scales = [ self.xaxis.integral() ]\n\t\tscale = self._scales.pop()\n\t\t\n\t\t# ensure numerical results are stored correctly\n\t\tassert np.isclose(self.xaxis.integral(), scale) and np.isfinite(self.xaxis.integral())\n\t\t\n\t\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tself.__recompute()\n\t\n@_H.property(HistogramScale)\ndef __recompute(self):\n\t\t\t\n\t# scale of 1.0 means no change, not even an implicit 0..0 one\n\t# so we need to compare histograms with the same scale\n\tif self == HistogramScale(1.0) or \\\n\t\t(self._histo is not None and \\\n\t\tall(float(value) == float(self._histo.at(value)) for value in self._histo) and \\\n\t\tself._bins"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tif other == None:\n\t\treturn self.scale\n\tif self.scale!= 0.0:\n\t\tself.scale=other\n\t\tif recompute:\n\t\t\tself.computeScales()\n\t\t\tself.scancfg=None\n\telse:\n\t\traise LenaValueError('Can not rescale zero-scale histogram')\n\t\ndef computeScales(self, max=None, init=1.0, factor=1.0, n=None):\n\t\"\"\"\n\tEstimate the integrated value of the histogram\n\tand set the scales.\n\t"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\tif other is None:\n\t\tdothist = self  # don't recompute scale\n\t\tcached = False\n\t\t\n\t\tif hasattr(self, '_cached_scale'):\n\t\t\t# this histogram is already normalized\t\n\t\t\treturn self._cached_scale\n\t\telse:\n\t\t\tif len(self.histogram) == 0:\n\t\t\t\tif self.norm is None:  # special case \n\t\t\t\t\tself.norm = 1.0 \n\t\t\t\t\t\n\t\t\t\tdothist._cached_scale = 1.0/self.norm\n\t\t\t\tc"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\tif not recompute:\n\t\tif self._scale is not None:\n\t\t\treturn self._scale\n\t\t\n\t\tself._compute_scale(other)\n\t\t\n\treturn self._scale\n\n \t\n\t\n\t\ndef _compute_scale(self, other=None, recompute=False):\n\t\n\tif not recompute and self._scale is not None:\n\t\treturn self._scale\n\t\n\told_hist = self.get_hist()\n\tif other is None:\n\t\t# find the maximum value of the list\n\t\tvalues = ["}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\t\n\tif other is None:\n\t\tscale = self._scale\n\t\tif scale is None and recompute:\n\t\t\tself._scale = self._histo.scale(1)\n\t\treturn self._scale\n\t\n\tif recompute:\n\t\tother = 1 / other\n\t\n\tscale = self._histo.scale(other)\n\tif scale is not None:\n\t\tself._scale = scale\n\telse:\n\t\traise self.ScaleFailed()\n\nclass ScaleFailed(LenaValueError):\n\t\"\"\"\n\tRaised when the histogram does not"}, {"generated_text": "def scale(self, other=None, recompute=False):\n\"\"\"\n\tCompute or set scale (integral of the histogram).\n\t\n\tIf *other* is ``None``, return scale of this histogram.\n\tIf its scale was not computed before,\n\tit is computed and stored for subsequent use\n\t(unless explicitly asked to *recompute*).\n\tNote that after changing (filling) the histogram\n\tone must explicitly recompute the scale\n\tif it was computed before.\n\t\n\tIf a float *other* is provided, rescale self to *other*.\n\t\n\tHistograms with scale equal to zero can't be rescaled.\n\t:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\treturn self.h_.scale_hist(other=other, recompute=recompute)\n\t\ndef log(self, minval=None, maxval=None):\n\t\"\"\"\n\tCalculate and store in ``self.log_(minval, maxval)`` :math:`\\log(|x|)`.\n\t\n\tRaise an *LenaValueError* exception\n\tif one tries to log in a histogram.\n\t\"\"\"\n\tif minval is None: minval = 0\n\tif maxval is None: maxval = self.max"}]}
{"task_id": "62b87b519a0c4fa8b80b3583", "prompt": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\t# XXX: not yet implemented\n\tif other is None:\n\t\treturn self._scale\n\t\n\told_scale, scale, coord = self.scale_info\n\tscale_ = scale\n\n\t# scale is used only (and only) in get_scale()\n\tif self.is_empty():\n\t\treturn old_scale\n\t\n\t# scale_ is used everywhere\n\tif other is not None and isnumber(other):\n\t\tscale_ = max(scale, other) or 1e-100\n\t\n\tif other is not None:\n\t\tif isn"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\treturn graph_attribute(self,'scale', 1.0, other)\n\t\n\t\n# The graph's data types.\n# TODO: replace those with a dict of the data types.\n# TODO: use this type system instead of the class names.\n#\n# Data,\n#    Point,\n\n# Error,\n#    LenaValueError,\n#    UnregisteredError,\n\n\n\nclass Error(Exception):\n\tpass\n"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\t\n\tif other is None:\n\t\treturn self._scale\n\telse:\n\t\toldScale = self._scale\n\t\tif isinstance(other, (numbers.Number, float)) and other!= 1:\n\t\t\t# try to preserve existing scale.\n\t\t\tfactor = oldScale/other\n\t\t\t# keep scale for later\n\t\t\tself._scale = other\n\t\t\t\n\t\t\t# scale coordinates:\n\t\t\tself._coords = map(lambda x: x*factor, self._coords)\n\t\telse:\n\t\t\toldScale = len(self._xCoords)\n\t\t\tself"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None: return self._scale\n\tif other<0: raise ValueError(str(other))\n\tif isnan(other): return self._scale\n\tif isnanarray(self): return self.set_scale(other)\n\n\tcoord = max(self.coord_arr(), axis=0)\n\tsize = coord.max()\n\t\n\tif isnanarray(self): self._validate()\n\t\n\tfor x in self.xyzkeys:\n\t\tscn = abs(self.scales[x]).max()\n\t\tif isnan"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None: return self.raw_scale\n\tscale = self.scale\n\tfor coord in self.fields:\n\t\tdelta = scale/other\n\t\tcoordvalue = self.data_type(coord).data.item\n\t\tfor x in range(coord_max(self.fields[coord])):\n\t\t\tnewvalue = coordvalue*delta\n\t\t\tself.data_type(coord).data [x] = newvalue\n\tself.raw_scale = other\n\ndef _rescale_delta(delta, newscale):\n\t\"\"\"\n\tRescale"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\tif self.X is None:\n\t\t\treturn None\n\t\tif self.X0 is not None:\n\t\t\treturn self.X0 / max(self.X)*self.Bounds[1][1]\n\t\telse:\n\t\t\treturn self.Bounds[1][1]/self.Bounds[0][1]\n\telse:\n\t\tif other < 0:\n\t\t\traise ValueError('scale of the current graph must be positive, but got %.2f'%other)\n\t\tif other > len(self.Bounds):\n\t\t\t"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tif other is None:\n\t\treturn self.scale\n\telif other is not numpy.nan:\n\t\ta = self * numpy.ones(3)\n\t\tself.rescale(other, a)\n\telse:\n\t\tself._scale = None\n\naddMethods(Graph.scale,'scaleable', **methodModifiers)\n\n@property\ndef scaleable(self):\n\treturn type(self).scaleable"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\treturn _rescale_graph(self, other)\n\t\n\t\n\n__all__ = [\n\t'get_graph_scale',\n\t'scale',\n]"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t# print (\"scale\", repr({k:v for v,k in self.items()}))\n\t# if other == 1:\n\t\t# print (\"scale now\", (self.x, self.y, self.z))\n\t# scale = self.get(other, 1) if isinstance(other, float) else other[0].scale\n\t# print (\"scale\", (self.x, self.y, self.z))\n\t# raise NotImplementedError(\"This graph only supports rescale by changing scale of its coordinate instead of its own value.\")\n\tif"}, {"generated_text": "def scale(self, other=None):\n\"\"\"\n\tGet or set the scale of the graph.\n\t\n\tIf *other* is ``None``, return the scale of this graph.\n\t\n\tIf a numeric *other* is provided, rescale to that value.\n\tIf the graph has unknown or zero scale,\n\trescaling that will raise :exc:`~.LenaValueError`.\n\t\n\tTo get meaningful results, graph's fields are used.\n\tOnly the last coordinate is rescaled.\n\tFor example, if the graph has *x* and *y* coordinates,\n\tthen *y* will be rescaled, and for a 3-dimensional graph\n\t*z* will be rescaled.\n\tAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t# check scale\n\tif other is None:\n\t\tscale = getattr(self, self.scale_name)\n\t\tscale_unit = getattr(self.unit_system(), scale)\n\telse:\n\t\trescale = True\n\t\tscale = None\n\t\tif other == 0:\n\t\t\traise ValueError(\"zero scaling\")\n\t\telif math.isinf(other) or math.isnan(other):\n\t\t\traise ValueError(\"non-number scaling\")\n\t\telif other > 0:\n\t\t\tscale = other\n\t\telif other < 0:\n\t\t\tscale ="}]}
{"task_id": "62b87b869a0c4fa8b80b35e1", "prompt": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\thist[:] = filter(lambda x: isinstance(x, (bin,)), hist)\n\n\tgraph = hist.new_graph()\n\tgraph.set_title(name)\n\tgraph[\"x\"] = hist.get_x_axis()\n\n\tif get_coordinate == \"left\":\n\t\tx_co = hist.x\n\telif get_coordinate == \"right\":\n\t\tx_co = hist.x - hist.x.range[1]\n\telif get_coordinate == \"middle\":\n\t\tx_co = (hist.x."}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\timport collections\n\timport numpy\n\t\n\t# Remove context & type casting a histogram.\n\t# The result may be a numeric type.\n\th = hist.remove_context().cast({})\n\t\n\t# Collect bin content & units\n\tbin_d = collections.defaultdict(lambda hist, unit=None: {\n\t\t'type': self.bin_to_histogram_type(hist),\n\t\t'unit': unit,\n\t\t'values': [\n\t\t\t(b.make(hist, self.context), None)\n\t\t\tfor b in self.bin"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tres = {}\n\tif scale is True:\n\t\thist = scale(hist)\n\tif isinstance(make_value, (str, type(None), type(''))):\n\t\tvalue_field = \"data\"\n\t\tmake_value_ = {}\n\t\tif field_names is None:\n\t\t\tget = lambda x: x\n\t\telse:\n\t\t\tfield_names = _field_names_from_field_names(field_names)\n\t\t\tget = lambda x: dict(zip(field_names, x.data))\n\t\t\tif hist.get_field_count"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tif len(hist) == 0:\n\t\treturn\n\n\txmin = min(hist[\"bins\"][\"minimum\"][\"value\"])\n\txmax = max(hist[\"bins\"][\"maximum\"][\"value\"])\n\tbin_number = len(hist[\"bins\"])\n\n\tif field_names == None:\n\t\tfield_names = [None]*bin_number\n\n\t# make x-domain\n\tx_domain = [xmin, xmax]\n\tmake_field_names = make_field_names_from_value_func"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t# make a copy\n\thist = deepcopy(hist)\n\tgraph = histgraph.graph()\n\tpoint_map = {}\n\tdim = len(hist.domain.attributes)\n\tfor index in range(len(hist.bins)):\n\t\tbins_list = hist.bins[index]\n\t\t\n\t\t# create bin\n\t\tname = (\"bin\"+str(index))\n\t\tbin = hist.domain.attributes.new(name, \"dummy\")\n\t\tbin.set_value(bins_list[0])\n\t\t\n\t\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tif get_coordinate == \"left\":\n\t\tcoordinate = lambda coord: coord\n\telif get_coordinate == \"right\":\n\t\tcoordinate = lambda x: -x\n\telse:\n\t\tcoordinate = lambda x: x\n\tfield_names = [field_name for field_name, field_type in iteritems(hist.field_types)]\n\ttry:\n\t\tdata = []\n\t\tfor (key, value), bin in iteritems(iteritems(iteritems(iteritems(hist.data))), 2):\n\t\t\tif isinstance(value, str"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\t#print(\"hist_to_graph()\", hist, make_value, get_coordinate, field_names, scale)\n\t##TODO\n\t\n@contextmanager\ndef graph(label=\"\", field_names=None, values=None, scale=True,\n\t\t    hist_x_axis_range=(0, None, 0), hist_y_axis_range=(None, 0, None),\n\t\t    hist_y_axis_scale=\"linear\", axis_colors=\"\"\n\t\t    ):\t###\n\t\"\"\"\n\t**Graph Context Generator**.\n\t"}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tfrom.graph import Graph\n\t#assert hasattr(make_value, \"__call__\")\n\t\n\tk, v, c = len(hist.field_names), list(hist.values), list(hist.context)\n\t\n\tv = v * 0\n\tfor b in hist:\n\t\tv[0] += b[0]\n\t\tv[1] += b[1]\n\t\tv[2] += b[1]**2\n\t\tfor i in range(3):\n\t\t\tv[i] += b[i]\n\t\t\n\tv ="}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tif isinstance(make_value, str):\n\t\tmake_value = lambda bin_: (bin_.value, bin_.value_error)\n\tif isinstance(hist, list) and all(isinstance(bin_, NumericBin) for bin_ in hist):\n\t\tassert len(hist) == len(field_names), \"dimensions of field_names (%s) must match the histogram's (%s)\"%(str(field_names), str(len(hist)))\n\t\tscales = [bin_.scale for bin_ in hist]\n\t\tdata = ["}, {"generated_text": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\t\n\t*make_value* is a function to set the value of a graph's point.\n\tBy default it is bin content.\n\t*make_value* accepts a single value (bin content) without context.\n\t\n\tThis option could be used to create graph's error bars.\n\tFor example, to create a graph with errors\n\tfrom a histogram where bins contain\n\ta named tuple with fields *mean*, *mean_error* and a context\n\tone could use\n\t\n\t>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\t\n\t*get_coordinate* defines what the coordinate\n\tof a graph point created from a histogram bin will be.\n\tIt can be \"left\" (default), \"right\" and \"middle\".\n\t\n\t*field_names* set field names of the graph. Their number\n\tmust be the same as the dimension of the result.\n\tFor a *make_value* above they would be\n\t*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\t\n\t*scale* becomes the graph's scale (unknown by default).\n\tIf it is ``True``, it uses the histogram scale.\n\t\n\t*hist* must contain only numeric bins (without context)\n\tor *make_value* must remove context when creating a numeric graph.\n\t\n\tReturn the resulting graph.\n\t\"\"\"\n\tfrom._graph import create\n\treturn create(hist, bin_name=None, scale=scale, field_names=field_names,\n\t\tmake_value=make_value, get_coordinate=get_coordinate())\n\n_histogram2d_options = \"scale, bins, field_names, name, get_coordinate\".split(\",\")\ndef create_hist_2d(hist, **kwargs):\n\t\"\"\"\n\tCreate a :class:`.hist.hist2d` from an existing\n\t:class:`.hist.hist1d` without"}]}
{"task_id": "62b8b4baeb7e40a82d2d1136", "prompt": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not candidate.__provides__(iface):\n\t\t\t\t# Not handled by candidate.\n\t\t\t\t# Check directly\n\t\t\t\t\n\t\t\tif checkIface(iface, False, vtype):\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\t# See if there are other valid ifaces.\n\t\t\t\tother = list(iface.__subClassSet__)\n\t\t\t\tfor other_iface in other:\n\t\t\t\t\tif checkIface(other_iface, False, vtype):\n\t\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\t# At least this one failed to satisfy the iface.\n\t\t\t\t\traise Invalid"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not isinstance(candidate, zope.interface.InterfaceClass) \\\n           and not candidate.providedBy(iface):\n\t\tif not tentative:\n\t\t\traise zope.interface.Invalid(\n                u\"Candidate does not provide %r: %r does not inherit from interface %r\"\n                % (iface, candidate, iface));\n\t\treturn False\n\n\tdef _get_declaration(function):\n\t\tif isinstance(function, method_type):\n\t\t\treturn function.__func__.__name__, function.im_func\n\t\telse:"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tdef _find_candidate_in_meta(class_):\n\t\tmeta = class_.__class__.__dict__\n\t\tname = class_.__name__ + 'Meta'\n\t\tfor meth, val in meta.items():\n\t\t\tif meth.find('_provides_')!= -1:\n\t\t\t\ttry:\n\t\t\t\t\tspec = vtypes.unwrap(val)\n\t\t\t\t\tif spec._providedBy is not class_:\n\t\t\t\t\t\traise TypeError\n\t\t\t\texcept TypeError:\n\t\t\t\t\tbreak\n\t\t\t\tiface = interface.providedBy(class_)\n\t\t\t\tif not iface"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tfrom zope.interface.exceptions import Invalid\n\ttry:\n\t\tchecker = getCompatibilityChecker()\n\texcept KeyError as zope_key_error:\n\t\t# The compatibility checker is not available. Try to register it\n\t\t# if it appears available in the registry.\n\t\t# TODO: There are two incompatible ways to provide\n\t\t# compatibility checkers. In one and only one, Zope 2 and 3 must\n\t\t# be integrated. The ZODB 3 compatibility checker actually\n\t\t# uses the Zope 3 compatibility checker. Once we get to the"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\ttry:\n\t\tprovides = iface.providedBy(candidate)\n\texcept ObjectDoesNotImplement:\n\t\tpass\n\telse:\n\t\tif not provides:\n\t\t\tif untrusted:\n\t\t\t\tifaceobj = iface\n\t\t\telse:\n\t\t\t\tifaceobj = _untrustedAdvisorObject(iface)\n\t\telse:\n\t\t\timport warnings\n\t\t\twarnings.warn(\"Interface implementation (%r) \"\n\t\t\t\t      \"implements %r\"\n\t\t\t\t      \"but does not correctly define \"\n\t\t\t\t      \"interface.declaredProvides()\" %"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif iface is None:\n        # this was already determined the following way\n        #zcml._zope_interface._verifyNone(candidate)\n        # return\n        # iface=candidate.__providedBy__(iface)\n        # if not iface:\n        # \treturn False\n        return True\n\n\tif issubclass(candidate, interfaces.Interface):\n\t\t# Check if we should make this a provider\n\t\tif concrete and (not tentative):\n\t\t\tif not candidate.isProvider():\n\t\t\t\treturn False\n\n\t\t# Make sure the"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not zope.interface._verify(iface, candidate, tentative=tentative):\n\t\treturn False\n\n\t# If candidate didn't declare its methods, we can't check them, so abort.\n\tfor method in [m for m in candidate.__class__.__dict__.values()\n\t\t\tif isinstance(m, classmethod)]:\n\t\tif method.im_self is None and method.im_class is iface:\n\t\t\terror = \"Candidate class %r does not declare its __str__ method.\" % (\n\t\t\t\tcandidate,\n\t\t\t)"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\timplementer = iface in candidate.classImplementedBy\n\tprovider = iface in candidate.classProvides\n\t\n\tif (provider or implementer) and (not tentative):\n\t\treturn True\n\t\n\trequired = _getrequiredattributes(iface)\n\tfor name in required:\n\t\tif not hasattr(candidate, name):\n\t\t\traise invalid(\"%s does not have %s\" % (candidate, name))\n\t\n\tfor methname in _getmethods(iface):\n\t\tfor pname, params in getargs"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif vtype is None:\n\t\tvtype = iface\n\tif not hasattr(vtype, '__provides__'):\n\t\t# The check will be skipped (this step is only relevant\n\t\t# for normal tests, not for unit tests), because the\n\t\t# interface doesn't allow us to do so without causing\n\t\t# a runtime error. (this is because we aren't implementing\n\t\t# the interface)\n\t\treturn True\n\t# If the candidate has the interface, but it is not providing it, the\n\t# test fails.\n\thasProvides = getattr"}, {"generated_text": "def _verify(iface, candidate, tentative=False, vtype=None):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\treturn _verify(iface, candidate, tentative, vtype, None)\n\ndef _verify(iface, candidate, tentative, vtype=None, version=None):\n\t# pylint: disable=W0613,R0904\n\t\n\t# NOTE: zope.interface does not guarantee that an instance has the\n\t# given version. We can't assume anything else.\n\tassert version is None or isinstance(version, tuple), repr(version)\n\tproviderClasses = _interfaceToClasses(iface, tentative=tentative)"}]}
{"task_id": "62b8b4baeb7e40a82d2d1137", "prompt": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\timplementedBy = getUtility(IImplifierRegistry).get(iface)\n\t# If there is no implemented-by registry entry for this, then we have not yet\n\t# registered this candidate. This could either mean we have not registered at\n\t# all yet, and thus there can be no problems, or it could mean that the problem\n\t# is transient, and will resolve later. We now throw the same exception as\n\t# Zope2 itself does in this case.\n\tif implementedBy is None:\n\t\traise Errors.NonRegistrable"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\timport warnings\n\n\ttry:\n\t\tinterfaceClass = iface.implementedBy\n\texcept AttributeError:\n\t\tinterfaceClass = iface\n\tif tentative:\n\t\treturn _verifyObjectTentative(interfaceClass, candidate)\n\treturn _verifyObject(interfaceClass, candidate)\n\n\n\ndef _verifyObject(ifaceClass, object):\n    if not hasattr(object, \"_providedBy_cache\"):\n        object._providedBy_cache = {}\n    provided = object._providedBy_cache\n    iface = ifaceClass.extension"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif iface is None:\n\t\treturn True\n\n\tprovided = iface.providedBy(candidate)\n\tif not provided \\\n\t   or candidate is provided or isinstance(candidate, (type, classobj)):\n\t\treturn False\n\n\terr = None\n\n\tfor ifaceIn, methOrAttr in iface.namesFromMixinDeclaration(candidate):\n\t\tif methIn(methOrAttr, '*'):\n\t\t\tcontinue\n\t\t# if methOrAttr is an attribute, test for type\n\t\t# (attribute should be of this type"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not len(iface):\n\t\traise errors.Invalid(\n\t\t\treason='cannot verify %r in \"%s\"; no iface name' % (candidate, candidate.__module__)\n\t\t)\n\tclass_iface = iface[0]\n\tif class_iface is None:\n\t\tclass_iface = getBases(candidate)[0]\n\tdeclared = class_iface==iface[0]\n\tprovided = inherited.providedBy(candidate)\n\tinvalid = False\n\tif not declared:\n\t\traise errors."}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t# make sure the candidate implements the interface\n\tif not (tentative and iface.providedBy(candidate)):\n\t\tif not (isinstance(candidate, type) and\n\t\t\tissubclass(candidate, iface) and\n\t\t\tcandidate is not iface and\n\t\t\tcandidate is not iface._providedBy):\n\t\t\t# zope.interface has a weird special-case implementation that is\n\t\t\t# implemented by zope.impex (this is an ugly hack, to be removed).\n\t\t\t# I'm not sure that my check for \"is not iface"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not candidate.providedBy(iface):\n\t\treturn False\n\tfor method in iface.__providedBy__(candidate):\n\t\tif not _verifyMethod(iface, method):\n\t\t\treturn False\n\tfor attribute in iface.__implementedBy__(candidate):\n\t\tif not _verifyAttribute(iface, attribute):\n\t\t\treturn False\n\tif tentative:\n\t\treturn True\n\tinheriting = iface.__implementedBy__(candidate)\n\trequiredMethods = _getRequiredMethods(iface)\n\trequired"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tif not iface.isProvidedBy(candidate):\n\t\treturn False\n\t\n\tif not tentative:\n\t\t# Check that the candidate implements all the necessary methods.\n\t\tif not candidate in __all_methods:\n\t\t\tmro = getattr(candidate, '__mro__', ())\n\t\t\tiface_mro = (candidate.__implemented__,) + getattr(iface, '__mro__', (object,)):\n\t\t\t    for _iface_mro in iface_mro:\n\t\t\t\t    if _iface_"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\tclass MyProperties(Properties):\n\t\t# It is a bit nasty to monkey-patch the class, as every other\n\t\t# monkeypatching call has to be preceded by a method call in\n\t\t# ObjectManager.getOrDiscover\n\t\tdef _get_Properties(self): return self\n\n\tclass MyProperties_getProperties(Properties._getProperties):\n\t\tdef __call__(self):\n\t\t\tproperties = super(MyProperties,self).__call__()\n\t\t\tif not isinstance(properties, Properties):\n\t\t\t\tfrom zope.interface.exceptions"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\t# This function needs to be imported explicitly\n\tfrom zope.interface.verify import verifyObject as _verifyObject\n\t\n\t# If we try to get the class of a class, we get the original class\n\t_candidate = candidate if inspect.isclass(candidate) else candidate.__class__\n\t\n\t# Make sure the candidate actually provides the type\n\t# that we expected\n\t\n\t# First check for direct interfaces\n\tcandidate_provided = iface.providedBy(candidate)\n\tif candidate_provided is True:\n\t\t#"}, {"generated_text": "def verifyObject(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\t\n\tThis involves:\n\t\n\t- Making sure the candidate claims that it provides the\n\t  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n\t  in which case this step is skipped). This means that the candidate's class\n\t  declares that it `implements <zope.interface.implementer>` the interface,\n\t  or the candidate itself declares that it `provides <zope.interface.provider>`\n\t  the interface\n\t\n\t- Making sure the candidate defines all the necessary methods\n\t\n\t- Making sure the methods have the correct signature (to the\n\t  extent possible)\n\t\n\t- Making sure the candidate defines all the necessary attributes\n\t\n\t:return bool: Returns a true value if everything that could be\n\t   checked passed.\n\t:raises zope.interface.Invalid: If any of the previous\n\t   conditions does not hold.\n\t\n\t.. versionchanged:: 5.0\n\t    If multiple methods or attributes are invalid, all such errors\n\t    are collected and reported. Previously, only the first error was reported.\n\t    As a special case, if only one such error is present, it is raised\n\t    alone, like before.\n\t\"\"\"\n\t\n\twantedPid = iface.__providedBy__\n\t_calls = [ x \\\n\t           for x in iface.provides \\\n\t           if x not in wantedPid ]\n\t\n\t# Make sure the candidate provides the interface:\n\t\n\tcandidateProvides = candidate.__class__.providedBy \\\n\t     if tentative else candidate.providedBy (iface)\n\t\n\tif candidateProvides!= wantedPid: \n\t\t# One candidate provides the interface, but the desired\n\t\t# object doesn't.  That should not"}]}
{"task_id": "62b8b4c1eb7e40a82d2d1139", "prompt": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tdef getProperties(interface):\n\t\tclassID = candidate.getClassID(interface)\n\t\tif classID[:6] == \"\\x7F\\x00\\x00\\x00\\x00\\x00\":\n\t\t\treturn classID[6:]\n\t\telse:\n\t\t\treturn str(classID)\n\n\tprops = getProperties(iface)\n\n\tif \".\" in props:\n\t\t# We can only get here if *candidate* implements a superclass\n\t\t# of *iface*. If not, the classID should be in classID,\n"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif not tentative and not canBeTentative(iface):\treturn False\n\tklass = type(candidate)\n\tif klass is not Interface: \n\t\ttry:\n\t\t\t# Try first to see if it is a subclass\n\t\t\t# of the *iface* class. Then let the\n\t\t\t# \"verifyClass\" method do the verification.\n\t\t\treturn type(iface) in candidate.__mro__\n\t\texcept AttributeError:\n\t\t\t# This class doesn't actually implement\n\t\t\t# the interface, so we can't really verify\n\t\t\t# that it"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif strict:\n\t\tcheckAccessible(iface, candidate)\n\telse:\n\t\tverifyInPackage(iface, candidate)\n\n\tiface = iface.makeNew()\n\tgetattr(iface, 'addPythonProvides', lambda *_: None)() # add dummy function\n\t# Verify that each module providing the iface is in the candidate list of Python providers (only checked once)\n\t# and that each class implementing the interface provides the iface\n\t# - not checked if not strict, since that would slow down all but some testcases\n\tadded ="}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t# If the candidate is a list, then each element must match itself.\n\tif type(candidate) is list:\n\t\tfor i in xrange(0,len(candidate)):\n\t\t\tif not verifyClass(iface, candidate[i], True):\n\t\t\t\treturn False\n\t\treturn True\n\t# If the candidate is a tuple, then each inner element must match itself.\n\telif type(candidate) is tuple:\n\t\tfor i in xrange(0, len(candidate)):\n\t\t\tif not verifyClass(iface, candidate, False):\n\t\t\t\t"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t# TODO: we need to call interfaces.Interface.verifyClass() for each interface!\n\t# The following is enough to pass this test for now:\n\tassert isinstance(candidate, (type, types.ClassType))\n\tifaceClass = iface or candidate.__class__\n\tiface = getInterfaces(ifaceClass)\n\treturn tryAllInterfaces(iface, candidate, tentative)\n\n\ndef tryClass(obj, c=None):\n\t\"\"\"\n\tTry to verify that the *obj* has a valid implementation for the\n\tinterface of"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tresult = False\n\ttry:\n\t\tcandidate = candidate()\n\texcept TypeError:\n\t\tpass\t# candidate is an instance of the class, so this is correct\n\telse:\n\t\t# is the current candidate the expected type?\n\t\tif inspect.isclass(candidate) and issubclass(candidate, iface):\n\t\t\t# is the current candidate a subclass of what we expect?\n\t\t\tif candidate is iface:\n\t\t\t\tresult = True\t# yup!!!\n\t\t\telse:\n\t\t\t\t# is there an inner class that has exactly one public method annotated as \""}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif iface.startswith(\"_\"):\n\t\tif candidate.startswith(\"_\"):\n\t\t\treturn # cannot verify the _ of iface_\n\tiface = normalizeClassName(iface)\n\tcandidate = normalizeClassName(candidate)\n\treturn __verifyClass(iface, candidate, tentative)\n\ndef normalizeClassName(name):\n\treturn string.upper(string.join(name.split('_')))\n\n\n# =============================================================================\n# Internal testing helpers\n# =============================================================================\n\nif __name__ == \"__main"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tif not candidate.startswith('java.lang.'):\n\t\traise Exception('This function must be in place of \"implements\" or \"extend\".')\n\n\tif not candidate.endswith('T'):\n\t\tcandidate += 'T'\n\tif not candidate.endswith(';'):\n\t\tcandidate += ';'\n\n\tif iface in candidate:\n\t\tclassName = getClassName(candidate)\n\t\tif className == 'void':\n\t\t\traise Exception('Can\\'t verify abstract interface methods!')\n\t\telif className =="}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tclassname=iface.replace(\"-\",\"_\")\n\tclassvalue=os.environ.get(\"__CAMPBELL_HAS_%s\"%classname,None)\n\tif classvalue:\n\t\treturn eval(classvalue)\n\tcandidates=dict(\n\t\txmpscan=campbellxmpscan.XMPPScan,\n\t\txmlrpc2=campbellxmlrpc.XMLRPC2,\n\t\t)\n\ttry: \n\t\tif candidate.lower()==\"xmpscan\": \n\t\t\timport os"}, {"generated_text": "def verifyClass(iface, candidate, tentative=False):\n\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tfrom. import exceptions\n\tassert iface.__module__ == \"gi.repository\", \"bad *iface* module: '%s'\" % iface\n\tfrom inspect import getsourcefile\n\tfrom os.path import dirname, basename\n\tif len(inspect.splittree(getsourcefile(candidate))) < 3:\n\t\tprint(candidate.__doc__)\n\tsource = \"%s/%s\" % (dirname(dirname(candidate.__module__)), basename(candidate.__module__))\n\tassert source"}]}
{"task_id": "62b8b559eb7e40a82d2d11f6", "prompt": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif len(bases) < 2:\n\t\treturn explicit_mc\n\t# TODO: __metaclass__ inheritance?\n\t# TODO: metaclass inheritance?\n\tif explicit_mc is None:\n\t\tmc = bases[0]\n\t\tfor b in bases[1:]:\n\t\t\tif 'Meta' in dir(b):\n\t\t\t\tmc = b\n\telse:\n\t\tmc = explicit_mc\n\t\t\n\treturn mc\n\n\ndef create_subclass(cls, name, bases, dict):\n\t\"\"\"\n\tCreate new class instance with the given"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmetas = []\n\tfor klass in bases:\n\t\tm = klass.__dict__.get('__metaclass__', None)\n\t\tif m is not None:\n\t\t\tif m is object:\n\t\t\t\tm = explicit_mc\n\t\t\treturn m\n\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\n\t# The classic metaclass\n\treturn type.__dict__['__metaclass__']\n\ndef getMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tReturns __metaclass__ instance of"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmetacls = explicit_mc\n\tif metacls is None:\n\t\tif len(bases) == 2 and bases[0] is Object and bases[1] is Object:\n\t\t\tmetacls = bases[0]\n\t\telse:\n\t\t\tmetacls = bases[0] or type(\"Metaclass\", (Object,), {}) # let the metaclass be Object if no metaclass can be found\n\treturn metacls\n\ndef resolveClassTree(base_mcs):\n\t\"\"\"Convert 1 or more baseclasses to their metaclass,"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc:\n\t\tif bases := [b for b in bases if hasattr(b, '__metaclass__')]:\n\t\t\treturn explicit_mc\n\t# if __metaclass__ set to class, fallback to class\n\treturn bases[0] if bases else type\n\ndef findClassInClasses(base_cls, cls):\n\t\"\"\"\n\tGiven base class instance and class instance, determine class\n\tof the derived instance.\n\tUsed by __new__ methods.\n\t\"\"\"\n\tfor c in base_cls.__mro"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is not None:\n\t\tbases = ( explicit_mc, ) + tuple(base.__class__.__name__ for base in bases)\n\tmeta = _meta_model_registry.getMetaClass(\"object\")\n\tfor baseClass in bases:\n\t\tif hasattr(baseClass, '__metaclass__'):\n\t\t\tmeta = createClass(baseClass.__metaclass__)\n\t\t\tbreak\n\tif not hasattr(meta, '__name__'):\n\t\traise TypeError('expected a MetaClass, got %s'"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc:\n\t\tmetaclass = explicit_mc\n\telse:\n\t\tbases = tuple(bases)\n\n\t\t# explicit __metaclass__ wins\n\t\tif '__metaclass__' in bases[0]:\n\t\t\tmetaclass = bases[0]['__metaclass__']\n\t\t\t_, parents = builtins.next(isinstance(b, collections.Sequence) and (b, []) or (b, b.__bases__))\n\t\t\tfor p in parents:\n\t\t\t\tif '__metaclass__' in locals():"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmeta = explicit_mc\n\tif not meta:\n\t\tmeta = bases and bases[0].__metaclass__\n\tif not meta:\n\t\tfor base in bases:\n\t\t\tmeta = base.__metaclass__\n\t\t\tif meta: break\n\treturn meta\n\n\ndef _inheritsmetaclass(bases):\n\t\"\"\"\n\tReturns True if the given bases list has a metaclass\n\t\"\"\"\n\tif not bases: return False\n\treturn getattr(bases[-1], '__metaclass__', False)\n\n\ndef getArg"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tklass = None\n\tif explicit_mc is None:\n\t\tfor base in bases:\n\t\t\tif hasattr(base, '__metaclass__'):\n\t\t\t\tklass = base.__metaclass__\n\t\t\t\ttry:\n\t\t\t\t\tklass.metaclass_exists = True\n\t\t\t\texcept TypeError:\n\t\t\t\t\tklass.metaclass_exists = False\n\t\t\telif isinstance(base, type) and issubclass(base, type(object)):\n\t\t\t\t# class based upon `object`; happens if `object` is a metaclass:"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tif explicit_mc is None:\n\t\texplicit_mc = __metaclass__ = None\n\n\tfor mbase in bases:\n\t\tif inspect.ismethod(mbase.__new__):\n\t\t\tfrom types import MethodType\n\t\t\tmbase.__new__ = MethodType(metaclass_call, mbase.thisclass(), bases)\n\t\t\tmbase.thisclass = mbase.thisclass.__name__\n\t\t\tmbase.__new__ = MethodType(mname_call, mbase.thisclass(), bases)\n\t\tmbase, explicit_mc"}, {"generated_text": "def determineMetaclass(bases, explicit_mc=None):\n\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tresolve_metaclass = None\n\tif explicit_mc is not None:\n\t\t# explicit metaclass supplied\n\t\tresolve_metaclass = explicit_mc\n\telif not any([issubclass(b, PythonModuleType) for b in bases]):\n\t\t# no metaclass supplied, or it's not a module, so there's only one\n\t\t# base class (either object or type)\n\t\treturn bases[0]\n\telse:\n\t\t# multiple bases, possibly with both object and type types\n\t\t# first round of guesses is based on known"}]}
{"task_id": "62b8d22a48ba5a41d1c3f47d", "prompt": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn dict.__getitem__(self, key)[0]\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\telse:\n\t\t\treturn default\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\treturn self.loads(self.dumps(key))\n\n\t# key is removed from self.d, returning self\n\n\t# key is removed from self.d, returning self\n\t# raise KeyError(key)\n\n\t# key is returned, returning self\n\n\n\n## Test\n#import doctest\n#doctest.testmod()\n\n## Run\n## python script1.py\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\treturn self.__getitem__(key, None).pop(key, default)\n\n\t\n\t"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\treturn self.get(key, default)\n\ndef popitem(self, **kwds):\n\t\"\"\"\n\tD.popitem() -> (k, v), return and remove a (key, value) tuple.\n\tPairs with D.setdefault() for dict-like behavior.\n\t\"\"\"\n\treturn self.pop(self.iteritems()[0][0], __marker)\n\ndef setdefault(self, key, default=None):\n\t\"\"\"\n\tD.setdefault(k[,d]) -> D.get(k,d), also set"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\t\n\twith self.make_lock():\n\t\ttry:\n\t\t\treturn self.dict.pop(key, default)\n\t\texcept KeyError:\n\t\t\treturn default\n\t\n\t\n\t\n\t\ndef clear(self):\n\t\"\"\"\n\tD.clear() -> None.\n\tRemove all items from D. \n\t\"\"\"\n\t\n\twith self.make_lock():\n\t\tself.dict.clear()\n\t\n\ndelete = pop\n\n##############################################################################################################\n\n"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\treturn self.__getitem__(key, default)\n\t\n\t\n\t\n\nclass _SortedDict:\n\t\"\"\"\n\tImplement dict() method on top of a sorted list.\n\t\"\"\"\n\t\n\tdef __init__(self, dict=None, **kwargs):\n\t\tself._listclass = SortedList\n\t\tself._listkws = dict, kwargs\n\t\tself._sortedlist = OrderedDict()\n\t\t\n\t\tself.update(dict, **kwargs)\n\t\n\tdef __getitem__(self,"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\treturn self.pipeline('pop',key,default)\n\ndef discard(self, key):\n\t\"D.discard(k[,d]) -> None. Remove specified key and return None. If key is not in D, d is returned if given, otherwise KeyError is raised.\"\n\tself.pipeline('discard',key)\n\ndef clear(self):\n\t\"D.clear() -> None. Remove all items from D.\"\n\tself.pipeline('clear')\n\ndef setdefault(self, key, default=None):\n\t\"D"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tassert self.contains(key)\n\tv = self.items.pop(key)\n\treturn v\n\n@_memoize\ndef popitem(self, key=__marker, value=__marker):\n\t\"\"\"\n\tD.popitem() -> (k, v), remove and return some (key, value),\n\traises KeyError if dict is empty\n\t\"\"\"\n\tassert self.contains(key)\n\treturn self.items.pop(key, value,)\n\n@_memoize\ndef setdefault(self, key, default"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\titem = self.get(key, default)\n\tif item is not __marker:\n\t\tdel self[key]\n\t\treturn item\n\traise KeyError(key)\n\nclass LRUCache(object):\n\t\"\"\"\n\tLRUCache(capacity) -> LRUCache\n\t__init__(capacity) -> LRUCache\n\n\tAn LRUCache copies data on-the-fly and never removes items from cache.\n\t\"\"\"\n\tdef __init__(self, capacity = 10, load_factor = 0.1):\n\t\t\"\"\"\n\t\tcapacity: maximum number"}, {"generated_text": "def pop(self, key, default=__marker):\n\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n\tIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tret = self._root[key]\n\tdel self._root[key]\n\treturn ret\n\n\t# if key in self._root: del (self._root[key])\n\t# else: return default\n\n\ndef popitem(self):\n\t\"\"\"\n\tD.popitem() -> (k, v), remove and return some (key, value), (k, v) pairs, and\n\traise KeyError if iterable is empty.\n\t\"\"\"\n\tret = self._root.popitem(False)\n\tif ret is None: raise KeyError\n\treturn ret"}]}
{"task_id": "62b8d23748ba5a41d1c3f497", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\ttry:\n\t\treturn self.lru.popitem()\n\texcept IndexError:\n\t\tpass\n\nclass LRUcache(object):\n\t\"\"\"\n\tLRU cache with the least frequently used keys removed from the cache\n\twhen the cache grows too big.\n\t\"\"\"\n\tHIT = \"HIT\"\n\tMISS = \"MISS\"\n\n\tdef __init__(self, maxlen=100, f=None):\n\t\t\"\"\"\n\t\tInitialize the cache with maximum size `maxlen`. If provided,\n\t\tthen the frequency (precision) of the keys"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\titems = self._heap.items()\n\n\tif len(self._items) > 0:\n\t\titem = self._heap.popitem(False)\n\t\tself._items.remove(item)\n\t\treturn item\n\telse:\n\t\traise IndexError(\"Cannot pop last (empty) item: the input queue is empty\")\n\ndef test():\n\t# test inputs\n\tsamples = [\n\t\t(10, [5, 6, 7, 2, 0, 2, 6, 2, 4, 2, 4, 4, 8, 8, 4, 0,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tkey, value = heapq.heappop(self.nodes)\n\theapq.heappop(self.nodes)\n\treturn key, value\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tmost_used = numpy.array([])\n\tmost_used_count = -numpy.inf\n\tfor (key, value) in self.iteritems():\n\t\tcount = self.count[key]\n\t\tif count < most_used_count:\n\t\t\tmost_used_count = count\n\t\t\tmost_used = (key, value)\n\tself.popitem()\n\treturn most_used\n            \n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t# NOTE: In the long term, consider sorting the items on insertion here so\n\t#       they can be popped from smallest-key-first instead of largest-\n\t#       value-first.\n\t(key, value) = heapq.heappop(self.list)\n\treturn (key, value)\n\n\t#if '<' in op:\t\t\n\t#\t\tself.list = sorted([(k,v) for k,v in self.list], key=self.cmp)\n\t#else:\n\t#\t\tself.list = sort_list(self"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t(k,v) = self._d.popitem()\n\tif not self.removed.has_key(k):\n\t\tself.removed[k] = v\n\tdel(self.added_removed[k])\n\treturn (k,v) # self.added_removed.popitem()\n\t\ndef __additem(self, k, v):\n\tself.add(k,v)\n\ndef __subitem(self, k, v):\n\tself.remove(k,v)\n\ndef setdefault(self, key,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t(k, v) = self.peekitem()\n\tself.q.remove(v)\n\treturn (k, v)\n\t\nif __name__ == '__main__':\n\timport sys\n\tif len(sys.argv) > 2:\n\t\timport p3\n\t\t(p3Path, kwargs) = p3.p3ParseArgs(sys.argv[1:])\n\t\tn = len(kwargs.keys())\n\t\tp = p3.p3Client(p3Path)\n\t\tfor kw in kw"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tmin_priority, priority_used = min([(x[0], x[1]) for x in self.frequencies.iteritems()])\n\t\t\n\tself.frequencies.pop(min_priority, None)\n\tself.priorities.remove(min_priority)\n\n\treturn priority_used\n\t\n\n#\t#  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  "}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t(key, value) = pd.Series().popitem()\n\treturn key, value\n\ndef get_random_key(key_count):\n\t\"\"\"\n\tReturn a list of random keys\n\t\"\"\"\n\tkeys = list(key_count.index)\n\tnp.random.shuffle(keys)\n\treturn keys[:key_count.index.size]\n\ndef get_list_of_keys_from_dic(dic):\n\t\"\"\"\n\tGiven a dictionary, returns all keys from the dictionary as a list of integers.\n\n\t:type"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t(key, value) = heapq.heappop(heap)\n\n\treturn (key, value)\n\n\t\ndef popitem(self=None):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\n\tRaises `KeyError` when the heap is empty.\n\t\"\"\"\n\t(key, value) = heapq.heappop(heap)\n\n\treturn (key, value)\n\n\n## \t# Remove and return the k-th least frequently used (k=0 is least)\n\t# ("}]}
{"task_id": "62b8d23a48ba5a41d1c3f499", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\ttry:\n\t\trVal = self.LRU.popitem()\n\texcept IndexError:\n\t\tlogger.info(\"There are no LRU entries yet.\")\n\telse:\n\t\tlogger.debug(\n\t\t\t\"{} : {}\".format(rVal[0], rVal[1]))"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\titem = heapq.heappop(self.heap)\n\tself.queue = deque(k for k, v in self.queue)\n\tself.heap = deque(k for k, v in self.heap if v)\n\treturn item\n\t\ndef peeking(self):\n\t\"\"\"Return the most-recently used `(key, value)` pair, or `None`.\"\"\"\n\tif self.queue:\n\t\treturn self.queue[0]\n\t\ndef peek(self):\n\t\"\"\"Return the most-recently used"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\theapid = self.size() - 1\n\tself.heap_decrease(self.root, self.size())\n\treturn heapid, self.data[self.heap_value]\n\n# Return an array of (key, value) pairs\ndef items(self):\n\t\"\"\"\n\tReturn an array of all `(key, value)` pairs in the Heap.\n\t\"\"\"\n\treturn [self.data[self.idx[key]] for key in self.idx[1:]]\n\n# Return the number of keys in the He"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tminkey, min= self.llru.poppo(self.rlru)\n\tself.llru.unpoppo(min, minkey)\n\tself.lastusage[minkey]= minusage.getclock()\n\tdel self.llru[minkey]\n\treturn minkey\n\n@classmethod\ndef load(cls, filename):\n\tif filename.endswith('.pickle'):\n\t\tf= open(filename, 'rb')\n\t\tr= PickleFile.load(f)\n\t\tf.close()\n\t\treturn"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\treturn self._heap.popitem()\n\nclass Dict(dict):\n\t\"\"\"\n\t``dict`` subclass that provides an __getitem__ method for lookup.\n\t\"\"\"\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\t``dict`` method overridden to take advantage of DictLFU.\n\t\t\"\"\"\n\t\treturn dict.__getitem__(self, key)\n\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tDictLFU is always used with this method.\n\t\t\"\"\"\n\t\treturn self.pop(dict.pop(self"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\treturn self.heap.popitem()\n\n# XXX should we use __hash__ here?\n# def __hash__(self):\n#\treturn self.lru.__hash__()\ndef __hash__(self):\n\treturn self.order\ndef __repr__(self):\n\treturn \"LRUList{%d, %d}\" % (self.order, self.size)\n\nclass LRUTaker:\n\tdef __init__(self):\n\t\tself.lru = LRU(4)\n\n\tdef add(self, key, payload"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkv_pairs = self.items()\n\n\tfor key, value in kv_pairs:\n\t\tif value.time_stamp < kv_pairs[0][1].time_stamp:\n\t\t\tvalue = kv_pairs.pop(0)\n\t\t\tbreak\n\t\telse:\n\t\t\tkv_pairs.sort()\n\n\treturn key, value\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\t# The stack returned by list.pop(0) is a borrowed reference,\n\t# i.e. the wrapper object is not bound to the instance\n\t# and should never be altered.\n\t# Return a copy of the stack so that it can be modified by\n\t# whatever code called _remove_least_recently_used.\n\tstack = self._stack[:]\n\twhile stack:\n\t\t(_key, stack, i_n_e) = stack[0] # _key is borrowed reference; the key/value pair\n\t\t\t\t\t\t\t\t\t\t# is not bound\n\t\t#"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkey, value = self.heap.popitem(True)\n\tself.dict[key] = value\n\tself.counter -= 1\n\treturn (key, value)\n\nclass LRUCache(dict):\n\t\"\"\"\n\tA dictionary-like implementation where\n\t\n\tpop(x[0]) deletes x[0] from the dictionary.\n\t\n\tpush(x, y) pushes x onto the heap, then\n\treplaces the least-recently-used item.\n\t\n\tremove() returns the least-recently-used item.\n\t\"\"\""}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\traise NotImplementedError\n\n_NODE_FIFO = _NodeSetType(lambda _: [], set)\n\nclass _SortedSet(object):\n\t\"\"\"\n\tA set without sorting.\n\t\"\"\"\n\tdef __init__(self, _factory):\n\t\tsuper(_SortedSet, self).__init__()\n\t\tself._factory = _factory\n\n\tdef update(self, s):\n\t\tif issubclass(s.__class__, Set):\n\t\t\tfor item in s:\n\t\t\t\tself.add(item)\n"}]}
{"task_id": "62b8d23c48ba5a41d1c3f49b", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tmost_recent = self.data.popleft()\n\tself.data[most_recent[0]] = most_recent[1]\n\treturn most_recent\n\n\t#-----------------------------------------------------------------------------\n\nclass ThreadSafeCounter(object):\n\t@staticmethod\n\tdef defaultInitVal():\n\t\t\"\"\"\n\t\tInitialize a counter with default value.\n\t\t\"\"\"\n\n\t\treturn 0\n\n\t#-----------------------------------------------------------------------------\n\nclass ThreadSafeMap(object):\n\t\"\"\"\n\tA mapping class that ensures any new method added to it are automatically run\n\tin the calling thread.\n\n\tNote: This"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\t(key, value) = self.pqueue.popleft()\n\tself.cache[key] = value\n\treturn(key, value)\n\n\t\nclass SortedListFromFunction (object):\n\t\"\"\"\n\tThis object allows you to specify a function of two arguments to order a list.\n\tUsing the Cython implementation, the cache will be a dictionary of lists of\n\tsorted data in the order that is returned from this function.\n\t\"\"\"\n\tdef __init__(self, compare=None, key=None):\n\t\t\"\"\"\n\t\tCompare(compare("}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\traise KeyError\n\n# The following two functions are for compatibility w/ the existing name space\n\ndef setdefault(self, key, default=None):\n\traise TypeError\n\ndef get(self, key, default=None):\n\traise\n\nclass _ContextLib:\n\n\t\ndef context(self):\n\t\"\"\"\n\tReturns a context manager that provides a `__exit__` function that restores\n\tthe *saved state* of this *libpy* context. This is used in case the user\n\twishes to alter values in a particular scope.\n\n\tAs an"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\titem = self.queue.popleft()\n\tdel self.history[item]\n\treturn item\n\t\n\t\n\t\nclass HistoryManager():\n\t\"\"\"\n\tAn example of how to use the history system in Python.\n\t\"\"\"\n\t\n\tdef __init__(self, historyfile=\"dict.txt\"):\n\t\tself.history = History()\n\t\tself.history_manager = HistoryManager(self.history)\n\t\t\n\t\tself.history_factory = None\n\t\t\n\t\tif historyfile and os.path.exists(historyfile):\n\t\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\treturn self.cache.popitem()\n\n\nclass memoize:\n\t\"\"\"\n\tA decorator for functions cache their outputs for reuse. The @memoize\n\tdecorator adds a cache to the function called `func`. Cache entries will\n\tbe stored in a dict and use a timestamp to determine if an item has been\n\talready considered recently.\n\t\"\"\"\n\n\tdef __init__(self, func):\n\t\tself.func = func\n\t\tself.cache = {}\n\t\tself.timestamps = {}\n\t\tself.timeout = -1 # set the default cache"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\ttry:\n\t\treturn self.__d.popitem()\n\texcept KeyError as e:\n\t\traise KeyError(e)\n\t\nclass MultiDict(dict):\n\t\"\"\"\n\tA `dict` subclass where `get` returns `MultiDictEntry` instances for the keys.\n\t\"\"\"\n\tdef __init__(self, arg=None):\n\t\t\"\"\"\n\t\tCreate a `MultiDict`, optionally initializing it with a dictionary.\n\t\t\n\t\t:param arg: The dictionary to initialize the `MultiDict` with.\n\t\t:type arg: dict"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\tk,v = heappop(self.items)\n\t\t\n\treturn (k, v)\n\n\ndef popleft(self):\n\t\treturn heappop(self.items)\n\ndef heapify(self):\n\tfor i in xrange(self.size -2,-1,-1):\n\t\t#heapify the item that is i elements to the left\n\t\ti_item = i\n\t\twhile 1:\n\t\t\tparent = i // 2\n\t\t\tif parent < 0 or parent >= self.size:\n\t\t\t\treturn\n\t\t\t\n\t\t\tif self.data[parent"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\treturn self.__dict[self.most_recent].value\n\t\n\nclass LRU_Cache2:\n\t\"\"\"\n\tA non-thread-safe cache, implemented as a dict where keys are\n\tintegers, while the values are cache entries themselves.\n\t\"\"\"\n\tdef __init__(self, capacity):\n\t\tself.capacity = capacity\n\t\tself.dict = {}\n\t\t\n\tdef __contains__(self, key):\n\t\treturn key in self.dict\n\t\t\n\tdef __getitem__(self, key):\n\t\treturn self.dict["}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\treturn self.cache.popitem()\n\n\ndef _getattributeview(item, *attributes):\n\t\"\"\"\n\tBuild a list of all attributes and a simple dictionary of the rest.  The former\n\tis needed because attribute lookup is done in the item, and the latter for\n\tperformance.\n\n\t:param item: The item to generate a view for.\n\t\n\t:param attributes: The attributes to list.  Either a string (single attribute)\n\tor a collection of attributes (multiple).\n\t\"\"\"\n\t# Generate the attributes\n\ttry: attributes = list("}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return the `(key, value)` pair most recently used.\n\t\"\"\"\n\ttry:\n\t\treturn self._cache.popitem()\n\texcept IndexError:\n\t\treturn None\n\nclass LRUDict(UserDict):\n\t\"\"\"\n\tA `UserDict` with `setdefault` and `popitem` support.\n\n\tNote: `iterkeys` and `itervalues` are inefficient.\n\n\t>>> d = LRUDict()\n\t>>> d.setdefault('a',1)\n\t>>> d['a']\n\t1\n\t>>> d.setdefault('b',1)\n\t>>> d['a']\n\t1\n\n"}]}
{"task_id": "62b8d23e48ba5a41d1c3f49e", "prompt": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn (self.popseq(), self.popseq())\n\n\nclass CacheSequencer(Sequencer):\n\t\"\"\"\n\tSequence keys using time.\n\t\"\"\"\n\tdef __init__(self):\n\t\tSequencer.__init__(self)\n\t\tself.lastcall = 0\n\n\t\t#self.c = 0\n\n\tdef check(self, t, args):\n\t\t\"\"\"\n\t\tCalled regularly with arguments to return a cache sequence.\n\t\t\"\"\"\n\t\treturn self._random(t = t, *args)\n\n\tdef _random(self, t = 0,"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn self.__gen.popitem()\n\ndef pop(self):\"\"\"\n\tRemove and return a random `key`.\n\t\"\"\"\n\treturn self.__gen.pop()\n\ndef setdefault(self, key, value):\"\"\"\n\tReturn `value` if `key` is not in the dictionary.\n\tOn return, the dictionary is the same as before.\n\t\"\"\"\n\treturn self.__gen.setdefault(key, value)\n\ndef update(self, **kwargs):\"\"\"\n\tUpdate and return a `dict_object`.\n\t\"\"\"\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn self.dictview.popitem()\n\ndef peekitem(self):\n\t\"\"\"\n\tPeek at the next item to be retrieved.\n\n\tIf no more items are available, `KeyError` will be raised.\n\t\"\"\"\n\tself.dictview.seek(0)\n\treturn self.dictview.read()\n\ndef pkgtuple(self, name):\n\treturn _PackageTuple(self, name)\n\ndef sort(self):\n\tself.pkgs.sort()\n\n\tdef sortkey(p) :\n\t\t\n\t\tif p"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tif not self.items:\n\t\t\treturn None\n\telse:\n\t\treturn heappop(self.items)\n\ndef pqueue(heap):\n\t\"\"\"\n\tThe priority queue's internal min-heap data structure, in which the first items and second\n\titems are swapped out to the top.\n\t\"\"\"\n\treturn heap\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\twhile True:\n\t\tkey = random.choice(self.keys())\n\t\tvalue = self.pop(key)\n\t\tif value is not None:\n\t\t\treturn key, value\n\nclass TestFuzzy(unittest.TestCase):\n\t#\n\tdef test_pickle(self):\n\t\t\"\"\"Test pickling / unpickling.\"\"\"\n\t\timport pickle\n\t\tfuzzy = FuzzyDictionary()\n\t\tfuzzy[1, 'b'] = 2\n\t\tfuzzy[2, 'a'] = 3\n\t\tfuzzy["}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tself.queue.popitem()\n\treturn self.queue.pop()\n\nclass Random:\n\t\"\"\"\n\tGenerates a random key in a given range, and returns it.\n\t\n\tIf the key's `range` attribute is set to ``False``, the random value\n\tis not generated.\n\t\"\"\"\n\tdef __init__(self, range):\n\t\tself.range = range\n\t\n\tdef next(self):\n\t\tif self.range:\n\t\t\tmin = 0\n\t\t\tmax = self.range\n\t\telse:\n\t\t\tmin = None\n\t\t\t"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tnew = self._n - 1, self._choices[self._cur: self._n]\n\tself._cur, self._choices[self._cur:self._n] = self._cur, self._choices[: self._cur + 1]\n\tself._n -= 1\n\treturn new\n\n\t\n\ndef init(self):\n    \"\"\":See `Stack` and `List`.\"\"\"\n    self._top = self._n = 0\n\n\ndef push(self, value):\n    \"\"\":See `Stack` and `List`.\"\"\"\n    self"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\ttry:\n\t\t\tkey, value = self.tally.popitem()\n\texcept IndexError:\n\t\t\treturn\n\treturn (key, value)\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\treturn next(iter(self.__dict__.items()))\n\n\t\n\n\nclass Stack:\n\t\"\"\"\n\tA stack implementation.\n\t\"\"\"\n\t\n\tdef __init__(self):\n\t\tself.__stack = []\n\t\n\t\n\tdef __len__(self):\n\t\treturn len(self.__stack)\n\n\n\n\tdef push(self, obj):\n\n\n\t\tindex = len(self.__stack)\n\t\tself.__stack = self.__stack[:index] + [(obj, index)] + self.__stack[index:]\n\n\n"}, {"generated_text": "def popitem(self):\n\"\"\"\n\tRemove and return a random `(key, value)` pair.\n\t\"\"\"\n\tself.__popsample()\n\treturn self.__items.pop()\n\ndef setdefault(self, key, default):\n\t\"\"\"\n\tAdd an item to the beginning of the lru list if it doesn't exist.\n\tUse default on the way if there is no such key.\n\t\"\"\"\n\tself.__popsample()\n\tif key not in self.__items:\n\t\tif len(self.__items) > self.__lru_treshold:\n\t\t\tself.__items.popitem()\n\t\tself.__items[key] ="}]}
{"task_id": "62b43425903eeb48555d3ea1", "prompt": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\t# Get all regexes.\n\tregexes = self.param_regexes.copy()\n\n\t# Create the regex to check for a parameter that's an exact match.\n\tself._check_id = '{0}'.format(self.__class__.__name__)\n\tregex = '^'.join(regexes[self.check_indexes[\"exact\"]])\n\n\t# Create the regex to check for multiple parameters that happen to have the same name (using lower-case letters).\n\tfor check_name, check_type in self"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tif self.in_style == \"plain\":\n\t\treturn \".*\"\n\telse:\n\t\tregex = \"\"\n\t\tif len(self.parameters) == 1:\n\t\t\treturn \"(\" + self.parameters[0] + \")\"\n\t\telse:\n\t\t\tregex = \"[\\\\s+]\"\n\t\tfor parameter in self.parameters:\n\t\t\tif \"*\" in parameter or \"?\" in parameter or \"!\" in parameter:\n\t\t\t\tregex += r\"\\\\\\(\" + parameter + r\"\\\\)\\\\\"\n\t\t\telse:\n\t\t\t\tregex += parameter +"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t# TODO: make this more robust\n\t# see _create_regex()\n\tregex = re.compile(r\"\"\"\n\t    ^\n\t\t ( \n\t\t   (?P<tag>[^\\s]+?) \n\t\t\t (?:\\s+ (?P<type>literal)/(string)?  )\n\t\t\t [ \\t]*\n\t\t ) \n\t\t (,\\s* )*\n\t    $\n\t    \"\"\", re.VERBOSE)\n\treturn regex\n\n\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tpattern = _create_regex()\n\t\n\tif self._param_type!= 'in':\n\t\tlogger.warning('Type has to be \\'in\\'.'.format(), extra=self._get_trace_info())\n\t\n\tif self._param_constraints is not None and not isinstance(self._param_constraints, bool):\n\t\tsorted_constraints = sorted(list(self._param_constraints.keys()), key=lambda x: (len(x), x), reverse=True)\n\t\t\n\t\tfor constraint_key"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn self._get_pattern(inregex=True)\n\n\n\n#\n#\tGetters\n#\ndef _get_numeric_regex(self) -> Pattern:\n\t\"\"\"\n\tGet a pattern that searches for numeric values, as defined by :func:`_compile_number_regex`.\n\t\n\tReturns the :class:`re.Pattern` created by :func:`_compile_number_regex`.\n\t\"\"\"\n\treturn self._get_pattern(regex=True, numeric=True)\n\n\ndef _get_number_"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tpattern = re.compile(\"\\(\" + self.attribute_name + \"\\s*in\\s*\\(\" + self.value_attribute_name + \"\\s*\\)\", re.IGNORECASE | re.DOTALL)\n\treturn pattern\n\n\ndef _validate_in_regex(self, in_regex: Pattern, value: str) -> bool:\n\t\"\"\"\n\tCheck if the given value is matching the in-style regular expression of a :class:`ParameterDict`.\n\t\n\tNote that the in-style regular expression will be"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(self.PATTERN[1])\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\treturn re.compile(self._re_param_pattern % '.*')\n"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r\"({})\".format(self.query))"}, {"generated_text": "def _create_in_regex(self) -> Pattern:\n\"\"\"\n\tCreate the in-style parameter regular expression.\n\t\n\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\treturn re.compile(r\"\"\"\n\t\t(\n\t\t\t?:                         # Begin non-captured group, non-captured part\n\t\t\t(?:{0:s}{1:s}*              # Allow one occurrence of: any characters preceding $ and one occurrence of the special in the parameter or name\n\t\t\t\\{(?P<escaped>\\w*)\\}       # The second occurrence of $ is escaped\n\t\t\t{1:s}\\w*\\{                 # The first occurrence of parameter or name which begins with \\w is captured in group \"$\" after capturing it with an non-capt"}]}
{"task_id": "62b43426903eeb48555d3ea2", "prompt": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting._Converter(self)\n\n\n# Initialize all converters.\n_converter = _create_converter()\n\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting._Converter()\n\ndef _parameter_styles():\n\t\"\"\"Retrieves a dictionary of all known parameter styles.\"\"\"\n\treturn {\n\t\t\"scalar\": _parameter_style_scalar,\n\t\t\"list\": _parameter_style_list,\n\t\t\"struct\": _parameter_style_struct,\n\t\t\"dict\": _parameter_style_dict,\n\t\t\"mapped\": _parameter_style_mapped,\n\t}\n\ndef _resolve(specified_style: str, style: str"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\t\n\treturn _Converter(self._parameter_types, self._dsl_defaults)\n\n\ndef _normalize_parameter_dsl(dsl_parameter: DslParameter):\n\t\"\"\"\n\tTransforms a :class:`DslParameter` into a :class:`_ParameterDef` class.\n\t\n\tReturns the :class:`_ParameterDef` class. \n\t\n\t**Returns**:\n\t\n\t\t:class:`_ParameterDef`: The converted :class:`DslParameter`.\n\t\"\"\"\n\t\n"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting.Converter(self.__module, False)\n\n\tfor param_name in self.__parameter_style:\n\t\tstyle = self.__parameter_style[param_name]\n\n\t\tif style == 'in':\n\t\t\tconverter.add_input(param_name)\n\t\telse:\n\t\t\tconverter.add_output(param_name)\n\n\treturn converter\n\n\n_parameter_style = [\n\t# In parameters\n\t'int:label',\n\t'string:id',\n\t'multi"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter_map: Dict[str, _converting._Converter] = {\n\t\t\t\"b\": self._b_converter,\n\t\t\t\"I\": self._parameter_value_converter,\n\t\t\t\"f\": self._parameter_value_converter,\n\t\t\t\"d\": self._parameter_value_converter,\n\t\t\t\"f8\": self._parameter_value_converter,\n\t\t\t\"f4\": self._parameter_value_converter,\n\t}\n\tconverter: _converting"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting._Converter(data, self.defaults, self.dummies_allowed, self.values + self.others, self.choices)\n\n\t\ndef _add_parser_options(cls) -> None:\n\t\"\"\"\n\tAdd options which should be added to the given parser object.\n\t\n\tReturns the newly added options.\n\t\"\"\"\n\tcls.add_argument(\"--dummies-allowed\", action=\"store_true\", help=\"Allow creation of DUMMIES!\")\n\tcls.add_argument(\"--others"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting._Converter()\n\n\nclass _Converter(Converter, ParameterMeta):\n\t\"\"\"\n\tParameter style converter.\n\t\n\tThis is how you use the converter:\n\t\n\t>>> from parameterize.convert import *\n\t>>> from my_class import my_class_function\n\n\t>>> converter = _Converter()\n\n\tIt looks like this::\n\n\t    >>> _Converter(my_class_function)\n\n\t.. versionadded:: 0.3.0\n\t.. versionchanged:: 0.4.5\n\t\t"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tif self.converter is None:\n\t\tassert self.parameter_style in [\n\t\t\t\"positional\", \"kwargs\", \"args\", \"format\"\n\t\t]\n\t\tif self.parameter_style == \"positional\":\n\t\t\tconverter_class = param_style.create_positional_converter\n\t\t\tself._converter = converter_class(self.conversion_override)\n\t\telif self.parameter_style in [\"kwargs\", \"format\"]:\n\t\t\tconverter_class = param_style.create_kwargs"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting._Converter(self.vartype.conversion.validator)"}, {"generated_text": "def _create_converter(self) -> _converting._Converter:\n\"\"\"\n\tCreate the parameter style converter.\n\t\n\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\treturn _converting.Converter(self.paramstyle)\n"}]}
{"task_id": "62b8966c755ee91dce50a154", "prompt": "def isoparse(self, dt_str):\n\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\t\n\tAn ISO-8601 datetime string consists of a date portion, followed\n\toptionally by a time portion - the date and time portions are separated\n\tby a single character separator, which is ``T`` in the official\n\tstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n\tcombined with a time portion.\n\t\n\tSupported date formats are:\n\t\n\tCommon:\n\t\n\t- ``YYYY``\n\t- ``YYYY-MM`` or ``YYYYMM``\n\t- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\t\n\tUncommon:\n\t\n\t- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n\t- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\t\n\tThe ISO week and day numbering follows the same logic as\n\t:func:`datetime.date.isocalendar`.\n\t\n\tSupported time formats are:\n\t\n\t- ``hh``\n\t- ``hh:mm`` or ``hhmm``\n\t- ``hh:mm:ss`` or ``hhmmss``\n\t- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\t\n\tMidnight is a special case for `hh`, as the standard supports both\n\t00:00 and 24:00 as a representation. The decimal separator can be\n\teither a dot or a comma.\n\t\n\t\n\t.. caution::\n\t\n\t    Support for fractional components other than seconds is part of the\n\t    ISO-8601 standard, but is not currently implemented in this parser.\n\t\n\tSupported time zone offset formats are:\n\t\n\t- `Z` (UTC)\n\t- `\u00b1HH:MM`\n\t- `\u00b1HHMM`\n\t- `\u00b1HH`\n\t\n\tOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n\twith the exception of UTC, which will be represented as\n\t:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n\tas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\t\n\t:param dt_str:\n\t    A string or stream containing only an ISO-8601 datetime string\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` representing the string.\n\t    Unspecified components default to their lowest value.\n\t\n\t.. warning::\n\t\n\t    As of version 2.7.0, the strictness of the parser should not be\n\t    considered a stable part of the contract. Any valid ISO-8601 string\n\t    that parses correctly with the default settings will continue to\n\t    parse correctly in future versions, but invalid strings that\n\t    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n\t    guaranteed to continue failing in future versions if they encode\n\t    a valid date.\n\t\n\t.. versionadded:: 2.7.0\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 76.00 MiB (GPU 0; 22.16 GiB total capacity; 20.55 GiB already allocated; 19.88 MiB free; 21.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b896de755ee91dce50a183", "prompt": "def parse(self, timestr, default=None,\n\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\t\n\t:param timestr:\n\t    Any date/time string using the supported formats.\n\t\n\t:param default:\n\t    The default datetime object, if this is a datetime object and not\n\t    ``None``, elements specified in ``timestr`` replace elements in the\n\t    default object.\n\t\n\t:param ignoretz:\n\t    If set ``True``, time zones in parsed strings are ignored and a\n\t    naive :class:`datetime.datetime` object is returned.\n\t\n\t:param tzinfos:\n\t    Additional time zone names / aliases which may be present in the\n\t    string. This argument maps time zone names (and optionally offsets\n\t    from those time zones) to time zones. This parameter can be a\n\t    dictionary with timezone aliases mapping time zone names to time\n\t    zones or a function taking two parameters (``tzname`` and\n\t    ``tzoffset``) and returning a time zone.\n\t\n\t    The timezones to which the names are mapped can be an integer\n\t    offset from UTC in seconds or a :class:`tzinfo` object.\n\t\n\t    .. doctest::\n\t       :options: +NORMALIZE_WHITESPACE\n\t\n\t        >>> from dateutil.parser import parse\n\t        >>> from dateutil.tz import gettz\n\t        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n\t        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n\t        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n\t        datetime.datetime(2012, 1, 19, 17, 21,\n\t                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\t\n\t    This parameter is ignored if ``ignoretz`` is set.\n\t\n\t:param \\*\\*kwargs:\n\t    Keyword arguments as passed to ``_parse()``.\n\t\n\t:return:\n\t    Returns a :class:`datetime.datetime` object or, if the\n\t    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n\t    first element being a :class:`datetime.datetime` object, the second\n\t    a tuple containing the fuzzy tokens.\n\t\n\t:raises ParserError:\n\t    Raised for invalid or unknown string format, if the provided\n\t    :class:`tzinfo` is not in a valid format, or if an invalid date\n\t    would be created.\n\t\n\t:raises TypeError:\n\t    Raised for non-string or character stream input.\n\t\n\t:raises OverflowError:\n\t    Raised if the parsed date exceeds the largest valid C integer on\n\t    your system.\n\t\"\"\"\n\t", "error": "CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 22.16 GiB total capacity; 20.60 GiB already allocated; 19.88 MiB free; 21.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF", "suggestions": []}
{"task_id": "62b8a4a4755ee91dce50a3d3", "prompt": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\treturn self.s_to_z(dt[0], dt[1])\n\n\ndef tobz(ds):\n\t\"\"\"\n\tConverts a date/time zone to a timezone-aware :class:`datetime.datetime` object.\n\t\n\t:param ds:\n\t    A :class:`datetime.datetime` object. Can be an ambiguous one\n\t    (e.g. if it's the first occurrence of a date in a recurring\n\t    date).\n\t\"\"\"\n\tif ds is None:\n\t\treturn None\n\t"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\treturn self.fromutc(dt.utctimetuple())\n\t\n\t\n\t\n"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t# Get the \"fold\" timezone.\n\tf_dt = dt.replace(tzinfo=_FOLD_TZ)\n\t\n\t# Try and get a previous \"fold\" time.\n\t# (This is really a sanity check for when we know there is no previous day to consider.)\n\tfl = _is_previous_ambiguous_day(f_dt)\n\tif fl:\n\t\treturn _fold_datetime(fl, dt)\n\t\n\t# Get the next \"fold\" time.\n\t# (This can be used to determine"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif dt.tzinfo is self:\n\t\t# TODO: use dt.astimezone(self) once we find an ambiguous datetime\n\t\treturn dt.replace(tzinfo=self)\n\t\n\telif dt.tzinfo is None:\n\t\treturn dt\n\t\n\t# In the rare case the zone-aware datetime already indicates a\n\t# timezone-aware object, we return it without further processing.\n\treturn dt\n\ndef fromtt(value, tzone):\n\t\"\"\"\n\tGiven a :class:`datetime."}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt = dt.astimezone(self.dst)\n\n\tif self.fold:\n\t\tif dt.date() in self.fold_dates:\n\t\t\t# Date is in the fold period, but the timezone can have changed.\n\t\t\t# (the fold period is inclusive of the year)\n\t\t\tdt = dt.replace(tzinfo=self)\n\t\telse:\n\t\t\tdelta = abs((dt.date() - self.ref_date) + 1)\n\t\t\tif delta <= self.dst_begin:\n\t\t\t\tdelta += 1\n\t\t\tdt ="}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt = dt.astimezone(self._tzone)\n\treturn datetime(dt.year, dt.month, dt.day, dt.hour, dt.minute, \\\n\t\tdt.second, dt.microsecond, dt.tzinfo)\n\n@deprecated\ndef removefold(self, dt):\n\twarnings.warn(\"The removefold method is deprecated. \"\n\t\t\"Use foldtime.is_ambiguous(dt) instead.\",\n\t\tDeprecationWarning)\n\treturn self.foldtime.is_ambiguous"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\t# Determine whether the target is unambiguous and ambiguous.\n\tambiguous = False\n\tif self.defaultdst == 0:\n\t\tambiguous = dt.timetuple().tm_hour < self.dst_start_hour\n\telif self.defaultdst > 0:\n\t\tambiguous = (dt.timetuple().tm_hour <= self.dst_start_hour) or \\\n\t\t\t(dt.timetuple().tm_hour > self.dst_end_hour)\n\telse:\n\t\t# If the timezone is"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt.astimezone(self)\n\t# Find out if the date is ambiguous in this timezone\n\tambiguous = self._is_ambiguous_datetime(dt)\n\tif ambiguous:\n\t\t# Set the local timezone to the canonical ambiguous datetime\n\t\tself.local.replace(dt)\n\telse:\n\t\t# Simply set local timezone to new timezone\n\t\tself.local.replace()\n\t# Force return type\n\treturn self.local\n\t\n\t\ndef get_utc_epoch_start(timezone=None):\n\t\"\"\""}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tfrom.funk import asctime\n\t\n\tif dt.tzinfo:\n\t\treturn dt.astimezone(self.zone) # return with timezone\n\telse:\n\t\treturn self.get(dt).astimezone(self.zone) # we don't have a tzinfo, don't convert dt\n\t\n\t# FIXME: do we really want to be this hard-wired? # FIXME: what the hell should we return?\n\nfromnow = fromutc\n\nfrom.funk import getfunk\nfrom.times import as"}, {"generated_text": "def fromutc(self, dt):\n\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\n\ttimezone-aware datetime in a new timezone.\n\t\n\tSince this is the one time that we *know* we have an unambiguous\n\tdatetime object, we take this opportunity to determine whether the\n\tdatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n\toccurrence, chronologically, of the ambiguous datetime).\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tif self.isambiguity(dt):\n\t\tres = dt.replace(tzinfo=None)\n\t\tif res.folded:\n\t\t\treturn res\n\treturn dt\n\t\ndef toutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime that is in UTC, calculates a\n\ttimezone-aware datetime in a given timezone.\n\t\n\t:param dt:\n\t    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tres = dt.replace"}]}
{"task_id": "62b8a7b2755ee91dce50a4a7", "prompt": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif not is_naive(dt): return dt\n\t# FIXME: need to test this\n\tif dt.tzinfo is None: dt = dt.replace(tzinfo=dflt_tz)\n\treturn dt.astimezone(gettz(tzinfo))\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\tif tzinfo is None:\n\t\t\treturn dt\n\t\telse:\n\t\t\tdt = dt.tzinfo._transition_dt_to_aware(dt, tzinfo)\n\telse:\n\t\tif tzinfo is None and dt.tzinfo.utcoffset(dt).total_seconds() is None:\n\t\t\treturn dt\n\n\treturn dt\n\ndef set_default_timezone(default_timezone, tzname=None):\n\t\"\"\"\n\tSets the default time zone to"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo or default_tzinfo())\n\telse:\n\t\treturn dt.astimezone(tzinfo)\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif tzinfo is not None and not dt.tzinfo:\n\t\ttzinfo_cls = tzinfo if isinstance(tzinfo, basestring) else tzinfo.__class__\n\t\tdt = tzinfo_cls(dt.utcoffset())\n\t\tif dt.utcoffset() is None:\n\t\t\tdt = dt.replace(\n\t\t\t\t\ttzinfo=tzinfo_cls(dt.dst()) if dt.dst() is not None else None\n\t\t\t\t\t)\n\treturn dt\n\"\"\"\nThe datetime"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t# Try just passing it a tzinfo\n\tif not hasattr(dt, 'tzinfo'):\n\t\ttry:\n\t\t\t# See if the tzinfo-format we are passed supports easy\n\t\t\t# parsing\n\t\t\tdt = tzinfo(dt, tzinfo)\n\t\texcept (TypeError, AttributeError):\n\t\t\t# Passed a non-aware object, or is aware but not tzinfo. We\n\t\t\t# pass this through.\n\t\t\treturn dt\n\n\t# Check if we need to replace the tzinfo\n\tif dt.utcoffset()"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\telse:\n\t\tdt = dt.astimezone(tzinfo)\n\treturn dt\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdef _parse(*args, **kwargs):\n\t\tkwargs[\"tzinfo\"] = tzinfo\n\t\treturn datetime.datetime(*args, ** kwargs)\n\t\t\n\tif hasattr(dt.replace(tzinfo=None), \"to\"):\n\t\t\treturn dt.replace(tzinfo=None)\n\treturn _parse(dt.strftime(\"%Y-%m-%d %H:%M:%S\"), **kwargs)\n\t\n@contextmanager \ndef get_timezone(tz):    \n\t\"\"\"\n\tA function"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt\n\t\n_UTC = tzoffset(None, 0)\n\n\n# -- UTC time zone --\nutc = tzoffset(None, 0)\n\n# -- Time zone definitions --\n\nTIMEZONE_BROKEN = True\nZERO = int(0)\n\n\ndef gmtimezone(offset):\n\t\"Builds a GMT object from an offset in seconds. Used by dateutil's gmtime.\"\n\treturn pytz"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\treturn dt.replace(tzinfo=tzinfo)\n"}, {"generated_text": "def default_tzinfo(dt, tzinfo):\n\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\t\n\tThis is useful for example when you are provided a datetime that may have\n\teither an implicit or explicit time zone, such as when parsing a time zone\n\tstring.\n\t\n\t.. doctest::\n\t\n\t    >>> from dateutil.tz import tzoffset\n\t    >>> from dateutil.parser import parse\n\t    >>> from dateutil.utils import default_tzinfo\n\t    >>> dflt_tz = tzoffset(\"EST\", -18000)\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n\t    2014-01-01 12:30:00+00:00\n\t    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n\t    2014-01-01 12:30:00-05:00\n\t\n\t:param dt:\n\t    The datetime on which to replace the time zone\n\t\n\t:param tzinfo:\n\t    The :py:class:`datetime.tzinfo` subclass instance to assign to\n\t    ``dt`` if (and only if) it is naive.\n\t\n\t:return:\n\t    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tif dt.tzinfo is None:\n\t\treturn local_timezone(dt).localize(dt, is_dst=None).astimezone(tzinfo)\n\treturn dt\n\n"}]}
{"task_id": "6305f9991d275c6667163c50", "prompt": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\n\tself.cut_chars_before = before\n\tself.cut_chars_after = after\n\nclass SplitFileName:\n\t\"\"\"\n\t\"\"\"\n\nclass EmptyFileHandle:\n\t\"\"\"\n\t\"\"\"\n\nclass NoFileHandle:\n\tpass\n\nclass FileName:\n\t\"\"\"\n\t\"\"\"\n\ndef file_name_is_valid(file_name: str) -> bool:\n\t\"\"\"\n\tCheck for an file name, ensuring it doesn't start or end with a slash,\n\twhich violates the file path rules for Windows, and that it"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\n\tassert isinstance(before, bytes), 'Input must be bytes or instance of str.'\n\t\n\tassert isinstance(after, bytes), 'Input must be bytes or instance of str.'\n\t\n\tself.before_char = before\n\tself.after_char = after\n\treturn\n\n# End of program\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself._split_chars = (before, after)\n\n\tself._regexp_split = regexp.compile('|'.join(self._split_chars))\n\n\tif not self._options['keep_non_dup_files']:\n\t\tdef is_duplicate_file(file: str) -> bool:\n\t\t\treturn self._regexp_split.match(file) is not None\n\t\t# TODO: If we would just copy into new list and sort it, we don't need to keep all files.\n\t\tself._duplicated_"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tassert isinstance(before, bytes)\n\tassert isinstance(after, bytes)\n\tself.before_split = before\n\tself.after_split = after\n\treturn\n\ndef validate_slice_chars(filename: str) -> None:\n\t\"\"\"\n\tValidate the slice chars used in the slice_chars for the given file for all the files.\n\t\n\tArgs:\n\t    filename: The file name used for the file to use the valid slice chars for.\n\t\"\"\"\n\tfor file in glob.iglob(filename):\n\t\tif not"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\t\n\t# Set the before and after character properties for each delimiter char\n\tself.before = before\n\tself.after  = after\n\tself.chr_lens = tuple([ len(x) for x in before ] + [ len(x) for x in after ] )\n\t\n\t\n\t# Compute the size of the file to be split\n\tfsize = os.stat(self.filename).st_size\n\tsplit_size = fsize / len(before)\n\tself.size = split_size * len(before) + fsize % len"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.cut_chars: t.List[bytes] = [before, after]\n\ndef find_file(search_path: bytes, split_at: bytes, **kwargs) -> FileSearchResult:\n\t\"\"\"\n\tFind a file based on the path and file split at the specified character.\n\t\n\tArgs:\n\t    search_path: The path to search.\n\t    split_at: Split the file path at this character.\n\t    kwargs: Passed on to `find_slices`.\n\t\n\tReturns:\n\t    If found:"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.__cut_chars = (before, after)\n\n    def get_cut_chars(self) -> Union[bytes, None]:\n        \"\"\"\n\t\tGet the bytes used to delimit slice points.\n\t\t\n\t\tReturns:\n\t\t\tA tuple containing the bytes before and after the slices.\n\t\t\tEmpty tuple is returned if cut characters were not explicitly set.\n\t\t\"\"\"\n        return self.__cut_chars\n    \n    def split(self, byte_to_split: bytes) -> bytes:\n        \"\"\"\n\t\tSplit a block of bytes by the bytes used to"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = list(before)\n\tself.after = list(after)\n\tself.cut_chars = 0\n\nclass SplitConfig(object):\n\tdef __init__(self, n, s, cut_chars, verbose=False):\n\t\tself.cut_chars = cut_chars\n\t\tself.n = n\n\t\tself.s = s\n\t\tself.verbose = verbose\n\ndef split_config(config: SplitConfig, n: int, s: int, cutoff: int, verbose: bool = False) -> int"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself._delims[0] = before\n\tself._delims[1] = after\n\n\tlog.debug('Set delims: %s and %s', list(self._delims[0]), list(self._delims[1]))\n\n\ndef split_cut(self, filename: bytes, split_points: bytes) -> (list, int):\n\t\"\"\"\n\tSplit up the file and return the list of new filenames.\n\tArgs:\n\t    filename: Full file path.\n\t    split_points: Full file path.\n\tReturns:\n"}, {"generated_text": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\"\"\"\n\tSet the bytes used to delimit slice points.\n\t\n\tArgs:\n\t    before: Split file before these delimiters.\n\t    after: Split file after these delimiters.\n\t\"\"\"\n\tself._cut_chars = (before, after)\n\n\t\nclass FileWrapper(object):\n\t\"\"\"\n\tWrapper that ensures files are closed.\n\t\"\"\"\n\t# pylint: disable=too-many-arguments\n\tdef __init__(self, fname: str, **kwds) -> None:\n\t\t\"\"\"\n\t\tArgs:\n\t\t    fname: Open a file.\n\t\t    kwds: Open parameters.\n\t\t\"\"\"\n\t\tif'mode' not in kwds:\n\t\t\tkwds['mode'] = 'rb'\n\t\t"}]}
{"task_id": "6306292052e177c0ba469f09", "prompt": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\tif isinstance(request.payload, DiasporaPayload):\n\t\treturn True\n\t\n\telif not request:\n\t\t# Diaspora has been refactored, it generates requests without payloads.\n\t\tif isinstance(request.payload, (DiasporaMessage,DiasporaMessagePublicPrivate)):\n\t\t\treturn True\n\t\t\t\n\treturn False\n\n\ndef get_private_msg_id(msg: DiasporaMessage):\n\t\"\"\"\n\tIf the message is not public and belongs to a private conversation, return the msg id of the conversation."}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\tif'messages' not in request.keys():\n\t\treturn False\n\n\t# Try finding first public message\n\ttry:\n\t\tpublic_message = filter(lambda message: message['type'] =='stream' and message['user']['is_bot'] == False, request['messages'])[0]\n\t\treturn public_message['content']\n\texcept (IndexError, KeyError):\n\t\tpass\n\n\t# Try finding a private message\n\ttry:\n\t    private_message = filter(lambda message: message['type'] == 'private', request['mess"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\ttry:\n\t\treturn message_payload(request)\n\texcept:\n\t\ttry:\n\t\t\treturn private_message_payload(request)\n\t\texcept:\n\t\t\treturn False\n\t\t\n########################################################################################################\n########################################################################################################\n########################################################################################################\n########################################################################################################\n\nif __name__ == '__main__':\n\tpass\n\n"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tif request.method!= \"GET\" or \"sender\" not in request.GET:\n\t\treturn\n\t\n\tif request.GET[\"sender\"] not in [message['sender'] for message in public_messages]:\n\t\ttry:\n\t\t\tpayload = json.loads(request.body)\n\t\texcept ValueError:\n\t\t\treturn\n\t\tlegacy = {\"contentId\": payload.get(\"contentId\", None)}\n\t\tif \"sender_name\" in payload:\n\t\t\tlegacy[\"senderName\"] = payload[\"sender_name\"]\n\t\telse:\n"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\t\n\tif not (request.subject_message or request.content_message):\n\t\treturn False\n\t\n\t# This is Diaspora request.\n\tif isinstance(request, PublicMessageRequest):\n\t    \n\t    # The type field should be set to `PublicMessagePayload`\n\t    \n\t    if not isinstance(request.content_message, PublicMessagePayload):\n\t        return False\n\t    \n\t    # The channel and topic should match to the topic of the incoming request\n\t    \n\t    if request.content_message.channel!= request"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tpublic_key = request.text\n\tprivate_key = request.get('payload_from')\n\treturn is_public_message(public_key, private_key) and do_receive_payload(private_key, request, request.get('payload_from'))\n\t\n\ndef is_requestor_key(request: RequestType):\n\t\"\"\"\n\tCheck that if the message is signed the requestor is in the payload from field\n\t@return bool\n\t\"\"\"\n\tpublic_key = request.text\n\tprivate_key = request.get"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tpayload = request.payload\n\ttry:\n\t\tpayload = json.loads(payload)\n\texcept (ValueError, TypeError):\n\t\treturn\n\t\n\tif isinstance(payload['msg'], dict):\n\t\tpayload['msg'] = payload['msg']['message']\n\t\n\tpayload = payload['msg']            # message\n\t\n\ttry:\n\t\tpayload = json.loads(payload) # message payload\n\texcept (TypeError, ValueError, AttributeError):\n\t\treturn\n\t\n\tif not 'post"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tuser = None\n\t\n\tif hasattr(request, \"user\"):\n\t\tuser = request.user\n\telif hasattr(request, \"peer_user\"):\n\t\tuser = request.peer_user\n\telif request.method in (\"POST\", \"GET\", \"OPTIONS\"):\n\t\ttry:\n\t\t\tpayload_data = json.loads(request.body)\n\t\t\tself_peer_user = payload_data[\"selfPeerUser\"]\n\t\t\tif self_peer_user and self_peer_user!= request.user.username:\n\t\t\t\ttry"}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tif \"content\" in request:\n\t\tif \"message payload\" in request[\"content\"]:\n\t\t\treturn_val = False #TODO: Determine if this the proper return value\n\n\telif \"payload\" in request: \n\t\tpayload_class = request[\"payload\"][\"class\"]\n\t\tpayload = json.dumps(request[\"payload\"])\n\t\tif payload_class == \"TextPayload\":\n\t\t\treturn_val = \"legacy\"\n\t\telif payload_class == \"DirectPayload\":\n\t\t\treturn_val = \"direct\""}, {"generated_text": "def identify_request(request: RequestType):\n\"\"\"\n\tTry to identify whether this is a Diaspora request.\n\t\n\tTry first public message. Then private message. The check if this is a legacy payload.\n\t\"\"\"\n\tcheck_public_message = lambda r: r and r[:3] == ['dm', 'from', 'public']\n\tcheck_private_message = lambda r: r and r[:3] == ['pm', 'from', 'id']\n\tif request.type == 'public' and check_public_message(request.payload):\n\t\treturn 'public'\n\telif request.type == 'private' and check_private_message(request.payload):\n\t\tif not check_private_message(parse('<message from=\"%s\" type=\"private\""}]}
{"task_id": "6306292152e177c0ba469f0d", "prompt": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tdata = json.loads(str(request))\n\tif \"type\" in data:\n\t\t# it's a Matrix request\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef make_identity_request(device: str, device_user_id: str, push_token: str, user_id: str,\n                          jti: str) -> str:\n\tidentity_request = {\"type\": \"IDENTITY_REQUEST\", \"push_service_token\": push_token,\n                         \"device_user_id\": device_user_id, \"device"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\ttry:\n\t\t# RequestType is a type(request)\n\t\trequest_id = str(request.request_id).split(\"_\")[-1]\n\t\trequests_info = [m for m in MatrixRequests if m['requestId'] == request_id]\n\t\tif len(requests_info):\n\t\t\trequests_data = requests_info[0]\n\t\t\tif len(requests_data[\"event\"]):\n\t\t\t\treturn True\n\t\treturn False\n\texcept Exception as e:\n\t\t# RequestType is not a type(request)\n\t\t# for"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tif request.has_header(\"Content-Type\"):\n\t\tif is_valid_matrix_request(request.body):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn \"MatrixRequestManager:\" in request.path or \"MatrixSieve:\" in request.path\n\n\ndef respond_fail(msg: str) -> Callable[..., Callable]:\n\tdef callable(request: RequestType, response: ResponseType) -> \"Response\":\n\t\tif not isinstance(response, Response):\n\t\t\tresponse = Response(StatusType.OK, \"\")  # type: ignore\n\n\t\tresponse.data(\"status\", \"bad response\")\n\t\tresponse.data(\"reason\", msg)\n\t\tresponse.headers[\"content-type\"] = \"application/json\"\n\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn (\n\t\trequest[\"MatrixRequest\"] is not None\n\t\tand (\n\t\t\t(\"To\" in request[\"MatrixRequest\"] if isinstance(request[\"MatrixRequest\"], dict) else False)\n\t\t\tor (\"From\" in request[\"MatrixRequest\"] if isinstance(request[\"MatrixRequest\"], dict) else False)\n\t\t)\n\t)\n\ndef verify_request_matrix(request: RequestType) -> bool:\n\t\"\"\"\n\tVerify that this is a request which has a MatrixRequest payload (and not an old-style request)\n\t\"\"\"\n\treturn (\"MatrixRequest\""}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\treturn request.matrix_request\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\turl_parts = urllib.parse.urlparse(request.path_and_query)\n\n\tif \"m.request\" not in url_parts.query or \"token=\" not in url_parts.query:\n\t\treturn False\n\n\ttry:\n\t\tif int(url_parts.query.split(\"token=\")[1])!= \"1\":\n\t\t\treturn False\n\texcept IndexError:\n\t\treturn False\n\n\treturn True\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tprotocol = \"matrix-client\" if request.method == \"GET\" else \"matrix\"\n\theaders = \"\".join(request.headers.get_all(b(\"content-type\")))\n\tmatrix_version_search = re.search(b(\"xmpp;type=\\\"{}\\\"\".format(protocol)), headers)\n\n\tif matrix_version_search:\n\t\ttry:\n\t\t\tmatrix_protocol = json.loads(\n\t\t\t\tbytes(matrix_version_search.group()[:matrix_version_search.end()]).dec"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\tfor key in request.keys():\n\t\tif matrix_identifier == key:\n\t\t\treturn True\n\treturn False\n"}, {"generated_text": "def identify_request(request: RequestType) -> bool:\n\"\"\"\n\tTry to identify whether this is a Matrix request\n\t\"\"\"\n\t# first try to figure out whether the HTTP request\n\t# can be a Matrix request, by matching a pair of headers\n\tmatrix_headers = (\n\t\t('M.frc', ['MatrixRequests']),\n\t\t('MatrixRequests', ['M.frc']),\n\t)\n\tfor header, value in matrix_headers:\n\t\tif request.META.get(header, None) == value:\n\t\t\treturn True\n\n\treturn False\n\n\ndef get_user_from_headers(request: RequestType, user_key: str = 'username', realm_key"}]}
{"task_id": "6306292252e177c0ba469f11", "prompt": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tret = dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f0000Z\")\n\twhile ret[-1:] == \"0\":\n\t\tret = ret[:-1]\n\treturn ret\n\n\nclass DATASTREAM:\n\t\"\"\"\n\tDataStream implementation with a list of (time, value) tuples.\n\tIt does little more than implement the 'add' method, so as to\n\tprovide a similar interface to simple, straight-line streams.\n\t\"\"\"\n\tdef __init__(self):"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\t# TODO: support datetime.date (the date portion will likely not be\n\t# useful)\n\treturn format_t(dt.astimezone(UTC))\n\ndef str_to_dt(s, fmt):\n\t\"\"\"\n\tReturn a DATETIME type with the given value parsed into a\n\tdatetime object using fmt.\n\t\"\"\"\n\td = datetime.strptime(s, fmt)\n\td = d.astimezone(UTC)\n\treturn d\n\ndef format_t(dt):\n\treturn dt.strftime"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif dt is None:\n\t\treturn None\n\t\n\treturn \"%sT%sZ\" % (dt.strftime(\"%Y-%m-%dT%H:%M:%S\"), \n\t\t\t\t\t   dt.strftime(\"%f\"))\n\ndef parse_dt(dtstr, tz=default_tz):\n\t\"\"\"\n\tParse a datetime string into a datetime.datetime value.\n\n\tReturns None if there is any problem, which means that the D*\n\tprotocol only expects the value to be a 32-byte string and"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime('%Y-%m-%dT%H:%M:%S')\n\ndef main(argv):\n\tif len(argv) == 1:\n\t\tprint \"You must pass a DIR\"\n\t\texit(1)\n\twith open(argv[1], 'rb') as f:\n\t\tparser = JSONParser().parse(f)\n\n\t# TODO: this currently assumes the DIR is an actual directory structure on disk\n\troot = parser.get('directoryNode', {})\n\tprint \"File %s\""}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn str(dt.days).zfill(2) +'' + dt.strftime(\"%H:%M:%S\").zfill(5)\n\n\ndef find_parental_node(node, target):\n\tfound = False\n\tfor childId in node.getElementsByTagName(\"child\"):\n\t\tfor id in childId.getElementsByTagName(\"id\"):\n\t\t\tif id.childNodes[0].data == target:\n\t\t\t\treturn childId\n\t\t\n\t\tif find_parental_node(childId, target):"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tdateparts = []\n\tfor p in dt.split()[1:5]:\n\t\tdatepart = p % 100\n\t\tif dt.month == 1:\n\t\t\tp -= 1\n\t\t\tif dt.day < 13:\n\t\t\t\tp += 32;\n\t\t\telse:\n\t\t\t\tp -= 13\n\t\tdateparts.append(p)\n\treturn dateparts\n\ndef add_date(dt: datetime, day_index: int, days_to_add: int) -> datetime:\n\treturn dt + timedelta(days=days_to_add)"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif isinstance(dt,list): dt = dt[0]\n\treturn \"%4d-%02d-%02dT%02d:%02d:%02dZ\" % (dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second)\ndef parse_dt(dt):\n\t\"\"\"\n\tParse a string in the form described by format_dt\n\t\"\"\"\n\treturn datetime.datetime.strptime(dt, \"%4d-%02d-%02"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\treturn dt.strftime('%c')\n\n\n@defun\ndef run(filename, outfilename):\n\n\t# Load the mesh\n\tmesh = Mesh.FromFile(filename)\n\tif mesh is None:\n\t\treturn\n\n\t# Get the material data\n\tmaterials = mesh.GetMaterials()\n\tnodemats = [materials[node.material] for node in mesh.nodes if 'nmat' in node.attributes]\n\tif len(nodemats)!= len(mesh.nodes):\n\t\tprint(\"WARNING: Mesh"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tdef fmt(x):\n\t\t\"\"\"\n\t\tFormat as in YYYY-MM-DD.HH:MI\n\n\t\t:param x:\n\t\t\tThe date to format.\n\n\t\t:returns:\n\t\t\tA formatted string formatted on YYYY-MM-DD.HH:MI.\n\t\t\"\"\"\n\n\t\treturn \"%s-%s-%sT%s:%s\" % (\n\t\t\tx.year,\n\t\t\tx.month,\n\t\t\tx.day,\n\t\t\tx.hour,\n\t\t\tx.minute,\n\t\t)\n\n\tif dt.tzinfo:\n"}, {"generated_text": "def format_dt(dt):\n\"\"\"\n\tFormat a datetime in the way that D* nodes expect.\n\t\"\"\"\n\tif not ':' in dt:\n\t\treturn datetime.datetime.strftime(dt, '%Y-%m-%dT%H:%M:%S')\n\telse:\n\t\treturn datetime.datetime.strftime(dt, '%Y-%m-%dT%H:%M:%S%z')\n\ndef str(o):\n\tif isinstance(o, datetime.datetime):\n\t\treturn str(format_dt(o))\n\telse:\n\t\treturn str(o)\n\ndef dt"}]}
{"task_id": "6306292352e177c0ba469f1d", "prompt": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tres = []\n\t\n\tfor tag in sorted(tags):\n\t\trepl = lambda x: \"\".join(list(reversed(replacer(x)))) if replacer else x\n\t\tfinds = [f for f in re.findall(r\"(\" + tag + r\")\", text, re.IGNORECASE)]\n\t\tif finds:\n\t\t\tres.append((tag.strip(), \"\".join(map(repl, finds))))\n\t\telse:\n\t\t\tres.append((tag.strip(), \"\"))\n\t\n\treturn tuple(res)\n\n@register"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\tfrom utils import get_tags, unescape_tag_text\n\t\n\tresult = set() ;\n\tprocess_text = text  # Text to be treated.\n\ttags = get_tags(text)\n\ttags = {\"</code>\" : \"<code{0}>\"}\n\t\n\tfor tag in tags.keys():\n\t\ttagtext = tags[tag]\n\t\t\n\t\t# Ignore tags inside code\n\t\tif tagtext.find(\"{% code{\") > -1 and tagtext.find(\"{% endcode%}\") > -1:\n"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\tdef repl(m: re.Match) -> str:\n\t\treturn replacer(m.group(0)) if replacer else m.group(0)\n\t\n\ts = \"**{**\" + \"\\n\" + text + \"**}\"\n\ttags = {\"**\\\\{\\\\***\": \"}*}> \"}\n\t\n\tnew = re.sub(rf'sys.*'.join(tags.keys()), repl, s, flags=re.IGNORECASE)\n\t\n\treturn set(tags.keys()), new\n\n\n# Returns whether"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\ttags = set()\n\tif replacer:\n\t\tdef replacer_func(tag: str, match: Match):\n\t\t\treturn replacer(tag)\n\t\n\ttag = re.compile(r'<!--([^>]*)-->', flags=re.ASCII)\n\tfor m in tag.finditer(text):\n\t\n\t\tword = m.group(1)\n\t\tif replacer:\n\t\t\ttext = tag.sub(replacer_func, text)\n\t\t\n\t\ttag = len(word.split(' ')) == 1\n\t\tif"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = {}\n\treplace_func = None\n\t\n\t# Find start of block\n\ttag_end = 1\n\twhile True:\n\t\tc = text[tag_end - 1]\n\t\tif c == \">\":\n\t\t\tbreak\n\t\tif c in (\"=\", \",\", \"=\") or c.isdigit():\n\t\t\tbreak\n\t\telse:\n\t\t\ttag_end += 1\n\t\n\t# Find tags\n\twhile tag_end < len(text):\n\t\ttag_content = \"\"\n\t\twhile True:\n\t\t\tc = text[tag_end"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t\n\tif replacer is None:\n\t\treplacer = lambda x: x\n\t\n\ttags = {}\n\t\n\tcur_pos = 0\n\t\n\twhile True:\n\t\ttag_end_pos = text.find('>', cur_pos) if text[cur_pos:tag_end_pos] in ('>', '\\t',' ') else -1\n\t\ttag_start_pos = text.find('<', cur_pos) if text[cur_pos:tag_start_pos-1] in ('<', '\\t',' "}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tr = re.compile(\n\t\tr'<\\s*(tag[\\w_]+)\\s*([^<>]+)\\s*>',\n\t\tre.IGNORECASE\n\t)\n\tm = re.search(r, text)\n\t\n\ttags = {}\n\t\n\twhile m:\n\t\ttag = m.group(1)\n\t\tword = m.group(2)\n\t\t\n\t\t# Ignore tags inside code blocks.\n\t\tif re.match(r'```', text[:m.start()]):\n\t\t\ttext ="}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttags = RE_TAG.findall(text)\n\tif replacer == None:\n\t\treturn tags, text\n\telse:\n\t\tresult = \"\"\n\t\tfor t in tags:\n\t\t\tresult += replacer(t)\n\t\treturn tags, result\n\t\n\n#------------------------------------------------------------------------------\n# Main functions that run this module\n\n# If we have a version number in module __future__, we run the python 2\n# version of this module, otherwise, we run the python 3 version.\ntry:\n\tversion = __future__.version_info\nexcept AttributeError:\n"}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\t# Remove and substitute tags\n\ttags = set()\n\ttext = text.replace(\"> \", \">  \") # Sub and add space to preserve format/style\n\tfor match in re.finditer(\"[\\w]+-[\\w]+\", text, re.I): # Ignore tags inside code\n\t\t# Only allow inside tags\n\t\tif not match.group(0) in \"[][}]\":\n\t\t\tcontinue\n\t\t\n\t\t# Try to find the next matching non-tag\n\t\tnext_match = None\n\t\twhile next_match is not None:\n\t\t\tnext_match ="}, {"generated_text": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\"\"\"\n\tFind tags in text.\n\t\n\tTries to ignore tags inside code blocks.\n\t\n\tOptionally, if passed a \"replacer\", will also replace the tag word with the result\n\tof the replacer function called with the tag word.\n\t\n\tReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tregex = re.compile(r'<h{1,2}(?:class=\"?(.+?)?\"?)?\\\\s*>\\\\s*(.*?)\\\\s*</h(?:1|2)(?:></h(?:1|2))?', re.DOTALL | re.MULTILINE)\n\tmatches = regex.finditer(text)\n\ttags = set()\n\tfor match in matches:\n\t\topen = match.group(1) or \"\"\n\t\tclose = match.group(2) or \"\"\n\t\tif replacer"}]}
{"task_id": "6306292352e177c0ba469f1e", "prompt": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t#print('Processing text...')\n\ttext_processed = []\n\tfor i, line in enumerate(text[:]):\n\t\t# Add a leading anchor if required\n\t\tif'' in line: # If there is text\n\t\t\t# Add an empty string between the anchor and the text to match it\n\t\t\t# with its previous one.\n\t\t\ttext_processed.append('')\n\t\t\ttext_processed.append(line.rstrip())\n\t\t\tcontinue\n\t\t# Don't bother adding links to blank lines.\n\t\tif not line:\n\t\t\ttext_processed"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tglobal LINK\n\ttmp = \"\"\n\tresult = \"\"\n\ttmp_re = re.compile(r\"<.?a href=('|\\\")?([^'>\\\"]+)('|\\\")?.?>\\s+\")\n\ttxt_re = re.compile(r\"<.?a\\s.*?>\\s+\")\n\tfor word in text.split(\" \"):\n\t\t# TODO: add more robust matching for regex\n\t\ttmp = word.strip()\n\t\tmatch = tmp_re.match(tmp)\n\t\tif (match is None):"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t# text = self.filter_url(text)\n\ttext = self.process_links(text)\n\ttext = self.add_link_attributes(text)\n\treturn text\n\nclass MarkdownConverter:\n\t\"\"\"Convert Markdown into HTML, using Pandoc.\"\"\"\n\tdef __init__(self, verbose: bool = False):\n\t\tif sys.platform == \"darwin\":\n\t\t\tprint(\"This is a Windows-only plugin\")\n\t\t\treturn None\n\t\tself.verbose = verbose\n\t\tglobal markdown\n\t\tif markdown"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = re.sub('<[^<]+?>', '', text)\n\tlink_regex = re.compile(\n\t\t\tr'''(https?://|(?<![^.\\n])\\.\\.(?:www\\.|m\\.|(?<![^\\.])\\.\\.))\\b(?!.*@)|\\b(?:#(?![\\d])|[^:\\s<\\$@]+@))(?:\\.(?:[^\\n]+))*(?:\\n|$)''',\n\t\t\tre.X"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tlinks = process_text_links.links\n\ttext = process_text_links.text_in_links(text)\n\treturn [i for i in links if i in text]\n\n# ------------------------------------------------------------------\n# ------------------------------------------------------------------\n# ------------------------------------------------------------------\n\nclass TextProcess:\n\tdef __init__(self):\n\t\tself.links  = [\n\t\t\tr'(?i)\\[(.+?)\\]',\n\t\t\tr'[\\[(](.+?)\\[\\]',\n\t\t\tr'\\[[^\\]]*\\]',\n\t\t\tr'^"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\t# First, do some whitespace conversions\n\ttext = text.replace('\"', '')\n\ttext = text.replace('  ','')\n\ttext = text.replace('\\n','')\n\ttext = text.replace(' (with spaces)', '(with spaces)')\n\n\t# Check to see if there were any links in text, and strip them out\n\tif not text.startswith('http'):\n\t\ttext = re.sub('http.*', '', text)\n\t\ttext = re.sub('\\n| |\\(|"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tnew_text = \"\"\n\tfor textPart in text:\n\t\tif type(textPart) == type(\"\") and \"http\" in textPart:\n\t\t\tnew_text += linkify_text(textPart,textPart)\n\t\telif textPart.__class__.__name__ == 'pagedoc.HTMLNode':\n\t\t\t# This is already HTML with a linkified text. Do nothing.\n\t\t\tpass\n\t\telse:\n\t\t\tnew_text += textPart\n\treturn new_text\n\ndef linkify_text(text, linkify_text):\n\t\"\"\""}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tdef replace_text_links(text):\n\t\thtml_regex=\"<.*?>(.*?)</.*?\"\n\t\tp = re.compile(html_regex)\n\t\tfor match in p.finditer(text):\n\t\t\tif re.match('.*?ftp', match.group(1)):\n\t\t\t\tlink = ''.join(('http://', match.group(1)))\n\t\t\telif re.match('.*?(http|https|ftp)', match.group(1)):\n\t\t\t\tlink = match.group(1)\n\t\t\t"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = text.replace('&mdash;','---')\n\ttext = text.replace('&nbsp;',' ')\n\ttext = text.replace('&copy;','\\xa9')\n\ttext = text.replace('&#039;','\\'')\n\ttext = text.replace('&gt;',' >')\n\ttext = text.replace('&quot;','\\\"')\n\ttext = text.replace('http://','https://')\n\treturn text\n\ndef get_pageinfo(url):\n    '''\n\tReturns data about page"}, {"generated_text": "def process_text_links(text):\n\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfor match  in url_re.finditer(text):\n\t\tfor match2 in url_re.finditer(text):\n\t\t\ttext = text.replace(match.group(0), match2.group(0))\n\t\t\turl, text = convert_unicode(match.group(0), match.group(1)) \n\t\t\turl = get_link(url, text)\n\t\t\ttext = linkify(replace_accelerator_re, text, \"?=\")\n\t\t\ttext = convert_accelcodes_to_lower_case(replace_accelerator_"}]}
{"task_id": "6306292652e177c0ba469f34", "prompt": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\thead_req = urllib.request.Request(url)\n\thead_req.get_method = lambda: \"HEAD\"\n\thead_req.add_header(\"accept\", \"application/zip\")\n\thead_req.add_header(\"If-Modified-Since\", \"Fri, 24 Dec 9999 16:40:25 UTC\")\n\n\ttry:\n\t\treturn urllib.request.urlopen(head_req).headers.get('content-type', None)\n\texcept urllib2.HTTPError as err:\n\t\tif err.code == 304"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\treq = Request(url, headers=get_fetch_headers())\n\ttry:\n\t\tconn = get_connection(req)\n\t\tres = conn.urlopen('HEAD', url)\n\texcept socket.error:\n\t\treturn None\n\n\tstatus = int(res.getheader('status'))\n\tif status == 200:\n\t\tctype = res.info().get_content_type()\n\t\treturn ctype\n\telse:\n\t\treturn None\n\ndef download_dir(url: str, path: str) -> None:\n\t\"\"\"\n\tDownload the file"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\trequest_params = {\n\t\t\"accept\": \"text/html\",\n\t}\n\ttry:\n\t\theaders = httplib2.Http(timeout=time_to_wait)\n\t\tresponse, resp_headers = headers.request(url, \"HEAD\", **request_params)\n\t\tif not response.status == 200:\n\t\t\traise RuntimeError(\"Cannot fetch {}: HTTP status code was {}\".format(url, response.status))\n\texcept HTTPResponseException:\n\t\traise RuntimeError(\"Cannot fetch url {}, error: {}\".format(url,"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\ttry:\n\t\tr = requests.head(url, allow_redirects=True)\n\n\t\t# If we got a redirect, follow the redirect, and keep caching this.\n\t\tif r.status_code == requests.codes['moved permanently'] or \\\n\t\t   r.status_code == requests.codes['moved']:\n\t\t\tlocation = r.headers['location']\n\t\t\treturn fetch_content_type(location)\n\t\telse:\n\t\t\treturn r.headers['content-type']\n\texcept (requests.exceptions.ConnectionError,\n\t\t   requests"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tcontent_type = None\n\n\ttry:\n\t\trequests_headers = requests.head(url)\n\t\trequests_content_type = requests_headers.headers.get('content-type')\n\n\t\tif requests_content_type is None:\n\t\t\t# If the request doesn't return a content type, check the\n\t\t\t# content-disposition header.\n\t\t\tcd_headers = requests.head(url).headers.get('content-disposition')\n\n\t\t\tif cd_headers is None:\n\t\t\t\t# If neither the content-type nor content-disposition headers are\n\t\t\t\t# available"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tr = requests.get(url)\n\n\tmatch: Optional[Match] = RE_CONTENT_TYPE.match(r.headers['Content-Type'])\n\tif not match:\n\t\treturn None\n\n\treturn match.groupdict()['type_']\n\ndef fetch_content_bytes(url: str) -> bytes:\n\tr = requests.get(url)\n\n\tr.raise_for_status()\n\n\ttry:\n\t\treturn r.text\n\texcept requests.ConnectionError as err:\n\t\tprint(err)\n\t\t\n\t\n\treturn ''\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\treq = Request(url)\n\treq.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8')\n\ttry:\n\t\tresponse = urlopen(req)\n\texcept HTTPError as e:\n\t\tif e.code == 404:\n\t\t\treturn None\n\t\telse:\n\t\t\ttraceback.print_exc()\n\t\t\traise e\n\texcept socket.timeout:\n\t\treturn None\n\tcontent_type = response.getheader('Content-Type')\n\treturn content"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\theaders = dict(max_retries=5)\n\tresponse = request_with_retries(\n\t\turl=url,\n\t\theaders=headers,\n\t\tmethod='HEAD',\n\t)\n\n\tif not response.ok:\n\t\tlogging.error('%s HEAD operation failed %s', url, response.text)\n\tresult = response.headers.get('Content-Type', None)\n\tif result:\n\t\treturn result\n\telse:\n\t\tlogging.error('No header response from %s header: %r', url, headers)\n\t\treturn None\n"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tlogger.info('Reading the remote url: %s', url)\n\ttry:\n\t\tresponse = scap_webutil.get_url(url)\n\texcept Exception:\n\t\tlogger.debug('Exception reading %s', url, exc_info=True)\n\t\treturn None\n\n\tif response.headers['Content-Type'] is None:\n\t\treturn None\n\tcontent_type = response.headers['Content-Type']\n\n\t# Check if the content-type is an image\n\timage_content_types = ('text', 'application')\n\tif content_"}, {"generated_text": "def fetch_content_type(url: str) -> Optional[str]:\n\"\"\"\n\tFetch the HEAD of the remote url to determine the content type.\n\t\"\"\"\n\tresponse = requests.get(url, allow_redirects=True)\n\n\treturn response.headers.get(\"content-type\")\n\n\ndef read_local_file(filepath: str) -> str:\n\t\"\"\"Return the contents of a file, skipping any errors\"\"\"\n\twith open(filepath, 'rb') as f:\n\t\treturn f.read()\n\n\ndef write_local_file(filepath: str, content: str) -> None:\n\twith open(filepath, 'wb+') as f:\n\t\tf.write(content)\n\n\ndef"}]}
{"task_id": "6306292a52e177c0ba469f41", "prompt": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif tag == \"a\" or tag == \"an\" or tag==\"another\":\n\t\treturn True\n\telif tag[0] == '@':\n\t\treturn True\n\telse:\n\t\treturn False\n\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tif tag == \"abbrev\"  or tag.startswith(\"abbrev_\") or tag.endswith(\"_abbrev\"):\n\t\t# ignore abbreviations and all the _abbrev endings\n\t\treturn False\n\n\tif tag == \"adverb\" or tag == \"adv\" or tag == \"adv_\"  or tag == \"adv_\" or tag.startswith(\"adv_\"):\n\t\t# ignore adverbs and all the 'adv' endings\n\t\treturn False\n\n\tif tag == \"noun\" or tag.startswith(\"noun"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn any(x in tag for x in ('\\\\<', '\\\\>', '/', '\\\\$', '\\\\_', '\\\\@', '\\\\+', '\\\\{', '\\\\}'))\n\n\ndef match_one_word(s: str, tags: List[str] = []) -> bool:\n\t\"\"\"\n\tMatch a word in one string.\n\t\"\"\"\n\tfor tag in tags:\n\t\tif s.startswith(tag):\n\t\t\treturn True\n\treturn False\n\n\ndef tokenize(s: str, tags: List[str] = ["}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\t# This case is already covered during the parsing.\n\tif tag!= '\\'' and '#' in tag:\n\t\treturn False\n\n\t# Check the tag length.\n\tif len(tag) == 0 or len(tag) >= min_tag_size:\n\t\treturn False\n\n\t# Test if the word is too short\n\tif len(word) < min_word_size:\n\t\treturn False\n\n\t# Test the last character that's not a number.\n\tif word[-1]!= '\\'' and is_word_word_char(word[-1"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn \"a\" <= tag <= \"z\" or \"l\" <= tag <= \"s\"\n\n\ndef test_tag_case(tag: str) -> bool:\n\t\"\"\"\n\tTest a word whether it could be accepted as a tag in UPPERCASE format.\n\t\"\"\"\n\treturn \"a\" <= tag <= \"z\"\n\n\ndef test_not_tag_case(tag: str) -> bool:\n\treturn \"A\" <= tag <= \"Z\"\n\n# TODO: Add this to the word list if required.\n\n\n@pytest.fixture\n"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tfor t in tags:\n\t\tif t == tag:\n\t\t\treturn True\n\telse:\n\t\tprint(f'Can not recognize {tag} as a tag.')\n\t\tsys.exit(1)\n\n\ndef create_tokenizer():\n\t\"\"\"\n\tCreate a tokenizer so that tokens can be split into sub-words.\n\t\"\"\"\n\treturn NLTK_WordTokenizer()"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn not tag\n\n\ndef check_tag(word: str, tag_name: str):\n\t\"\"\"\n\t\tChecks whether the word could be accepted as a tag. If no, return False\n\t\tto say so. Otherwise, return True.\n\t\t\"\"\"\n\treturn tag_name in word.upper()\n\n\ndef to_list(str: str, split: bool = False) -> list:\n\t\"\"\"\n\t\tSplits a string on a comma and returns a list. \n\t\"\"\"\n\treturn str.split(',') if split else [str]\n\t\t\t\n\ndef"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treg = re.compile(r'.*?{1}[{0}]:.*?{0}[{1}]'.format(tag, tag))\n\treturn reg.match(word) is not None\n\ndef load(path: str) -> str:\n    \"\"\"\n    \tLoad a word from a line in the dictionary and return the word.\n\n    \"\"\"\n    for line in open(path, 'r'):\n        if line.strip().lower().endswith('?'):\n            word = line[:-1].strip().lower()\n            if word"}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\treturn word.regex(tag) is not None\n\ndef test_lemma(lemma: str) -> bool:\n\t\"\"\"\n\tTest a word whether it could be accepted as a lemma.\n\t\"\"\"\n\treturn word.regex(lemma) is not None\n\ndef test_sense(sense: str) -> bool:\n\t\"\"\"\n\tTest a word whether it could be accepted as a sense.\n\t\"\"\"\n\treturn sense.regex(sense) is not None\n\ndef test_lexical(lexical: str) -> bool:\n\t\"\"\""}, {"generated_text": "def test_tag(tag: str) -> bool:\n\"\"\"\n\tTest a word whether it could be accepted as a tag.\n\t\"\"\"\n\tfor v in WORDS: v[1] = v[1].replace(tag, '')\n\tfor w in WORDS:\n\t\tif w[1].endswith(tag): w[1] = w[1][:-len(tag)]\n\treturn True\n\n# for a given word, return a set of every other word that can follow it\ndef get_following_words(w: str) -> set:\n\ti = 0\n\tfor v in WORDS:\n\t\tw_pos = w.find(v[0]) + len(v[0"}]}
{"task_id": "6306298b52e177c0ba469fdc", "prompt": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\timport time\n\t\n\tresults = {}\n\t\n\tfor c in node.children:\n\t\ttry:\n\t\t\tif type(c) == etree._Element:\n\t\t\t\tresults[c.tag] = xml_children_to_dict(c)\n\t\texcept TypeError:\n\t\t\tif type(c) == type(''):\n\t\t\t\tresults[c] = xml_children_to_dict(c)\n\t\n\t\n\treturn results\n\t\n\nfrom urllib2 import urlopen\n\ndef download(url, dest=''):\n\t\"\"\"\n\tDownload"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\treturn {child.nodeName: child.childNodes[0].nodeValue}\n\nclass Tester(threading.Thread):\n\ttest_func = None\n\t@classmethod\n\tdef test(cls, test_func):\n\t\tcls.test_func = test_func\n\tdef run(self):\n\t\tprint \"Running \" + self.test_func\n\t\timport time\n\t\tstart = time.time()\n\t\tself.test_func()\n\t\tprint \"Done \" + self.test_func + \" in \" + str(time.time()-"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tobj = {}\n\tfor child in node.xpath('*/text()'):\n\t\tfor t in child.split():\n\t\t\tobj[child.text] = t.strip()\n\treturn obj\n"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t\n\tpairs = {}\n\tfor child in node.xml.children:\n\t\tname = child.tag\n\t\tpairs[name] = _xml_children_as_dict(child)\n\treturn pairs\n\ndef load_config(filename):\n\t# print(\"Loading config file: \" + filename)\n\tdoc = parse(filename)\n\troot = doc.getroot()\n\t\n\tassert(root.tag == \"cib\")\n\t\n\tnodes = xml_children_as_dict(root)\n\t\n\tnodes[\"status\"] = _"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\t# Get a list of all children:\n\tlist = [x for x in node.findall('.//*')]\n\t# Get a sorted list of all node names:\n\tslist = list[:] # Copy!\n\tsorted(slist)\n\t# Convert the list into a dict:\n\tdata = {x.tag:x for x in slist}\n\treturn data\n\n@pytest.fixture\ndef xml_data_as_dict(xml_data):\n\t\"\"\"\n\tConvert the xml_data object into a dict:\n\t"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tretval = {}\n\tfor child in node.childNodes:\n\t\tif (child.nodeType == xml.dom.Node.ELEMENT_NODE):\n\t\t\tretval[child.tagName] = _xml_children_as_dict(child)\n\t\telse:\n\t\t\tif (child.nodeType == xml.dom.Node.TEXT_NODE):\n\t\t\t\tretval[child.tagName] = child.data\n\treturn retval\n\t\t\t\n\t\n\t\n\t\t\n            # from xml.dom.ext\n\nif __name__ == '__"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\td = collections.defaultdict(list)\n\tfor subnode in node.getchildren():\n\t\td[subnode.tag].append(subnode.tag)\n\treturn d\n\t\ndef tag_as_attr(node, tag):\n\treturn \"%s %s %s[@%s='%s']\" % (xml_type_as_str(node.get('type')),\n\t\txml_children_as_dict(node).get(tag, []),\n\t\ttag,\n\t\tnode.get('attr'),\n\t\tnode.get(tag))\n"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tresult = {}\n\tfor (tag, childNode) in node.childNodes:\n\t\tif childNode.nodeType!= Node.ELEMENT_NODE or (tag == '@' and childNode.nodeType == Node.ATTRIBUTE_NODE): continue\n\t\tresult[tag] = xml_get_child(childNode, 0)\n\treturn result\n\ndef xml_get_child(node, number):\n\t\"\"\"\n\tGet the child nodes as a list of a given level.\n\t\n\tnode - the original root node\n\tnumber - the"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tres = {}\n\tfor n in node.iterchildren('*'):\n\t\tres[n.tag] = n\n\treturn res\n\ndef xml_node_as_lst(node):\n\t\"Returns the <xml> children as an list.\"\n\treturn [c for c in node.iterchildren() if c.tag == 'xml']\n\ndef xml_node_as_dict(node):\n\t\"Returns the <xml> children as a dict, keyed by tag name.\"\n\tres = {}\n\tfor c in node.iterchildren():\n\t\tres"}, {"generated_text": "def xml_children_as_dict(node):\n\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\n\tThis is only a shallow conversation - child nodes are not recursively processed.\n\t\"\"\"\n\tnodelist = node.getchildren()\n\tchildren = {}\n\tfor child in nodelist:\n\t\tname = child.tagname\n\t\tif name in [\"comment\", \"meta\"]: continue\n\t\tchildren.setdefault(name, []).append(xml_children_as_xml(child))\n\treturn children\n\ndef xml_children_as_string(node):\n\treturn \"\\n\".join(v for v in xml_children_as_xml(node))\n\t\ndef xml_children_as_xml(node):\n\treturn ''.join(ET."}]}
{"task_id": "6306299052e177c0ba469fe8", "prompt": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\ttry:\n\t\tif sender_handle or not entity_handle:\n\t\t\treturn True;\n\t\tif sender_handle!= entity_handle:\n\t\t\traise ValueError()\n\texcept ValueError: # Just to be nice to the caller\n\t\treturn False\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle!= \"0x00000000\" and sender_handle!= entity_handle:\n\t\t\n\t\traise DiasporaAuthenticationError(\"Incorrect entity handle: %s\" % entity_handle)\n\n\nclass DiasporaSession:\n\t\n\tdef __init__(self):\n\t\tself.data = {}\n\t\tself.handlers = {}\n\t\tself.initialize_session()\n\n\tdef initialize_session(self):\n\t\tsession_data = get_session_data()\n\n\t\tself.initialize_handler(session_data, self.on_init)\n\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle!= entity_handle and not sender_handle == None and not entity_handle == None:\n\t\traise Exception('Sender and entity handles must match')\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tassert(sender_handle.is_canonical())\n\tassert(entity_handle.is_canonical())\n\t\n\tcanonical_sender_handle = sender_handle.canonical()\n\tif canonical_sender_handle.is_canonical()==True:\n\t\tcanonical_sender = canonical_sender_handle.to_string()\n\telse:\n\t\tcanonical_sender = canonical_sender_handle.to_string() if canonical_sender_handle.object().is_canonical() else canonical_sender_handle.object"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\t# If the sender is the entity, we can be certain they're the entity we say they are. However, we can't be sure\n\t# about the sender, since it might be the entity themselves. We can only check if the sender handle is what\n\t# we received when receiving the payload.\n\t\n\t# If the sender is not this entities owner, we can only be certain the sender is a valid person. We can therefore\n\t# be certain if the sender is the entity handle.\n\tif sender_handle!= entity_handle and sender_handle!= \"me\" and not is_owner"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\treturn sender_handle == g.user.get_handle() or sender_handle == get_entity_handle(payload)\n\ndef get_entity_handle(payload):\n\treturn payload.objectFrom if payload.objectFrom else ''\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle is None:\n\t\traise DiasporaError('cannot verify that handles do not match')\n\tif sender_handle!= entity_handle:\n\t\traise DiasporaError('entity-received and sender-sent payloads do not match')\n\ndef check_user_validity_and_entity_handle_match(user, sender_handle, entity_handle):\n\t\"\"\"\n\t\n\t\"\"\"\n\tif user is None:\n\t\traise DiasporaError('cannot verify that user IDs do not match')\n\tif user.key not in sender_hand"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle == entity_handle:\n\t\treturn\n\n\ttry:\n\t\tsender_attributes = json.loads(sender_handle)\n\t\tentity_attributes = json.loads(entity_handle)\n\t\t# Check if these are the same - otherwise this isn't the same person doing this\n\t\treturn sender_attributes['id'] == entity_attributes['id']\n\texcept:\n\t\treturn False\n"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tassert sender_handle == entity_handle\n\t\nclass ReceiveMessagePayload(Payload):\n\t\"\"\"\n\tProcess payload.\n\t\"\"\"\n\t@property\n\tdef entity_handle(self):\n\t\treturn self.object['ds_id']\n\n\t@property\n\tdef sender_handle(self):\n\t\tassert self.object\n\t\treturn self.object['ds_id']\n\n\t@property\n\tdef message(self):\n\t\treturn self.object['object']\n\n\t@property\n\tdef date(self):\n\t\ttimestamp = str(dateutil.parser"}, {"generated_text": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\"\"\"\n\tEnsure that sender and entity handles match.\n\t\n\tBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\n\tbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n\tAND the object. We must ensure they're the same.\n\t\"\"\"\n\tif sender_handle is not None:\n\t\tassert sender_handle == entity_handle\n\treturn sender_handle\n\n# In order to support custom fields as headers, this converts an email message to a dictionary mapping field names to values.\ndef prepare_message(msg):\n\t\"\"\"\n\t\tConvert a message into a dictionary that can be used for sending.\n\t\t\n\t\tIf the message has no headers at all, it is returned as is because it should not be converted. Returning None\n\t\tis the same as removing all headers when the message turns out to have no headers.\n\t\n\tArg"}]}
{"task_id": "630629b952e177c0ba46a043", "prompt": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t# Generate the NodeInfo.well-known document path\n\t# - with sub-documents if requested in the client\n\tif document_path:\n\t\t# Prepend the NodeInfo base path to the supplied path\n\t\t# to ensure proper urls\n\t\tdoc_path = urlparse(document_path).path.lstrip('/')\n\t\tdoc_path = '/'.join([get_nodeinfo_document_path(), doc_path])\n\n\t\tdns_path = urllib.quote_plus(doc_path)\n\t\twell_known_doc_path"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tnodeinfo_url = make_canonical_url((url, \"nodeinfowrite\"), document_path)\n\tresp = req_json(nodeinfo_url)\n\t\n\tcheck_resp_ok(resp)\n\t\n\twell_known_doc = resp[\"well_known_doc\"]\n\t\n\treturn well_known_doc\n\t\n\t\n# -- nodeinfo\n\n_make_canonical_url = None\n_well_known_document = (b\"%s/nodeinfo/%s\" % (NODEINFO_URL_PREFIX"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\t# Create the base nodeinfo document\n\tbody = \"\"\n\t\n\tbody += (BODY_HEADER\n\t         % ({'url':url}\n\t            if not document_path\n\t            else {'url':url, 'path':document_path}\n\t         )\n\t         )\n\t\n\tbody += \"\"\"<services>\n\t  <namespace>\n        <nodeinfo>\n          <version>${version}</version>\n        </nodeinfo>\n        <keycloak>\n \t\t  <version>${version}</version"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\tscheme, _, host, port, _, _, path, _, query, _ = urlparse(url)\n\n\t# Default nodeinfo document path to what you like (or custom otherwise)\n\tdocument_path = document_path or DEFAULT_NODINFO_DOCUMENT\n\n\treturn_doc = {\n\t\t\"version\": \"0.1\",\n\t\t\"data\": {\n\t\t\t\"@context\": {\n\t\t\t\t\"nodeinfo\": \"https://nodeinfo.diaspora.software/v1/context.json\",\n\t\t\t\t\"schema\": \"https"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\tif document_path is None:\n\t\tdocument_path = '%(doc_id)s.json' % \\\n\t\t\t{'doc_id': _get_nodeinfo_doc_id()}\n\t\n\t_LOGGER.info(\"Generating NodeInfo document: %s\", document_path)\n\t\n\tdoc = {\n\t\t'name': 'NodeInfo',\n\t\t'description': 'Diaspora NodeInfo',\n\t\t'extensions': {\n\t\t\t'extensions': {\n\t\t\t\t'js': {\n\t\t\t\t\t'publicPath': 'nodeinfo/"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tif not (document_path is None):\n\t\turl = url+\"/\"+document_path\n\t\n\twell_known_document = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<!DOCTYPE nodeinfo SYSTEM \"nodeinfo.dtd\">\n<nodeinfo>\n\t<author>\"\"\"\n\tif document_path is not None:\n\t\twell_known_document += \"<document_path>{}</document_path>\".format(document_path)\n\tif document_path is None:\n\t\twell_known_document"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tnodeinfo_url = url\n\tif document_path is not None:\n\t\tnodeinfo_url += document_path\n\tr = requests.get(nodeinfo_url)\n\tr.raise_for_status()\n\treturn r.json()\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\t\n\tif document_path is None:\n\t\t\tdocument_path = get_nodeinfo_document_path()\n\t\n\tdocument = get_nodeinfo_document(url=url)\n\tdocument['document_path'] = document_path\n\tfor path in document['document_path']:\n\t\tdocument['document_path'][path] = document['document_path'].get(path)['href']\n\t\n\treturn document\n\t\n\n"}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tdoc = {\n\t\t'@context': 'http://nodeinfo.diaspora.software/0.1/context.json',\n\t\t'default': {\n\t\t\t'node': '/nodeinfo/node/',\n\t\t\t'container': '/nodeinfo/container/'\n\t\t},\n\t\t'links': {\n\t\t\t'self': {\n\n\t\t\t}\n\t\t}\n\t}\n\tdoc['links']['self'] = url\n\tif document_path is not None:\n\t\tdoc['default'] = dict(doc['default'], node=document_path, container="}, {"generated_text": "def get_nodeinfo_well_known_document(url, document_path=None):\n\"\"\"\n\tGenerate a NodeInfo .well-known document.\n\t\n\tSee spec: http://nodeinfo.diaspora.software\n\t\n\t:arg url: The full base url with protocol, ie https://example.com\n\t:arg document_path: Custom NodeInfo document path if supplied (optional)\n\t:returns: dict\n\t\"\"\"\n\tnodeinfo = {}\n\tnodeinfo['@context'] = Context.build_path(url)\n\tif document_path is not None:\n\t\tnodeinfo['@doc'] = document_path\n\n\tfor node in [self.Node, self.Edge]:\n\t\tnodeinfo['%s-nodes'] = node.query()\n\t\tnodeinfo['%s-edges'] = node.get_query(source='%s-nodes', target='%s-nodes')\n\t\tnodeinfo['%s-subGraphs'] = node.get_query(source='"}]}
{"task_id": "630629d052e177c0ba46a0a1", "prompt": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t# First ensure the signatures match\n\tif not verify_digital_signature(public_key, signature):\n\t\traise InvalidVerificationError\n\n\t# Now verify the timestamp\n\tfrom datetime import datetime\n\tfrom pylint.messages import TokenNotFound\n\tfrom twisted.enterprise import adbapi\n\timport twisted.persist.who\n\ttry:\n\t\twith who.find(adbapi.Transaction(\"select * from tokens where key='%s'\" % public_key, 'token')) as tokens:\n\t\t\tfor token in tokens:\n\t\t\t\tif token"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t# this is the hash of '<ns0:data><ns0:data-element>'\n\thash_doc = SHA1(doc.xpath('child::node()')[0]).hexdigest()\n\t# this is the verifier that verifies the signature\n\tp2 = xmpppy.JID(signature.get('from')).getNode()\n\tsig = Signature(signature.text)\n\tcert = load_certificate(NSC_CRT, p2.encode('base64'))\n\tif not p2:"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t# verify the signature\n\ttry:\n\t\tparsed_signature = simple_base64_decode(signature)\n\texcept (TypeError, binascii.Error):\n\t\treturn False\n\n\tif len(parsed_signature)!= 40:\n\t\treturn False\n\n\tkey_id, sequence = parsed_signature, struct.unpack(\">I\",\n\t\t\tparsed_signature[:4])\n\tif key_id!= public_key:\n\t\treturn False\n\treturn doc.tamper_proof(sequence)\n\n"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tif not signature:\n\t\treturn (True, \"The message is not signed.\")\n\tpublic_key_sig = openpgp.Sig(text=public_key)[0]\n\trsa_verify(doc, public_key_sig, base64.decodestring(signature))\n\treturn (True, \"\")\n\ndef rsa_verify(doc, signer, sig):\n\tverifier = openpgp.PyOpenPGP.Verifier().init()\n\tverifier.set_pubkey(signer)\n\tver"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tfor element, signing_key, hash_alg in elements_of_doc(doc.documentElement):\n\t\tsigtext = \" \".join((element.tagName, signing_key, hash_alg, signature))\n\t\te = element.ownerDocument.importNode(\n\t\t\t\telement.ownerDocument.createTextNode(sigtext), True\n\t\t\t\t)\n\t\tpkey = ssl.match_hostname(e.firstChild.nodeValue)\n\t\te.firstChild.nodeValue = pkey\n\t\tif not verify_with_cert(cert_file):\n\t\t\te"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\t#TODO: verify the sigtype is relaying\n\ttry:\n\t\tsigned = doc.find('Signature')\n\texcept:\n\t\tmsg = 'XML signature was not found in received XML'\n\t\ttry:\n\t\t\tverification_report = verify_documents(doc)\n\t\t\treturn (msg, verification_report)\n\t\texcept Exception as e:\n\t\t\tprint 'XML verification report:'+ str(e)\n\t\t\tmsg = str(e)\n\t\t\treturn (msg,'Exception')\n\t\n\t#Get the signature type and validate the signature\n\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\ttry:\n\t\tm = sks_public.VerifySignature(public_key, doc, signature, verify_algorithm = \"rsa_sha1_pkcs1_v1_5\")\n\t\treturn m.match\n\texcept:\n\t\treturn False\n\ndef verify_signature(public_key, xmlstring):\n\t\"\"\"verify the signed parts and return True/False\"\"\"\n\tis_pkcs1 = False\n\n\tif isinstance(xmlstring,unicode):\n\t\txmlstring = xmlstring.encode(\"utf-8\")\n\t"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\telem = doc.getroot()\n\tsig = elem[-1].get(\"signature\")\n\tif not sig == signature:\n\t\traise Exception(\"Incorrect signature.\")\n\n\tconfi = elem[-1].get(\"confirmation\")\n\tif confi is None:\n\t\treturn False\n\tconfi = confi.strip()\n\treturn confi in signature\n\n\ndef parse_receipt(receipt):\n\treturn json.loads(receipt)\n\n\ndef verify_receipt(pubkey, signature):\n\t\t# print"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tfrom Crypto.PublicKey import RSA\n\tfrom Crypto.Signature import PKCS1_v1_5\n\n\tpublic_key = RSA.importKey(public_key)\n\tkey = PKCS1_v1_5.new(public_key)\n\n\tdoc_sig = doc.find('{http://www.w3.org/2000/09/xmldsig#}Signature', namespaces)\n\tif not doc_sig.tag.endswith(self.DIGESTALGORITHM):\n\t\tprint 'SIG TAG"}, {"generated_text": "def verify_relayable_signature(public_key, doc, signature):\n\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\n\tauthor did actually generate this message.\n\t\"\"\"\n\tlogging.debug(\"verifying that signed XML document has proper verification code\")\n\ttry:\n\t\tfrom openid.consumer import consumer\n\t\tfrom openid.extensions import sreg\n\t\tvalidated_msg = consumer.XMLVerificationRequest(identifier=self._generateId(), return_to=\n\t\t\tself._getReturnToURL(), timestamp=timestamp(), nonce=nonce)\n\t\tfrom openid.message import Message\n\t\tmsg = Message.fromOpenIDArgs(validated_msg.toMessage())\n\n\t\t# If this is a verified message, see if"}]}
{"task_id": "630629e052e177c0ba46a0c4", "prompt": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\ttry:\n\t\treturn json.loads(document)\n\texcept ValueError:\n\t\ttry:\n\t\t\tparts = document.split('<', 1)\n\t\t\tassert len(parts) > 1\n\t\t\tparts[-1] = '</' + parts[-1]\n\t\t\treturn {\n\t\t\t\t'is_current': parts[0].split('<')[0].split('>')[1].split(':')[0],\n\t\t\t\tparts[0] + '/' + parts[-1].rsplit('<', 1)[0]\n\t\t\t}\n\t\texcept IndexError"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\ttry:\n\t\tparsed_document = json.loads(document)\n\texcept json.JSONDecodeError as err:\n\t\tif error_is_old_webfinger_json(document):\n\t\t\tparsed_document = old_to_new_diaspora_webfinger_parser(document)\n\t\telse:\n\t\t\traise err\n\n\treturn parsed_document\n\ndef is_old_webfinger(document: str, required: str=None) -> bool:\n\t\"\"\"\n\tCheck if a given document is an old (version 1) Diaspora Webfinger"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tif \"<div class='webfinger'>\" not in document: # new format\n\t\treturn json_decode(document)\t\t\n\t\t\n\ttry:\n\t\treturn ast.literal_eval(document)\n\texcept Exception as e:\n\t\traise Exception(\"Error parsing Diaspora webfinger: %s\\nDocument: %s\\n\" % (e, document))\n\t\ndef parse_diaspora_account_name(document: str) -> Dict:\n\tname_re = re.compile(r'^-?[0-9]{4"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tfor match in re.finditer(re_webfinger, document):\n\t\ttry:\n\t\t\txrd = match.group(1).split(\"/\")\n\t\t\t\n\t\t\t# Parse \"old\" json format\n\t\t\tif len(xrd) == 4:  # old format, xrd.json = {\"email\": \"email\"}\n\t\t\t\treturn next(parse_json_file(json.loads(xrd[1])[\"email\"], xrd[2], xrd[3]])\n\t\t\telif len(xrd) == 5:  # Diaspora 6.1 JSON"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\ttry:\n\t\twebfinger = json.loads(document)\n\texcept:\n\t\twebfinger = xrd.parse_xrd_v1(document)\n\turl = webfinger[\"url\"]\n\tif \"email\" in webfinger[\"identities\"]:\n\t\treturn {\n\t\t\t\"identity\": webfinger[\"identities\"][\"email\"],\n\t\t\t\"claimed_groups\": frozenset(webfinger[\"identities\"][\"groups\"],\n\t\t\t                                   ignore_case=True),\n\t\t\t\"claimed_user\": webfinger[\"identities\"][\"user\"],\n"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\tif JINJA and document:\n\t\ttry:\n\t\t\treturn dict(**JSON.loads(JINJA.compile_or_identify_template_string(document)))\n\t\texcept:\n\t\t\tlogger.error(\"failed to parse json\")\n\t\t\treturn {}\n\t\n\tweb_finger_values()\n\t\n\tassert document\n\t\n\ttry:\n\t\treturn dict(**json.loads(document))\n\texcept:\n\t\treturn {}\n\t\n\t"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tresult: Dict = None\n\ttry:\n\t\tif document.startswith(')'):\n\t\t\t# JSON format\n\t\t\tjson_data = json.loads(document[2:])\n\t\t\tresult = json_data.get('results', [])\n\t\telif ':' in document:\n\t\t\t# XRD format\n\t\t\tparsed_document = diaspora.webfinger.split_xrd(document,'')\n\t\t\t# Remove spaces between tokens\n\t\t\tparsed_document = [tok.strip() for tok in parsed_document]\n\t\t\t# Check"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\tresult: Dict = {} # type: Dict[str, Any]\n\n\tif not isinstance(document, str):\n\t\traise DiasporaWebfingerNotParseable(\"Provided document type \" + str(type(document)) + \" isn't string\")\n\t\n\ttry:\n\t\tif json_to_xrd:\n\t\t\tresult = json.loads(document, object_pairs_hook=OrderedDict)\n\t\telse:\n\t\t\tresult = xrp.loads(document)\n\texcept Exception as e:\n\t\t\n\t\traise DiasporaWeb"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t# The API seems to return errors more often than we need. \n\t# Just ignore them.\n\trecover_errors(document)\n\n\ttry:\n\t\treturn get(document)\n\texcept RequestException as err:\n\t\tlogger.warn('Webfinger request for %s failed with %s: %s %s', url, err, type(err), err.message if hasattr(err,'message') else '')\n\t\tlogger.debug('Webfinger request for %s failed with %s: %s', url, err, type(err), err.response if"}, {"generated_text": "def parse_diaspora_webfinger(document: str) -> Dict:\n\"\"\"\n\tParse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\t\n\thttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n\t\"\"\"\n\t\n\ttry:\n\t\tif b64decode(document).startswith(b'{'):\n\t\t\t# new format - XRD format\n\t\t\tdocument_diaspora = json.loads(document[1:-1].decode('utf-8'))\n\texcept ValueError:\n\t\t# old format - JSON format\n\t\tdocument_diaspora = json.loads(document)\n\n\treturn document_diaspora\n\n\ndef diaspora_webfinger_response(document: str) -> str:\n\treturn f'Webfinger document is {document.strip().replace"}]}
{"task_id": "630629e152e177c0ba46a0d1", "prompt": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tlogger.trace(\"Attempting to receive the RFC7033 webfinger document of %(handle)r\", {\"handle\": handle})\n\t\ttry:\n\t\t\tlogger.trace(\"Attempting to receive the RFC7033 webfinger document of %(handle)r\", {\"handle\": handle})\n\t\t\twebfinger = requests.get(webfinger_uri(handle)).json()\n\t\t\tlogger.trace(\"Received the RFC7033 webfinger document in %(handle)r\", {\"handle\": handle})\n\t\texcept (requests.ConnectionError, requests.exceptions.Read"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t# pylint: disable=import-outside-toplevel\n\timport xmlrpclib as xmlrpc\n\n\turi = \"http://webfinger.io/{0}\".format(handle)\n\tif sys.version_info[0] == 2:\n\t\turi = uri.encode(\"utf-8\")\n\t\turi = urllib.quote_plus(uri)\n\n\tconn = xmlrpc.Server('.')\n\ttry:\n\t\tdoc = conn.service.fetch(uri)\n\texcept IOError as error:\n\t\tif"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\turl = \"https://certificates.zulip.org.v6/webfinger?address=%s\" % (handle,)\n\n\t\tresponse = requests.head(url, allow_redirects=True)\n\n\t\tis_webfinger = response.headers.get(\"Location\")\n\t\tif is_webfinger:\n\t\t\tresponse.raise_for_status()\n\n\t\t\treturn is_webfinger\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tif not validate(WEBFINGER_PUBLIC_SUBDOMAIN, handle):\n\t\treturn None\n\ttry:\n\t\tdocuments = list(webfinger(handle))\n\t\treturn documents[0]\n\texcept (\n\t\tWebFingerNoDocumentFoundException,\n\t\tWebFingerQueryErrorException,\n\t\tURLError,\n\t\tTimeout,\n\t\tInvalidURLError,\n\t\tUnicodeDecodeError,\n\t):\n\t\t# This function only retrieves document, so\n\t\t# we do not want that there are errors raised in an exception."}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\t# try: webfinger_document:str = db.scalar(\n\t\t# db.scalar(\n\t\t\t# 'SELECT id, fingerprint, jti FROM webfinger_document '\n\t\t\t# 'ORDER BY order_', '%s'), handle)\n\t# except: webfinger_document = None\n\twebfinger_document = None\n\t# TODO: remove this when we have a Webfinger Document in 0.8.0\n\ttry:\n\t\tprintdbg(\"webfinger_document:stderr\", 0, \"webfinger-document:%s:"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tif not is_valid_webfinger_link(handle):\n\t\treturn None\n\n\trequest_method = \"GET\"\n\turl_params = {\n\t\t\"sver\": \"https://pkserver.appspot.com/lookup?sver=https://github.com/%s\" % handle\n\t}\n\tresponse = requests.request(\n\t\trequest_method,\n\t\t\"https://pkserver.appspot.com?callback=https://pkserver.appspot.com/callback\",\n\t\tparams=url_params,\n\t\ttimeout=0."}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tresponse = requests.get(os.environ['SERVER_PORT_CHAT'] + '/webfinger/' + handle,\n\t\t\t\t\t\t\t\theaders={\"accept\": \"application/json\"}\n\t\t)\n\texcept Exception as exc:\n\t\tpass\n\tif not 400 <= response.status_code < 600:\n\t\treturn None\n\treturn response.text\n\ndef create_or_read_user_profile(username: str, user_id: str, email: Optional[str]):\n\t\"\"\" Creates or reads a user profile with a default nickname of username. \"\"\""}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tlog.debug(\"Attempt to retrieve webfinger document for %s\" % handle)\n\ttry:\n\t\tf = urllib.request.urlopen(WebfingerProtocol.WebFingerDocumentURLTemplate % handle)\n\t\treturn f.read()\n\texcept urllib.error.HTTPError as e:\n\t\tif e.code in (301, 302, 401, 403):\n\t\t\treturn None\n\t\telse:\n\t\t\traise e\n\ndef retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tRetrieve the webfinger document for"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\timport pytan\n\texcept ImportError:\n\t\t# The following are the supported PYTONS\n\t\tpass\n\n\ttry:\n\t\timport webfinger\n\texcept ImportError:\n\t\t# This is required to be on the PYTONS path\n\t\timport sys\n\t\tsys.path.append(\"/opt/pyion_sdk/\")\n\t\timport webfinger\n\telse:\n\t\ttry:\n\t\t\tdoc, url = webfinger.discover(handle, version=webfinger.Verb.GET)\n\t\texcept Exception as e:\n\t\t\treturn None"}, {"generated_text": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tret = get_webfinger_document(handle)\n\tif not ret:\n\t\tprint(\"WARNING: Failed to retrieve a webfinger document or empty response\")\n\treturn ret\n\n# --------------------------------------------------------------------------------------------------------------------\n# M E S S A G E S\n# --------------------------------------------------------------------------------------------------------------------\n\ndef setup_message():\n\tglobal message\n\n\t# The name of the bot should not need changing.\n\tmessage = Message()\n\tmessage.set_author(BOT_USERNAME, BOT_NAME, BOT_REAL_NAME)\n\tmessage.add_repo(PROPACK"}]}
{"task_id": "630629e152e177c0ba46a0d2", "prompt": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tr = handle_to_resource(handle)\n\tif not r:\n\t\tprint 'Error parsing remote Webfinger'\n\t\tsys.exit(1)\n\tinfo=r['info']\n\tbody=info['body']\n\tuser_id,name=body.split('/')[0].split(':')\n\tdiaspora_id=body.split('/')[1]\n\n\t#\n\t# The Diaspora webfinger contains three fields: http://www.diaspora.org\n\t#  /services/org/{org-id}/"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tlogging.info('Retrieving %s document...', handle)\n\n#\tretrieve_document(RemoteURL(handle, \"https://res.diaspora.org/api/v1/me\"), parse_diaspora_webfinger)\n\t\n\tdocument_id = None\n\tsoup, _ = html.parse(RemoteURL(handle, \"https://res.diaspora.org/api/v1/me\"))\n\n\tif soup is not None:\n\t\tfor a in soup.find_all('a'):\n\t\t\tif a.text!= '["}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tif not diaspora_webfinger.retrieve_document(handle):\n\t\ttraceback.print_exc()\n\t\traise DiasporaServerError\n\treturn diaspora_webfinger.parse_document(handle)\n\ndef parse_diaspora_webfinger_document(document):\n\t\"\"\"\n\tParse a Diaspora \"webfinger\" document.\n\t\n\t:arg document: string\n\t:returns: dict\n\t\"\"\"\n\tif document is None:\n\t\treturn {}\n\telse:\n\t\treturn diaspora_webfinger.parse_"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tresult =  {}\n\t\n\t# Retrieve document\n\tresponse = diasp.rest.get_webfinger(handle)\n\tif response.status_code < 200 or response.status_code > 201:\n\t\tlogging.error('Retrieve webfinger response:')\n\t\tlogging.error(response.headers)\t\t\t\n\t\tlogging.error('Failed to retrieve webfinger. Check the URL.')\t\t\n\t\treturn result\n\tdocument = response.text\n\t\n\t# Parse\n\tresult = json.loads(document)\n\tfor item in result"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tpass\n\ndef retrieve_and_parse_github_webfinger(handle):\n\t\"\"\"\"\n\tRetrieve a and parse a remote GitHub webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tpass\n\ndef set_webfinger_data(data=None, remote_handle=None):\n\t\"\"\"\n\tSet webfinger data\n\t\n\t:arg data: Dictionary containing the webfinger data\n\t:arg remote_handle: The Remote handle to retrieve webfinger data from and parse the data from"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t# Build a HTTP GET request request\n\treq = tornado.httpclient.HTTPRequest(remote_addr,\n\t\t\"{}/v1{}\".format(get_route(\"api\", \"webfinger\"), urlencode({get_route('webfinger', \"%s\" % handle): handle})))\n\treq.headers[\"User-Agent\"] = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36\"\n\t"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\tif type(handle) == str:\n\t\thandle = {'diaspora_webfinger': handle}\n\t\n\tf = open(handle['diaspora_webfinger'])\n\tlines = f.readlines()\n\tf.close()\n\t\n\t# Parse out the attributes of the webfinger document.\n\t\n\twebfinger = dict()\n\t\n\tfor line in lines:\n\t\tattr = line.split(':')\n\t\tif'' in attr[0]:\n\t\t\tattr[0] = attr[0].split(' '"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tr = requests.get(diaspora_site_url + handle)\n\tr.raise_for_status()\n\txml_content = json.dumps(r.content, separators=(',',':')).replace('null', 'None')\n\treturn ET.fromstring(xml_content)\n\t\ndef send_message(handle, destination_handle, content):\n\t\"\"\"\n\tSend a message to the given destination handle.\n\t\n\t:arg handle: Remote handle to send message to\n\t:arg destination_handle: Remote handle to send message to\n\t"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\tdoc = requests.get(diaspora_api + handle)\n\tif not doc.status_code == 200:\n\t\tlog.exception('Retrieve remote Diaospra handle failed!')\n\t\treturn\n\t\n\tbody = json.loads(doc.content.decode('utf-8'))\n\t\n\tif 'webfinger' in body:\n\t\treturn body['webfinger']\n\telse:\n\t\treturn None\n\t\n\t\n\t\ndef retrieve_and_parse_diaspora_remote_handles_for_topic(handle, topic, max"}, {"generated_text": "def retrieve_and_parse_diaspora_webfinger(handle):\n\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\t\n\t:arg handle: Remote handle to retrieve\n\t:returns: dict\n\t\"\"\"\n\t\n\tdiaspora_webfinger = handle\n\tif not diaspora_webfinger:\n\t\thandle = DiasporaWebfingerParser().retrieve(handle)\n\t\tdiaspora_webfinger[0] = handle\n\t\n\tdata = diaspora_webfinger[0]\n\tparsed_json = json.loads(data) if data else {}\n\t\n\treturn parsed_json\n\ndef generate_new_token(handle, options, user_agent_id=None, user_agent_version=None):\n\t\"\"\"\n\tGenerate a"}]}
{"task_id": "630629e252e177c0ba46a0d6", "prompt": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn XRD(\"https://diaspora.org/meta/\", host)\n\t\t\t\t\t\n@cache.memoize(60*60*24*7)\ndef get_remote_meta(host):\n\treturn retrieve_diaspora_host_meta(host).data\n\nclass GitHub:\n\t\"\"\"\n\tA class for interacting with GitHub API resources.\n\t\"\"\"\n\tdef __init__(self): pass\n\n\t@cache.memoize(60*60*24*7)\n\tdef authenticate(self, consumer_key, consumer_secret, username"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\treturn XRDCollection(host.service.get(host._meta(\"base_url\") + \"meta.xml\"))\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tif not type(host) in types.StringTypes:\n\t\thost = str(host)\n\t# Retrieve from Diaspora\n\tdocument = get_document(host, \"https://diaspora.example.org/v3/meta/hosts/{url_encoded_host}\".format(url_encoded_host = url_escape(host)))\n\n\ttry:\n\t\thost_meta = create_host_meta_proxy(document)\n\texcept InvalidUrl:\n\t\tset_host_meta_valid(False)\n\t\treturn\n\t\n\tset"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\tr = None\n\t\n\tremote = remote_diaspora_host_meta(host=host)\n\tr = remote.get()\n\t\n\treturn r\n"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn _get_data(\"data/diaspora-\" + host + \"/meta.json\", False)\n\t\ndef retrieve_diaspora_host_posts(host):\n\treturn _get_data(\"data/diaspora-\" + host + \"/posts-api.json\", False)\n\t\ndef write_diaspora_host_meta(host, meta):\n\t\"\"\"\n\tWrite a remote Diaspora host-meta document to the server.\n\t\n\t:arg host: Host to write to\n\t:arg meta: Meta data to write\n\t\"\"\"\n\t"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn DiasporaXrdService.get_meta(host)\n\n\nclass DiasporaXrdService(XrdService):\n\t\"\"\"\n\tDiaspora xrd service.\n\t\n\t:arg api_url: Diaspora REST API URL (default: None)\n\t:arg dry_run: False (default: None)\n\t\"\"\"\n\tdef __init__(self, api_url=None, dry_run=None):\n\t\tif api_url is None: api_url = DiasporaXrdService.default_uri\n\t\tif dry_run"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\tmeta = RemoteDiasporaHostMeta()\n\tmeta._raw = api.posts.get_single(host=host)['meta']\n\n\treturn meta\n\n\ndef retrieve_diaspora_webhook(diaspora_id=None, webhook_id=None, source=None, source_id=None):\n\t\"\"\"\n\tRetrieve a single webhook document.\n\n\t:arg diaspora_id: Diaspora webhook identifier\n\t:arg webhook_id: Webhook document identifier\n\t:arg source: The ``DiasporaSource`` to use"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\txrd = XRD()\n\tretcode, xrd.host = xrd.cli(\"info\", host)\n\tfor m in xrd.meta:\n\t\tif m[\"key\"] == \"name\":\n\t\t\txrd.name = m[\"value\"]\n\t\tif m[\"key\"] == \"public\":\n\t\t\txrd.is_public = m[\"value\"]\n\t\tif m[\"key\"] == \"domain\":\n\t\t\txrd.domain = m[\"value\"]\n\tif xrd.domain is None:\n\t\traise HostParseError(\"Unable to retrieve domain for host"}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\t\n\tresult = []\n\thost_meta = XRD(host)\n\t\n\t# Fetch remote document\n\tremote_doc = host_meta.document\n\tassert(remote_doc is not None), 'Remote Diaspora host-meta document not found. [%s]' % (host)\n\tassert(remote_doc.id is not None), 'Remote Diaspora host-meta document has no id. [%s]' % (host)\n\tdiaspora_host = remote_doc.id\n\t\n\t# Fetch related documents (e.g."}, {"generated_text": "def retrieve_diaspora_host_meta(host):\n\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\t\n\t:arg host: Host to retrieve from\n\t:returns: ``XRD`` instance\n\t\"\"\"\n\treturn self.request_xrd(host.id(), \"GET\", \"diaspora/meta\", dict())\n\t\t\n\t\n\t\n\t\ndef retrieve_diaspora_members(host):\n\t\"\"\"\n\tRetrieve all Diaspora Users/Organizations (from cache)\n\t\n\t:arg host: Host to retrieve users/orgs from\n\t:returns: :class:`Users` object.\n\t\"\"\"\n\tmembers = cache.get(host.id())\n\tif members is None:\n\t\tmembers = (Members()\n\t\t\t\t\t.all()\n"}]}
{"task_id": "630629e752e177c0ba46a0fb", "prompt": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\t# build a request with the correct method\n\treq = Request(url, data=_form_encode(data), auth=None, timeout=timeout)\n\t\n\ttry:\n\t\t# use the method specified from the 'args' list as kwargs\n\t\tkwargs.update(method=method)\n\n\t\t# post via the defined URL\n\t\tresponse = urlopen(req, *args, **kwargs)\n\n\texcept HTTPError as err:\n\t\t# return a dict of request info (except if the response is a json\n\t\t# object - we don't need"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\ttry:\n\t\tif isinstance(data, bytes):\n\t\t\tdata = data.decode(\"utf-8\")\n\texcept: \n\t\tpass\n\t\n\ttry:\n\t\tif isinstance(data, io.RawIOBase):\n\t\t\tresponse = requests.post(url, data, timeout=timeout, **kwargs)\n\t\telif isinstance(data, io.BytesIO):\n\t\t\tresponse = requests.post(url, data, timeout=timeout, **kwargs)\n\t\telse:\n\t\t\traise ValueError(\"Unknown input type\")\n\texcept:\n\t\terr"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\tif method in (\"post\", \"get\"):\n\t\t# Normalize method argument; if it's not \"post\" or \"get\", assume it's \"post\"\n\t\tmethod = \"POST\"\n\tif not hasattr(data, \"read\"):\n\t\tdata = BytesIO(data)\n\t\n\ttry:\n\t\t# Set default timeout, or let user override or config if missing\n\t\ttimeout = get_config(\"DEFAULT_TIMEOUT\", timeout)\n\t\tsession = requests.Session()\n\t\tresponse = session.request(method, url, data=data,"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tclient = requests.Session()\n\t\n\tif isinstance(data, dict):\n\t\tdata_form = data['data_form']\n\telif isinstance(data, bytes):\n\t\tdata_form = \"application/x-www-form-urlencoded\"\n\telif hasattr(data,'read'):\n\t\tdata_form = 'form_multipart/form-data; boundary={0}'.format(uuid.uuid4())\n\t\tparts = []\n\t\tfor part in data:\n\t\t\tparts.append('--{0}'.format"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\theaders = kwargs.get(\"headers\", get_headers())\n\tif headers.get(\"Content-Type\"):\n\t\theaders.update(kwargs.get(\"headers\", {\"Content-Type\":headers[\"Content-Type\"]}))\n\telse:\n\t\theaders.update(kwargs.get(\"headers\", {\"Content-Type\":content_type(kwargs.get(\"data\"))}))\n\t\n\ttry:\n\t\tr = requests.request(method, url, data=data, *args, **kwargs)\n\t\tif hasattr(r,'status_code"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tif hasattr(data, \"read\"):\n\t\tdata = data.read()\n\tif hasattr(data, \"readlines\"):\n\t\tdata = \"\\n\".join(data.readlines())\n\n\t# if the url is already a full URL, just pass it through\n\tif (len(url) < 10) and (url[:7] == \"http\"):\n\t\treturn http_request(url, data, timeout=timeout)\n\n\turl = fix_path(url)\n\n\ttry:\n\t\tkwargs[\"timeout\"] = timeout\n\t\tkwargs[\"data"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tif isinstance(data, dict): data = urlencode(data)\n\tif isinstance(data, bytes): data = data.decode('utf8','replace')\n\t\n\tif data is None:\n\t\treturn urequests.send(url=url, method=method, timeout=timeout, **kwargs)\n\telse:\n\t\treturn urequests.Session().post(url, data=data, timeout=timeout, **kwargs)\n\n"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t# Don't send anything unless we have at least one file-like object\n\tif data is None and not hasattr(data, \"read\"):\n\t\treturn -1, \"No data passed for sending\"\n\t\n\tresponse = requests.request(method, url, data=data, *args, **kwargs)\n\tstatus = response.status_code\n\tstatus_message = response.headers.get(\"X-Status-Message\", None)\n\tif status_message:\n\t\tlog.warn(\"Received Status-Message: %s\", status_message)\n\treturn"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\t\n\turl = url.rstrip(\"/\")\n\t\n\tif method == \"get\":\n\t\tresp = requests.get(url, params=data, stream=True, timeout=timeout)\n\telif method == \"post\":\n\t\tif type(data) is dict:\n\t\t\tdata = urllib.parse.urlencode(data)\n\t\tresp = requests.post(url, data, timeout=timeout)\n\telif method == \"del\":\n\t\tresp = requests.delete(url, timeout=timeout)\n\telse:\n\t\traise Exception(\"Method argument"}, {"generated_text": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\"\"\"\n\tHelper method to send a document via POST.\n\t\n\tAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\t\n\t:arg url: Full url to send to, including protocol\n\t:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n\t:arg timeout: Seconds to wait for response (defaults to 10)\n\t:arg method: Method to use, defaults to post\n\t:returns: Tuple of status code (int or None) and error (exception class instance or None)\n\t\"\"\"\n\tif data is not None:\n\t\tif isinstance(data, bytes):\n\t\t\tdata = requests.utils.bytes_(data)\n\t\tif isfilelike(data):\n\t\t\twith open(data, \"rb\") as fd:\n\t\t\t\tdata = fd.read()\n\t\tif isinstance(data, dict):\n\t\t\tmultipart = None\n\t\t\tfor (name, value) in data.items():\n\t\t\t\tif isinstance(value, (dict, list)):\n\t\t\t\t\tmultipart = _multipart_encode(value, multipart=multipart)\n"}]}
{"task_id": "6306091073426c38ae68acac", "prompt": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tdic.setdefault(key, {})  # if not given default dictionary is used\n\tfor k in keys:\n\t\tnested = k.split('.')\n\t\t# if the given key is not already in the nested dictionary, store the value and descend\n\t\tif not keys and k not in dic.keys():\n\t\t\tdic[k] = val\n\t\t# if the deepest key is already present, insert the value\n\t\telif len(nested) == 1:\n\t\t\tdic[k] = val\n\t\t# if the last nested key is known\n\t\telse"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor key in keys:\n\t\tif not key in dic:\n\t\t\tdict_insert(dic, val, key, keys[0:0:-1])\n\t\tdic[key] = val\n\t\t\ndef ndexpand(x, sep='', is_iter=True):\n\t\"\"\"\n\tExpand nested dictionary(dict), list(list), tuple(tuple)\n\t\n\tthe given dictionary is returned as is(dict), list(list), tuple(tuple)\n\tit's value is not expanded recursively\n\t\n\texample:\n\t  "}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tidx = len(keys)\n\tfor i in range(len(keys)):\n\t\tdic = dic.setdefault(keys[i], {})\n\t\tidx -= 1\n\tdic.setdefault(key, {})[idx] = val\n\ndef dict_union(dic, val, key, *keys):\n\t\"\"\"\n\textand a key chain in a dictionary by given value, without change existing value\n\t\n\texample:\n\t  dict_union({'key': 123}, 'val', '.'.split('.'))\n\t\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\t\n\tfor key in keys:\n\t\tif key not in dic.keys():\n\t\t\tdic[key] = {}\n\t\t\n\t\tdic = dic[key]\n\t\n\tdic[key] = val\n\n### end of module\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor k in keys:\n\t\tdic = dic[k]\n\tdic[key] = val\n\ndef dict_delete(dic, val, key, *keys):\n\t\"\"\"\n\tdelete value of a nested key and return the object\n\t\n\tto remove a value of a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_delete({}, \"val\", \"key1.key2\".split('.'))\n\t\n\t:param d: a dictionary object to remove the nested key value from\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tvalue = OrderedDict()\n\tfor key in keys:\n\t\ttry:\n\t\t\tvalue = value[key]\n\t\texcept KeyError:\n\t\t\tvalue = OrderedDict()\n\t\t\tvalue[key] = OrderedDict()\n\t\tif value is not OrderedDict():\n\t\t\tvalue = value[key]\n\t\tvalue = value[key.split('.')[0]]\n\n\tvalue[key.split('.')[1]] = val\n\n\ndef dict_insert_list(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tval = \"%s\" % val\n\tfor key in keys:\n\t\tif '.' in key:\n\t\t\tpkey = key.split('.')[0]\n\t\t\tif pkey not in dic:\n\t\t\t\tdict_insert(dic, val, key, dic.split('.'))\n\t\telse:\n\t\t\tdic[key] = val\n\t\t\treturn\n\treturn\n\n# function for checking is given key is a key of the nested data structure\ndef nested_key_validation(key):\n\t\"\"\"\n\tCheck if a key is valid nested key\n\n\t:"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tobj = dic\n\tif len(keys) > 1:\n\t\tfor i in range(1, len(keys)):\n\t\t\tobj = obj[keys[i]]\n\tobj[keys[0]] = val\n\t\n"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tif not isinstance(dic,dict):\n\t\traise BadParameter('type(dic): {}, should be dict'.format(type(dic)))\n\tfor sub_key in keys:\n\t\tif not isinstance(sub_key, str):\n\t\t\traise BadParameter('value for keys: {}, should be str type instead of {}'.format(sub_key,type(sub_key)))\n\t\ttmp_dic = dic\n\t\tidx = 1\n\t\twhile idx < len(keys):\n\t\t\ttmp_dic = tmp_dic.get(sub"}, {"generated_text": "def dict_insert(dic, val, key, *keys):\n\"\"\"\n\tinsert a value of a nested key into a dictionary\n\t\n\tto insert value for a nested key, all ancestor keys should be given as\n\tmethod's arguments\n\t\n\texample:\n\t  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\t\n\t:param dic: a dictionary object to insert the nested key value into\n\t:param val: a value to insert to the given dictionary\n\t:param key: first key in a chain of key that will store the value\n\t:param keys: sub keys in the keys chain\n\t\"\"\"\n\tfor _key in keys[:-1]:\n\t\tif not key in dic:\n\t\t\tdic[_key] = {}\n\t\tdic = dic[_key]\n\tdic[keys[-1]] = val\n\treturn dic\n\ndef update_dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tupdate a value of a nested key into a dictionary, without the keys need to \n\tbe mentioned\n\t\n\texample:\n\t  update_dict_insert({}, 'val', 'key1.key2'.split('.'"}]}
{"task_id": "6306091a73426c38ae68acc8", "prompt": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tout = '''\n\ttype \"%(type)s\" of string\n\tsubtype \"%(subtype_name)s\" of IniType\n\tattribute Name { string := \"%(subtype_name)s\" }\n\tattribute Value { \"%(type)s\" }\n\t''' % {\n\t\t'type' : format_type(type(spec_option)),\n\t\t'subtype_name' : str(fq_name(namespace.name, spec_option['Name'])),\n\t\t'type' : format_type(type(spec_option['"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\treturn \"\"\"<?xml version=\"1.0\" encoding=\"windows-1252\" standalone=\"yes\"?><xsd:schema version=\"1.0\" xmlns=\n\"urn:schemas-microsoft-com:xml-analysis\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:tns=\"http://docs.oasis-open.org/ns/analysis/odf/v1.0/prof"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tresult_type = None\n\t\n\tif settings_dirs.__class__.__name__ == \"list\":\n\t\t\tprint \"Spec option is a list\"\n\t\t\tfor d in settings_dirs:\n\t\t\t\tif os.path.isdir(d)==True:\n\t\t\t\t\tprint \"Valid directory:\" +str(d) + \", will create a ini-type object file for it\"\n\t\t\t\telse:\n\t\t\t\t\tprint \"Invalid directory:\" +str(d)+\", will not create an ini-type object file\"\n\n\t\tif settings_dirs.__class__.__"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tif spec_option.type == \"IniType\" and len(spec_option.list_of_file_names) > 0:\n\t\treturn IniType(\n\t\t\tsettings_dirs=settings_dirs,\n\t\t\tconfig_option=spec_option,\n\t\t\tobject_type=\"List\",\n\t\t\tcomposite_type=\"IniType\"\n\t\t)\n\telse:\n\t\treturn IniType(config_option=spec_option, object_type=\"File\", composite_type=\"IniType\")\n\nclass IniSettings: \n\t\"\"\"\n\t"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\treturn {'options': \n\t\t\t[{'name': 'file_path', \n\t\t\t'type':'string', 'enum': list(settings_dirs)}],\n\t\t\t'type': 'object', 'extends': 'FilePathType'}\n\ndef list_of_file_data_structures(config_option, struct_type=IniType):\n\t\"\"\"\n\tCreate a map between config option and IniStructure\n\t\"\"\"\n\toption_to_section = \n\t\t{config_option:('section',) for config_option in config_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tdef convert(settings_dirs, spec_option, default_values={}, value=None):\n\t\tif not isinstance(default_values, type) or not issubclass(default_values, IniType):\n\t\t\traise TypeError('default_values should be an instance of IniType or its subclass')\n\t\tsettings_dirs_list = settings_dirs if isinstance(settings_dirs, type) and issubclass(\n\t\t\tsettings_dirs, list_of_strings) else [settings_dirs]\n\t\tfor settings_dir in settings_dir"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tfrom..ini_type.IniType_ListOfFileNames import IniType_ListOfFileNames\n\treturn IniType_ListOfFileNames(settings_dirs, spec_option)\n\n\n\t\n\ndef option_group(settings_dirs, spec_option):\n\tfrom..ini_type.OptionGroup import OptionGroup\n\tfrom..ini_type.IniFactory import ini_type_for_class\n\t\n\tfrom..core.BaseSettings import  BaseSettings\n\tfrom..convert.settings import ConverterBaseSettings\n\t\n\tclass"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tt_ = \"Inistype (File List)\"\n\treturn IniType(t_, \"File Name List\", spec_option)\n\n\"\"\"\n\tDefine IniTypes for all defined data types/subtypes\n\"\"\"\n## Data type\n# Basic types\n\n# For now, use the default options\nDataTypes = IniType.list_of_type_file_names(\n\t[sys.path[0], \"DataTypes\"],\n\t\"Data Type\")\n\n# Subtype\nDataTypes.new_type(\"Integer\", \"\", Integer, False).new_"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\tspecification = ((\"type\",\"file\"), spec_option)\n\tnew_type = IniType(specification, {})\n\treturn(new_type)\n#----------------------------------------------------------------------------------------\n# Main section of file\n#----------------------------------------------------------------------------------------\n\nprint(\"Creating the file in 'tutorials/read_list.py'\\n\")\n#----------------------------------------------------------------------------------------\n# List the current working directory as a string\n#----------------------------------------------------------------------------------------\nprint(\"Current working dir: \")\ncd_str = os.getcwd()\nprint cd_str\n#----------------------------------------------------------------------------------------\n# List the current working directory as a"}, {"generated_text": "def list_of_file_names(settings_dirs, spec_option):\n\"\"\"\n\tCreate a new IniType complex type\n\t\"\"\"\n\ttype_name = \"list_of_file_names\"\n\ttype_attributes = {\n\t\t\t\"filename\":IniType.attribute_type(\"string\").set_parser_function(\"filename_parser\"),\n\t\t\t}\n\ttype_extra = {\n\t\t}\n\n\treturn IniType(type_name, type_attributes, type_extra)\n\n@register_parser\ndef filename_parser(settings_dirs, spec_option):\n\t\"\"\"\n\t\tParse the filename that will be added to the IniType attribute\n\t\"\"\"\n\tif ('-' in"}]}
{"task_id": "6306091b73426c38ae68acd7", "prompt": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tif \"ansible_config_manager\" not in cls.__dict__:\n\t\tcls.ansible_config_manager = configobj.ConfigObjFile(cls.ansible_config_file)\n\treturn cls.ansible_config_manager\n@__inject__\n@__instance__\n#@__permanent__\n#@__class__\nclass ansible_run(Base):\n\t\"\"\"\n\tThe ansible run helper class.\n\t\t\"\"\"\n\t__instance_name__ = \"AnsibleRun\"\n\t@classmethod\n\t"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn ConfigManager(cls._path(\"ansible.cfg\"))\n\n\n@Memoized\ndef path(key):\n\t\"\"\"\n\tGets the path to a specific file.\n\t:type key: str\n\t:rtype: str\n\t\"\"\"\n\treturn os.path.join(ansible_config_manager().config['path'], *key.split('/'))\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tif not AnsibleConfigManager:\n\t\tAnsibleConfigManager = cls()\n\n\treturn AnsibleConfigManager\n\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tcfg_manager = cls.ansible_config.config_manager\n\treturn cfg_manager\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\tpass\n\n\n@singleton(cls.__name__)\nclass config(tq_config.TqConfig):\n\t\"\"\"\n\tSets up and configures the ``tq_config`` object.\n\n\tThe properties are defined by the tq_config.TqConfig metaclass, which\n\tmakes it easy to extend and override as needed. To customize the\n\tTqConfig object, simply define an object of that metaclass with the\n\tsame name as the module being loaded.\n\n\tAny functions or class methods of the ``tq_config`` object\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn get_service_manager(cls.__module__, \"ansible\", \"ansible_config_manager\")\n\n\nclass HostManager(Manager):\n\t\"\"\"\n\tGets the services manager.\n\tThis is a special service manager which manages the ansible hosts.\n\t\"\"\"\n\n\t_name = \"ansible_host_manager\"\n\t_dependency = [\"ansible_config_manager\"]\n\n\tdef setup(self):\n\t\t\"\"\"\n\t\tSets up the manager. This involves importing the hosts as a list so we can\n\t\tuse them easily. This also initial"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls.context.ansible_config_manager\n\n@classmethod\ndef ansible_config_editor(cls):\n\t\"\"\"\n\tGets the ansible config editor.\n\t\"\"\"\n\treturn cls.context.ansible_config_editor\n\n@classmethod\ndef ansible_config_edit(cls):\n\t\"\"\"\n\tGets the ansible config edit.\n\t\"\"\"\n\treturn cls.context.ansible_config_edit\n\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls.ansible_config.manager\n"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn cls.lookup_provider('ansible_config_manager')\n\ndef ansible_default(cls):\n\t\"\"\"\n\tIs used by the module.\n\t\"\"\"\n\treturn cls.lookup_provider('ansible_default')\n\ndef zendesk(cls):\n\t\"\"\"\n\tIs used by the module.\n\t\"\"\"\n\treturn cls.lookup_provider('zendesk')\n\ndef gitlab(cls):\n\t\"\"\"\n\tIs used by the module.\n\t\"\"\"\n\treturn cl"}, {"generated_text": "def ansible_config_manager(cls):\n\"\"\"\n\tGets the ansible config manager.\n\t\"\"\"\n\treturn getattr(cls, \"_ansible_config_manager\", None)\n\n\n\tdef ansible_module(cls, **kwargs):\n\t\tansible_dict = dict(kwargs)\n\t\treturn call_method(cls._ansible_config_manager, \"ansible_module\", True, kwargs)\n\n\n\tdef ansible_cmd(cls, **kwargs):\n\t\tansible_dict = dict(kwargs)\n\t\treturn call_method(cls._ansible_config_manager, \"ansible_cmd\", True, k"}]}
{"task_id": "6306091b73426c38ae68acd9", "prompt": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tglobal __workspace_manager\n\tif __workspace_manager is None:\n\t\t__workspace_manager = WorkspaceManager()\n\treturn __workspace_manager\n\ndef active(cls):\n\t\"\"\"\n\tA decorator which ensures the function is invoked on an object of the object type.  \n\tUse this decorator with WorkspaceManager.async_method() in order to ensure that \n\tthe function is invoked only when the given WorkspaceManager is active.\n\t\"\"\"\n\tdef _dec(func):\n\t\tdef wrapper(self, *args, **"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls.__workspace_manager\n\n\ndef workspace_loader(cls):\n    \"\"\"\n    Gets the workspace loader.\n    \"\"\"\n    return cls.__workspace_loader\n\n\ndef __workspace_manager(app, name):\n    \"\"\"\n    Get a new workspace manager.\n    \"\"\"\n    mgr = app.get_service('workspace_manager')\n    key = app.config['workspace_loader']._workspace_cache_key_by_name(name)\n    module, attr = app.config['workspace"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tif cls._workspace_manager:\n\t\treturn cls._workspace_manager\n\n\t__name__ = \"workspace_manager\"\n\tif \".\" not in __name__:\n\t\t__name__ = \".\".join((__name__,))\n\tif cls not in locals():\n\t\tlocals()[cls._swcachedir] = cls\n\tif cls not in locals():\n\t\traise NameError('Workspace Manager \"%s\" not found.' % cls.__name__)\n\treturn locals()[cls._name]()"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn 'workspace_manager'\n\n\ndef is_workspace_mgr(instance):\n\t\"\"\"\n\tGets true if instance is a workspace manager.\n\t\"\"\"\n\treturn 'workspace_manager' in dir(instance)\n\n\ndef workspace_manager_manager(cls):\n\t\"\"\"\n\tGets the workspace manager manager.\n\t\"\"\"\n\treturn 'workspace_manager_manager'\n\n\ndef is_workspace_manager_manager(instance):\n\t\"\"\"\n\tGets true if instance is a workspace manager manager.\n\t\"\"\"\n\treturn 'works"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls.__workspaces_manager\n\n  def __initialize_workspaces(self):\n\t\"\"\"\n\tCreates and initializes the workspace manager.\n\t\"\"\"\n\tws_manager = WorkspaceManager()\n\tws_manager.initialize(app_config)\n\treturn ws_manager\n\n  def _create_workspace(self, ws_name, ws_default_view, ws_default_format, ws_type):\n\t\"\"\"\n\tCreates a workspace.\n\t\"\"\"\n\tworkspace = Workspace()\n\tworkspace"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls._workspace_manager\n\ndef get_main_container_manager():\n    \"\"\"\n    Gets the \"main\" container manager. This may not always be the case since\n    we don't have it instantiated until certain code paths are finished.\n    \"\"\"\n    return workspace_manager().get_container_manager()\n"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn Manager(cls)\n\n\nclass Mapper(object):\n\t\"\"\"\n\tMapper that handles mapping objects to/from a given\n\tDjango-specific model.\n\t\"\"\"\n\tdef __init__(self, model):\n\t\t\"\"\"\n\t\tInitialise a Mapper that uses the given Django model.\n\n\t\t:param model: The Django model.\n\t\t:type: class\n\n\t\t>>> from django.db.models.models import Model\n\t\t>>> instance = Model(id=1, name='Test instance')\n\t\t>>> instance.save()\n\t\t<pym"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\tglobal workspaceManager\n\tif not(workspaceManager):\n\t  #import pudb;pudb.set_trace()\n\t  ws=Workspace()\n\t  workspaceManager=ws.WorkspaceManager(maxWorkers=20)\n\t  return workspaceManager\n\telse:\n\t  return workspaceManager\n\nclass Workspace:\n\t#_singleton = None\n\n\t\"\"\"\n\tWorkspace class.\n\t\n\t\"\"\"\n\t\n\t#_singleton = None\n\n\tdef __iter__(self):\n\t\t#import pudb;p"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls.instance\n"}, {"generated_text": "def workspace_manager(cls):\n\"\"\"\n\tGets the workspace manager.\n\t\"\"\"\n\treturn cls._singleton._manager\n    \n    @classmethod\n    def get_default_workspace(cls):\n        \"\"\"\n            Gets a new managed workspace object.\n            Returns None if no workspace can be \n            found.\n        \"\"\"\n        for workspace_manager in cls.workspace_managers():\n            workspace = workspace_manager.get_default_workspace()\n            if workspace:\n                return workspace\n\n    @classmethod\n    def get_workspace(cls, name):\n        \"\"\"\n            Gets a workspace by a"}]}
{"task_id": "6306091b73426c38ae68acda", "prompt": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls.__mgr.get_plugin_manager(cls, '')\n\n\nif __name__ == '__main__':\n\timport doctest\n\tdoctest.testmod()\n\t\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\t#return gtk.PluginsManager()\n\treturn cls.global_plugins_manager\n\nclass GuiPluginManager(plugins_manager):\n    \"\"\"\n       A plugin manager that manages GUI and non-GUI plugins.\n    \"\"\"\n    def __init__(self, pluginfiles=None, main_window = None):\n        plugins_manager.__init__(self, pluginfiles)\n        self.main_window = main_window\n        self.preferences_manager = gui_preferences.gui_preferences_manager(self)\n        "}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls.registry[\"plugins\"]\n\n\nclass PluginProxy(object):\n\t\"\"\"\n\tAn abstract class that manages the interaction with different plugin types.\n\tIt allows extensions to simply specify plugins that handle the extension type.\n\t\"\"\"\n\n\tplugin_name = None\n\t\n\t@abstractmethod\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tCalled by the core at initialisation.\n\t\tPlugin managers must not hold any other objects.\n\t\t\"\"\"\n\t\t#self.plugins = []\n\t\tself.plugins = None\n\t\t\n\tdef load_config("}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tif not hasattr(cls, '_plugin_manager'):\n\t\tcls._plugin_manager = plugins_manager.PluginsManager(cls.Plugins, cls._plugin_options)\n\treturn cls._plugin_manager\n\n\nclass PluginsManager (object):\n\t\"\"\"\n\tManages the plugin.\n\t\"\"\"\n\tplugins = None\n\tplugins_options = None\n\tplugin_options = None\n\tplugins_order = None\n\tplugin_order = None\n\n\tdef __init__(self, plugins_list=None, plugin_options_"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tif not cls._instance:\n\t\tcls._instance = PluginManager()\n\n\t\t\"\"\"\n\t\tInitializes the plugin manager.\n\t\t\"\"\"\n\t\tcls._instance.initialize()\n\n\treturn cls._instance\n\n\nclass pluginmanager(object):\n\t\"\"\"\n\tManages the plugins of FME.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitializes the plugin manager.\n\t\t\"\"\"\n\t\tself._plugins = PluginManager()\n\n\tdef addplugin(self, name, factory, category=\"__default\", autorun=False):\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls.__plugins_manager\nplugin_manager = cls().get_plugin_manager()\n\ndef plugins_manager_get_plugin(name):\n\treturn plugin_manager.get_plugin_by_name(name)\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tif cls._instance is not None:\n\t\treturn cls._instance.__plugin\n\n\treturn cls._manager\n\n\t# cls._instance = None\n\t# cls._manager = PluginsManager()\n\t# cls._instance = cls\n"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tplugins_dict = {}\n\tfor filename in cls.plugin_file_list():\n\t\twith open(filename) as f:\n\t\t\tplugins = f.readlines()\n\n\t\tfor plugin in plugins:\n\t\t\tplugin_name, plugin_args = plugin.strip().split(' ', 1)\n\t\t\tplugins_dict[plugin_name] = plugin_name, plugin_args\n\tprint '# loaded'+ str(len(plugins_dict)) +'plugins'\n\treturn plugins_dict\n\t\t\nclass PluginManager:\n\t\"\"\"\n\t\"\"\"\n\tplugin_dict = {}"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\treturn cls.plugins_managers[\"default\"]\n\n@Plugin(\"system\")\ndef plugins_manager(cls):\n    \"\"\"\n    \tGets the plugin manager.\n    \tThis method is designed so that system plugins can register plugins globally.\n    \t\"\"\"\n    return cls.plugins_managers[\"system\"]\n\n@Plugin(\"system\")\ndef plugins_manager(cls):\n    \"\"\"\n    \tGets the plugin manager.\n    \tThis method is designed so that system plugins can register plugins globally.\n    \t\"\"\"\n    return cls.plugins_"}, {"generated_text": "def plugins_manager(cls):\n\"\"\"\n\tGets the plugin manager.\n\t\"\"\"\n\tif not cls.instance:\n\t\tcls.instance = PluginsManager()\n\treturn cls.instance\n\n# vim: tabstop=8 expandtab shiftwidth=4 softtabstop=4\n"}]}
{"task_id": "6306091c73426c38ae68acdc", "prompt": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif not spec_content and is_exists(cls.PATH_SPEC):\n\t\t# spec_content not provided\n\t\topen(cls.PATH_SPEC).close()\n\t\t# check if existing in case of existing\n\t\t# TODO: remove this exception to avoid confusion \n\t\traise(Exception('Spec does not exists.'))\n\ttry: \n\t\twith open(cls.PATH_SPEC, 'r') as reader:\n\t\t\tyaml.load(reader)\t\n\texcept Exception as exc:\t\n\t\t# file does not exist or was not created with y"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif not spec_content:\n\t\tspec_content = cls.load_spec(cls.spec_filename)\n\tvalid = True\n\treturn_data = dict()\n\t# parse the specification into a dictionary\n\ttry:\n\t\treturn_data = common.yaml_to_dict(spec_content)\n\texcept yaml.YAMLError, exc:\n\t\t# try to find a reasonable filename in case of errors\n\t\tif 'while mapping a mapping' in text_type(exc):\n\t\t\traise exc\n\t\t# a string containing the yaml"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\t\n\tdef validate_key_present(key):\n\t\t\"\"\"\n\t\tCheck if key is present in spec\n\t\t\n\t\t:param key: key string\n\t\t:raise IRValidatorException: when key is not present in spec\n\t\t:return: spec content\n\t\t\"\"\"\n\t\t\n\t\tif not key in spec_content and len(key.split(\".\")) > 1 and not \".\".join(key.split(\".\")[:-1]) in spec_content:\n\t\t\traise IRValidatorException(\"Missing key(s): %s\" % key)\n\t\treturn spec_content\n"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tfrom vistrails.core import debug\n\tassert spec_content is not None, \"must be given\"\n\tdata = {}\n\ttry:\n\t\tyaml.safe_load(spec_content)\n\texcept Exception, e:\n\t\t# no exception expected for validation, only exception thrown\n\t\traise ValueError(str(e))\n\tassert spec_content is not None, \"must be given\"\n\treturn data\n\t\nclass ValidationError(Exception): pass\nclass ValidationWarning(Exception): pass\n\n\n##############################################################################\n# Validator base class that"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif spec_content is None:\n\t\tspec_content = load_spec_from_files()\n\trequired_fields = cls._required_fields()\n\n\terror = []\n\tfor field, value in required_fields.items():\n\t\tif not field in spec_content:\n\t\t\terror.append(\"missing %s in definition.yml\" % field)\n\n\tmsg = \"\\n\".join(error)\n\tif error:\n\t\traise IRValidatorException(msg, error)\n\t\t\n\n\treturn spec_content\n\t\t\n\t\t\n\t\t\n\ndef validate_from"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_content = spec_content or get_from_content(cls.class_name().upper(), cls.CLASS, cls.SPEC_FILE)\n\tvalidated = cls(spec_content)\n\terrors = {\n\t\t\t\"spec\": validated.validate,\n\t}.get(\n\t\t\tvalidated.validate(),\n\t\t\t{\n\t\t\t\t\"error\": \"No data loaded.\",\n\t\t\t}\n\t)\n\tif any(errors):\n\t\traise ValidationException(\n\t\t\tvalidated.validate(),\n\t\t\t'{} {}'.format(validated."}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\treturn cls.validate_from_file(cls.get_validation_file_fullpath(spec_content))\n\n@classmethod\ndef validate_from_file(cls, spec_file_fullpath=None):\n\t\"\"\"\n\tvalidates that any YAML specifications file (see 'validate_from_content')\n\thas all required fields\n\t\n\t:param spec_file_fullpath: absolute path of a spec (YAML) file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tfrom cps2.helpers.specfiles import validate_from_file\n\tvalidate_from_file(cls, spec_content)\n\nvalidate_from_content.__dict__.setdefault(\"version\", \"0.1\") # TODO remove when 1.0 released\nvalidate_from_content.__doc__ = validate_from_content.__doc__.format(version=validate_from_content.version)"}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif not spec_content:\n\t\traise Exception(\"spec can not be empty\")\n\t\n\t#load the spec_data\n\tcls.spec_data = cls.parse_spec_file(spec_content)\n\t# check for required fields\n\tif \"repositories\" not in cls.spec_data:\n\t\traise IRValidatorException(\"no repositores specified\")\n\tif \"files\" not in cls.spec_data:\n\t\traise IRValidatorException(\"no files specified\")\n\tif not cls.spec_data.get(\""}, {"generated_text": "def validate_from_content(cls, spec_content=None):\n\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\t\n\t:param spec_content: content of spec file\n\t:raise IRValidatorException: when mandatory data\n\tis missing in spec file\n\t:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tif spec_content:\n\t\tyaml_data = yaml.load(spec_content)\n\tyaml_validator = Validator()\n\tresult = yaml_validator.validate(yaml_data, cls.REQUIRED_KEYS)\n\tif len(result):\n\t\traise IRValidatorException(\"Not all of specified values are defined\", result)\n\n\treturn yaml_data\n\n\ndef load_from_files(path_spec_file=None, path_meta_file=None, path_report_file=None):\n\t\"\"\""}]}
{"task_id": "6306091c73426c38ae68acdd", "prompt": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tfrom validations_utils import validate_yaml_file\n\n\tif not yaml_file:\n\t\traise IRValidatorException(\"Specified yaml file was not passed. Please check usage. (Error:1)\")\n\n\treturn validate_yaml_file(yaml_file)\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t# Load YAML file, and check fields\n\ttry:\n\t\tcontent = yaml.load(yaml_file)\n\texcept yaml.YAMLError as exc:\n\t\tmsg = \"Cannot parse YAML file {!r}: {}\".format(yaml_file, exc)\n\t\traise Exception(msg)\n\t\n\t# Check for required fields\n\tfor name, attr in cls.__attrs__.items():\n\t\tif not content.get(name):\n\t\t\traise cls.IRMissingAttributeException()\n\t\n\t"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif yaml_file is None: \n\t\t_, ext = os.path.splitext(yaml_file)\n\t\tif ext!= '.yml':\n\t\t\tmsg = _(\"YAML file must have '.yml' extension.\")\n\t\t\traise IRValidatorException(msg)\n\t\n\ttry:\n\t\tdata = yaml.load(open(yaml_file, \"r\"))\n\t\treturn cls.validate(data)\n\texcept yaml.YAMLError as ex:\n\t\tif ex.context is None:\n\t\t\tmsg ="}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\twith open(yaml_file, \"r\") as f:\n\t\tif f.readline().strip().startswith('!yaml:'):\n\t\t\tyaml_file = f.readline().split(\"\\n\")[0].split(\"\\t\")[1].split(\":\")[0] + \".yml\"\n\t\t\tf.close()\n\t\t\n\t\treturn cls.load_from_file(yaml_file)\n\t\n\t\n\t\nclass IRValidatorException(BaseException):\n\tdef __init__(self, value):\n\t\t"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif yaml_file is None:\n\t\t_module_logger.error('yaml_file not given - validation skipped')\n\t\treturn None\n\t\n\t_module_logger.debug(_('validating \\'%s\\'') % yaml_file)\n\t\n\tdata = dict()\n\t\n\terr_msg = _('validator cannot load %s') % yaml_file\n\t\n\ttry:\n\t\twith open(yaml_file) as yaml_f:\n\t\t\tdata = yaml.load(yaml_f)\n\t\t\t\n\t\t"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\twith open(yaml_file) as stream:\n\t\treturn cls.from_stream(\n\t\t\tcls.load(stream)\n\t\t).validate_dict()\n\t\t\n\t\nclass IRValidatorException(Exception):\n\t\"\"\"Exception to raise when validation fails\"\"\"\n\tpass\n\t\n\t\nclass IRValidator(object):\n\t\"\"\"\n\tValidator and helper methods to verify and format IRI data\n\t\n\tA validator is able to check whether an IRI is valid\n\tand transform it into it's internal representation\n\t\n\t:iv"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tif yaml_file is None:\n\t\t\tyaml_file = os.path.join(\n\t\t\t\tos.path.dirname(\n\t\t\t\t\tos.path.abspath(\n\t\t\t\t\t__file__)),\n\t\t\t\t\"test_config\")\n\n\ttry:\n\t\tresult = yaml.load(\n\t\t\topen(yaml_file))\n\n\texcept yaml.YAMLError as e:\n\t\tprint e\n\t\traise e\n\n\tif not isinstance(result, dict):\n\t\traise IRValidatorException(\"YAML file is not a valid YAML dictionary"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\td = {}\n\tif not yaml_file or not os.path.isfile(yaml_file):\n\t\traise Exception('Invalid YAML file')\n\twith open(yaml_file, 'r') as f:\n\t\td = yaml.safe_load(f)\n\ttry:\n\t\td['id']\n\texcept KeyError:\n\t\tmessage =  \"Missing mandatory key from metadata: {0}\".format('id')\n\t\traise IRValidatorException(message)\n\ttry:\n\t\td['description']\n\texcept KeyError:\n"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tyaml_values = None\n\twith open(yaml_file) as yml_file:\n\t\tyaml_values = yaml.load(yml_file)\n\t\tfor key in yml_values:\n\t\t\tif key not in cls.required_fields:\n\t\t\t\tlogger.error(\"Missing field in {}. Required key: {} : {}\".format(yaml_file, key, cls.required_fields[key]))\n\t\t\t\traise IRValidatorException(cls.__name__)\n\treturn yml_values\n\t\nclass IRValid"}, {"generated_text": "def validate_from_file(cls, yaml_file=None):\n\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\t\n\t:param yaml_file: Path to YAML file\n\t:raise IRValidatorException: when mandatory data is missing in file\n\t:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\treturn cls(yaml_file=yaml_file).run()\n\t\n\t\ndef add_to_dict_from_yaml(data_dict, parent_dict, yaml_file=None):\n\t\"\"\"\n\tAdds data loaded from a YAML file - see :func:`IRDictValidation.validate_from_file` \n\t:param parent_dict: Parent dictionary\n\t:param data_dict: Dictionary with data loaded from a YAML file\n\t\n\t:raise KeyError: When a parent key isn't present"}]}
{"task_id": "6306091d73426c38ae68ace5", "prompt": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tself['include_groups'] = {}\n\n\tfor spec, val in parser_dict.iteritems():\n\t\tif spec not in self:\n\t\t\tself[spec] = {}\n\t\tif type(val) is list:\n\t\t\tfor val in val:\n\t\t\t\tself._include_groups(val)\n\t\telse:\n\t\t\tself._include_groups(val)\n\n\tparse_log.debug('Included groups: {}'.format(self['include_groups']))\n\n\treturn self\n\n"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tspecfile_list = []\n\ttry:\n\t\tinclude_group = Find(included.name.ljust(7) + \" \" * 64)\n\t\tfor (inc_file, included_text, included_dict) in \\\n\t\t\tincluded.include_group_as_list(include_group):\n\n\t\t\tif any(\n\t\t\t\t\t# FIXME use.file_type instead of.kind\n\t\t\t\t\tinclude_kind.lower() in [\"class\", \"enum\"]\n\t\t\t\t\tfor include_kind in included_dict.get(\"__kinds\", [])\n\t\t\t):\n\n\t\t\t\tif"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tself.included_specs = {}\n\t\n\tif \"*\" in parser_dict:\n\t\tself.included_specs = dict.fromkeys(parser_dict.iterkeys(), True)\n\t\t\n"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor group in self._get_groups(parser_dict):\n\t\tself.includes.append(group)\n\nclass Group(object):\n\t\"\"\"\n\tRepresents a group in the spec files.\n\tInherits from object so the Group can be used as a dictionary key.\n\t\"\"\"\n\tdef __init__(self, group, parent_group=None):\n\t\tself.group = str(group).split('/')\n\n\t\tif parent_group is None:\n\t\t\tself.parent_group = None\n\t\telse:\n\t\t\tself.parent_group = parent_"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t# For each entry with an include_groups specification, the corresponding value is a list. For each one,\n\t# the list elements are the group names to be considered.\n\tfor group_name in parser_dict.get('include_groups',{}).keys():\n\t\t# We don't check for group_name being an absolute group name because the only one that currently exists\n\t\t# is group.group_included_by_default. This will be deleted soon, see group.py\n\t\t# This will fail if it is a relative group name...\n\t\tif group_name in parser"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t# In all the specs that require a include groups definition these are all the\n\t# groups that need to be added to the parser first or the other way around.\n\t#\n\t# e.g. if the spec is\n\t#  included_files[i]\n\t#       include_files[j]\n\t#       include_files[k]\n\t# then\n\t#  groups[u]\n\t#       is_include_files[u]\n\t#       is_included_files[u]\n\t# is set to True so it's included files"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif \"groups\" in parser_dict:\n\t\tfor group in parser_dict[\"groups\"]:\n\t\t\tparser_dict['groups'][group] = dict(parser_dict['groups'][group])\n\t\tdel parser_dict['groups']\n\n\ndef _include(self, target_file):\n\t\"\"\" Parses the file and includes the content.\n\n\tExpects a parsed dictionary and an included file path.\n\t\"\"\"\n\tincluded_file = self.manager.include(target_file)\n\tself.manager.merge(self.parsed, included_file"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfor name, group_dict in parser_dict.items():\n\t\tif \"include\" in group_dict:\n\t\t\t# print(\"Resolving '%s'\" % \",\".join(group_dict[\"include\"]))\n\t\t\tfor file_type, pkg_specs in group_dict[\"include\"].items():\n\t\t\t\tspecs = self.read_specs(file_type, pkg_specs,\n\t\t\t\t\t\t\t\t\t\tgroup_dict[\"name\"], 0)\n\t\t\t\tsys.stdout.write(\"%s: %s\" % (name, \",\".join(spe"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t# The parser dict includes a set of group names to include.\n\n\t# If the include statement comes from a different group, only add the\n\t# groups this parser can resolve.\n\t# This allows one spec file or any number of included files to specify\n\t# which group to include and then exclude other groups.\n\n\tfor group in parser_dict['groups'].copy():\n\t\tif group in self._all_parser_groups:\n\t\t\tself.include_group.add(group)\n\n\tfor key, values in parser_dict.copy().items():\n\t\tif isinstance(values"}, {"generated_text": "def _include_groups(self, parser_dict):\n\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tif hasattr(parser_dict, '_include_groups'):\n\t\treturn\n\tparser_dict._include_groups = collections.deque()\n\tfor subname in parser_dict.get('include', '').split():\n\t\tif subname in self._specials:\n\t\t\tif '[' in subname:\n\t\t\t\tself._print(subname, 0)\n\t\t\tself._load_plugin_spec(subname)\n\t\telse:\n\t\t\tself._load_plugin_file(subname)\n\tparser_dict._include_groups.clear()\n\n\tif has"}]}
{"task_id": "6306092373426c38ae68acfa", "prompt": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tret = {}\n\tspec = vars(self)\n\tfor key, value in dict(spec).items():\n\t\tvalue = value.resolve()\n\t\tif value is not None:\n\t\t\tret[key] = value\n\n\treturn ret\n\n\ndef build(cls):\n\tret = cls.__new__(cls)\n\tret.__init__(**get_spec_defaults(cls))\n\treturn ret\n\n\nclass ComponentInfo(object):\n\t\"\"\"\n\tBase class for components, mixins and providers.\n\t\"\"\"\n\n\n\t"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\treturn dict(\n\n\t\t# Common\n\t\t#:\n\t\tbase_dir = '../build/',\n\t\tbuild_dir = 'build/',\n\t\ttest_dir = 'test/',\n\t\tscript_template ='script',\n\n\t\t# Python\n\t\t#:\n\t\textension = 'py',\n\t\t#compiler = 'python3.4',  # `python3.3`\n\t\tpython_include = '${PYTHON_INCLUDES}',\n\t\tlibraries = '${PYTHON_LIBS}',\n\t\toptions = '${P"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\t# if self._satisfied:\n\t# \tself.add_satisfied_spec()\n\n\t# self.verify_dependencies()\n\n\treturn self\n\n\ndef resolve_all(module, dependencies, args, sources, options):\n\tself = build_module(module, dependencies, args, sources, options)\n\twith trace_time('resolver.resolve_all'):\n\t\treturn self.get_spec_defaults()\n\n\ndef add_satisfied_spec(self):\n\tif not self.satisfied:\n\t\tself."}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tspec_args = self.sources.get_args()\n\t\n\ttry:\n\t\t# If the arguments source exist use it.\n\t\tspec_args.update(self.sources.eval_args())\n\t\t\n\t\tfor key, val in list(defaults.items()):\n\t\t\tif key not in spec_args:\n\t\t\t\tspec_args[key] = defaults[key]\n\t\treturn spec_args\n\texcept Config.ConfigurationError as e:\n\t\tprint('Error in configuration:'+ e.message)\n\t\tsys.exit(1)\n\t\n"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\td = {}\n\tfor key, val in self.kwds.items():\n\t\tdval = val.resolve()\n\t\tif dval is None:\n\t\t\traise exception.ArgDefaultError(arg=key, default=val)\n\t\td[key] = dval\n\treturn d\n\n"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\treturn self.resolve_value(self.parser)\n\n"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\td = self.__get_dict(\"spec\")\n\tfor name in list(self.__get_list(\"args_env\")):\n\t\tv = self.get_environment(name)\n\t\tif v:\n\t\t\td[name] = v\n\tfor k, v in defaults.iteritems():\n\t\tif not k in d:\n\t\t\td[k] = v\n\n\tif self.get_config(\"generate-sources-section\"):\n\t\td[\"sources_section\"] = \"%s section\" % self.conf.name\n\n\treturn d\n\ndef"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tresolved = {}\n\tfor a_str, a_spec, a_default in zip(self.args, self.specs, self.defaults or ()):\n\t\tif a_str not in resolved:\n\t\t\tif isinstance(a_spec, list) and len(a_spec) == 0:\n\t\t\t\tresolved[a_str] = a_default\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tresolved[a_str] = a_spec.eval(self.values)\n\t\t\t\texcept Exception as ex:\n\t\t\t\t\tself.args.append(a_str"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\targuments = dict()\n\n\tif '-c' not in self.env:\n\t\tself.env['-c'] = os.path.abspath(os.path.join(self.out_dir, '.'))\n\n\tfor source in self.get_sources(in_sources=False):\n\t\targuments[source] = self.get_inputs()[source]\n\n\tfor source in self.get_sources(in_sources=True):\n\t\targuments[source] = self.get_outputs()[source]\n\n\treturn"}, {"generated_text": "def get_spec_defaults(self):\n\"\"\"\n\tResolve arguments' values from spec and other sources.\n\t\"\"\"\n\tdefaults = {}\n\n\t#: spec default values (in the order they are given)\n\t#: {name: value}\n\tdefaults.update(self.__dict__['__defaults__'])\n\tif'spec' in self:\n\t\t#: spec argument dictionary\n\t\tdefaults.update(self.spec.arg_dict)\n\telse:\n\t\t#: global dictionary\n\t\tdefaults.update(global_defaults)\n\n\t#: user arguments overrides spec ones\n\t#: {name: value}\n\tif 'user' in self:"}]}
{"task_id": "6306092973426c38ae68ad01", "prompt": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tmyargs = OrderedDict()\n\tfor name in self.options:\n\t\tdefdep = self.opt.get_value_def(name)\n\t\tif defdep:\n\t\t\tmyargs[name] = defdep\n\t\telse:\n\t\t\targs = self.opt.get_value_defaults(name)\n\t\t\tif args:\n\t\t\t\tmyargs[name] = args\n\t\t\telse:\n\t\t\t\tmyargs[name] = ''\n\t# check if there are any deprecated options\n\tdeprecated = False\n\tfor dep in self.deprecated:\n\t\tif dep"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {}  # Not deprecated.\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {}\n\n#\tTODO:\n#\t\tsupport for deprecated args\n#\t\tfind other arguments which deprecated\n#\t\tprovide warnings\n#\t\tremove deprecated args\n\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tfrom qgis.core import QgsWkbTypes\n\tret = {}\n\tret['renderer'] = {'type': {QgsWkbTypes.Point: {1: 'PointSVG',\n\t\t\t\t\t\t\t      3: 'PointSVG',\n\t\t\t\t\t\t\t      4: 'PointDMS',\n\t\t\t\t\t\t\t      5: 'PointDMS'},\n\t\t                      QgsWkbTypes.LineString: {1: 'LineSVG',\n\t\t\t\t\t\t\t\t          3: 'LineSVG',\n\t\t\t\t\t\t\t\t          4: 'LineDMS',\n\t\t\t\t\t\t\t\t          5: 'LineDMS'},"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn {\n\t\t'bbox'\t: '-b',\n\t\t'subset'\t: '-g'\n\t}\n\n\t\n\t\n\t\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tdeprecated_args = {\t'num_nodes': ['num_points_x', 'num_points_y', 'points_x[x]', 'points_y[y]', 'num_points', 'points'],\n\t\t\t\t\t\t\t'minsize_x' : ['min_x','mapping_x','mapping','min_points_x'],\n\t\t\t\t\t\t\t'minSize_y*1': ['min_y','mapping_y'],\n\t\t\t\t\t\t\t'min_x':['min_x','mapping_x','min_points"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\treturn [{\n\t\t\"name\": \"log_level\",\n\t\t\"value\": \"5\"\n\t}, {\n\t\t\"name\": \"enable_remote_logging\",\n\t\t\"value\": \"2\"\n\t}]\n\ndef get_defaults(self):\n\t\"\"\"\n\tReturning dict with default options.\n\t\"\"\"\n\treturn {\n\t\t\"log_level\": 5,\n\t\t\"max_log_size\": 5 * 1024 * 1024,\n\t\t\"enable_remote_logging\": 0,\n\t\t\"enable_remote_logging_ssl\": 0"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\topt = [(o[0], o[1].get(\"opt\", o[1]['default'])) for o in self._all]\n\tres = {}\n\tfor p in OPT_PARAMS.split():\n\t\tif p[0] in opt:\n\t\t\tres[p[0]] = {'name':p[1], 'help':p[2], 'opt' : p[3], 'flag':p[4], 'default':opt[p[0]]['default']}\n\treturn res\n\n\t\nif __name__ == '__main__':\n\n"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\twarnings=[]\n\tdef is_deprecated(v, dep_args, func_name_):\n\t\tif not dep_args.has_key(re.sub('__', '.', v)):\n\t\t\treturn (func_name_, v)\n\t\tif not dep_args[v]==None:\n\t\t\twarnings.append(\"Parameter '\"+re.sub('__', '.', v)+\"' of function '\"+func_name_+\"' is deprecated. Parameter '\"+dep_args[v]+\"' is used.\")\n\t\treturn (func"}, {"generated_text": "def get_deprecated_args(self):\n\"\"\"\n\tReturning dict with options which deprecate others.\n\t\"\"\"\n\tdeprecated_args = {\n\t\t\"name\": [\"name\", \"namespace\", \"names\", \"namespace_name\",\n\t\t\t\t \"namespace_sname\", \"namespace_sname_name\", \"project_id\"],\n\t\t\"version\": [\"version\", \"versions\", \"version_name\"],\n\t\t\"author\": [\"author\", \"authors\", \"author_name\"]\n\t}\n\treturn deprecated_args\n\n\tdef get_args(self, kwargs):\n\t\targs = {}\n\t\t\n\t\tfor opt, val in kwargs.iteritems():\n\t\t\tif"}]}
{"task_id": "6306092c73426c38ae68ad02", "prompt": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_arg_dict = {\n\t\t\"dynamic_load\": \"auto_dynamic_load\",\n\t\t\"data_format\": \"data_format\",\n\t\t\"save_dataset\": \"save_file_path\",\n\t\t\"show_matplotlib\": \"show\",\n\t\t\"show_plot_hist\": \"show_hist\",\n\t\t\"show_plot_hist_cdf\": \"show_hists\",\n\t\t\"test_plot_hist\": \"plot_test\",\n\t\t\"test_plot_plot\": \"plot_test\",\n\t\t\"save"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tfor arg in answer_file_args.keys():\n\t\tif cli_args.get(arg)!= answer_file_args.get(arg):\n\t\t\tdep_arg = arg \n\t\t\tif not cli_args.get(arg):\n\t\t\t\tvalue = answer_file_args.get(arg)\n\t\t\telse:\n\t\t\t\tvalue = cli_args.get(arg)\n\t\t\t\n\t\t\tprint >> sys.stderr, \"WARNING! Argument '\" + str(dep_arg) + \"' is now deprecated. Use argument \" + str(arg) + \" instead"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tvalid_keys = ['deprecated_flag', 'answer_file', 'question_text', 'yes_option', 'no_option', 'choices', 'help_text', 'choices_with_defaults', 'choices_without_defaults']\n\t# valid options and arguments list must have the same order\n\t# so, when reading all keys, make sure it is there\n\tvalid_arg_list = dict()\n\tfor arg in answer_file_args:\n\t\tfor key in arg.keys():\n\t\t\tif key not in valid_keys:\n\t\t\t\traise"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecated_args = [cli_name for cli_name in cli_args if cli_name not in answer_file_args] \n\tif len(deprecated_args) > 0:\n\t\t#print a list of deprecated arguments\n\t\tprint(\"\"\"\n----------------------------- WARNING\nSome deprecated arguments are here: \n%s \n-----------------------------\n\"\"\".strip() % list(zip(deprecated_args, answer_file_args)))\n"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\t\n\tfor k,v in cli_args.iteritems():\n\t\tif isinstance(v, str):\n\t\t\tif is_deprecated_argument(k, v, answer_file_args):\n\t\t\t\tdeprecated_print(v, k, version=answerfile.ANSWER_FILE_VERSION)\n\t\n\t\ndef print_arg_deprecation(out, line, k, v, version=0):\n\t\"\"\"\t\t\n\t\tPrints the message for the deprecated argument on the given line.\n\t\t\n\t\t:param out: the stream object to write to\n"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tfor key in answer_file_args:\n\t\tif key not in cli_args:\n\t\t\tprint \"Argument {0} not available from command line\".format(key)\n\n\n# Parsing the arguments\narg_parser, remaining_args = self._parse_args()\n\nparser.add_argument('--version',\n                   action=\"version\",\n                   version=\"%(prog)s {0}.{1}.{2}\".format(__version__[0],\n                                                         __version__[1],\n                                   "}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif not ansible_utils.validate_answer_file(answer_file_args):\n\t\t\traise SystemExit(\"Error: The parameter `%s` does not appear to be a file\" % answer_file_args['ANSWER_FILE'])\n\tansible_utils.validate_cli_args(cli_args)\n\tansible_utils.validate_play_context_for_legacy_args(ansible_utils.resolve_cli_args(cli_args),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tanswer_file_args)\n\tansible_utils.valid"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tfor argument in (cli_args, answer_file_args):\n\t\tif argument:\n\t\t\tprint_deprecated_args(argument)\n\ndef validate_command_line_arguments(cmdline_args):\n\t\"\"\"\n\tCalls the validation method for each command that has a command-line version\n\t\n\t:param cmdline_args: the list of unparsed and parsed arguments to process  \n\t\"\"\"\n\tfor command in CommandRegistry:\n\t\targument = CommandRegistry[command].validate_arguments(cmdline_args)\n\t\tif argument:"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tmessages = [\n\t\t\"%s was deprecated in %s.\" % (cli_args['--input-data'][0], cli_args['--input-data'][1]),\n\t\t\"%s was deprecated in %s.\" % (cli_args['-w'][0], cli_args['-w'][1]),\n\t\t\"%s was deprecated in %s.\" % (cli_args['-h'][0], cli_args['-h'][1]),\n\t\t\"%s was deprecated in %s.\" % (cli_args['-"}, {"generated_text": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\"\"\"\n\tValidates and prints the deprecated arguments.\n\t\n\t:param cli_args: the dict of arguments from cli\n\t:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tif cli_args is not None:\n\t\tto_delete = []\n\t\t\n\t\tcli_pairs = dict(cli_args)\n\t\tanswer_file_pairs = dict(answer_file_args)\n\t\tintersection_cli_answer = answer_file_pairs.values()\n\n\t\t# add pairs from the CLI to the dict, to remove, then replace\n\t\tkeys_to_remove = []\n\n\t\tfor k,v in cli_args.iteritems():\n\t\t\tif (v in intersection_cli_answer):\n\t\t\t\tanswer_file_pairs["}]}
{"task_id": "6306092d73426c38ae68ad04", "prompt": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t\n\tparser = utils_misc.OmniCommand(ignore_status=True, debug=True)\n\tparser.add_command(command_name)\n\tparser_specs = parser.process_arguments(None)\n\n\topt_keys = list_options(command_name)\n\treturn parser_specs['options']\n\t\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tparser_option_specs = set()\n\tparser_options = self.parser.option_list\n\tfor (opt_spec, opt_flags, _, help_txt, default, _) in parser_options:\n\t\t# Skip the options specified in this class file itself\n\t\tassert opt_spec is not None\n\t\tif str(opt_spec) == command_name:\n\t\t\tcontinue\n\t\t# Exclude any arguments options from the list of the remaining options\n\t\t# except if the -s or --single options was specified.\n\t\tif (help_txt is None and opt"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tif command_name.endswith('main'):\n\t\t\tcommand_name = command_name[:-4]\n\tcommand_name = command_name.replace('hyper-', '')\n\tcommand = self.module.get_command_from_command_name(command_name)\n\t\n\tspecs = command.get_parser_option_specs()\n\tspecs_with_args = {}\n\tfor spec in specs:\n\t\tname = spec['name'].replace('-', '_')\n\t\tspecs_with_args[name] = spec"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tcmd_name = command_name.title() + \"_OPTIONS\"\n\treturn self._parser.get_option_specs()\n\t\n# ==============================================================================\n\n\n# ==============================================================================\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n\n# ==============================================================================\n\n\n"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\t# We need to get optparser by command name\n\ttry:\n\t\tcmd_dict = self.parser\n\texcept AttributeError:\n\t\treturn {}\n\n\tparser_specs = {}\n\tfor options, command in cmd_dict.items():\n\t\tif command!= command_name:\n\t\t\tcontinue\n\t\t\n\t\t# Retrieve the OptionParser as well, if it exists\n\t\tif \"subopts\" in options:\n\t\t\tparser_specs['subopts'] = options[\"subopts\"]\n\t\telse:\n\t\t\tbreak\n\t\t\n\treturn parser_specs"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\toption_specs = []\n\tfrom distutils.spawn import find_executable\n\t\n\t# find full path of command\n\tcommand = None\n\tcmd = None\n\tpath = ''\n\tfor c in os.name, os.sys, os.path:\n\t\tfor s in c:\n\t\t\tcmd = find_executable('%s' % comm, path, c, s)\n\t\t\tif cmd:\n\t\t\t\tpath = cmd[:]\n\t\t\t\tbreak\n\t\n\tif cmd:\n\t\tcmd = '%s -help %s' % (cmd, command_"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tparser_option_specs = self.parser.get_option_specs()\n\tfor attr in dir(self):\n\t\tobj = getattr(self, attr)\n\t\tif hasattr(obj, \"command_name\") and obj.command_name == command_name:\n\t\t\tparser_option_specs.append(attr)\n\t\t\t\t\n\treturn parser_option_specs\n\ndef get_parser_opt_from_opt_name(parser_option_specs, opt_name):\n\t\"\"\"\n\tGets the index and value of the parser"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\treturn [\n\t\t(option, { \"default\": default })\n\t\tfor option,default in self._option_specs.iteritems()\n\t\tif command_name[0:len(option)] == option\n\t]\n\nclass SimpleOptionList(OptionParser):\n\tdef _parse_option(self, command_name, option_name, option_value):\n\t\tif not option_value:\n\t\t\treturn \"\"\n\t\t# XXX: make it dynamic.\n\t\tif option_name == \"command\":\n\t\t\treturn command_name\n\n\t\t# XXX: add option values type validation in"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\toption_specs = []\n\t\n\tfor command in self.commands.keys():\n\t\tif command_name!= command:\n\t\t\tcontinue\n\t\t\n\t\tfor option in self.commands[command]['options']:\n\t\t\toption_specs.append(option)\n\n\treturn option_specs\n\t\ndef get_parser_option_specs_for_command(command, command_name='main'):\n\t\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command: the command name (main, virsh, ospd"}, {"generated_text": "def get_parser_option_specs(self, command_name):\n\"\"\"\n\tGets all the options for the specified command\n\t\n\t:param command_name: the command name (main, virsh, ospd, etc...)\n\t:return: the list of all command options\n\t\"\"\"\n\tcmd_group = get_command_group(command_name)\n\tif cmd_group is not None:\n\t\treturn cmd_group.optional_options\n\telse:\n\t\treturn []\n\n\n"}]}
{"task_id": "6306092d73426c38ae68ad05", "prompt": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t# This is just for backward compatibility with the default function.\n\treturn {'no_option_spec' : []}\n\n\ndef get_options(self):\n\t\"\"\"\n\tGets a list of options which may affect behaviour by the\n\tcommands, or None if there are no such options.\n\t\"\"\"\n\t# It could be that this command should return no options,\n\t# but we have to treat the function as though it were set\n\t# in the `Option` object anyway.\n\treturn [ Option('', '--no-options', 'Suppress all options,"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tif hasattr(self.options_specs, \"%s_OPTIONS_SPECIFICATIONS\" % command_name): \n\t\tfor option_spec in self.options_specs % {\"command_name\": command_name}:\n\t\t\tif option_spec % {\"argument_name\": argument_name}!= 0:\n\t\t\t\treturn option_spec\n\treturn None\n\t\n\t\n\t\n\t\n\t\n\t\n\t"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self.get_options(command_name).options.get((argument_name, None))\n\t\n\t\ndef parse_options(opts, context, error_function): \n\t\"\"\"\n\tParses a set of options and then calls various functions to\n\tprocess the values and callbacks specified by the user.\n\t\"\"\"\n\tparser = self._make_option_parser(context, opts)\n\toptions = getattr(context, 'options', {})\n\n\t# do we have a command name specified?\n\tcommand = context.command\n\tcommand_"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\toption_spec_map = {}\n\toption_spec_map[command_name] = {}\n\toption_spec_map[command_name][argument_name] = {\n\t\t\"type\": \"bool\",\n\t\t\"required\": True,\n\t\t\"default\": False\n\t}\n\treturn option_spec_map\n\ndef get_default(self, command_name, argument_name):\n\tcommand = self.get_dict(command_name)[\"command\"]\n\targument_name_list = command[\"arguments\"][argument_name][\"optional_defaults"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\ttry:\n\t\treturn self.options[(command_name, argument_name)]\n\texcept KeyError:\n\t\traise CommandNotFoundError(\"You can not get option spec for command %s argument %s\" % (command_name, argument_name))\n\n\ndef get_arguments(command_name):\n\t\"\"\"\n\tGets the arguments of the specified command.\n\t\"\"\"\n\ttry:\n\t\treturn self.arguments[command_name]\n\texcept KeyError:\n\t\traise CommandNotFoundError(\"Command %s not found!\" % (command_name))\n\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn self.option_spec[argument_name].value\n\n\nclass TextWidget(AbstractWidget):\n\t\"\"\"\n\tText widget.\n\t\"\"\"\n\tdef set_string(self, value):\n\t\t\"\"\"\n\t\tParses the input value and sets the text widget to the specified value.\n\t\t\"\"\"\n\t\tpass\n\n\nclass FileTreeWidget(AbstractWidget):\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(FileTreeWidget,self).__init__(*args, **kwargs)\n\t\tself.__data = None\n\t\t\"\"\"\n"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\t_ = command_name\n\t_ = argument_name\n\tfor option in self.options:\n\t\tif option['name'] == argument_name:\n\t\t\treturn option\n\n\treturn None"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\toptions = OptionSpec()\n\toptions.new(argument_name, command_name, required=True, help=_help_for(command_name))\n\treturn options\n\ndef get_option_parser(command_name, parser_name=\"Command\", option_spec=None):\n\t\"\"\"\n\tReturns a OptionParser instance for the command. Option can be\n\tspecified using the command line (the command_name) and its\n\targument (a tuple of the command with command arguments). The\n\tparser_name is the name that the command will appear is returned\n\tby shell"}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\tspec = dict(type = \"string\", label = gajim.nicks[self.account],\n\t\t\texample = \"%s value\" % argument_name, help = \"help\")\n\treturn spec\n\ndef create_listener(account):\n\t\"\"\"\n\tCreates a listener on the specified account.\n\t\"\"\"\n\tdef __run(conn, event):\n\t\tif event.type() == _core_events.EventType.CONNECTION_REMOVED:\n\t\t\tself._log(\"Received event of type connection_removed\")\n\t\t\tself.disconnect("}, {"generated_text": "def get_option_spec(self, command_name, argument_name):\n\"\"\"\n\tGets the specification for the specified option name.\n\t\"\"\"\n\treturn OptionSpecification.from_command_line_option(self._parser,\n\t\tcommand_name, argument_name)\n"}]}
{"task_id": "6306092d73426c38ae68ad06", "prompt": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tresult = []\n\n\tfor arg in self._arguments:\n\t\tif '-' in arg and '=' in arg and len(arg) > 2:\n\t\t\tname = arg.replace(' - ', '')\n\t\t\tvalue = ''\n\t\t\ttry:\n\t\t\t\tvalue = args[index(arg)]\n\t\t\texcept IndexError:\n\t\t\t\tpass\n\n\t\t\tif len(value):\n\t\t\t\tresult.append(name)\n\n\treturn result\n"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tname_dict = dict()\n\n\tfor n, v in list(six.iteritems(vars(args)|{k: vals(args)[k] for k, v in list(six.iteritems(self.silent_arguments))})):\n\t\tif v and (type(v)!= list) and (type(v) not in six.string_types):\n\t\t\tname_dict[n] = v\n\t\tif v and ((type(v) in six.string_types) and (v not in list(self.silent_arguments))):\n"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\treturn [x for x in reversed(args) if not x in self.options]\n\t        \nclass SilentCommandCaller(SilentCommandBase):\n\t\t    \n\tdef __init__(self, name, target, cmd, silent_args, *args, **kwargs):\n\t\tself.name = name\n\t\tself.target = target\n\t\tself.cmd = cmd\n\t\tself.silent_args = silent_args\n\t\tself.args = list(args)\n\t\tself.kwargs = kwargs\n\t\tself._check_cmd_args()\n\t\t\n"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tslist = []\n\tfor arg in args:\n\t\t\tif arg not in self.args.argv:\n\t\t\t\tslist.append(arg)\n\treturn slist\n\n\n"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tif args.silenced:\n\t\ts_map = [arg.split(\"=\")[0] for arg in args.silenced]\n\t\ts_list = []\n\t\tfor item in s_map:\n\t\t\tif item in self.definitions:\n\t\t\t\titem = self.definitions[item]\n\t\t\t\ts_list.append(item.name)\n\t\treturn s_list\n\telse:\n\t\treturn []\n\n\nclass ArgumentError(RuntimeError):\n\t\"\"\"Argument error\"\"\"\n\tpass\n\nclass ArgumentTypeError(RuntimeError):\n\t\"\"\"Type error in an"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilenced = []\n\tif len(args) == 0:\n\t\tpass\n\t\n\telif len(args) == 1:\n\t\tsilenced = [\"--type\"]\n\t\tpass\n\t\n\telif len(args) == 2:\n\t\tsilenced = [\"--type\", \"--name\"]\n\t\tpass\n\t\n\telif len(args) == 3:\n\t\tsilenced = [\"--type\", \"--name\", \"--action\", \"--user\"]\n\t\tpass\n\t\n\telif len(args) >= 4:\n\t\tsilenced = list("}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tresult = list(set(list(filter(lambda x: not args.has_key(x), args.keys()))))\n\treturn result\n\t\n\t\ndef get_full_args(self, command, args):\n\t\"\"\"\n\tList the expected arguments for the provided command.\n\t\n\t:param command: The current command string.\n\t:param args: The actual arguments.\n\t:return: list of the expected arguments\n\t\"\"\"\n\tresult = list(command)\n\tif self.silenced_args:\n\t\tresult += [''.join(x) for"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tsilent_args = []\n\tslisted = set(args)\n\tfor arg in args:\n\t\tif arg == \"--hide\":\n\t\t\tsilent_args.append(arg)\n\t\telse:\n\t\t\tslisted.add(arg)\n\treturn silent_args\n\n\ndef get_quiet_args(args):\n\t\"\"\"\n\tlist of silenced argument\n\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tquiet_args = []\n\tslisted = set(args)\n\tfor arg in args:"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\targs = {}\n\t# args['input_directory'] = self.__input_directory\n\t# args['output_directory'] = self.__output_directory\n\targs['simulation'] = self.__simulation\n\t\n\treturn args\n\n\n\t# ===================================================================\n\t# private methods\n\t# ===================================================================\n\tdef _do_init(self):\n\t\tLogger.root.info(\"\\nStarting simulation %s...\" % self.__input_name)\n\t\tself.__simulation = 'r'\n\t\tLogger.root.info(self.__sim"}, {"generated_text": "def get_silent_args(self, args):\n\"\"\"\n\tlist of silenced argument\n\t\n\t:param args: The received arguments.\n\t:return: list, slienced argument names\n\t\"\"\"\n\tnew_args = []\n\tfor arg in args:\n\t\tif arg.lower() in SILENT_ARGS:\n\t\t\tcontinue\n\t\tnew_args.append(arg)\n\treturn new_args\n\n\ndef get_log_args(op, args):\n\t\"\"\"Return the arguments to log.\n\t:param op: The operator to log.\n\t:param args: The received arguments.\n\t:return: A list of strings, each entry representing a parameter.\n\t\"\"\"\n\t\n\tif op == 'execute':\n\t\taction = u\"Execute\"\n\t"}]}
{"task_id": "6306092d73426c38ae68ad07", "prompt": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif len(args)< self.min_args:\n\t\t\traise ValueError(\"The minimum number of arguments to \"+self.commandline+\" is \"+str(self.min_args))\n\tif len(args) > self.max_args:\n\t\t\traise ValueError(\"The maximum number of arguments to \"+self.commandline+\" is \"+str(self.max_args))\n\t\t\nclass Option:\n\t\"\"\"\n\tOption class, used for storing information about command line options.\n\n\t:param name: Option name, e.g. 'user' for the -u or"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif len(args) < 5:\n\t\traise CommandError('Missing required arguments for %s.Usage: %s' % (self.__class__.__name__, self.parser.format_usage()))\n\n\tfor param in self.args.values():\n\t\tif param.required:\n\t\t\tif not param.value:\n\t\t\t\traise CommandError('Required argument missing: %s.Usage: %s.%s' % (param.name, self.__class__.__name__, self.parser.format_usage()))\n"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tpass\n\n\n\t\nclass IValidator(Interface):\n\t\"\"\"\n\tInterface for validators. Every validator needs a validate_* method.\n\tAll validation messages are sent to the validator's message queue.\n\t\"\"\"\n\t\n\t\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tConstructor.\n\t\t\"\"\"\n\t\tself.validation_messages = []\n\t\n\t\n\tdef validate(self, *args):\n\t\t\"\"\"\n\t\tValidate a given args tuple\n\t\t\"\"\"\n\t\tself.validation_messages = []\n\t\tself."}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\treturn not any(arg is None for arg in args)\n\n\nif __name__ == '__main__':\n\timport doctest\n\tdoctest.testmod()\n"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tmissing = []\n\n\tfor required_argument, arg_name in self._requires.items():\n\t\tif isinstance(arg_name, list):\n\t\t\t# the arg_name is a list - check every value in the list is present here\n\t\t\tfor required in required_argument:\n\t\t\t\tif required not in args:\n\t\t\t\t\tif required in self._flags:  # if the required argument is flagged,\n\t\t\t\t\t\t# then the user might be asking for information they do not need.\n\t\t\t\t\t\t# we do not warn the user.\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\tmissing += [arg_name]\n\n\t\t\t\t#"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\t# If one of the required arguments is missing, the error thrown\n\t# contains the name of the missing argument.\n\tif args!= []:\n\t\tdelimiter =''\n\t\targ_error = list()\n\t\tfor arg in args:\n\t\t\targ_error.append(\n\t\t\t\t'--' + arg + delimiter + self.missing_args.get(arg, ''))\n\t\tif arg_error!= []:\n\t\t\targ_error = delimiter + 'Errors:' + delimiter + delimiter + '\\n' + \\\n\t\t\t\t' '.join(arg_"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tparams = [('user_id', int)]\n\tself.validate_params(args, params)\n\treturn args\n\n#@auth.requires_permission('admin')\n#def delete_user(args):\n\t#\"\"\"\n\t\t#\"\"\"\n\t#user = args[\"user\"]\n\t#self.delete_user(user)\n\t#self.flash(\"User '\"+user.fullname+\"' has been deleted.\")\n\t#redirect_to(\"admin\")\n\n\t## Redirect user to login page\n\t#redirect_to(\"/admin\")"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif len(self.options) - len(self.args) < self.nargs:\n\t\t\traise ValueError(\"Missing %s arguments\" % (self.nargs-len(self.args)))"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\timport sys\n\timport os\n\tf = open(os.path.join(os.getcwd(), os.path.dirname(__file__)), \"r\")\n\tfound_required_args = False\n\tfor line in f:\n\t\tif re.search(\"\\[[lL]u\\]\", line):\n\t\t\tname = re.sub(\"\\[[lL]\\]\", \"\", line)\n\t\t\tif name == \"receiver_args\".lower():\n\t\t\t\tfound_required_args = True\n\tf.close()\n\tif not found_required_"}, {"generated_text": "def validate_requires_args(self, args):\n\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tif len(args) == 0:\n\t\tname_not_provided.pop()\n\t\treturn False\n\n\tif len(args) == 1:\n\t\tif not self.required:\n\t\t\tname_not_provided.pop()\n\t\t\treturn False\n\n\t\tif self.values_scheme:\n\t\t\tif not self.values_scheme(args[0]):\n\t\t\t\tname_not_provided.append(args[0])\n\t\t\t\treturn False\n\t\tif not self.type_scheme:\n\t\t\tname_not_provided.append(args[0])\n\t\t\treturn False\n\n"}]}
{"task_id": "6306092d73426c38ae68ad08", "prompt": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t) -> None:\n\t\tdef _validate_and_split_args(args):\n\t\t\tif not len(args):\n\t\t\t\treturn []\n\t\t\targ_map_spec = self._get_spec(command_name)(**self._extract_cmd_arg_map_spec_kwargs(command_name))\n\t\t\targ_name_list: List[str] = []\n\t\t\tfor arg_name in args:\n\t\t\t\tfor arg_spec in arg_map_spec:\n\t\t\t\t\tif arg_name in arg_spec.split:\n\t\t\t\t\t\targ_name_list.append"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t) as cond_req_args:\n\t\tregexen = util.make_regexen(command_name)\n\t\tall_args = util.make_args_regex(options_spec, regexen)\n\t\tregex_args = util.make_args_regex(options_spec, regexen,\n\t\t\tmatch_once=True)\n\t\tmatch, matched_args = re.search(all_args, ''.join(args))\n\t\tfor m in match:\n\t\t\tfor r in regex_args:\n\t\t\t\tif re.search(r, matched_"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\tif options_spec.conditional_required:\n\t\toptions_spec_set.update_conditional_options(options_spec)\n\t\treq_conds = options_spec.conditional_required\n\t\treturn [ arg for arg, conditions in req_conds.iteritems() if (conditions, args[i]) for i in range(len(args)) ]\n\telse:\n\t\treturn [ arg for arg in args if args[i] for i in range(len(args)) ]\n"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\toptions_spec = split_option_string(options_repr(options_spec))\n\targs = split_option_string(args)\n\tcommand_spec = get_command_spec(command_name, options_spec)\n\n\tcmd_args = []\n\tfor name, value in iter_matching_cmd_opts(command_spec, args):\n\t\tcmd_arg = [name]\n\t\topts = options_spec.pop(name)\n\t\tif opts['required_when']:\n\t\t\treq = False\n\t\t\tfor f in opts['required"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t):\n\t\t# make sure the required_when is an integer\n\t\tif type(required_when)!= int:\n\t\t\traise RuntimeError(\"Required when should be an integer.\")\n\t\telif required_when < 1:\n\t\t\traise RuntimeError(\"Required when should be greater than zero.\")\n\n\t\t# check the required_when condition\n\t\tif required_when not in args:\n\t\t\treturn list()\n\t\telse:\n\t\t\targs = list(args)\n\t\t\treturn list(args[required_when - 1:])\n\n\tdef _get_returns_tuple(self, name):"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t) -> None:\n\t\treturn list(map(_resolve_conditionally_required_args,\n\t\t([(command_name, k) for k in options_spec], args, ))\n\t\t)\n\tif __name__ == '__main__':\n\t\targs = sys.stdin.readlines()\n\t\t(options,) = parse_arguments((args[0][:-1].split(',')))\n\t\tresult = _exec_command(\n\t\t\t(args[0][:-1],),\n\t\t\t(options,),\n\t\t)\n\t\tprint('{0}_{1"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\targs, options = self.parse_options_and_arguments(options_spec=options_spec, args=args)\n\toption_pairs = list()\n\tfor option in options:\n\t\tif option in ['help','version'] or option in options_spec:\n\t\t\tcontinue\n\t\trequired_when = None\n\t\tfor r in options_spec[option]:\n\t\t\tif r.startswith('required_when:'):\n\t\t\t\trequired_when = r[len('required_when:'):]\n\t\t\t\tbreak\n\t\tif required_when is not None:\n\t\t\tfor"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\n\tmatch = False\n\tconditional_args = []\n\t# for each positional argument in options_spec\n\tfor arg in options_spec:\n\t\tif arg.condition:\n\t\t\tmatch = check_required_arg_condition(arg.condition, args[arg.pos])\n\t\tif match:\n\t\t\t# store positional argument value to a list: required_args_pos\n\t\t\tconditional_args.append(arg.pos)\n\t# if the last command's positional arguments should not be used,\n\t# so return immediately\n\t# if the flag should be set in args"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t\n\targs = []\n\tfor option_name, option_args in options_spec:\n\t\twhen_list = _required_when_list(option_name, command_name)\n\t\tif not when_list:\n\t\t\tcontinue\n\n\t\tfor when_spec in when_list:\n\t\t\tif not when_spec:\n\t\t\t\tbreak\n\n\t\t\tcond_eval, _, arg_name = when_spec.partition('-')\n\t\t\tcond = _get_conditional_value(cond_eval, args)\n\t\t\tif cond:\n\t\t\t\targs.append(arg_name)"}, {"generated_text": "def _get_conditionally_required_args(self, command_name, options_spec,\n\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\t\n\t:param command_name: the command name.\n\t:param options_spec:  the list of command spec options.\n\t:param args: the received input arguments\n\t:return: list, list of argument names with matched ``required_when``\n\t    condition\n\t\"\"\"\n\t) -> List[Tuple[str, str]]:\n\t\tconditional_options = []\n\n\t\tfor name, spec in options_spec:\n\t\t\t# TODO: move this function to settings\n\t\t\ttry:\n\t\t\t\tvalidate_required_when(\n\t\t\t\t\t**({\"command_name\": command_name, \"name\": name, \"value\": args[name]}))\n\t\t\texcept ValidationError as error:\n\t\t\t\traise CommandLineError(\n\t\t\t\t\t\"{}: {}\".format(error.message, error.errors[\"name\"][0][\"__code__\"]))\n\t\t\telse"}]}
{"task_id": "6306092e73426c38ae68ad09", "prompt": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args)!= len(self._length):\n\t\ttry:\n\t\t\treturn self._length[0] < len(args)\n\t\texcept IndexError, e:\n\t\t\treturn False\n\telse:\n\t\treturn True\n\nclass LengthTargets(Target):\n\t\"\"\"\n\tCheck if the user's input is a certain length.\n\t\n\tThe received argument will be stored in ``$len``:\n\t\n\t.. code-block:: python\n\t\n\t\tclass LengthTargets(Target):\n\t\t\n\t\t\tdef validate_target(self, value, *"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) < self.numArguments:\n\t\t\tprint self.usage\n\t\t\treturn False\n\treturn True\n\t\t\n\t\t\n\t\t\n\t\t\n############################################################################################\nprint'start'\nfor i in range(0, len(sys.argv)):\n\tprint sys.argv[i]\n\tprint \" \"\n\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\tif len(args) > self.length:\n\t\t\tlog.error(\"Number of arguments is longer than expected!\")\n\t\t\traise CommandError('Invalid number of arguments.')\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tvalue = args[self.parameter]\n\tlength = self.data.get(\"length\", 0)\n\n\tif (value is None) or (value.__len__() < length):\n\n\t\traise ValueError(\"Parameter value length too small.\")\n\n\tif length == 0:\n\t\treturn\n\n\ttotal_length = sum(x.__len__() for x in self.data.get(\"children\", []))\n\n\tif (value.__len__()!= length) or (total_length >= length):\n\t\t# TODO: Error message for children with different length\n\t\traise"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif not (1 < len(args) <= 6):\n\t\traise Exception(\"Invalid number of arguments, expected length between (1,6], but was %i\" % len(args))\n\tif len(args) < 5:\n\t\traise Exception(\"Invalid number of arguments, argument 5 is '%s'\" % args[len(args)-1])\n\nclass SimpleTransmitter(Transmitter):\n\t\"\"\"\n\t\tSimple (one-way) Transmitter that forwards data to all connected receivers.\n\n\t\t:param sender: Sender which is used to send the data.\n\t\t:param receiver"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) < self.length:\n\t\traise ValueError(\"Error: No such argument value\")\n\"\"\"\n\n\"\"\"\ndef create_decorator(name):\n\t\"\"\"\n\tDecorator to create validator functions.\n\t\"\"\"\n\tdef dec(f):\n\t\t\"\"\" Create the validator. \"\"\"\n\t\t\n\t\tdef validator(value):\n\t\t\t\n\t\t\tvalidator_args = list()\n\t\t\t\n\t\t\t# Check parameters.\n\t\t\tvalidator_args.extend([value])\n\t\t\t\n\t\t\t# Check if '_value' is present. If it"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t# Get length of the method\n\tif ((not self.length) and (self.length in args)):\n\t\t\tself.length = args.pop(self.length)\n\t# Get maximum length of the method\n\tif (not self.max_length) and (self.max_length in args):\n\t\t\tself.max_length = args.pop(self.max_length)\n\t# Check that the number of passed arguments is valid\n\tif ((self.length and (max(len(args), self.length-1) < self.length)) and \n\t\t"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args)!= self.args_nums:\n\t\t\treturn (False, None, \"Not enough arguments. Provide the same amount as arguments to the function.\")\n\n\tif len(args)!= self.args_nums:\n\t\treturn (True, len(args), \"The length of the arguments does not match the length of the function\")\n\n\n# @classmethod\n# def parse_args(self, data):\n# \t\"\"\"\n# \tDecode a list of strings into a dictionary according to format defined. The format has exactly the same as specified by the function itself"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) > len(self.args):\n\t\traise TypeError, _(\"Invalid number of arguments\")\n\treturn args\n"}, {"generated_text": "def validate_length_args(self, args):\n\"\"\"\n\tCheck if value of arguments is not longer than length specified.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\ttry:\n\t\tlength = int(self.arguments.index(\"length\"))\n\texcept:\n\t\traise RequestError(\"Argument 'length' is not present or not an integer.\")\n\t\n\tif length <= 0:\n\t\traise RequestError(\"Number of characters is not positive.\")\n\t\n\tif len(args) > length:\n\t\traise RequestError(\"Received {} additional arguments.\".format(len(args)-length))\n\nclass RequestError(ValueError):\n\t\"\"\"\n\tError indicating that request received cannot be processed.\n\t\"\"\"\n\n\tdef __init"}]}
{"task_id": "6306092e73426c38ae68ad0a", "prompt": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif type(args)!= list: return False\n\tfor arg in args:\n\t\tif arg not in self.choices: return False\n\treturn True\n\nclass CmdAddItemArgsValidator(ArgsValidator):\"\"\"\n\tCheck if parameters of a add command are correct.\n\t\n\t:param str item: The item to add to the list.\n\t:param list choices: The item types.\n\t\"\"\"\n\tdef __init__(self, item):\n\t\tself.required_args = ['item']\n\t\tself.item = item\n\t\tself.choices"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor (i, arg) in enumerate(args):\n\t\tif not isinstance(arg, basestring):\n\t\t\traise InvalidArgumentsError(\"arg[%d] has wrong type (expected str, got %s)\" % (i, repr(arg.__class__)))\n\t\tif arg not in self._choices:\n\t\t\traise InvalidArgumentsError(\"arg[%d] refers to an invalid choice: %s\" % (i, arg))\n\treturn True\n\n\nclass ConfigBase(object):\n\t\"\"\"\n\tConfiguration base class.\n\t\"\"\"\n\tdef __"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t# The received arguments must appear as a list of tuples\n\tif not isinstance(args, list):\n\t\t\traise TypeError(\"First argument must be a list\")\n\tif not all(isinstance(arg, tuple) for arg in args):\n\t\t\traise TypeError(\"Arguments not a list of tuples\")\n\tfor arg in args:\n\t\t\tif not isinstance(arg, tuple) or len(arg)!= 2:\n\t\t\t\t\traise TypeError(\"Arguments must be tuples of two arguments, i.e. (name, value)\")\n\t\t\tname, value = arg"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\treturn [c[0] for c in self._choices if c[1] == args][0]\n\t\n\t#def __str__(self):\n\t#\t\treturn self.description\n\t\n\t\n\t#def __init__(self, choices):\n\t\t#self._choices = tuple(choices)\n\t\n\t#def __str__(self):\n\t\t#return \"%s (%s)\" % (self.description, repr(self.value))\n\t\t\nif __name__ == '__main__':\n\t\n\tapp = QtGu"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.__meta_args[\"choices\"]\n\ttry:\n\t\tif args not in choices:\n\t\t\treturn {'success': False, \"details\": \"Invalid choice\"}\n\texcept KeyError:\n\t\treturn {'success': False, \"details\": \"Choices are not provided\"}\n\n\treturn {'success': True}\n\nclass BaseCommand(object):\n    @classmethod\n    def handle_view(cls):\n        #: Get the path to the current module.\n        if sys.argv:\n            module_name = inspect.getmodule"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tchoice_ids = self.get_choice_ids(args)\n\tvalid_choices = self.choices.keys()\n\treturn Choice.objects.filter(id__in=choice_ids).exclude(\n\t\t\tid__in=valid_choices).count() == 1\n\n\t"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\tvalue = get_single_choice_argument(args)\n\tif value not in vars(self.choices):\n\t\t\n\t\t# TODO: Change the help string of this method to something more verbose.\n\t\traise ValueError(\"Unexpected choice value. This is either unsupported or invalid choice: {0}\".format(value))\n\t\n\treturn value\n\n\nclass TextChoices(Choices):\n\t\"\"\"\n\tThis class adds additional functions to the standard Choices class:\n\n\t* TextChoices provides get_text_choices to parse input of\n\t"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif not any(choice.check(args)\n\t           for choice in self.choices_args):\n\t\traise ArgumentValueError(args, self.choices_args)\n\n\ttry:\n\t\treturn args.pop(*self.choices_args)\n\texcept IndexError as err:\n\t\traise ArgumentValueError(set(args), self.choices_args)\n"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif os.path.exists(args[0]):\n\t\t\treturn args[0]\n\telse:\n\t\tself.caller.msg(\"Must be given a valid file path.\")\n\t\treturn False\n"}, {"generated_text": "def validate_choices_args(self, args):\n\"\"\"\n\tCheck if value of choice arguments is one of the available choices.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tchoice_args = [AR_TARGET_ID_TYPE, AR_TARGET_ID_VALUE]\n\t\n\tif not self.check_choices(choice_args, args):\n\t\t\treturn False\n\treturn True"}]}
{"task_id": "6306092e73426c38ae68ad0b", "prompt": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t\n\t# If no limits are set, return True\n\tif self.min == None and self.max == None:\n\t\treturn True\n\t\n\t# Otherwise, check if they are out of bounds\n\ttry:\n\t\t# If min is defined\n\t\tif self.min!= None:\n\t\t\tmin_limit = self.min\n\t\t\t\n\t\t\t# If we have a list as the minimum value\n\t\t\tif type(self.min) is list:\n\t\t\t\t# Do the logical logic of the comparison between min and self.value\n\t\t\t\t# This is to preserve order\n\t\t\t\tif args["}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif type(args) == types.TupleType:\n\t\targs = list(args)\n\t\n\tif len(args) < 2:\n\t\targs.append(self.min)\n\tif len(args) < 3:\n\t\targs.append(self.max)\n\t\n\tfor (num, value) in enumerate(args):\n\t\tif type(value) == types.IntType:\n\t\t\tvalue = int(value)\n\t\t\n\t\tif value < self.min[num] or value > self.max[num]:\n\t\t\traise ValueError(unic"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tmin = float(self.config.get_min_double())\n\tmax = float(self.config.get_max_double())\n\tif self.min_less(args,min):\n\t\traise ValueError(\"Min value invalid '{0}' > '{1}'\".format(args, min))\n\tif self.max_greater(args,max):\n\t\traise ValueError(\"Max value invalid '{0}' < '{1}'\".format(args, max))\n\t\t\n\"\"\"\n\tCalculates a double from a string provided:\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tdef is_valid(value):\n\t\tif value < self.min or value > self.max: \n\t\t\traise ValueError('arguments should comprise between %s and %s.'% (self.min, self.max))\n\t\treturn super(IntegerRange, self).validate(value)\n\t\n\treturn list(islice(map(is_valid, args), 0, 10))\n\n\n\n__all__ = [\n\tUnlimited,\n\tLimitedNumber,\n\tOptionalInteger, OptionalFloat,\n\tLimitedPositiveNumber,\n\tLimitedNegativeNumber,\n"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif len(args) < 2:\n\t\terr = ErrorMessage(\"You do not specify minimum and maximum value\")\n\t\terr.min = str(self.minimum_values[0])\n\t\terr.max = str(self.maximum_values[0])\n\t\traise err\n\tfor i in range(len(args[0])):\n\t\tif type(args[0][i]) == type(1):\n\t\t\tif type(args[1][i]) == type(1):\n\t\t\t\tif args[0][i] < args[1][i]:\n\t\t\t\t\terr = ErrorMessage"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\t_validator = utils.AttributeDict(min_value=None, max_value=None)\n\t\t\n\tresult = _validator(**utils.get_args_and_validators(args))\n\tif result.min_value:\n\t\tself._min_value = result.min_value\n\tif result.max_value:\n\t\tself._max_value = result.max_value\n\t\n\t_validator = utils.AttributeDict(min_value=-1, max_value=1)\n\targs_with_validators = ut"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\targ_count = len(args)\n\tif arg_count == 2:\n\t\t\traise ValueError(\"Not enough parameters for this function call. A min and max parameter are required.\")\n\telif arg_count == 3:\n\t\t\tif args[0] > args[2]:\n\t\t\t\t\traise ValueError(\"Minimum must be less greater than maximum.\")\n\t\t\telif args[0] < args[2]:\n\t\t\t\t\traise ValueError(\"Maximum must be greater than minimum.\")\n\telse:\n\t\t\traise ValueError(\"too many arguments to this function call.\")\n\"\"\"\n\tConvert number to list"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tassert isinstance(args, tuple)\n\t\n\tfor v_pos, v in enumerate(args):\n\t\tassert isinstance(v, int)\n\t\tminv = maxv = v\n\t\t\n\t\tif v_pos == 0: # first argument - minimum value\n\t\t\tminv = maxv = args[0]\n\t\telif v_pos == 1: # second argument - max value\n\t\t\tmaxv = args[1]\n\t\t\n\t\tfor min_value, max_value in self.validator.limits:\n\t\t\t# check if min/max values are same"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tif not self.is_min_arg(args.args.min):\n\t\t\traise ValueError('Invalid min argument %s, the minimum value is %s.' %\n\t\t\t\t(args.args.min, self.get_min_arg()))\n\tif not self.is_max_arg(args.args.max):\n\t\t\traise ValueError('Invalid max argument %s, the maximum value is %s.' %\n\t\t\t\t(args.args.max, self.get_max_arg()))\n\tif args.args.min > args.args.max:\n\t\t\traise"}, {"generated_text": "def validate_min_max_args(self, args):\n\"\"\"\n\tCheck if value of arguments is between minimum and maximum values.\n\t\n\t:param args: The received arguments.\n\t\"\"\"\n\tfor index, arg in enumerate(args, 0):\n\t\ttry:\n\t\t\tminvalue = None if arg is None else float(args[index])\n\t\t\tmaxvalue = None if arg is None else float(args[index])\n\t\texcept ValueError:\n\t\t\traise CommandParameterError(\n\t\t\t\t\"Argument %s must be a float. Received '%s' instead.\"\n\t\t\t\t% (index, str(arg))\n\t\t\t)\n\n\t\t# Minimum of -1 is to force an error because this is the default.\n\t\tif minvalue!= -1:\n\t\t\tif not self."}]}
{"task_id": "6306092e73426c38ae68ad0d", "prompt": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tobj = self.modeler.create_complex_argumet_type(subcommand, type_name, option_name,\n\t'''\n\t@param {%s:%s} command The %s command instance\n\t@param {%s} option the option value name in %s command instance\n\t\"\"\"\n\t@return {%s} the complex value object instance\n\t'''\n\t% (subcommand, type_name, subcommand, subcommand,\n\t\tcommand, option_name\n\t)\n\t@return\n\t\tdef create_option_type(option"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tif not self._config.has_section(type_name):\n\t\tspec = \"\"\n\t\tfor entry in spec_option:\n\t\t\tif \"name\" in entry:\n\t\t\t\tname_entry = entry\n\t\t\t\tif \"default\" in entry:\n\t\t\t\t\tspec += \"\\t\"+ type_name + \" %s %s %s\" % (subcommand, option_name, name_entry[\"name\"])\n\t\t\t\telse:\n\t\t\t\t\tvalue_entry = entry  + \", \"\n\t\t\t\t\tspec += \"\\t\"+ type_name + \" %s %s %s\" % (subcommand"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t# create the complex type\n\targs = argparse.ArgumentParser()\n\targs.add_argument(\"--%s\"%option_name, type_name)\n\ttype_instance =  args.parse_args()\n\treturn type_instance\n\nclass Subcommands(object):\n\t\"\"\"\n\tThis class allow to create a CLI sub-commands based on\n\tthe basic subcommands specified by the module.\n\t\"\"\"\n\n\tdef __init__(self, type_name, option_name, type_spec, group_name, description):\n\t\tself"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tfrom.arguments import Type, Arguments\n\tif type(option_name) == unicode:\n\t\tpass\n\telif type(option_name) == str:\n\t\toption_name = option_name.encode('utf-8')\n\telif hasattr(option_name, \"encode\"):\n\t\toption_name = option_name.encode('utf-8')\n\telse:\n\t\traise TypeError(\"OptionName must be unicode, str or unicode/str: %s\"%(option_name))\n\toption_name_copy"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tclass_name = 'ComplexArguments'\n\tbase_argument_types = ['ComplexArguments']\n\targ_type = '%s%sComplexArgumentsType' % (preprefix, cmd_name)\n\n\n\tfor element in spec:\n\n\t\tfor base_name in base_argument_types:\n\t\t\tif element.type.startswith('Complex%sComplexArgumentsType' % (base_name)):\n\t\t\t\tbreak\n\n\t\t# If it was already processed\n\t\telse:\n\t\t\tcontinue\n\n\t\tif not element.name:\n\t\t\tcontinue"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t# get the option value\n\toption = self.cmd.options[option_name]\n\t# create the complex type\n\tcomplex_type = self.create_complex_type(option, type_name, subcommand)\n\t# register the complex type\n\tself.cmd.register_complex(complex_type, option_name)\n\n\tsubcommand_instance = get_complex_type_instance(self.cmd, option_name)\n\treturn subcommand_instance\n\n\nclass SpecificationHelper(object):\n \n\tdef __init__(self, cmd):\n\t\t"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t):\n\t'''\n\tBuild the complex argument type\n\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t'''\n\ttype_instance = ArgparseTypeInstance()\n\ttype_instance.create_simple_argument_type(subcommand, option_name, spec_option) \n\ttype_instance.name = type_name\n\t\n\treturn type_instance\n\n\n"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\t@type_decorator\n\tdef type_init_fct(self, options):\n\t\t\"\"\"\n\t\tReturns an argument type with a 'com_type_name' (prefixed by '*' if used).\n\t\t\n\t\t:param options: the command option dictionaries\n\t\t:return: the type\n\t\t\"\"\"\n\t\ttype_obj = create_arguement_type(subcommand, type_name)\n\t\ttype_obj.extend(option_name, get_option_values(options))\n\t\treturn type_obj\n\n\tget_type_name = create"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tassert type_name!= 'None'\n\tassert option_name!= 'None'\n\tassert not isinstance(option_name, basestring)\n\toption = {\n\t\t'name': option_name,\n\t}\n\toption_type = self.build_complex_type(subcommand, type_name, option)\n\treturn option_type\n\t\"\"\"\n\tGenerate and return a complex type instance according option's specification\n\tThis function must receive the following parameters:\n\t:param subcommand: the command name\n\t:param type_name: the complex type name"}, {"generated_text": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n\"\"\"\n\tBuild the complex argument type\n\t\n\t:param subcommand: the command name\n\t:param type_name: the complex type name\n\t:param option_name: the option name\n\t:param spec_option: option's specifications\n\t:return: the complex type instance\n\t\"\"\"\n\tspec = self.options.spec[option_name][type_name]\n\toption_value = self.option_values.get(option_name, None)\n\n\tif spec.has_key(\"set\"):\n\t\ttype_ = create_list_or_tuple_type(option_value.parse(spec['set']))\n\n\t\tif option_name.endswith(\"_s\"+type_name):\n\t\t\tisinstance = get_composite_type_type(option_value.parse(type_name), type_name)\n\t\t\tisinstance."}]}
{"task_id": "6306092e73426c38ae68ad0f", "prompt": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif arg[0] in (\"-C\", \"-v\", \"-vv\", \"-vvv\", \"-vvvv\", \"-vvvvv\"):\n\t\t    # Control arguments. \n\t\t    # The last argument should be a list of the control variables.\n\t\t    for idx in range(len(arg) - 2):\n\t\t\t        control_args[\"ansible_%s\" % arg[idx + 2]] = arg[idx + 3]\n\t\t    continue\n\t\tel"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrols_key = \"CONTROLS\"\n\t# All controls are always lists, not dictionaries (i.e. do not support k/v)\n\tcontrols = list()\n\tfor item in args[controls_key]:\n\t\tif item[0] == '@':\n\t\t\tnested_args = get_nested_custom_and_control_args(item)\n\t\t\tnested_args[controls_key] = item[1:]\n\t\t\treturn controls_key, nested_args\n\t\telse:\n\t\t\tcontrols.append(item)\n"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\n\t# control_args = [v[0][:-4], v[1][:-4] for v in list(self.controller_args.items()) if v[1]:]\n\t# nested_args = [v[0][:-4], v[1][:-4] for v in list(self.nested_args.items()) if v[1]:]\n\t# control_args = list(set(control_args))\n\t# nested_args = list(set(nested_args))\n\n"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tres = {}\n\tres_control = {}\n\tcontrol_keys = set(CONTROL_KEYS)\n\tres_control.update(vars(args))\n\tres.update(res_control)\n\targnames = vars(args).keys()\n\tfor argname in argnames:\n\t\tif argname not in control_keys:\n\t\t\tif argname in self.nested_spec_keys:\n\t\t\t\tres[argname] = args.get(argname, None)\n\t\t\telif argname in self.custom_spec_keys:\n"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tkey = arg.split('=')[0]\n\t\tif key.startswith('--') and (key not in control_args):\n\t\t\tcontrol_args[key] = arg\n\t\telif key not in control_args:\n\t\t\tnested_args[key] = arg\n\treturn control_args, nested_args\n\n"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\t# Collect all the control arguments first\n\tcontrol_args = defaultdict(dict)\n\targs_spec = defaultdict(dict)\n\tfor key, value in vars(args).items():\n\t\tkey = str(key)\n\t\t# Skip this arg if there is a control arg with the same key\n\t\tif key in control_args:\n\t\t\tcontinue\n\t\t# Get the list of all possible arguments for the nested arg\n\t\tnested_list = args_spec[key].keys()\n\t\t# Skip this arg if there is a custom arg with the same key\n\t\t"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\t\n\tcontrols = []\n\tnested = []\n\t\n\tfor arg in args:\n\t\tif arg in self.CONTROL_KEYS:\n\t\t\tcontrols.append(arg)\n\t\telse:\n\t\t\t# normal use, use it for nested spec\n\t\t\tnested.append(arg)\n\t\n\t# make sure control args are in strict mode\n\tcontrols = self.make_custom_args(controls, mode='strict',\n\t\t\tcustom_key_names=self.CONTROL_KEYS)\n\t\n\t# make sure nested args"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tnested_args = {}\n\tcontrol_args = {}\n\t\n\tfor arg in args.split(self.get_argument_separator()):\n\t\tif not arg:\n\t\t\tcontinue\n\t\targ = arg.strip()\n\t\tvalue_dict = arg_to_dict(arg)\n\t\tif self.is_nested_argument(arg):\n\t\t\tnested_args.update(value_dict)\n\t\t\tcontrol_args.update(value_dict)\n\t\telse:\n\t\t\tcontrol_args.update(value_dict) \n\t\t\tnested_args."}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = dict()\n    \tcontrol_args.update(self.get_custom_vars(args))\n\tcontrol_args.update(self.get_vars_from_env())\n\t\n\tnested_args = dict()\n\tnested_args.update(self.get_nested_args(args))\n\t\n\tcontrol_args, nested_args = self.order_into_dicts(control_args, nested_args)\n    \treturn control_args, nested_args\n\n"}, {"generated_text": "def get_nested_custom_and_control_args(self, args):\n\"\"\"\n\tSplit input arguments to control nested and custom.\n\t\n\tControls arguments: control the IR behavior. These arguments\n\t    will not be put into the spec yml file\n\tNested arguments: are used by the Ansible playbooks and will be put\n\t    into the spec yml file.\n\tCustom arguments: Custom ansible variables to be used instead of the\n\t    normal nested usage.\n\t\n\t:param args: the collected list of args.\n\t:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif arg in self.CUSTOM_KWARGS_NESTED:\n\t\t\tnested_args[arg[0]] = arg[1]\n\t\telif arg == \"control\":\n\t\t\t# control_args[arg[0]] = arg[1]\n\t\t\tpass\n\t\telif arg.find(\"=\") == -1:\n\t\t\tcontrol_args[arg] = arg\n\t\telse:\n\t\t\tnested_args[arg] = arg\n\treturn control_args"}]}
{"task_id": "6306092e73426c38ae68ad11", "prompt": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn extra_vars\n\tfor item in extra_vars:\n\t\ttry:\n\t\t\tvars_dict.update(item)\n\t\texcept:\n\t\t\tpass\n\treturn extra_vars\n\n\ndef get_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\t\tMerge extra-vars into ``vars_dict``\n\t\n\t:param vars_dict: Dictionary to copy values into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tif extra_var not in vars_dict:\n\t\t\t\tvars_dict[extra_var] = 'True'\n        return vars_dict\n\t\ndef merge_params(params_dict, param_list):\n    '''\n    Merge a dictionary containing a list of params into a dictionary\n    '''\n    return_dict = {}\n    for param in param_list:\n        if param in params_dict:\n            return_dict[param] = [return_"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tnew_vars = None\n\tif extra_vars:\n\t\tnew_vars = dict(vars_dict)\n\t\tnew_vars.update(extra_vars)\n\telse:\n\t\tnew_vars = vars_dict\n\n\treturn new_vars\n\ndef merge_noparams2params(input_params):\n    \"\"\"\n    Replace 'noparams' in input_params to 'params', if that is in fact a nested list\n    :returns: input_params if 'noparams' isn"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tfor key, val in six.iteritems(extra_vars):\n\t\t# If we have a key in the vars-dict (but not `extra_vars`), then we need\n\t\t# to delete it!\n\t\tif key in vars_dict:\n\t\t\tdel vars_dict[key]\n\t\telse:\n\t\t\tvars_dict[key] = val\n\tif extra_vars:\n\t\t# Add all newly registered extra-vars to the 'vars_dict'\n\t\tfor key, val in six.iteritems(extra_vars):"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars is None:\n\t\treturn vars_dict\n\t\n\t# add only non-empty items\n\tfor key in set(extra_vars).intersection(extra_vars.viewkeys()):\n\t\tvars_dict[key] = extra_vars[key]\n\t\n\treturn vars_dict\n"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif not extra_vars:\n\t\treturn vars_dict\n\tfor v in vars_dict.keys():\n\t\tif type(vars_dict[v]) == dict:\n\t\t\tif v not in extra_vars:\n\t\t\t\tfor v2 in extra_vars:\n\t\t\t\t\tvars_dict[v2] = ExtraVarDict()\n\t\t\telif v not in extra_vars + [v2 for v2 in vars_dict if type(vars_dict[v2]) == dict]:\n\t\t\t\t# merge two variables (not a sub-dict"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif extra_vars is None : return\n\tfor key in extra_vars.keys() :\n\t\tvars_dict[key] = extra_vars[key]\n\n\"\"\"\n\tGet ``vars_dic`` for the first host in the list. This needs to work using the \n\t``@[ip]`` notation\n\"\"\"\ndef get_vars(hosts_list):\n\t\"\"\"\n\tRetrieve variables \n\t\n\t:param list of hosts: list of potential hostname to query for \n\t\"\"\"\n\t\n\tv"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tfor k in extra_vars:\n\t\tvars_dict[k] = extra_vars[k]\n\ndef merge_meta(meta, extra_key=None):\n\textra_meta = meta or {}\n\textra_meta['__META__'] = meta\n\n\t# if the metakey has been set.\n\tif extra_key:\n\t\t# if the metakey is defined, add it to the meta dictionary\n\t\tif extra_meta.get(extra_key):\n\t\t\tmeta[extra_key] = extra_meta[extra_key]"}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars:\n\t\tfor key, val in extra_vars.items():\n\t\t\t# Avoid copying if already in vars_dict\n\t\t\tif key not in vars_dict:\n\t\t\t\tvars_dict[key] = val\n\t\t\telif type(vars_dict[key]) == list:\n\t\t\t\tvars_dict[key].extend(val)\n\n\t\t\t# Avoid deep merging in vars_dict\n\t\t\telif type(vars_dict[key])!= list:\n\t\t\t\tvars_dict[key] = list(vars_dict["}, {"generated_text": "def merge_extra_vars(vars_dict, extra_vars=None):\n\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\t\n\t:param vars_dict: Dictionary to merge extra-vars into\n\t:param extra_vars: List of extra-vars\n\t\"\"\"\n\tif extra_vars:\n\t\tmap(vars_dict.update, extra_vars)\n\treturn vars_dict\n"}]}
{"task_id": "6306092f73426c38ae68ad13", "prompt": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n\t):\n\t\n\t\tif not verbose:\n\t\t\tansible_verbose = \"--verbose -vvvv\"\n\t\telif verbose == \"normal\":\n\t\t\tansible_verbose = \"-vvvv\"\n\t\telif verbose == \"debug\":\n\t\t\tansible_verbose = \"-vv\"\n\t\telse:\n\t\t\tansible_verbose = \"\"\n\t\t\n\t\tcmd = \"{0} {1} -i {2} -c '{3} {4}' {5}\".format(\n\t\t\tir_workspace.options['ansible"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t)\ndef ansible_playbook_cmd(ir_workspace, ir_plugin, playbook_path):\n\t\"\"\"\n\tExecutes the named ansible_playbook playbook\n\n\t:param ir_workspace:\n\t:param ir_plugin:\n\t:param playbook_path:\n\t\"\"\"\n\tfrom infrared import utils\n\n\tassert ir_plugin is not None\n\tassert ir_workspace is not None\n\tassert playlist_path is not None\n\t\n\tif verbose is not None:\n\t\tif verbose > 0:\n\t\t\tverbose = 1\n"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t) as ansible_subprocess:\n\t\tsubprocess.run(\n\t\t\t[\"os.environ[IR_PATH]=\" + \";\".join([\"'%s'\" % ir_workspace.path] +\n\t\t\t                                    [\"%s=%%(p)s\" % p for p in\n\t\t\t\t                                  [(\"BIN_MODULE\", \"ir_module\"),\n\t\t\t\t                                   (\"PLUGIN_PATH\", ir_plugin.path)]]),\n\t\t\t  \"ansible-playbook\"]\n\t\t\t+ list(ansible_args.items()"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n\ndef ansible_runner(ir_workspace, ir_plugin, playbook_path, context=False, verbose=None,\n\"\"\"\n\tWraps the 'ansible-run' CLI.\n\n\t:param ir_workspace: An Infrared Workspace object represents the active workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param context: flag to indicate that the CLI invokes with context; to only\n\t    use for the inventory, not for command/module execution\n\t"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\"infrared-ansible-playbook\") as ansible_playbook:\n\t\tir_plugin.log(\"Executing Infrared Ansible Playbook at: %s\" % playbook_path)\n\t\n\t\tif extra_vars is not None:\n\t\t\tansible_args = ansible_args or dict()\n\t\t\tif extra_vars is not None:\n\t\t\t\tfor k, v in extra_vars.items():\n\t\t\t\t\tkwargs = {\"env_vars\": self.ansible_env_vars, \"extra_vars\": extra_vars}\n"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\t\ttry:\n\t\t\tfor v in ir_workspace.puppet_plugins:\n\t\t\t\tif ((type(v) == type('./modules/ansible/')\n\t\t\t\t\tand v.endswith(pip_module)) or\n\t\t\t\t\tirs_utils.plugin_can_load(v)):\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\treturn False\n\n\t\texcept ValueError, e:\n\t\t\treturn irs_utils.IRSPluginException('Can find ansible module')\n\t\texcept Exception, e:\n\t\t\treturn irs_utils.get_exception_string"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\t\n    if'verbose' in ansible_args:\n        verbose = ansible_args.pop('verbose')\n\n    playbook_args = ansible_args.pop('playbook_args', {})\n\n    if verbose is True:\n        verbose = _VERBOSE\n    else:\n        verbose = 3\n\n    env = {\n            'WORKSPACE_PATH': ir_workspace,\n            'PYTHONPATH': [\n                '{}'.format(os.environ.get('PYTHONPATH"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t):\n\ttry:\n\t\timport ansible\n\t\timport ansible.playbook\n\texcept ImportError as e:\n\t\tif verbose is None:\n\t\t\tverbose = self.verbose\n\t\traise e\n\telse:\n\t\t# Use args.check mode by default if no arg is passed\n\t\targs_check_mode = args.check\n\t\t# Check args for arg/args\n\t\tif args.check_mode is not None:\n\t\t\targs_check_mode = args.check_mode\n\t\tif args.verbose and args.verbose_level is not None"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t\n):\n\t\"\"\"\n\tReturns:\n\t    An InfraredJob object.\n\t\"\"\"\n\t_ansible_verbose = verbose\n\t_ansible_extra_vars = extra_vars\n\t_ansible_args = ansible_args\n\tansible_env = _ansible_build_environment()\n\t# Create a list of tuples so that we can specify\n\t# both the playbook to execute and the optional\n\t# parameters in the same list; the first item in\n\t# each tuple is the parameter name; the second\n\t# item"}, {"generated_text": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\t\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\tworkspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:param playbook_path: the playbook to invoke\n\t:param verbose: Ansible verbosity level\n\t:param extra_vars: dict. Passed to Ansible as extra-vars\n\t:param ansible_args: dict of ansible-playbook arguments to plumb down\n\t    directly to Ansible.\n\t\"\"\"\n\t)):\n\tdef __init__(self, ir_plugin, playbook_path, verbose=None, extra_vars=None, ansible_args=None):\n\t\tself.ir_plugin = ir_plugin\n\t\tself.verbose = verbose\n\t\tsuper().__init__()\n\t\tself.ansible_args = ansible_args if ansible_args else {}\n\t\tself.extra_vars = extra_vars or {}\n\n\t\tif self.verbose is None:\n\t\t\tself.verbose = (2 if self.ir_plugin.config"}]}
{"task_id": "6306093273426c38ae68ad15", "prompt": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\ttry:\n\t\tplaybook_path = ir_plugin.get_workspace_path(ir_workspace)\n\t\tplaybook_str =''.join(str(x) for x in cli_args)\n\t\tplaybook_res = ir_cli.execute_ansible(\n\t\t\tplaybook_path, playbook_str, vars_dict)\n\t\treturn playbook_res\n\texcept InfraredError as e:\n\t\tir_logging.get_logger('ir_cli').error(\n\t\t\t\"%s while executing the '%s' command\" %"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tpass\n\tir_logger.info(\"{} Running ansible playbook {} for the workspace {}.\".format(PLUGIN_NAME,\n\tcli_args[0], ir_workspace._name))\n\n\t# Run ansible cli with vars\n\trc, out_ansible_return, ansible_log_path = run_ansible(cli_args, vars_dict, [ir_workspace],\n\tir_plugin)\n\n\t# return ansible return code and ansible result\n\t# FIXME: For now, just add the last few lines"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t# Set plugin class\n\tif (ir_plugin.get_conf().has_section('ANSIBLE') and\n\t    ir_plugin.get_conf().get('ANSIBLE', 'class') == 'INFRA_SIMPLE_TEST'):\n\t\tclass_plugin = InfraSimpleTestTrait(ir_plugin)\n\telse:\n\t\tclass_plugin = InfraTrait(ir_plugin)\n\n\tir = class_plugin(ir_plugin, ir_workspace)._run_run_plugin(cli_args, vars_dict)\n\n\treturn"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tif ir_plugin.verbose_debug:\n\t\tprint \"Ansible client - Run playbook - %s\"%cli_args\n\tansible_command = \"{0:s} {1:s}\".format(AnsibleClient.bin_path, cli_args)\n\tansible_cmd_proc = subprocess.Popen(ansible_command.split(), stdout=subprocess.PIPE)\n\treturn_results = [line.decode('utf-8') for line in ansible_cmd_proc.stdout]\n\treturn return"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tif not os.path.exists(ir_workspace.path):\n\t\traise Exception(\"Directory: {} does not exist\".format(ir_workspace.path))\n\n\tif not os.path.isdir(ir_workspace.path):\n\t\traise Exception(\"Directory: {} is not a directory\".format(ir_workspace.path))\n\n\tir_command = IRCli(vars_dict)\n\tp = subprocess.Popen(cli_args,\n\t                     stdin=subprocess.PIPE,\n\t                     stdout=subprocess."}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\treturn ansible.run(ir_plugin, vars_dict,\n\t\thost_list=ir_workspace.get_hosts(),\n\t\tinventory=ir_workspace.get_hosts(),\n\t\tmodule_name=\"\",\n\t\tinventory_options={},\n\t\tcheck=False,\n\t\tconnection_timeout=300,\n\t\tforks=6,\n\t\tbecome_method='su',\n\t\tbecome_user=ir_plugin.cli_args.become_user,\n\t\tsudo_user=ir_plugin.cli_args.become_user,"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\t\n\t# check if current plugin is toplevel\n\ttry:\n\t\t# if True, get the toplevel plugin as infrared_plugin\n\t\tir_plugin = _get_current_plugin(ir_workspace)\n\texcept AssertionError as e:\n\t\t# get the immediate underline of plugin\n\t\tir_plugin = _get_current_plugin(ir_workspace)[:-4]\n\t\t# if there are subplugins of the current plugin\n\t\tassert \".\" in ir_plugin, \"subplugin detected is not a subplugin. please remove '.'"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\treturn ansible(\n\t\t*cli_args,\n\t\ttask_vars={**vars_dict},\n\t\tsubset=ir_workspace.subset(ir_plugin),\n\t\tprivate=(ir_workspace._private)\n\t)\n"}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\tansible_opts = cli_args[\"args\"]\n\tansible_opts.extend([\"--inventory\", \"./inventory/\"+ ir_plugin.id + \".yml\"])\n\tansible_opts.extend(['--extra-vars', json.dumps(vars_dict)])\n\tresult_obj = None\n\twith tempfile.TemporaryDirectory() as tempdir:\n\t\t# Save the current user directory inside to a temporary directory\n\t\t# if a different directory was set in the plugin configuration file\n\t\tif os.path."}, {"generated_text": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\"\"\"\n\tRuns ansible cli with vars dict\n\t\n\t:param vars_dict: dict, Will be passed as Ansible extra-vars\n\t:param cli_args: the list  of command line arguments\n\t:param ir_workspace: An Infrared Workspace object represents the active\n\t workspace\n\t:param ir_plugin: An InfraredPlugin object of the current plugin\n\t:return: ansible results\n\t\"\"\"\n\ttry:\n\t\targs = cli_args\n\t\tansible = AniS.run_cli(args.split(' '), vars_dict, 'iron_cli', ir_plugin, ir_workspace)\n\t\treturn ansible\n\texcept (OSError, IOError) as e:\n\t\tprint_stderr_info(\"Cli execution error:\" + str(e))\n\t\treturn None\n\ndef _get_infra_plugins():\n\t\"\"\"\n\tReturns an InfraredPluginSet containing all the active plugins\n\t:return: InfraredPluginSet containing all"}]}
{"task_id": "63060ada73426c38ae68ad31", "prompt": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t_convert_common_arguments(parser_name, values_dict)\n\t_convert_single_element_numeric_argument(parser_name, values_dict,\n\t\t\t\t\t\t'cpuset', _convert_cpuset_argument)\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor key, val in values_dict.items():\n\t\tif key == \"value\":\n\t\t\tif isinstance(val, list):\n\t\t\t\t# TODO(bogdan): This need to figure out if the value is already an array\n\t\t\t\tvalues_dict[key] = str(' '.join(val))\n\t\t\telse:\n\t\t\t\tvalues_dict[key] = str(val)\n\t\telif key == \"count\":\n\t\t\tif val is not None:\n\t\t\t\tvalues_dict[key] = int(val)\n\t\telif hasattr(parser_name, key):\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tdef _validate(func):\n\t\"\"\"\n\tValidates that parameter for the function is valid.\n\t\t\n\tChecks if the type of the parameter matches the function's expected\n\targument type.\n\t:param func: The function to be validated\n\t\"\"\"\n\t@functools.wraps(func)\n\tdef wrapper(self, parser, values_dict):\n\t\tif 'arg_list' not in values_dict:\n\t\t\t_message = self._message % values_dict\n\t\t\tparser.print_usage()\n\t\t\tself.parser.error(_message)\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif values_dict:\n\t\tall_vars = self.script_params.keys()\n\t\tfor var in all_vars:\n\t\t\tif var in values_dict.keys():\n\t\t\t\tif values_dict[var] in (\"True\", \"false\"):\n\t\t\t\t\tvalues_dict[var] = eval(values_dict[var])\n\t\t\t\telse:\n\t\t\t\t\tvalues_dict[var] = str(values_dict[var])\n\treturn\n\t\n"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tvargs = self.parser.values\n\tvalues = {}\n\tfor pname in self.parser._get_all_params():\n\t\tvalue = self.parser._get_default(pname)\n\t\tif pname in parser_name + ('name',): # do not pass the cli args to the parameters that are in the parser's list\n\t\t\tcontinue\n\t\telif pname in vars(parser_name): # only pass the parser variables\n\t\t\tvalue = getattr(vargs, pname) if callable(getattr(vargs, pname)) else vargs"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tvalues_dict = dict(values_dict)\t\t# makes copy of dict\n\tfor value in values_dict:\n\t\ttry:\n\t\t\tvalues_dict[value] = types.is_integer(values_dict[value])\n\t\texcept:\n\t\t\traise argparse.ArgumentTypeError(\"The argument %s is not string or number\" % value)\n\t\n\treturn values_dict\n\ndef _add_connect_args(namespace):\n\t\"\"\"\n\t\tAdds connect flags to argument parser, if user enabled them.\n\t\"\"\"\n\tif CONNECT_OPTIONS"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tfor key, value in values_dict.items():\n\t\t_type = re.match(r'^(str|type):?([^=]|$)', key).group(2)\n\t\tif _type:\n\t\t\tif value is not None:\n\t\t\t\ttry:\n\t\t\t\t\tif _type == '':\n\t\t\t\t\t\tvalues_dict[key] = value\n\t\t\t\t\telif _type =='str':\n\t\t\t\t\t\tif isinstance(value, str):\n\t\t\t\t\t\t\tvalues_dict[key] = value\n\t\t\t\t\telif _type == 'type':\n\t\t\t\t\t\tif isinstance(value, str):"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tcmd_map = self.__dict__[parser_name]['cmd_map']\n\tfor (key, val) in values_dict.items():\n\t\ttry:\n\t\t\tret = cmd_map[key](val)\n\t\t\tvalues_dict[key] = ret\n\t\texcept KeyError:\n\t\t\tpass\n\treturn values_dict\n\ndef _parse_non_cli_args(self, parser_name, cmd_values_dict):\n\t\"\"\"\n\tParses arguments for a sub-command or a list of sub-commands.\n\t:param parser_"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\tif not values_dict:\n\t\tvalues_dict={}\n\tfor key in self.arguments_types[parser_name]:\n\t\tif key not in values_dict:\n\t\t\tif self.arguments_types[parser_name][key] in (\"integer\", \"float\"):\n\t\t\t\tif self.arguments_defaults[parser_name][key] in (\"True\", \"True\", \"False\", \"0\", \"1\"):\n\t\t\t\t\tif self.arguments_defaults[parser_name][key].lower() in (\"true\", \"yes\"):\n\t\t\t\t\t\t"}, {"generated_text": "def _convert_non_cli_args(self, parser_name, values_dict):\n\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\t\n\tBy default all the values are strings.\n\t\n\t:param parser_name: The command name, e.g. main, virsh, ospd, etc\n\t:param values_dict: The dict of with arguments\n\t\"\"\"\n\tif \"output_file\" in values_dict:\n\t\tvalues_dict[\"output_file\"] = path(*values_dict[\"output_file\"])\n\t\n\tfor arg in self._non_cli_args:\n\t\tv = values_dict.get(arg, None)\n\t\tif type(v)!= str:\n\t\t\tif parser_name in self._argparse_name:\n\t\t\t\tvalues_dict[arg] = str(v)\n\t\t\telse:\n\t\t\t\traise TypeError(\"Expecting argument of type string\")\n\ndef _is_str(x):\n\t"}]}
{"task_id": "63060b1a73426c38ae68ad3e", "prompt": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_dir = os.path.join(plugin_dir, \"plugin.spec\")\n\t\n\t# Load plugin spec\n\tspec = yade.utils.read_yaml(plugin_dir)\n\t\n\t# Set the plugin as plugin instance to ensure\n\t# we load correctly in the plugins list\n\t#FIXME\n\t#plugin = yade.config.plugins.get(spec.name)\n\tplugin = eval(spec.name)()\n\t#plugin_func = yade.plugins.func_from_plugin(plugin)\n\t\n\t# Initial"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugins = get_plugins(plugin_dir)\n\t\t\n\tdef _clean_list(lst):\n\t\treturn sorted(lst, key=lambda x: (len(x), ord(x[0])))\n\t\n\tdef _make_key(lst):\n\t\tname = ''\n\t\tfor lst in lst:\n\t\t\tname += lst if name else lst\n\t\treturn name\n\t\t\n\treturn _make_key(map(lambda x: (x.name, x.author), plugins))\n\t\ndef check_plugin(plugin_dir):"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tret_dict = {}\n\tfor (plugin_name, plugin_path) in iter_plugin_list(plugin_dir):\n\t\tconfig = load_configs_from_path(plugin_path)\n\t\tconfig = config['plugin-properties']\n\t\tflattend_dict = config.flatten()\n\t\tret_dict[plugin_name] = flattend_dict\n\treturn ret_dict\n\ndef save_to_config(config, key, dest_dir):\n\tconfig_flatten = flatten_dict(config)\n\ttry:\n\t\tyaml"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_spec = None\n\twith open(plugin_dir + 'plugin.json') as file:\n\t\tplugin_spec = jsonpickle.decode(file)\n\t\n\treturn plugin_spec\n\ndef read_file(file_path):\n\t\"\"\"\n\t\n\t:param file_path:\n\t:return: raw (string) file content\n\t\"\"\"\n\n\twith open(file_path) as file:\n\t\treturn file.read()\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tif not exists(plugin_dir):\n\t\texit('Error: the plugin directory '+plugin_dir+' does not exist\\n')\n\tfmt_spec = parse_spec(plugin_dir)\n\treturn dict(zip(fmt_spec.keys(), fmt_spec.values()))\n\ndef validate_param(type, arg):\n\t\"\"\"\n\tValidates the argument type\n\t\n\t:param type: A python type\n\t:param arg: An argument given to the parameter\n\t:return: A string repreatnig the problem, and the argument\n"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_spec = dict()\n\tplugin_dir_name = plugin_dir.split(os.sep)\n\tplugin_dir_name.pop()\n\t\n\tfor i, plugin_dir_name_item in enumerate(plugin_dir_name):\n\t\tplugin_dir_item_path = plugin_dir_name_item + os.sep\n\t\tif os.path.isdir(plugin_dir_item_path):\n\t\t\tplugin_name = plugin_dir_name[i]\n\t\t\tplugin_obj = __class__(plugin_name, os"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_spec_file = os.path.join(plugin_dir, 'plugin.spec')\n\tif not os.path.exists(plugin_spec_file):\n\t\traise FileNotFoundError(\"Could not locate plugin.spec\")\n\tfile_contents = yaml.safe_load(open(plugin_spec_file))\n\n\t# Create the flattend dictionary\n\tflatten_dict = {'id':'','version':'', 'name':file_contents['name'], \\\n\t\t\t\t\t\t'path':plugin_dir, 'author':file_cont"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\t# Create the flat dictionary\n\td = dict()\n\t# Walk over all the modules and attributes\n\tfor root, directories, file_name in walk(plugin_dir):\n\t\t# If this is a directory\n\t\tif os.path.isdir(os.path.join(root, file_name)):\n\n\t\t\t# If this directory is the main directory of the plugin (which is not considered a plugin), skip\n\t\t\tif file_name == get_plugin_main_file_name(root):\n\t\t\t\tcontinue\n\n\t\t\t# If plugins are named in the form of plugins.conf,"}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_spec = parse_plugin_spec(plugin_dir + '/plugin.json')\n\tplugin_name = os.path.basename(plugin_dir)\n\tif plugin_spec is None:\n\t\tprint(\"Can't load plugin \"+plugin_name)\n\t\tsys.exit(0)\n\t\n\tplugin_properties = plugin_spec['Properties']\n\t\n\tflat_dict = {}\n\tfor attr_name, attr_values in plugin_properties.items():\n\t\tif attr_values is None:\n\t\t\tcontinue\n\t\tflat_dict["}, {"generated_text": "def get_plugin_spec_flatten_dict(plugin_dir):\n\"\"\"\n\tCreates a flat dict from the plugin spec\n\t\n\t:param plugin_dir: A path to the plugin's dir\n\t:return: A flatten dictionary contains the plugin's properties\n\t\"\"\"\n\tplugin_spec = dict()\n\tmodule_name, module_object = sys.modules[plugin_dir.split(\"modules/\")[1].split(\"/\")[0]].__name__.split(\".\")[0], sys.modules[plugin_dir.split(\"modules/\")[1].split(\"/\")[0]].__name__\n\tplugin_spec['name'] = module_object\n\tplugin_spec['type'] = plugin_dir.split(\"plugins/\")[1].split(\"/\")[0]\n\tplugin_spec['class_name'] ="}]}
{"task_id": "63060b1b73426c38ae68ad42", "prompt": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tfor var in(\n\t\t\t\"SENDGRID_USER_CONFIG_PATH\",\n#\t\t\t\"SENDER_CONFIG_PATH\"\n\t\t\t\"SENDER_USER_CONFIG_PATH\"\n\t\t\t):\n\t\tif os.environ.has_key(var):\n\t\t\tglobals()[var] = os.environ[var]\n\t\t\tcontinue\n\t\telse:\n\t\t\tglobals()[var] = self.install_path + \"/config\"\n###\n##    # For new code\n###    \tself.user_config_path = self"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\treturn \n\ndef init_config_files(self):\n\tprint \"init_config_files()\"\n\n\t# Check if the config files exist and exist.\n\tif not os.path.exists(self.config_file):\n\t\treturn\n\tprint \"init_config_files(), file %s exists\" % self.config_file\n\n\t# Set the config-path env var\n\tif os.getenv(\"KITPHONO_CONFIG_PATH\"):\n\t\treturn\n\tprint \"set system environment\"\n\tos.environ['KITPH"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\ttry:\n\t\tos.environ[self.ENV_CONFIG]\n\texcept KeyError:\n\t\tos.environ[self.ENV_CONFIG] = self._config_path()\n\telse:\n\t\tlogging.debug(\"config path was already defined\")\n\n\n# def _config_path():\n# \t\"\"\"\n# \tFind the path for the config directory and return it\n# \t\"\"\"\n# \tlogging.debug(\"looking for configuration directory\")\n# \thome = os.environ['HOME']\n# \tfor p"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif not \"Config\" in os.environ.keys():\n\t\tos.environ[\"Config\"] = os.path.join(\n\t\t\tos.path.expanduser(\"~\"),\n\t\t\t\".config\",\n\t\t\t\"dia\"\n\t\t)\n\n## Main\nif __name__ == \"__main__\":\n\tinject_config()\n\tdia = Dia(config_path = os.environ[\"Config\"])\n\tif dia.dia_version > \"0.60\":\n\t\tprint(\"Dia version is {}\".format(dia.d"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif os.getenv(self.CONFIG_PATH_KEY, False) == False:\n\t\tos.environ[self.CONFIG_PATH_KEY] = '%s/config' % (self.CORE_PATH)\n\ndef main():\t\n\"\"\"\n\tIt runs the application, and calls the parse function. \n\t\"\"\"\n\t# Inject the CONFIG_PATH_KEY environment variable, if necessary.\n\tInjectConfig()\n\t# The command-line arguments.\t\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\ttry:\n\t\tdel os.environ[\"CONFIGPATH\"]\n\texcept:\n\t\tpass\n\n\tconfig_file = os.path.join(self.parent.path, str(self.name) + '.ini')\n\tos.environ[\"CONFIGFILE\"] = config_file\n\n\tConfigParser.Interpolation.DEFAULT_INTERPOLATION = configparser.ExtendedInterpolation()\n\n\tself.data = {}\n\n\n\tself.load_config()\n\n#### ConfigManager(config)\nclass ConfigManager(object):\n\t\"\"\"\n\tConfigManager for"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif not have_path(self.config) or not have_path('.'):\n\t\tsys.exit('Invalid path to config file.')\n\n\nclass ProjectConfig(ConfigParser):\n\t\"\"\"\n\tRepresents the configuration object of a project which should be loaded by\n\tthe Project class with the inject_config function.\n\t\"\"\"\n\n\tdefault_option_name = '__DEFAULT__'\n\tdefault_option_value = ''\n\n\tdef __init__(self):\n\t\t\"\"\"\n\t\tInitialize the object.\n\t\t\"\"\"\n\n\t\tConfigParser.__init__(self"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif 'CONFIG' not in os.environ:\n\t\tos.environ['CONFIG'] = os.path.expanduser('~/.favcord')\n\tif 'CONFIG_FILENAME' not in os.environ:\n\t\tif 'CONFIG_SRC_DIR' in os.environ:\n\t\t\tos.environ['CONFIG_FILENAME'] = '{}.json'.format(os.path.join(\n\t\t\t\tos.environ['CONFIG_SRC_DIR'], 'config'))\n\t\telse:\n\t\t\t"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif not 'CONFIG' in os.environ:\n\t\tos.environ['CONFIG'] = self.cfg_path\n\n\t# If config file is not already in path, add it to path\n\tif not re.search('^%s$' % self.cfg_path,sys.path): \n\t\tsys.path.insert( 0, self.cfg_path )\n"}, {"generated_text": "def inject_config(self):\n\"\"\"\n\tSet the environment variable for config path, if it is undefined.\n\t\"\"\"\n\tif 'PATH' not in os.environ:\n\t  os.environ['PATH'] += \";%s/bin;\"%(os.path.dirname(os.path.abspath(__file__)))\n"}]}
{"task_id": "63060b1b73426c38ae68ad43", "prompt": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tself.spec = subparser.add_parser('spec')\n\tself.spec.add_argument('--path', type=FileType('r'), help='path to the spec')\n\t\n\tself.specal.add_parser(parser_specal.CLI_PACKAGE)\n\tself.specal.add_argument('PACKAGE', help='the package/version/spec to build')\n\t\n\tself.specal.add_parser(parser_specal.CLI_BUILD)\n\tself.specal.add_argument('path',"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tself._add_common_options(root_subparsers)"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tself.parser = root_subparsers\n\t\n\tself.parser.add_parser('spec',\n\t\thelp='specify a name for the specification',\n\t\tparents=[self.parser.rpn_config])\n\t\n\tself.parser.add_parser('-p',\n\t\thelp='print a specification',\n\t\tparent_parser=self.parser.rpn_config)\n"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tfrom. import cli\n\tfrom. import cim; cim.extend_cli(subparser); del cim\n\timport pykern.pkunit; pykern.pkunit.UnitTest.extend_cli(subparser); pykern.pkunit.pkunit.UnitTests.extend_cli(subparser); del pykern.pkunit\n\n\ndef extend_parser(self, parser):\n\t\"\"\"\n\tSets the subparser object and the parser's root parser.\n\t\"\"\"\n\timport pykern."}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tself.parse_options = subparser.add_parser('spec', help='Create and/or modify a spec', \n\t\t\t\t\t\t\t\t\t\t\t  description='Create specifications and add/remove/rename keys, '\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t 'modify settings, or change the value of the specs tag')\n\tself.parse_options.set_defaults(func=self.spec_options)\n\tparse_spec = self.parse_options.add_subparsers()\n\tspecs_cmd = parse_spec.add_parser('list', help='list all the specs')"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tparser = root_subparsers.add_parser('add', help='add user spec')\n\tparser.add_argument('spec_name')\n\tparser.add_argument('spec_id')\n\t\n\tparser = root_subparsers.add_parser('list', help='list spec names')\n\n\nclass SpecManager(object):\n\t\"\"\"\n\tThe SpecManager handles the global state of specs and handles user actions.\n\t\n\t:params base_dir: base directory of the application.\n\t\n\t:ivar spec_dir: directory to store specs"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tfrom.cli import spec_cli\n\tself.add_command(spec_cli)\n\n#------------------------------------------------------------------------------\n\n@extend_cli\nclass SpecUpdateCli(subparser.Subparser):\n\t\"\"\"\n\tCommand-line interface for spec update operations.\n\n\t\"\"\"\n\tdef __init__(self, version=\"1.0\", **kwargs):\n\t\t\"\"\"Constructor.\n\n\t\t:param version: program version.\n\t\t\"\"\"\n\t\tsuper(SpecUpdateCli, self).__init__(\"SpecUpdate\", version, **kwargs)\n\t\tself.add_"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tif self.get_all_scopes() is not None:\n\t\tall_scopes = set(self.get_all_scopes())\n\t\tall_scopes.add(ALL_SCOPES_CHOICE)\n\t\t\n\t\tsparse_scopes = filter(lambda scope:(scope, '') in all_scopes, all_scopes)\n\t\t\n\t\tfor (i, scope_tuple) in enumerate(sparse_scopes):\n\t\t\tname = self.get_scope_name(scope_tuple)\n\t\t\targs = scope_tuple"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\tassert isinstance(root_subparsers, ArgumentParser)\n\n\tspec = root_subparsers.add_parser(name='spec', description='Reads a json spec and prints its content for quick and dirty editing.', help='Reads a json spec and prints its content for quick and dirty edit.')\n\tspec.set_defaults(func=self.cmd_spec)\n\tself.spec = spec\n\ndef init_cli(root_subparsers):\n\t\"\"\"\n\tCalls the command cli entry point to start the program and attaches all the"}, {"generated_text": "def extend_cli(self, root_subparsers):\n\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\t\n\t:param subparser: the subparser object to extend.\n\t\"\"\"\n\t\n\tparser_id = \"id\"\n\tparser_name = \"name\"\n\t\n\tparser_filter = \"filter\"\n\tparser_filters = \"filters\"\n\t\n\tparser_target = \"target\"\n\tparser_target_group = \"target_group\"\n\t\n\tparser_target_id = \"id\"\n\tparser_target_name = \"name\"\n\tparser_target_filters = \"filters\"\n\t\n\t# id must appear first\n\t\n\tparser = root_subparsers.add_parser(root"}]}
{"task_id": "62ece4982e6aefcf4aabbd64", "prompt": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\t", "suggestions": [{"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\treturn path.replace(root, \"\", 1)\n\ndef read_file(path, root=None):\n\t\"\"\"Reads the specified file using utf-8 encoding\"\"\"\n\twith io.open(strip_root(path, top_root=root), \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n\t\ttry:\n\t\t\treturn f.read()\n\t\texcept UnicodeDecodeError:\n\t\t\traise Exception(\"This path does not seem to be encoded correctly: {0}\".format(path))\n\telse:\n\t\traise FileNotFoundError(\""}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tend_point = path.rfind(os.path.sep)\n\tassert path[:end_point] == root.rstrip(os.path.sep), 'Path \"{}\" doesn\\'t match root \"{}\"'\\\n\t\t.format(path, root)\n\treturn path[end_point + 1:]\n\n\ndef split_path(path):\n\t\"\"\"\n\tSplit a path into:\n\t\t\t- Path\n\t\t\t- OS path\n\t\t\t- Root\n\t\"\"\"\n\troot_path, rest = os.path.split(path.rstrip(os."}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\treturn path.lstrip(\"\" + root)\n\n\"\"\"\n\tTest for whether path is a symlink or not. Return\n\tTrue if path is a symlink\n\t\"\"\"\ndef is_symlink(path):\n\treturn os.path.islink(path)\n\n\"\"\"\n\tCreate a subdirectory for test cases\n\t\"\"\"\n\"\"\"\nDirectory to create the testcase folder inside of\n\"\"\"\ndirname_test = 'test_cases'\n\ndef set_dir():\n\tos.chdir(path_dir)\n\tos.mkdir("}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif path.startswith(root):\n\t\tresult = path[len(root):]\n\t\tif not result.startswith(\"/\"):\n\t\t\traise ValueError\n\t\treturn result\n\traise ValueError\n\n\ndef get_all_roots():\n\tresult = [ ]\n\troot = config.get(\"general\", \"root\")\n\tif root.endswith(\"/\"):\n\t\troot = root[:-1]\n\tresult.append(root)\n\tfor entry in config.get(\"general\", \"roots\").split(\";\"):\n\t\tresult.append(strip_"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif root:\n\t\tif path.startswith(root):\n\t\t\treturn path[len(root):]\n\telse:\n\t\traise Exception(\"Invalid root.\")\n\n\n\ndef is_image_extension(path, root):\n\t\"\"\"\n\tReturn True/False depending on if the filename in path\n\tis an image.\n\t\"\"\"\n\treturn (is_file_name_jpeg(path, root) or is_file_name_png(path, root)\n\t\t\tor is_file_name_gif(path, root) or is_file_name_"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif len(path.split(root)) <= 1:\n\t\troot_tup = (root,)\n\telif len(path.split(root)) > 1:\n\t\troot_tup = path.split(root)\n\t\tfor i in range(1, len(root_tup)):\n\t\t\tif not os.path.isdir(root_tup[0] + os.sep + ''.join((root_tup[1], ''))):\n\t\t\t\troot_tup = (root_tup[0] + os.sep + ''."}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif path.startswith(root):\n\t\troot_len = len(root)\n\t\tresult = path[root_len:]\n\telse:\n\t\traise RuntimeError(\"Not a relative path. %s isn't relative to %s.\" % (path, root))\n\t# remove trailing slash if present.\n\tif len(result) > 1 and result[len(result)-1] == \"/\":\n\t\treturn result[0:len(result)-1]\n\telse:\n\t\treturn result\n\n\ndef resolve_path(env, root, path):\n\t\"\"\"\n\t"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\ttry:\n\t\t\tret = path[len(root):].rstrip('/')\n\t\t\tif ret == '': raise IOError(\"Empty root\")\n\t\t\treturn ret\n\texcept:\n\t\t\terr = \"No root supplied.\"\n\t\t\treturn err\n\nroot = strip_root(path, '/root')\n\nclass File(object):\n\t\"\"\"\n\t\"\"\"\n\tdef __init__(self, path):\n\t\tif os.path.isdir(path):\n\t\t\traise IOError(\"Directory not allowed\")\n\t\tself.path = os.path.realpath(path)\n\n"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise Exception(\"Attempted to strip root from %s, but didn't find it.\"%(path,))\n\nclass ResourceNode(object):\n\t\"\"\"\n\tThis is an abstract class that represents a single resource in the system\n\t(e.g. an API key, file, etc).\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tself.parent = None\n\t\tself.childs = []\n\n\tdef parent(self):\n\t\treturn self.parent\n\n\tdef"}, {"generated_text": "def strip_root(path, root):\n\"\"\"\n\tRemove root from path, throw exception on failure.\n\t\"\"\"\n\tres = remove_root(path)\n\tif res and res.startswith(root):\n\t\t# path is fully inside root, everything OK.\n\t\treturn res\n\traise PathException('path is not inside root', path, root)\n\ndef get_path_depth(path):\n\t\"\"\"\n\tRetrieve depth of a path.\n\t\"\"\"\n\tif path.startswith('/'):\n\t\t# path is absolute, return len(path)\n\t\treturn len(path)\n\telse:\n\t\t# path is relative, get path as absolute, and\n"}]}
