76024224,Differences/Comparison between the results of scipy.fsolve and scipy.minimize,"<p>I'm a little confused between fsolve and minimize. There are 5 questions I'm looking to try and answer using the below setup, where I have an exact system of equations with 2 solutions.</p>
<p>These are the 5 questions (I restate them at the end of all the code):</p>
<ol>
<li>Why does basinhopping get different answers? If the same initial guesses are used, should you not always get the same output?</li>
<li>If one solution has residuals of 7e-15 and the other 2e-22, considering how small these numbers are, are both valid solutions?</li>
<li>How to get basinhopping to jump out of local minima? It is quite clear if initial conditions are used for solution 1, basinhopping can sometimes get stuck in solution 2 (question 1). However the reverse is not true, if initial conditions used are solution2, basinhopping can never jump out and find solution1. This is quite concerning considering the global minima is solution1, not solution2. So you would get erronous solutions depending on your initial conditions.</li>
<li>Considering fsolve is so heavily dependent on initial guesses (i.e. fsolve easily breaks if initial guesses are even slightly off from the exact solutions). Is minimize the best method always, even for exact systems?</li>
<li>Why are the residuals for fsolve and minimization different? I presume I may have set something wrong here, since they should be exactly the same.</li>
</ol>
<p>Let's say you have a problem</p>
<pre><code>solution1=13.4
solution2=600
solution3=7100
solution4=95600

def equations(initial):
    equation1=a*b/(1+b)
    equation2=a*b*c**2/(1+b*c)
    equation3=a*b*d**2/(1+b*d)
    equation4=a*b*c**2*d**2/(1+b*c*d)
</code></pre>
<p>Now this is an exact system. Therefore fsolve can be used to solve this</p>
<pre><code>from scipy.optimize import fsolve
import numpy as np

solution1=13.4
solution2=600
solution3=7100
solution4=95600

def equations(initial):
    a,b,c,d=initial
    equation1=a*b/(1+b)-solution1
    equation2=a*b*c**2/(1+b*c)-solution2
    equation3=a*b*d**2/(1+b*d)-solution3
    equation4=a*b*c**2*d**2/(1+b*c*d)-solution4
    return (equation1,equation2,equation3,equation4)

fsolve(equations(7,30,0.02,500))
</code></pre>
<p>Now from my understanding, minimize can also be used to solve this same system</p>
<pre><code>def equations2(initial):
    a,b,c,d=initial
    equation1=a*b/(1+b)-solution1
    equation2=a*b*c**2/(1+b*c)-solution2
    equation3=a*b*d**2/(1+b*d)-solution3
    equation4=a*b*c**2*d**2/(1+b*c*d)-solution4
    return np.sum(np.array([equation1,equation2,equation3,equation4])**2))
minimize(equations2,x0=(7,30,0.02,500),bounds=((0,np.inf),4)*4) #added bounds to this one
</code></pre>
<p>From my understanding, these should theoretically gave the same output</p>
<pre><code>using_fsolve=fsolve(equations(7,30,0.02,500))
using_minimize=minimize(equations2,x0=(7,30,0.02,500),bounds=((0,np.inf),4)*4)
print(using_fsolve[0],using_minimize.x)
print(sum(using_fsolve[1]['fvec']**2),using_minimize.fun) 
&gt;&gt;&gt; 
[7.23,3.09e1,2.78e-2,4.94e2] [7.2,3.08e1,2.76e-2,5e2]
2.11e-22 8.1e-3
</code></pre>
<p>It can be observed fsolve found the solution with very small residuals which is great. However, the minimizer actually failed. While it found those values, the status for it was fail. So I tried it again, using Nelder-Mead which I've often had success with.</p>
<pre><code>using_fsolve=fsolve(equations(7,30,0.02,500))
using_minimize=minimize(equations2,x0=(7,30,0.02,500),bounds=((0,np.inf),4)*4,method='Nelder-Mead',options={'maxiter':10000}) #had to increase iterations otherwise it would fail
print(using_fsolve[0],using_minimize.x)
print(sum(using_fsolve[1]['fvec']**2),using_minimize.fun) 
&gt;&gt;&gt; 
[7.23,3.09e1,2.78e-2,4.94e2] [7.23,3.09e1,2.78e-2,4.94e2]
2.11e-22 8.86e-12
</code></pre>
<p>This time it was able to get the exact solution, but for some reason the residual sum is higher in minimize than in fsolve.</p>
<p>Now the whole purpose behind this is because fsolve is highly dependent on the initial guess. I'd like to find the solution to this system of equations independent of what the initial guess is. To that end, I decided to use basinhopping with the previous setup to do this</p>
<pre><code>from scipy.optimize import basinhopping
using_fsolve=fsolve(equations(7,30,0.02,500))
using_minimize=basinhopping(equations2,x0=(7,30,0.02,500),minimizer_kwargs={'bounds':((0,np.inf),4)*4,'method':'Nelder-Mead','options':{'maxiter':100000'}})
print(using_fsolve[0],using_minimize.x)
print(sum(using_fsolve[1]['fvec']**2),using_minimize.fun) 
&gt;&gt;&gt; 
[7.23,3.09e1,2.78e-2,4.94e2] [7.23,3.09e1,2.78e-2,4.94e2]
2.11e-22 5.92e-13
</code></pre>
<p>It was again able to find the exact solution as fsolve, albeit higher residual. Now to test whether this is the true solution, or other solutions exist, I used basin hopping with random inputs</p>
<pre><code>using_fsolve=fsolve(equations(7,30,0.02,500))
using_minimize=basinhopping(equations2,x0=(70,300,0.002,5000),minimizer_kwargs={'bounds':((0,np.inf),4)*4,'method':'Nelder-Mead','options':{'maxiter':100000'}})
print(using_fsolve[0],using_minimize.x)
print(sum(using_fsolve[1]['fvec']**2),using_minimize.fun) 
&gt;&gt;&gt; 
[7.23,3.09e1,2.78e-2,4.94e2] [7.23,3.09e1,2.78e-2,4.94e2]
2.11e-22 5.92e-13
#run it again, same inputs
[7.23,3.09e1,2.78e-2,4.94e2] [13.15 128.08 0.30 56.83]
2.11e-22 1.24e-10
#run fsolve this time with the new solution of 13.15, 128.08, 0.3, 56.83
[13.15 128.08 0.30 56.83] [7.23,3.09e1,2.78e-2,4.94e2]
7.40e-15 4.39e-13
#run fsolve and basinhopping using the new solution
[13.15 128.08 0.30 56.83] [13.15 128.08 0.30 56.83]
7.40e-15 1.24e-10
#run again
[13.15 128.08 0.30 56.83] [13.15 128.08 0.30 56.83]
7.40e-15 1.24e-10
</code></pre>
<p>From here, I have 5 questions:</p>
<ol>
<li>Why does basinhopping get different answers? If the same initial guesses are used, should you not always get the same output?</li>
<li>If one solution has residuals of 7e-15 and the other 2e-22, considering how small these numbers are, are both valid solutions?</li>
<li>How to get basinhopping to jump out of local minima? It is quite clear if initial conditions are used for solution 1, basinhopping can sometimes get stuck in solution 2 (question 1). However the reverse is not true, if initial conditions used are solution2, basinhopping can never jump out and find solution1. This is quite concerning considering the global minima is solution1, not solution2. So you would get erronous solutions depending on your initial conditions.</li>
<li>Considering fsolve is so heavily dependent on initial guesses (i.e. fsolve easily breaks if initial guesses are even slightly off from the exact solutions). Is minimize the best method always, even for exact systems?</li>
<li>Why are the residuals for fsolve and minimization different? I presume I may have set something wrong here, since they should be exactly the same.</li>
</ol>
<p>Putting all the code together for a MVE</p>
<pre><code>from scipy.optimize import fsolve
from scipy.optimize import basinhopping
import numpy as np

solution1=13.4
solution2=600
solution3=7100
solution4=95600

def equations(initial):
    a,b,c,d=initial
    equation1=a*b/(1+b)-solution1
    equation2=a*b*c**2/(1+b*c)-solution2
    equation3=a*b*d**2/(1+b*d)-solution3
    equation4=a*b*c**2*d**2/(1+b*c*d)-solution4
    return (equation1,equation2,equation3,equation4)

def equations2(initial):
    a,b,c,d=initial
    equation1=a*b/(1+b)-solution1
    equation2=a*b*c**2/(1+b*c)-solution2
    equation3=a*b*d**2/(1+b*d)-solution3
    equation4=a*b*c**2*d**2/(1+b*c*d)-solution4
    return np.sum(np.array([equation1,equation2,equation3,equation4])**2))

using_fsolve=fsolve(equations(7,30,0.02,500))
using_minimize=minimize(equations2,x0=(7,30,0.02,500),bounds=((0,np.inf),4)*4,method='Nelder-Mead',options={'maxiter':10000}) 
#using_minimize=basinhopping(equations2,x0=(70,300,0.002,5000),minimizer_kwargs={'bounds':((0,np.inf),4)*4,'method':'Nelder-Mead','options':{'maxiter':100000'}})
print(using_fsolve[0],using_minimize.x)
print(sum(using_fsolve[1]['fvec']**2),using_minimize.fun)
</code></pre>
<p>Edit:</p>
<p>Just wanted to add an extra bit here, I have tried this exact problem in Mathematica suite using SOLVE. And that gives a range of solutions, and if ignoring all negative solutions, it very nicely provides both of the above solutions I found in python. So is there an option for fsolve to find all viable solutions and display them like in Mathematica (which doesn't take in an input of initial guesses, so I presume it runs a range)?</p>
",https://stackoverflow.com/questions/76024224/differences-comparison-between-the-results-of-scipy-fsolve-and-scipy-minimize,False,
76024202,Install in intelijin Jetbreank,"<p>'https://plugins.jetbrains.com/pluginManager/?id=aws.toolkit&amp;build=IU-231.8109.175&amp;uuid=1504231465a5296-ea28-4998-8d23-530acc9ebd8e': Connection reset</p>
<p>'https://plugins.jetbrains.com/pluginManager/?id=aws.toolkit&amp;build=IU-231.8109.175&amp;uuid=1504231465a5296-ea28-4998-8d23-530acc9ebd8e': Connection reset result</p>
",https://stackoverflow.com/questions/76024202/install-in-intelijin-jetbreank,False,
76024167,Creating a new column based on values of two other columns,"<p>This is my dataframe:</p>
<pre><code>df = pd.DataFrame({'a': [10, 20, 50], 'b': [5, -2, 20]})
</code></pre>
<p>And this is the output that I want:</p>
<pre><code>    a   b      c
0  10   5     105
1  20  -2     20.58
2  50  20     12.348
</code></pre>
<p>I have an initial value which is 1000. The first row of <code>c</code> is calculated like this:</p>
<p>(1000 * 0.1) * 1.05</p>
<p>Columns <code>a</code> and <code>b</code> are percent. That is 10 percent of 1000 is 100 and then I want to add 5 percent to 100 which gives me 105 for first row. Then for the next row of <code>c</code>, I want to calculate 20 percent of 105 which is 21 and then subtract 2 percent of it which is 20.58.</p>
<p>I have tried this code but it doesn't work:</p>
<pre><code>df['b'] = df['b'] / 100
df['c'] = (df.a / 100).cumprod() * 1000
df['c'] = df.c * (1 + df.b)
 
</code></pre>
<p>Feel free to edit the title. I want something more specific but couldn't figure it out.</p>
",https://stackoverflow.com/questions/76024167/creating-a-new-column-based-on-values-of-two-other-columns,True,76024214
76024153,Retrieve large objects with multiprocessing.Pipe,"<p>I'm current working in a RAM monitoring decorator:</p>
<pre><code>import time
import asyncio
import multiprocessing as mp

import psutil
import numpy as np


class MemoryMonitor:

    def __init__(
        self,
        threshold=115343360, # Bytes
        delay=1              # Seconds
    ):
        self.threshold = threshold
        self.delay = delay

    async def __memory_info(self, pid):
        memory = psutil.Process(pid).memory_info().rss
        print(f'Monitoring RAM: current value={memory}')
        if memory &gt; self.threshold:
            print(f'Memory threashold exceeded: thr={self.threshold}, current={memory}')
            return True
        return False
    
    async def __ram_monitoring(self, process):
        while process.is_alive():
            memory_error = await asyncio.gather(self.__memory_info(process.pid))
            if memory_error[0]:
                if process.is_alive():
                    process.kill()
                raise MemoryError(f'Memory exceeded threshold={self.threshold}')
            await asyncio.sleep(self.delay)

    def __call__(self, **monitoring_kwargs):
        def monitoring_decorator(func):
            def wrapper(*args, **kwargs):
                # Target function to collect retrived valued of decorated function
                def target(*args, **kwargs):
                    try:
                        cconn.send([func(*args, **kwargs), False])
                    except Exception as exc:
                        cconn.send([exc, True])

                pconn, cconn = mp.Pipe(duplex=False)

                # Call the function in a different process 
                # to be able to kill the process if memory is exceeded
                process = mp.Process(target=target, args=args, kwargs=kwargs)
                process.start()

                # Run the ram monitorin process in async mode
                asyncio.run(self.__ram_monitoring(process=process))

                # Collect the decorated function retrived valued
                value, error = pconn.recv()
                if error:
                    raise value
                return value
            return wrapper
        return monitoring_decorator


if __name__ == '__main__':
    memory_monitor = MemoryMonitor(threshold=2.5e+7)

    @memory_monitor()
    def test_function(n):
        for i in range(n):
            v = np.ones([i,i])
            time.sleep(0.1)
        return v

    v = test_function(100)
    print(v)
</code></pre>
<p>I'm using:</p>
<ol start=""0"">
<li>Linux operating system</li>
<li>Asynchronous functions to collect the RAM memory value with <code>psutil</code> library.</li>
<li><code>multiprocessing.Process</code> to run the decorated function</li>
<li>And <code>multiprocessing.Pipe</code> to collect the retrieved value of the decorated function</li>
</ol>
<p>The process works fine, it kills the process if the RAM memory is exceeded:</p>
<pre><code>Memory threashold exceeded: thr=25000000.0, current=25014272
Traceback (most recent call last):
  File &quot;main.py&quot;, line 98, in &lt;module&gt;
    v = test_function(500)
  File &quot;main.py&quot;, line 77, in wrapper
    asyncio.run(self.__ram_monitoring(process=process))
  File &quot;/usr/lib/python3.8/asyncio/runners.py&quot;, line 44, in run
    return loop.run_until_complete(main)
  File &quot;/usr/lib/python3.8/asyncio/base_events.py&quot;, line 616, in run_until_complete
    return future.result()
  File &quot;main.py&quot;, line 56, in __ram_monitoring
    raise MemoryError(f'Memory exceeded threshold={self.threshold}')
MemoryError: Memory exceeded threshold=25000000.0
</code></pre>
<p>And returns the value of the decorated function:</p>
<pre><code>Monitoring RAM: current value=22700032
[[1. 1. 1. 1.]
 [1. 1. 1. 1.]
 [1. 1. 1. 1.]
 [1. 1. 1. 1.]]
Main.py DONE
</code></pre>
<p>The problem is when I'm trying to recover a large value (like a <code>numpy.ones([100,100])</code>); the process gets stuck in <code>cconn.send(func(*args, **kwargs))</code> and never ends.</p>
<p>Question:</p>
<p>How can I recover large values with <code>multiprocessing.Pipe</code> or <code>multiprocessing.Queue</code>?  <a href=""https://medium.com/analytics-vidhya/using-numpy-efficiently-between-processes-1bee17dcb01"" rel=""nofollow noreferrer"">I read this post:</a>, he suggests <code>multiprocessing.Array</code> and <code>multiprocessing.Value</code>, however, I cannot always predict the type and shape of the returned value.</p>
<p>Or am I using a wrong approach (asynchronous functions + multiprocessing)?</p>
",https://stackoverflow.com/questions/76024153/retrieve-large-objects-with-multiprocessing-pipe,False,
76024137,Python Django Model.save() saved wrong value in column,"<p>I'm new in Django, and maybe question is dumb, but anyway...
I've problems with MyModel.save() saving in database, i'm trying to save some specific column which is used as filter in another part of my code... but this calue is not saving properly in Database as i expected from saving query...</p>
<p>models.py:</p>
<pre><code>class TemplateAdvert(models.Model):
&quot;&quot;&quot; Черновик &quot;&quot;&quot;
profile = models.ForeignKey(Profile, verbose_name='Профиль пользователя', on_delete=models.CASCADE, blank=True,
                            null=True)
category = models.ForeignKey(Category, verbose_name='Категория 1', on_delete=models.CASCADE, null=True,
                             blank=True)
category2 = models.ForeignKey(Category2, verbose_name='Категория 2', on_delete=models.CASCADE, null=True,
                              blank=True)
category3 = models.ForeignKey(Category3, verbose_name='Категория 3', on_delete=models.CASCADE, null=True,
                              blank=True)
category4 = models.ForeignKey(Category4, verbose_name='Категория 4', on_delete=models.CASCADE, null=True,
                              blank=True)
subject = models.CharField('Тема', max_length=120, null=True,
                           blank=True)
description = models.TextField('Объявление', max_length=10000, null=True,
                               blank=True)
canton = models.CharField('Кантон', choices=Advert.CANTON_CHOICES, default='Zürich', max_length=100)
city = models.ForeignKey(City, verbose_name='Город', on_delete=models.CASCADE, blank=True, null=True)
latlng_lat = models.DecimalField('Координаты latitude', max_digits=60, decimal_places=30, blank=True, null=True)
latlng_lng = models.DecimalField('Координаты longitude', max_digits=60, decimal_places=30, blank=True, null=True)
filters_vals = models.TextField('Значения фильтров', blank=True, null=True)
filters_vals_completed = models.BooleanField('Без фильтров', default=False)
images = models.ForeignKey(
    Gallery,
    verbose_name='Фото',
    blank=True,
    null=True,
    on_delete=models.SET_NULL
)
is_bold = models.BooleanField('Выделено жирным', default=False)
bold_days = models.PositiveIntegerField('Осталось дней выделения жирным', default=0)
price = models.DecimalField('Цена', max_digits=8, decimal_places=2, null=True,
                            blank=True)
exp_days = models.PositiveIntegerField('Осталось дней', default=0)
progress = models.PositiveIntegerField('Прогресс', default=0)
created = models.DateTimeField('Дата создания', auto_now_add=True)
country = models.CharField('ISO код страны', max_length=5, default='CH')
slug = models.SlugField('url', max_length=200)

def cat_path(self):
    ret = self.category.name
    if self.category2:
        ret += ' &gt; ' + self.category2.name
    if self.category3:
        ret += ' &gt; ' + self.category3.name
    return ret

def __str__(self):
    return '{} - {}'.format(self.profile.user.username, self.subject)

def save(self, *args, **kwargs):
    p = 0
    if self.category:
        p += 20
    if self.subject:
        p += 20
    if self.filters_vals_completed:
        p += 20
    if self.images:
        if self.images.complete:
            p += 20
    self.progress = p
    super().save(*args, **kwargs)

class Meta:
    verbose_name = 'Черновик'
    verbose_name_plural = 'Черновики'
</code></pre>
<p>views.py, here I'm trying to save specific column in database:</p>
<pre><code>    def profile_create_ad_template_save(request, username, step):
    if request.user.username == username:
        profile = Profile.objects.get(user=request.user)
        currentCountry = settings.COUNTRY

        if step == 1:
            cats_ids = []
            for k, v in request.POST.items():
                if k[0:3] == 'cat':
                    cats_ids.append(v)
            qs = {
                'category': None,
                'category2': None,
                'category3': None,
                'category4': None
            }
            if len(cats_ids) &gt;= 1:
                if len(Category.objects.filter(id=cats_ids[0], is_active=True)) == 1:
                    qs['category'] = Category.objects.get(id=cats_ids[0], is_active=True)
            if len(cats_ids) &gt;= 2:
                if len(Category2.objects.filter(id=cats_ids[1], is_active=True)) == 1:
                    qs['category2'] = Category2.objects.get(id=cats_ids[1], is_active=True)
            if len(cats_ids) &gt;= 3:
                if len(Category3.objects.filter(id=cats_ids[2], is_active=True)) == 1:
                    qs['category3'] = Category3.objects.get(id=cats_ids[2], is_active=True)
            if len(cats_ids) &gt;= 4:
                if len(Category4.objects.filter(id=cats_ids[3], is_active=True)) == 1:
                    qs['category4'] = Category4.objects.get(id=cats_ids[3], is_active=True)
            qs['profile'] = profile
            t_id = request.POST.get('template_id')
            if len(t_id):
                template = TemplateAdvert.objects.filter(id=int(t_id))
                template.update(**qs)
                template = template[0]
            else:
                more_than_20 = TemplateAdvert.objects.filter(profile=profile).order_by('-created')[19:]
                for item in more_than_20:
                    item.delete()
                template = TemplateAdvert.objects.create(**qs)
            city_filter = False
            use_map = False
            for k, v in qs.items():
                if 'category' in k:
                    if v:
                        city_filter = v.city_filter
                        use_map = v.use_map
            response = {
                'success': True,
                'template_id': template.id,
                'city_filter': city_filter,
                'use_map': use_map,
                'path_to_marker': f'{settings.STATIC_URL}img/marker.png',
                'hex_color': get_hex_from_profile(profile)
            }
#            response['cantons'] = [{'id': canton.id, 'name': canton.canton_name} for canton in Canton.objects.all().order_by('canton_name')]
            if city_filter:
#                response['cities'] = [{'id': city.id, 'name': city.name} for city in City.objects.filter(canton=_canton)]
                response['cities'] = [{'id': city.id, 'name': city.name} for city in City.objects.all().filter(country=currentCountry).order_by('name')]
#            response['cantons'] = sorted([canton[0] for canton in Advert.CANTON_CHOICES])
            response['cantons'] = sorted([canton.canton_name for canton in Canton.objects.all().filter(country=currentCountry).order_by('canton_name')])
            return JsonResponse(response)
        if step == 2:
            response = {'success': False}
            template_id = request.POST.get('template_id')
            if template_id:
                if len(TemplateAdvert.objects.filter(id=template_id)) == 1:
                    template = TemplateAdvert.objects.get(id=template_id)
                    template.subject = request.POST.get('subject')
                    template.description = request.POST.get('description')
                    if 'city' in request.POST.keys():
                        template.city = City.objects.get(id=request.POST.get('city'))
                    if 'latlng-lat' in request.POST.keys():
                        template.latlng_lat = request.POST.get('latlng-lat')
                        template.latlng_lng = request.POST.get('latlng-lng')
                    if 'canton' in request.POST.keys():
                        template.canton = request.POST.get('canton')
                    template.country = currentCountry
                    template.save()
                    all_main_params_select = []
                    all_main_params_other = []
                    all_additional_params = []
                    if template.category2:
                        all_main_params_select.extend(p2 for p2 in template.category2.params.filter(type='select'))
                        all_main_params_other.extend(
                            p2 for p2 in template.category2.params.exclude(type='bool').exclude(type='select'))
                        all_additional_params.extend(p2 for p2 in template.category2.params.filter(type='bool'))
                    if template.category3:
                        all_main_params_select.extend([p3 for p3 in template.category3.params.filter(type='select')])
                        all_main_params_other.extend(
                            [p3 for p3 in template.category3.params.exclude(type='bool').exclude(type='select')])
                        all_additional_params.extend([p3 for p3 in template.category3.params.filter(type='bool')])
                    if template.category4:
                        all_main_params_select.extend([p4 for p4 in template.category4.params.filter(type='select')])
                        all_main_params_other.extend(
                            [p4 for p4 in template.category4.params.exclude(type='bool').exclude(type='select')])
                        all_additional_params.extend([p4 for p4 in template.category4.params.filter(type='bool')])
                    if len(all_main_params_select):
                        response['main_params_select'] = [
                            {
                                'id': param.id,
                                'name': param.name,
                                'required': param.required,
                                'options': [
                                    {
                                        'id': param_item.id,
                                        'text': param_item.name
                                    }
                                    for param_item in ParamItem.objects.filter(param=param)
                                ]
                            }
                            for param in all_main_params_select
                        ]
                    if len(all_main_params_other):
                        response['main_params_other'] = [
                            {
                                'name': param.name,
                                'type': ('text' if param.type == 'str' else 'number'),
                                'id': param.id,
                                'required': param.required,
                            }
                            for param in all_main_params_other
                        ]
                    if len(all_additional_params):
                        response['additional_params'] = [
                            {
                                'id': param.id,
                                'name': param.name,
                                'checks': [
                                    {
                                        'id': param_item.id,
                                        'name': param_item.name
                                    }
                                    for param_item in ParamItem.objects.filter(param=param)
                                ]
                            }
                            for param in all_additional_params
                        ]
                    response['success'] = True
            return JsonResponse(response)
        if step == 3:
            response = {'success': False}
            template_id = request.POST.get('template_id')
            if template_id:
                if len(TemplateAdvert.objects.filter(id=template_id)) == 1:
                    template = TemplateAdvert.objects.get(id=template_id)
                    data = ''
                    bool_params = {}
                    exclude_items = ['csrfmiddlewaretoken', 'template_id']
                    required_params = []
                    if template.category2:
                        for param in template.category2.params.filter(required=True):
                            required_params.append(param.id)
                    if template.category3:
                        for param in template.category3.params.filter(required=True):
                            required_params.append(param.id)
                    if template.category4:
                        for param in template.category4.params.filter(required=True):
                            required_params.append(param.id)
                    required = []
                    for param in required_params:
                        if str(param) not in request.POST.keys():
                            required.append({'id': param})
                        elif not request.POST[str(param)]:
                            required.append({'id': param})
                    if not len(required):
                        for k, v in request.POST.items():
                            if v:
                                if len(v) &lt;= 12:
                                    if v == 'on':
                                        pid = k.split('_')[0]  # Param ID
                                        piid = k.split('_')[1]  # ParamItem ID
                                        if pid not in bool_params:
                                            bool_params[pid] = [piid]
                                        else:
                                            bool_params[pid].append(piid)
                                    elif k not in exclude_items:
                                        param = Param.objects.get(id=int(k))
                                        value = v
                                        if param.type == 'select':
                                            value = ParamItem.objects.get(id=int(value)).slug
                                        data += '\r\n' + param.type + ':' + param.slug + ':' + value
                        data = data[2:]
                        for pid, piid_mass in bool_params.items():
                            data += '\r\nbool:' + Param.objects.get(id=int(pid)).slug + ':'
                            for piid in piid_mass:
                                data += ParamItem.objects.get(id=int(piid)).slug + ', '
                            data = data[:-2]
                        template.filters_vals = data
                        template.filters_vals_completed = True
                        template.country = currentCountry
                        template.save()
                        response['success'] = True
                    else:
                        response['required'] = required
            return JsonResponse(response)
        if step == 4:
            response = {'success': False}
            template_id = request.POST.get('template_id')
            if template_id:
                if len(TemplateAdvert.objects.filter(id=template_id)) == 1:
                    template = TemplateAdvert.objects.get(id=template_id)
                    indexes = request.POST.get('indexes')
                    gal_name = '{}-template{}'.format(profile.user.username, template.id)
                    if Gallery.objects.filter(name=gal_name):
                        gal = Gallery.objects.get(name=gal_name)
                    else:
                        gal = Gallery.objects.create(name=gal_name, slug=gal_name, country=currentCountry)
                    for i in range(len(indexes)):
                        img_name = 'gallery{}-photo{}'.format(gal.id, indexes[i])
                        template.images.images.filter(slug=img_name).update(order=i+1)
                    template.images.images.all().update(is_main=False)
                    template.images.images.filter(order=1).update(is_main=True)
                    template.images.complete = True
                    template.images.save()
                    template.images.country = currentCountry
                    template.save()
                    response['success'] = True
            return JsonResponse(response)
        if step == 5:
            response = {'success': False}
            template_id = request.POST.get('template_id')
            if template_id:
                if len(TemplateAdvert.objects.filter(id=template_id)) == 1:
                    template = TemplateAdvert.objects.get(id=template_id)
                    is_bold = True if request.POST.get('is_bold') == 'on' else False
                    bold_days = 0
                    if is_bold:
                        bold_days = request.POST.get('bold_days')
                    if 'price' in request.POST.keys():
                        template.price = request.POST.get('price')
                    price_per_meter_filter = False
                    last_cat = template.category
                    if template.category2:
                        price_per_meter_filter = template.category2.price_per_meter_filter
                        last_cat = template.category2
                    if template.category3:
                        price_per_meter_filter = template.category2.price_per_meter_filter
                        last_cat = template.category3
                    if template.category4:
                        price_per_meter_filter = template.category4.price_per_meter_filter
                        last_cat = template.category4
                    if price_per_meter_filter and template.price:
                        for line in template.filters_vals.split('\r\n'):
                            if 'meters2' in line:
                                meters_square = float(line.split('meters2:')[1])
                                template.filters_vals += '\r\nint:price-per-meters2:' + str(round(float(template.price)/meters_square, 2))
                    exp_days = request.POST.get('exp_days')
                    template.is_bold = is_bold
                    template.bold_days = bold_days
                    template.exp_days = exp_days
                    template.country = currentCountry
                    template.save()
                    response = {
                        'success': True,
                        'enough_points': (True if int(template.exp_days) * last_cat.points_days_price + int(template.bold_days) * last_cat.points_bold_days_price &lt;= profile.points else False),
                        'days': exp_days,
                        'bold_days': bold_days
                    }
            return JsonResponse(response)
    return error403(request, 'No Permissions')
</code></pre>
<p>but when I'm doing <code>SELECT country from table</code> I got different value from what I'm saving in views.py... also this value is different from SQL query which I've got from logging queryies in Django console...</p>
<p>logged query:</p>
<pre><code>(0.005) UPDATE &quot;my_app_templateadvert&quot; SET &quot;profile_id&quot; = 60, &quot;category_id&quot; = 14, &quot;category2_id&quot; = 213, &quot;category3_id&quot; = 2120, &quot;category4_id&quot; = NULL, &quot;subject&quot; = 'test comment LV', &quot;description&quot; = 'test comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LV', &quot;canton&quot; = 'testCountryCanton', &quot;city_id&quot; = 163, &quot;latlng_lat&quot; = NULL, &quot;latlng_lng&quot; = NULL, &quot;filters_vals&quot; = NULL, &quot;filters_vals_completed&quot; = false, &quot;images_id&quot; = NULL, &quot;is_bold&quot; = false, &quot;bold_days&quot; = 0, &quot;price&quot; = NULL, &quot;exp_days&quot; = 0, &quot;progress&quot; = 40, &quot;created&quot; = '2023-04-15T16:54:06.747890+00:00'::timestamptz, &quot;country&quot; = 'LV', &quot;slug&quot; = '' WHERE &quot;my_app_templateadvert&quot;.&quot;id&quot; = 145; args=(60, 14, 213, 2120, 'test comment LV', 'test comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LVtest comment LV', 'testCountryCanton', 163, False, False, 0, 0, 40, datetime.datetime(2023, 4, 15, 16, 54, 6, 747890, tzinfo=datetime.timezone.utc), 'LV', '', 145); alias=default
</code></pre>
<p>so can anyone help me with this mistake?</p>
<p>P.S Django 4.1.1
P.P.S database is same and it's set in settings.py</p>
",https://stackoverflow.com/questions/76024137/python-django-model-save-saved-wrong-value-in-column,False,
76024129,ERROR in using beautiful Soup to extract SEC filling header information with returning NONE,"<p>When I'm trying to use beautiful soup to get the header information from the SEC I'm getting a a return value of none. The tag I'm using is <code>&quot;&lt;SEC-HEADER&gt;&quot;</code>.</p>
<p>Any idea on how to get this code work correctly?</p>
<p>My code is:</p>
<pre><code>
import requests
from bs4 import BeautifulSoup

# URL of the SEC filing
url = 'https://www.sec.gov/Archives/edgar/data/1166036/000110465904027382/0001104659-04-027382.txt'

# Fetch the SEC filing data
response = requests.get(url)
content = response.content

# Parse the content with Beautiful Soup
soup = BeautifulSoup(content, 'html.parser')

# Find the SEC-HEADER tag using a custom function
def find_sec_header(tag):
    return tag.name == 'sec-header'
sec_header = soup.find(find_sec_header)

# Extract the header data
if sec_header:
    header_data = sec_header.get_text()
    print(header_data)
else:
    print(&quot;SEC-HEADER not found in the document.&quot;)

</code></pre>
<p>From the website link in the ode you can see their is header information. Copy of some of the text is:</p>
<pre class=""lang-none prettyprint-override""><code>-----BEGIN PRIVACY-ENHANCED MESSAGE-----
Proc-Type: 2001,MIC-CLEAR
Originator-Name: webmaster@www.sec.gov
Originator-Key-Asymmetric:
 MFgwCgYEVQgBAQICAf8DSgAwRwJAW2sNKK9AVtBzYZmr6aGjlWyK3XmZv3dTINen
 TWSM7vrzLADbmYQaionwg5sDW3P6oaM5D3tdezXMm7z1T+B+twIDAQAB
MIC-Info: RSA-MD5,RSA,
 Qv2jQwkNcHK0jmh1uw9JmsapJjhSCceQJLdEQsrmZh5YPKB2tMIC07tAARLtOJNv
 RF+yO70MOHHbXjXQABtM/g==

&lt;SEC-DOCUMENT&gt;0001104659-04-027382.txt : 20040913
&lt;SEC-HEADER&gt;0001104659-04-027382.hdr.sgml : 20040913
&lt;ACCEPTANCE-DATETIME&gt;20040913074905
ACCESSION NUMBER:       0001104659-04-027382
CONFORMED SUBMISSION TYPE:  8-K/A
PUBLIC DOCUMENT COUNT:      7
CONFORMED PERIOD OF REPORT: 20040730
ITEM INFORMATION:       Completion of Acquisition or Disposition of Assets
ITEM INFORMATION:       Financial Statements and Exhibits
FILED AS OF DATE:       20040913
DATE AS OF CHANGE:      20040913

FILER:

    COMPANY DATA:   
        COMPANY CONFORMED NAME:         MARKWEST ENERGY PARTNERS L P
        CENTRAL INDEX KEY:          0001166036
        STANDARD INDUSTRIAL CLASSIFICATION: CRUDE PETROLEUM &amp; NATURAL GAS [1311]
        IRS NUMBER:             270005456
        FISCAL YEAR END:            1231

    FILING VALUES:
        FORM TYPE:      8-K/A
        SEC ACT:        1934 Act
        SEC FILE NUMBER:    001-31239
        FILM NUMBER:        041026639

    BUSINESS ADDRESS:   
        STREET 1:       155 INVERNESS DR WEST
        STREET 2:       STE 200
        CITY:           ENGLEWOOD
        STATE:          CO
        ZIP:            80112
        BUSINESS PHONE:     303-925-9275

    MAIL ADDRESS:   
        STREET 1:       155 INVERNESS DR WEST
        STREET 2:       STE 200
        CITY:           ENGLEWOOD
        STATE:          CO
        ZIP:            80112
&lt;/SEC-HEADER&gt;
&lt;DOCUMENT&gt;
&lt;TYPE&gt;8-K/A
&lt;SEQUENCE&gt;1
&lt;FILENAME&gt;a04-10341_18ka.htm
&lt;DESCRIPTION&gt;8-K/A
&lt;TEXT&gt;
&lt;html&gt;

&lt;head&gt;



&lt;/head&gt;
</code></pre>
",https://stackoverflow.com/questions/76024129/error-in-using-beautiful-soup-to-extract-sec-filling-header-information-with-ret,True,
76024124,Python &quot;import as&quot; changing global namespace,"<p>I'm automatically translating some perl code to python with my <a href=""https://github.com/snoopyjc/pythonizer"" rel=""nofollow noreferrer"">pythonizer</a> tool.  The perl code uses the Date::Manip package, which has a submodule Date::Manip::Date.  I use a <code>Date</code> variable (which I defined in <code>builtins</code>) in my generated code to hold the namespace where I define <code>Date.Manip</code>.  When I import the Date.Manip.Date package, even though I import it as <code>_Date_Manip_Date</code>, the import still sets a global <code>Date</code> variable preventing access to my <code>Date</code> namespace.  I'm looking for ideas on how to avoid this issue.  I can't change the name of the submodule.  Here is code that reproduces the problem (note I'm using <code>Date</code> as a string in this test case and not a namespace):</p>
<pre><code># date_issue.py:
import sys
sys.path[0:0] = '.'
import Date.Manip as _Date_Manip

# Date/Manip/__init__.py:
Date = '2023-04-15'
import Date.Manip.Date as _Date_Manip_Date
assert Date == '2023-04-15', f&quot;Date got changed to {Date}&quot;

# Date/Manip/Date.py (empty file)
</code></pre>
<p>I get this error on the assert:</p>
<pre><code>AssertionError: Date got changed to &lt;module 'Date.Manip.Date' from '/mnt/c/pythonizer/play/./Date/Manip/Date.py'&gt;
</code></pre>
<p>I'm thinking of maybe saving the value of <code>Date</code> before the import and restoring it afterwards, and only doing this if there is a module name being imported that is the same name as any of the parent namespaces, but do you have any better ideas?</p>
",https://stackoverflow.com/questions/76024124/python-import-as-changing-global-namespace,True,
76024118,Mutiple Regression for Multiple dependent variables by using stats model,"<p>I have one dataframe having multiple independent variables (say ind_var['X1','X2','X3','X4']) and multiple data frames having different number of dependent variables (say dep1[&quot;y3,'y4;..'y10']). I have 100 of such data frames for dependent variables where column names and count of dependent variables are not fixed. I want to run regression for each dependent variable (preferably using the stats model) and save the intercept, beta ,p value and adj R2 in a dataframe.</p>
<p>any help would be highly appreciated</p>
<p>Currently i am doing it one at a time as below:
model=sm.OLS(Y1,ind_var).fit()</p>
",https://stackoverflow.com/questions/76024118/mutiple-regression-for-multiple-dependent-variables-by-using-stats-model,False,
76024117,Get loss and accuracy graphic based on saved model,"<p>I forgot to save the history during training (I just call the <code>model.fit()</code> directly without save the return into variable), luckily I saved the model every epoch into <code>unet_model.h5</code>.</p>
<p>So how do I show loss and accuracy based on saved model?
You need to know that I'm using train and test generator.</p>
<pre><code>#@markdown 6. Mendefinisikan generator data untuk *training* dan *testing*.
# Ini dilakukan karena data yang digunakan adalah data gambar agar dapat masuk ke neuron.

def train_generator(images_path, labels_path, batch_size, image_size):
  image_paths = [os.path.join(images_path, filename) for filename in os.listdir(images_path)]
  label_paths = [os.path.join(labels_path, filename) for filename in os.listdir(labels_path)]
  assert len(image_paths) == len(label_paths)

  while True:
    indices = np.random.choice(len(image_paths), batch_size)
    batch_images = []
    batch_labels = []

    for i in indices:
      image_path = image_paths[i]
      label_path = label_paths[i]

      image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
      image = cv2.resize(image, image_size)
      image = image.astype(np.float32) / 255.

      label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)
      label = cv2.resize(label, image_size)
      label = label.astype(np.float32) / 255.
      label[label &gt; 0.5] = 1.0
      label[label &lt;= 0.5] = 0.0

      batch_images.append(np.expand_dims(image, axis=2))
      batch_labels.append(np.expand_dims(label, axis=2))

    yield np.array(batch_images), np.array(batch_labels)

def test_generator(images_path, labels_path, image_size):
  image_paths = sorted([os.path.join(images_path, filename) for filename in os.listdir(images_path)])
  label_paths = sorted([os.path.join(labels_path, filename) for filename in os.listdir(labels_path)])
  assert len(image_paths) == len(label_paths)

  images = []
  labels = []
  for i in range(len(image_paths)):
    image_path = image_paths[i]
    label_path = label_paths[i]

    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    image = cv2.resize(image, image_size)
    image = image.astype(np.float32) / 255.

    label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)
    label = cv2.resize(label, image_size)
    label = label.astype(np.float32) / 255.
    label[label &gt; 0.5] = 1.0
    label[label &lt;= 0.5] = 0.0

    images.append(np.expand_dims(image, axis=2))
    labels.append(np.expand_dims(label, axis=2))

  return np.array(images), np.array(labels)
</code></pre>
<p>So how do I show the loss and accuracy graphic of training/testing?</p>
<p>ChatGPT answer give me answer to train few epochs from saved model then show the graph which it's useless since it didn't give the real graphic start from epoch 0.</p>
",https://stackoverflow.com/questions/76024117/get-loss-and-accuracy-graphic-based-on-saved-model,False,
76024112,How to change the order of USSD menu based on user frequently used; using k-clustering python or other methods,"<p>I want to use machine learning methods to order USSD menus based on users frequently USSD menu number
using python</p>
<p>i try to use to create a USSD and based on the k-clustering i want to classify how many frequently used menu number but i cant combine the two projects</p>
",https://stackoverflow.com/questions/76024112/how-to-change-the-order-of-ussd-menu-based-on-user-frequently-used-using-k-clus,False,
76024008,Can I describe function params in PyDantic?,"<p>I want to describe function params in PyDantic, here example what I mean</p>
<pre class=""lang-py prettyprint-override""><code>def abc(a: int, b: float, c:str):
    s=a+b
    return f'{a}+{b}={s}, {c}'
print(abc(a=2,b=4,c='asdsd'))
</code></pre>
<p>and I want to get something like this</p>
<pre class=""lang-py prettyprint-override""><code>class Abc(BaseModel):
    a:int
    b:float
    c:str
def abc(Abc):
    s=a+b
    return f'{a}+{b}={s}, {c}'
print(abc(a=2,b=4,c='asdsd'))
</code></pre>
<p>i.e. I want to extract parameters from Pydantic and use it in function</p>
<p>UPD:
here example with a little more detailed I have a basic class and function &quot;insert&quot; take each time different params, as in the example above, I would like that when I inherit from this class, the function parameters are redefined</p>
<pre class=""lang-py prettyprint-override""><code>class DaoBase:
    model = None
    @classmethod
    async def insert(cls, **data):
        async with async_session_maker() as session:
            query = insert(cls.model).values(**data).returning(cls.model.id)  # type: ignore
            result = await session.execute(query)
            await session.commit()
            return result
</code></pre>
",https://stackoverflow.com/questions/76024008/can-i-describe-function-params-in-pydantic,False,
76024007,How to perform AES gcm encryption using Python PYKCS11,"<p>I am trying to simulate an AES-GCM encryption using soft-HSM and Python PyKCS11 library.
according to Oasis documentation. C_EncryptFinal needs to be called after C_Encrypt step to get Ciphertext and tag. I am not sure whether the library mechanism calls the EncryptFinal automatically, cause the ciphertext i get doesn't include the tag.</p>
<pre><code>    import PyKCS11
    from PyKCS11 import LowLevel
    from PyKCS11 import PyKCS11Lib
    import os
    from PyKCS11 import *
    
    pkcs11 = PyKCS11Lib()
    pkcs11.load('C:\\SoftHSM2\\lib\\softhsm2-x64.dll')
    
    slot = pkcs11.getSlotList(tokenPresent=True)[0]
    # print(pkcs11.getSlotInfo(slot))
    # print(slot)
    
    session = pkcs11.openSession(slot, PyKCS11.LowLevel.CKF_SERIAL_SESSION | PyKCS11.LowLevel.CKF_RW_SESSION)
    
    session.login('Simple10!')
    
    
    def generatesessionkey():
        AES_TEMPLATE = [
            (LowLevel.CKA_CLASS, LowLevel.CKO_SECRET_KEY),
            (LowLevel.CKA_KEY_TYPE, LowLevel.CKK_AES),
            (LowLevel.CKA_LABEL, &quot;aes-256key234234234&quot;),
            (LowLevel.CKA_VALUE_LEN, &quot;32&quot;),
            (LowLevel.CKA_TOKEN, CK_TRUE)
        ]
        key = session.generateKey(AES_TEMPLATE, PyKCS11.MechanismAESGENERATEKEY)
        return key
    
    
    # print(session.findObjects([(LowLevel.CKA_CLASS,LowLevel.CKO_SECRET_KEY)]))
    
    
    encryption_key = session.findObjects([(LowLevel.CKA_LABEL, &quot;aes-256key234234234&quot;)])[0]
     
    
    def AES_encryption():
        Nonce = session.generateRandom(96 // 8)
        M = PyKCS11.AES_GCM_Mechanism(iv=Nonce, aad=b'test', tagBits=16)
        ciphertext = session.encrypt(encryption_key, b'Hello World', M)
        return bytes(ciphertext)
AES_encryption()
</code></pre>
",https://stackoverflow.com/questions/76024007/how-to-perform-aes-gcm-encryption-using-python-pykcs11,False,
76023998,Create an Azure Keyvault with Python,"<p>I'm trying to create a keyvault with Python by running the code from <a href=""https://github.com/Azure-Samples/key-vault-python-manage"" rel=""nofollow noreferrer"">here</a>:</p>
<pre><code>import os
import json
from azure.common.credentials import ServicePrincipalCredentials
from azure.mgmt.keyvault import KeyVaultManagementClient
from azure.mgmt.resource.resources import ResourceManagementClient
from haikunator import Haikunator

haikunator = Haikunator()

WEST_US = &quot;westus&quot;
GROUP_NAME = &quot;azure-sample-group&quot;
KV_NAME = haikunator.haikunate()
# The object ID of the User or Application for access policies. Find this number in the portal
OBJECT_ID = &quot;401e9294-xxxx-xxxx-xxxx-xxxx&quot;

# Manage resources and resource groups - create, update and delete a resource group,
# deploy a solution into a resource group, export an ARM template. Create, read, update
# and delete a resource
#
# This script expects that the following environment vars are set:
#
# AZURE_TENANT_ID: with your Azure Active Directory tenant id or domain
# AZURE_CLIENT_ID: with your Azure Active Directory Application Client ID
# AZURE_CLIENT_SECRET: with your Azure Active Directory Application Secret
# AZURE_SUBSCRIPTION_ID: with your Azure Subscription Id
#


def run_example():
    &quot;&quot;&quot;Resource Group management example.&quot;&quot;&quot;
    #
    # Create the Resource Manager Client with an Application (service principal) token provider
    #
    subscription_id = os.environ[&quot;AZURE_SUBSCRIPTION_ID&quot;]

    credentials = ServicePrincipalCredentials(
        client_id=os.environ[&quot;AZURE_CLIENT_ID&quot;],
        secret=os.environ[&quot;AZURE_CLIENT_SECRET&quot;],
        tenant=os.environ[&quot;AZURE_TENANT_ID&quot;],
    )
    kv_client = KeyVaultManagementClient(credentials, subscription_id)
    resource_client = ResourceManagementClient(credentials, subscription_id)

    # You MIGHT need to add KeyVault as a valid provider for these credentials
    # If so, this operation has to be done only once for each credentials
    resource_client.providers.register(&quot;Microsoft.KeyVault&quot;)

    # Create Resource group
    print(&quot;\nCreate Resource Group&quot;)
    resource_group_params = {&quot;location&quot;: WEST_US}
    print_item(
        resource_client.resource_groups.create_or_update(
            GROUP_NAME, resource_group_params
        )
    )

    # Create a vault
    print(&quot;\nCreate a vault&quot;)
    vault = kv_client.vaults.create_or_update(
        GROUP_NAME,
        KV_NAME,
        {
            &quot;location&quot;: WEST_US,
            &quot;properties&quot;: {
                &quot;sku&quot;: {&quot;name&quot;: &quot;standard&quot;},
                &quot;tenant_id&quot;: os.environ[&quot;AZURE_TENANT_ID&quot;],
                &quot;access_policies&quot;: [
                    {
                        &quot;tenant_id&quot;: os.environ[&quot;AZURE_TENANT_ID&quot;],
                        &quot;object_id&quot;: OBJECT_ID,
                        &quot;permissions&quot;: {&quot;keys&quot;: [&quot;all&quot;], &quot;secrets&quot;: [&quot;all&quot;]},
                    }
                ],
            },
        },
    )
    print_item(vault)

    # List the Key vaults
    print(&quot;\nList KeyVault&quot;)
    for vault in kv_client.vaults.list():
        print_item(vault)

    # Delete Resource group and everything in it
    print(&quot;\nDelete Resource Group&quot;)
    delete_async_operation = resource_client.resource_groups.delete(GROUP_NAME)
    delete_async_operation.wait()
    print(&quot;\nDeleted: {}&quot;.format(GROUP_NAME))


def print_item(group):
    &quot;&quot;&quot;Print an instance.&quot;&quot;&quot;
    print(&quot;\tName: {}&quot;.format(group.name))
    print(&quot;\tId: {}&quot;.format(group.id))
    print(&quot;\tLocation: {}&quot;.format(group.location))
    print(&quot;\tTags: {}&quot;.format(group.tags))


if __name__ == &quot;__main__&quot;:
    run_example()


</code></pre>
<p>After receiving comments, I assigned the <code>Contribute</code>role to the service principal.</p>
<p>My only modification to the code was substituting the object id with the one from my service principal (see image below):</p>
<p><a href=""https://i.stack.imgur.com/Q22Sc.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q22Sc.jpg"" alt=""enter image description here"" /></a></p>
<p>I'm receiving following error:
<a href=""https://i.stack.imgur.com/i3ZWL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i3ZWL.png"" alt=""enter image description here"" /></a></p>
<p><strong>EDIT:</strong>
Accepted @sridev's answer because I solved the issue by running the sample code which was linked in the comments</p>
<p>Here's my working code, based on <a href=""https://github.com/Azure-Samples/azure-samples-python-management/tree/main/samples/keyvault"" rel=""nofollow noreferrer"">this GitHub repo</a>, assuming that the Azure environment variables are set properly:</p>
<pre><code>import os

from azure.identity import DefaultAzureCredential
from azure.mgmt.keyvault import KeyVaultManagementClient
from azure.mgmt.resource import ResourceManagementClient


def main():
    TENANT_ID = os.environ.get(&quot;AZURE_TENANT_ID&quot;, None)
    SUBSCRIPTION_ID = os.environ.get(&quot;AZURE_SUBSCRIPTION_ID&quot;, None)
    GROUP_NAME = &quot;resource_group_name&quot;
    VAULT = &quot;vault_name&quot;
    LOCATION = &quot;azure_location_eg_westus&quot;
    OBJECT_ID = &quot;service_principal_object_id&quot;

    # Create client
    # Other authentication approaches: https://pypi.org/project/azure-identity/
    resource_client = ResourceManagementClient(
        credential=DefaultAzureCredential(), subscription_id=SUBSCRIPTION_ID
    )
    keyvault_client = KeyVaultManagementClient(
        credential=DefaultAzureCredential(), subscription_id=SUBSCRIPTION_ID
    )

    # Create resource group
    resource_client.resource_groups.create_or_update(GROUP_NAME, {&quot;location&quot;: LOCATION})

    # Create vault
    vault = keyvault_client.vaults.begin_create_or_update(
        GROUP_NAME,
        VAULT,
        {
            &quot;location&quot;: LOCATION,
            &quot;properties&quot;: {
                &quot;tenant_id&quot;: TENANT_ID,
                &quot;sku&quot;: {&quot;family&quot;: &quot;A&quot;, &quot;name&quot;: &quot;standard&quot;},
                &quot;access_policies&quot;: [
                    {
                        &quot;tenant_id&quot;: TENANT_ID,
                        &quot;object_id&quot;: OBJECT_ID,
                        &quot;permissions&quot;: {
                            &quot;keys&quot;: [
                                &quot;encrypt&quot;,
                                &quot;decrypt&quot;,
                                &quot;wrapKey&quot;,
                                &quot;unwrapKey&quot;,
                                &quot;sign&quot;,
                                &quot;verify&quot;,
                                &quot;get&quot;,
                                &quot;list&quot;,
                                &quot;create&quot;,
                                &quot;update&quot;,
                                &quot;import&quot;,
                                &quot;delete&quot;,
                                &quot;backup&quot;,
                                &quot;restore&quot;,
                                &quot;recover&quot;,
                                &quot;purge&quot;,
                            ],
                            &quot;secrets&quot;: [
                                &quot;get&quot;,
                                &quot;list&quot;,
                                &quot;set&quot;,
                                &quot;delete&quot;,
                                &quot;backup&quot;,
                                &quot;restore&quot;,
                                &quot;recover&quot;,
                                &quot;purge&quot;,
                            ],
                        },
                    }
                ],
                &quot;enabled_for_deployment&quot;: True,
                &quot;enabled_for_disk_encryption&quot;: True,
                &quot;enabled_for_template_deployment&quot;: True,
            },
        },
    ).result()
    print(&quot;Create vault:\n{}&quot;.format(vault))

    # Get vault
    vault = keyvault_client.vaults.get(GROUP_NAME, VAULT)
    print(&quot;Get vault:\n{}&quot;.format(vault))

    # Update vault
    vault = keyvault_client.vaults.update(
        GROUP_NAME, VAULT, {&quot;tags&quot;: {&quot;category&quot;: &quot;Marketing&quot;}}
    )
    print(&quot;Update vault:\n{}&quot;.format(vault))

    # Delete vault
    keyvault_client.vaults.delete(GROUP_NAME, VAULT)
    print(&quot;Delete vault.\n&quot;)

    # Purge a deleted vault
    keyvault_client.vaults.begin_purge_deleted(VAULT, LOCATION).result()
    print(&quot;Purge a deleted vault.\n&quot;)

    # Delete Group
    resource_client.resource_groups.begin_delete(GROUP_NAME).result()


if __name__ == &quot;__main__&quot;:
    main()

</code></pre>
",https://stackoverflow.com/questions/76023998/create-an-azure-keyvault-with-python,True,76027040
76023983,Script raises &#39;TypeError: Credentials.__init__() got an unexpected keyword argument &#39;auth_method&#39;,"<p>I am writing script to send email messages.</p>
<p>Here is the main file code:</p>
<pre class=""lang-py prettyprint-override""><code>import config
from exchangelib import Credentials, Account, Message, Mailbox
from exchangelib.errors import UnauthorizedError, TransportError

credentials = Credentials(
    username=config.exchange_username,
    password=config.exchange_password,
    auth_method='ntlm',
)
account = Account(
    primary_smtp_address=config.exchange_email,
    credentials=credentials,
    autodiscover=True,
)
</code></pre>
<p>here is the config file code:</p>
<pre class=""lang-py prettyprint-override""><code>token = &quot;6666666666:GGGGvbe-cHjjDjuUtRggHtGfFFfF&quot;
exchange_username = 'Asdfgh'
exchange_password = 'AkjhHhgHGhV'
exchange_email = 'Asdfgh@example.com'
exchange_server = 'https://some.mail.gov/owa/'
</code></pre>
<p>I found on the Internet that the error occurs in the <code>Credentials(username=config.exchange_username, password=config.exchange_password, auth_method='ntlm')</code> line, since there is no <code>auth_method</code> parameter in the Credentials constructor in the exchangelib library. Instead, the <code>auth_type</code> parameter is used. Changed, but the error occurs again already with the description of <code>auth_type</code></p>
<p>If I remove <code>auth_method='ntlm'</code> from the code, it gives an error &quot;Authorization error on the Exchange server: No compatible auth type was reported by server&quot;</p>
",https://stackoverflow.com/questions/76023983/script-raises-typeerror-credentials-init-got-an-unexpected-keyword-argum,False,
76023945,Converting from Flask to Django,"<p>What is Django's equivalent of the data type flask.wrappers.Response?</p>
<pre><code>if request.method == &quot;GET&quot;:
    data = open(&quot;print.txt&quot;,&quot;r&quot;)        
    response_r = app.response_class(
        response=data,
        status=200,
        mimetype='text/plain'
    )
</code></pre>
<p>As shown in the code above, response_r's datatype() is flask.wrappers.Response and I want to use this code in Django (not flask).</p>
",https://stackoverflow.com/questions/76023945/converting-from-flask-to-django,True,
76023938,Creating a new column that shows percent change based on values of another column,"<p>This is my dataframe:</p>
<pre><code>df = pd.DataFrame({'a': [10, 20, 50]})
</code></pre>
<p>And this is the output that I want:</p>
<pre><code>    a      b      
0  10    100
1  20    20  
2  50    10
</code></pre>
<p>I have an initial value which is 1000. Then I want to create column <code>b</code>. The first row of <code>b</code> is 10 percent of 1000 which is 100. The second row is 20 percent of 100 that we got previously. And the last row is 50 percent of 20.</p>
<p>I have tried this code but it doesn't work:</p>
<pre><code>df = df.reset_index(drop=True)
df.loc[0, 'b'] = 1000 * (df.a.iloc[0] / 100)
df['b'] = (df.a / 100) * df.b.shift(1)
</code></pre>
",https://stackoverflow.com/questions/76023938/creating-a-new-column-that-shows-percent-change-based-on-values-of-another-colum,True,76023955
76023933,"FUNCTION_INVOCATION_FAILED Error when deploying django application to vercel, but on localhost everything works","<p>I connect freedb remote mysql database, it is works on localhost, but not on vercel.
Displayed this error in vercel logs</p>
<blockquote>
<p>Unknown application error occurred
Runtime.Unknown</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/XneC8.png"" rel=""nofollow noreferrer"">And this is Local Js Console errors,</a> but I think that the server cannot fail to start because of this</p>
<p>this is my first time uploading django to versel and i can't imagine what could be causing this error. I would really appreciate your help, thanks in advance</p>
<p>I changed the database, because already vercel not suported sqlite database.Connect the remote mysql database.<a href=""https://i.stack.imgur.com/o6sHe.png"" rel=""nofollow noreferrer"">If it works, it should show a screen like this</a></p>
",https://stackoverflow.com/questions/76023933/function-invocation-failed-error-when-deploying-django-application-to-vercel-bu,False,
76023916,"Layer &quot;decoder&quot; expects 1 input(s), but it received 3 input tensors","<p>I'm trying to train an autoencoder (and actually the fit seems to go correctly). Then I want to test my models:</p>
<pre><code>encoded_imgs = encoder.predict(images[:10])
decoded_imgs = decoder.predict(encoded_imgs)
</code></pre>
<p>where images is an array of images (224,224) and the latent vector is equal to 1024. I'd expect encoded_imgs to be 10x1024, but instead it is 3x10x24, which results in the error of the title when I perform the decoder.predict. Why the result of the encoder has that shape?</p>
<p>I'll add the structure of both encoder and decoder, while the predict uses the standard training.py library</p>
<pre><code>latent_dim = 1024
encoder_inputs = Input(shape=(224, 224))
x = layers.Reshape((224, 224, 1))(encoder_inputs) # add a batch dimension
x = layers.Conv2D(32, 3, activation=&quot;relu&quot;, strides=2, padding=&quot;same&quot;)(x)
x = layers.MaxPool2D()(x)
x = layers.Conv2D(64, 3, activation=&quot;relu&quot;, strides=2, padding=&quot;same&quot;)(x)
x = layers.MaxPool2D()(x)
x = layers.Conv2D(128, 3, activation=&quot;relu&quot;, strides=2, padding=&quot;same&quot;)(x)
x = layers.Flatten()(x)
x = layers.Dense(4096, activation=&quot;relu&quot;)(x)
z_mean = layers.Dense(latent_dim, name=&quot;z_mean&quot;)(x)
z_log_var = layers.Dense(latent_dim, name=&quot;z_log_var&quot;)(x)
z = Sampling()([z_mean, z_log_var])
encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=&quot;encoder&quot;)

latent_inputs = Input(shape=(latent_dim,))
x = layers.Dense(7 * 7 * 64, activation=&quot;relu&quot;)(latent_inputs)
x = layers.Reshape((7, 7, 64))(x)
x = layers.Conv2DTranspose(128, 3, activation=&quot;relu&quot;, strides=2, padding=&quot;same&quot;)(x)
x = layers.UpSampling2D(size=(2, 2))(x)
x = layers.Conv2DTranspose(64, 3, activation=&quot;relu&quot;, strides=2, padding=&quot;same&quot;)(x)
x = layers.UpSampling2D(size=(2, 2))(x)
x = layers.Conv2DTranspose(32, 3, activation=&quot;relu&quot;, strides=2, padding=&quot;same&quot;)(x)
x = layers.Conv2DTranspose(1, 3, activation=&quot;sigmoid&quot;, padding=&quot;same&quot;)(x)
decoder_outputs = layers.Reshape((224, 224))(x)
decoder = Model(latent_inputs, decoder_outputs, name=&quot;decoder&quot;)
</code></pre>
<p>If you think that some additional information is required to answer, tell me and I'll add it.</p>
",https://stackoverflow.com/questions/76023916/layer-decoder-expects-1-inputs-but-it-received-3-input-tensors,True,76023972
76023889,&quot;numpy&quot; is not accessed Pylance,"<p>After importing numpy, I get an error &quot;No module named 'numpy'&quot;.</p>
<p>vs code also tell me &quot;&quot;numpy&quot; is not accessed&quot;</p>
<blockquote>
<p>import numpy
ModuleNotFoundError: No module named 'numpy'</p>
</blockquote>
<p>This isn't because I don't have numpy installed. I've installed and used it in the same directory I'm using at the moment.</p>
<p>I've already tried to install numpy multiple times in an infinite amount of ways, but none resolve the issue.</p>
<p>I've come across solutions that say to select the python interpreter, but none work.</p>
<p>I should add, not all modules have this issue.</p>
<p>What can I do to be able to use numpy in vs code?</p>
",https://stackoverflow.com/questions/76023889/numpy-is-not-accessed-pylance,False,
76023880,Returning a random value from some column from Google Sheets only returns the first character,"<pre><code>def search_promo_name2(
    self,
    data: List[List[Union[str, bool]]],
    search_col: int = 1,
    promo_col: int = 2
) -&gt; int:
    googlesheet_client: pygsheets.client.Client = self._get_googlesheet_client()
    wks: pygsheets.Spreadsheet = self._get_googlesheet2_by_url(googlesheet_client)
    values = wks.get_col(promo_col, include_empty=False)
    if not values:
        return -1
    promo = random.choice(values)

    return promo
</code></pre>
<p>in column</p>
<pre><code>BLLR48IQR9
CLL45G3WMP
WLL31HGJ64
TLLJ6A4FCJ
ELLFYF5B14
</code></pre>
<p>but it's returning only one ,first letter like B or E,C and etc</p>
<p>I am just new in Python(3 month) and in pygsheets too,i have no idea how to return a full string from cell ,like <code>BLLR48IQR9</code></p>
<pre><code>get_col(col, returnas='matrix', include_tailing_empty=True, **kwargs)[source]
Returns a list of all values in column col.
</code></pre>
<p>but it's returning only one ,first letter like B or E,C and etc</p>
",https://stackoverflow.com/questions/76023880/returning-a-random-value-from-some-column-from-google-sheets-only-returns-the-fi,False,
76023858,automate saving info from a dictionary in an excel table on a daily basis,"<p>I have a dictionary with 10 keys – the keys remain the same but the values are updated each day.</p>
<p>How can I save the daily changes in the value while preserving the previous ones?</p>
<p>try csv in pycharm but every time i run the program it replaces the old data</p>
",https://stackoverflow.com/questions/76023858/automate-saving-info-from-a-dictionary-in-an-excel-table-on-a-daily-basis,False,
76023852,How i can avoid writing this 120 times?,"<p>im currently coding a rainfall model in python and am at the stage where i need to average the monthly rainfalls over the 10 years, seperated by year.</p>
<p>how can i avoid writing this code 120 times?</p>
<p>i really cannot find a way to do it, ive tried writing a function but the one i wrote required me to have a list for every month of every year - so would still need 120 times.</p>
<p>below is the code i wrote for 1, 1 - being january of the first year:</p>
<pre><code>seasonal_rainfall = []
january_rainfall_amount =[]


for i in range(len(rainfall)):
    if rainfall.loc[i, &quot;Year Tag&quot;] ==1 and rainfall.loc[i, &quot;Month Tag&quot;] ==1:
        daily_amount = rainfall.loc[i, &quot;Amount (mm)&quot;]
        january_rainfall_amount.append(daily_amount)
    else:
        pass
month_amount = sum(january_rainfall_amount)
</code></pre>
<p>the full code needed for this to run is below - some of it could probably be made a lot better but i am very new to python:</p>
<pre><code>import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
import openpyxl
import tempfile
import xlsxwriter

#pww = float(input(&quot;PWW value:\n&quot;))
pww = 0.7
#pdd = float(input(&quot;PDD Value: \n&quot;))
pdd = 0.6
pwd = float(1 - pww)
pdw = float(1 - pdd)
#onelam = float(input(&quot;1 / lambda value:\n&quot;))
onelam = 3.1
lda = 1/onelam



rainfall = pd.DataFrame({
    &quot;Day&quot;: range(1, 3651),
    &quot;Random 1&quot;: np.random.rand(3650),
    &quot;Random 2&quot;: np.random.rand(3650)
})

state = [1]

for i in range(1, len(rainfall)):
    prev_state = state[-1]
    random_value = rainfall.loc[i, &quot;Random 2&quot;]

    if prev_state &gt; 0:
        new_state = 1 if random_value &lt; pww else 0
    else:
        new_state = 1 if random_value &lt; pdw else 0

    state.append(new_state)


rainfall[&quot;State&quot;] = state

amounts = []

for i in range(0, len(rainfall)):
    random = rainfall.loc[i, &quot;Random 1&quot;]
    one_random = 1 - random

    if state[i] &gt; 0:
        amount = (-abs(1) * math.log(one_random))/lda
    else: amount = 0

    amounts.append(amount)

rainfall[&quot;Amount (mm)&quot;] = amounts


total_rainfall = rainfall[&quot;Amount (mm)&quot;].sum()

january = []
february =[]
march =[]
april =[]
may =[]
june =[]
july =[]
august =[]
september =[]
october =[]
november =[]
december =[]

for i in range(1,29):
    february.append(2)

for i in range(1,32):
    january.append(1)
    march.append(3)
    may.append(5)
    july.append(7)
    august.append(8)
    october.append(10)
    december.append(12)
    
for i in range(1,31):
    april.append(4)
    june.append(6)
    september.append(9)
    november.append(11)

year = january + february + march + april + may + june + july + august + september + october + november + december
ten_years_monthly = 10 * year
rainfall[&quot;Month Tag&quot;] = ten_years_monthly

year1 =[]
year2=[]
year3=[]
year4=[]
year5=[]
year6=[]
year7=[]
year8=[]
year9=[]
year10=[]

for i in range(0,364):
    year1.append(1)
for i in range(364,730):
    year2.append(2)
for i in range(730, 1095):
    year3.append(3)
for i in range(1095, 1460):
    year4.append(4)
for i in range(1460, 1825):
    year5.append(5)
for i in range(1825,2190):
    year6.append(6)
for i in range(2190,2555):
    year7.append(7)
for i in range(2555, 2920):
    year8.append(8)
for i in range(2920, 3285):
    year9.append(9)
for i in range(3285, 3650):
    year10.append(10)

ten_years_yearly = year1 + year2 +year3+year4+year5+year6+year7+year8+year9+year10
rainfall[&quot;Year Tag&quot;] = ten_years_yearly

seasonal_rainfall = []
january_rainfall_amount =[]


for i in range(len(rainfall)):
    if rainfall.loc[i, &quot;Year Tag&quot;] ==1 and rainfall.loc[i, &quot;Month Tag&quot;] ==1:
        daily_amount = rainfall.loc[i, &quot;Amount (mm)&quot;]
        january_rainfall_amount.append(daily_amount)
    else:
        pass
month_amount = sum(january_rainfall_amount)
print(month_amount)
</code></pre>
<p>Thanks in advance for any help.</p>
",https://stackoverflow.com/questions/76023852/how-i-can-avoid-writing-this-120-times,True,76024374
76023809,Using a list in pytholog,"<p>I'm trying to define a knowledge base where a disease has a list of symptoms. Here's a snippet:</p>
<pre><code>import pytholog as pl
diseases_kb = pl.KnowledgeBase()
diseases_kb([
     &quot;disease(diabetes, [frequent_urination, extreme_thirst])&quot;,
     &quot;disease(pneumonia, [cough, fever, shortness_of_breath])&quot;,
])
</code></pre>
<p>However, when I query this:</p>
<pre><code>print(diseases_kb.query(pl.Expr(&quot;disease(diabetes, Symptoms)&quot;)))
</code></pre>
<p>it just returns <code>['No']</code>.</p>
<p>It looks like, lists are not being recognized by pytholog. I tried using (), list(), whatever syntax that I hope would work but nothing works. The library removes '(' and ')', and splits the queries by ',' which I think is the reason why it's not recognizing lists. Is there a workaround for this?</p>
",https://stackoverflow.com/questions/76023809/using-a-list-in-pytholog,False,
76023799,Operate on one dataframe based on the condition in another dataframe,"<p>I have two dataframes: one for days &amp; another for distance. First dataframe contains difference in days about when an activity is performed at a location &amp; second dataframe contains distance between different locations. Both the dataframes are in the form of matrices of about 60,000*60,0000 records. Please see below for an example with 5 locations:</p>
<p>Days difference Matrix</p>
<pre><code>    location 1  location 2  location 3  location 4  location 5
location 1  0       3              -5                0             2
location 2  3       0               9                2            -3
location 3  -5      9               0                0            -5
location 4  0       2               0                0             6
location 5  2      -3              -5                6             0
</code></pre>
<p>Distance matrix</p>
<pre><code>    location 1  location 2  location 3  location 4  location 5
location 1  0      64              46                69             68
location 2  64      0              31               100             36
location 3  46     31               0                78             41                                     
location 4  69    100              78                 0             53
location 5  68     36              41                53              0

</code></pre>
<p>I want to compare the days difference between each location pair from dataframe 1 and if the difference is negative, I want to change the distance value for that pair in dataframe 2 to 1,000.</p>
<p>For example, since the record (3,1) in  dataframe 1 is negative, I want to change the record (3,1) in dataframe 2 to 1,000.</p>
<p>Index names and column names are same in both the dataframes.</p>
<p>I am currently doing it using two nested for loops. This is what I am doing:</p>
<pre><code>for i in range(len(days_difference)):
    for j in range(len(days_difference.columns)):
        if days_difference.iloc[i,j] &gt; 0 :
            continue
        else:
                df_dist_matrix.iloc[i,j] = 1000
</code></pre>
<p>What is a better/more efficient way to do this.</p>
",https://stackoverflow.com/questions/76023799/operate-on-one-dataframe-based-on-the-condition-in-another-dataframe,True,76023845
76023785,How can I sync Raspberry Pi audio output with LED string?,"<p>I'm working on a project for school, it's basically a voice assistant running in a Python script on Raspberry Pi 4B, and I was wondering if there's a way to sync an LED strip to the audio output of the board: in other words, the higher the output volume is in a specific moment, the more LEDs should be powered on, in order to follow the vocal pattern of the assistant's response.</p>
<p>The whole project runs in a python script, and uses Microsoft Azure speech services as Text-to-speech, so the synthesizer generates an audio file that's then played, could it be easier to do using it?</p>
",https://stackoverflow.com/questions/76023785/how-can-i-sync-raspberry-pi-audio-output-with-led-string,False,
76023771,Why doesn&#39;t the position of the object after a loop iteration get updated in a Pygame script?,"<p>This is a test script for the navigation of a square block.</p>
<pre><code>import pygame as pg
import sys

pg.init()

FPS = 3
FramePerSec = pg.time.Clock()

# Predefined colors
BLACK = (0, 0, 0)
BLUE = (0, 0, 255)
WHITE = (255, 255, 255)
GREEN = (0, 255, 0)

SCREEN_W = 900
SCREEN_H = 600

DISPLAYSURF = pg.display.set_mode((SCREEN_W, SCREEN_H))
DISPLAYSURF.fill(WHITE)
pg.display.set_caption(&quot;Control&quot;)

x = 0
y = 0
testSq = pg.rect.Rect(x, y, 15, 15)

while True:
    eset = pg.event.get()
    for event in eset:
        if event.type == pg.QUIT:
            pg.quit()
            sys.exit()
        if event.type == pg.KEYDOWN:
            if event.key == pg.K_SPACE:
                pg.quit()
                sys.exit()
            if event.key == pg.K_UP:
                y-=1
            if event.key == pg.K_DOWN:
                y+=1
            if event.key == pg.K_LEFT:
                x-=1
            if event.key == pg.K_RIGHT:
                x+=1
    print(x,y)
    pg.draw.rect(DISPLAYSURF, BLACK, testSq)
    pg.display.update()
    FramePerSec.tick(FPS)
</code></pre>
<p>Though the coordinates (x, y) does indeed get updated, it is not reflected on the screen after each iteration of the main <code>while</code> loop.</p>
<p>Does it have something to do with the pygame draw.rect() function here?</p>
",https://stackoverflow.com/questions/76023771/why-doesnt-the-position-of-the-object-after-a-loop-iteration-get-updated-in-a-p,False,
76023766,"MLRunPreconditionFailedError: 412 Client Error, &quot;API is waiting for migrations to be triggered&quot;","<p>I got this error:</p>
<pre><code>MLRunPreconditionFailedError: 412 Client Error: Precondition Failed for url: http://localhost:8080/api/v1/projects: Failed creating project tutorial-&lt;name&gt; details: 
MLRunPreconditionFailedError('API is waiting for migrations to be triggered. Send POST request to [/api/operations/migrations](https://file+.vscode-resource.vscode-cdn.net/api/operations/migrations) to trigger it')
</code></pre>
<p>when I called this source code:</p>
<pre><code>import mlrun
...
mlrun.set_env_from_file(envFile)
project = mlrun.get_or_create_project(&quot;test&quot;, &quot;./&quot;, user_project=True)
</code></pre>
<p>I used MLRun CE 1.3.0 in Desktop Docker. Did you solve the issue?</p>
",https://stackoverflow.com/questions/76023766/mlrunpreconditionfailederror-412-client-error-api-is-waiting-for-migrations-t,True,76080956
76023764,Text files exploration with Python,"<p>I have been trying to solve the problem of searching and extracting information from a text file for a long time.</p>
<p>I need to get the data of &quot;Fully diluted earnings per common share&quot; for &quot;1993&quot; from text file &quot;text.txt&quot; with Python?</p>
<p>I will be glad of any help...</p>
<p>text.txt:
<code>&lt;TABLE&gt; &lt;CAPTION&gt; Fiscal Years Ended September 30, September 24, September 25, 1994 1993 1992 &lt;S&gt; &lt;C&gt; &lt;C&gt; &lt;C&gt; Primary Earnings Per Share Earnings Net income applicable to common stock $310,178 $ 86,589 $530,373 Shares Weighted average number of common shares outstanding 117,808 117,096 119,198 Adjustment for dilutive effect of outstanding stock options 927 2,029 3,292 Weighted average number of common and common equivalent shares used for primary earnings per share 118,735 119,125 122,490 Primary earnings per common share $2.61 $0.73 $4.33 Fully Diluted Earnings Per Share Earnings Net income applicable to common stock $310,178 $ 86,589 $530,373 Shares Weighted average number of common shares outstanding 117,808 117,096 119,198 Adjustment for dilutive effect of outstanding stock options 1,002 2,174 3,388 Weighted average number of common and common equivalent shares used for fully diluted earnings per share 118,810 119,270 122,586 Fully diluted earnings per common share $2.61 $0.73 $4.33 &lt;/TABLE&gt;</code></p>
<p>When calling
<code>print(df['Fully diluted earnings per common share'].loc['1993'])</code></p>
<p>I would like to get '$0.73'</p>
<p>And '$2.61' when calling
<code>print(df['Fully diluted earnings per common share'].loc['1994'])</code></p>
",https://stackoverflow.com/questions/76023764/text-files-exploration-with-python,False,
76023760,Write a Python function that accepts a string and calculates the number of upper case letters and lower case letters,"<p>I am trying to write a python code that accepts a string and calculates the number of upper case letters and lower case letters through function.</p>
<pre><code>`#`why this code is running perfect`
def up_low(s):
d={&quot;upper&quot;:0, &quot;lower&quot;:0}
for c in s:
if c.isupper():
d\[&quot;upper&quot;\]+=1
elif c.islower():
d\[&quot;lower&quot;\]+=1
else:
pass
print(&quot;Original String : &quot;, s)
print(&quot;No. of Upper case characters : &quot;, d\[&quot;upper&quot;\])
print(&quot;No. of Lower case Characters : &quot;, d\[&quot;lower&quot;\])
</code></pre>
<pre><code>#but this is not running
def up_low(s):
upper=\[0\]
lower=\[0\]
for i in s:
if i.isupper():
upper+=1
elif i.islower():
lower+=1
else:
pass
print(s)
print('No. of upper case letter is: {}'.format(upper))
print('No. of lower case letter is: {}'.format(lower))
</code></pre>
<p>this is showing TypeError: 'int' object is not iterable</p>
<pre><code>



I am expecting an answer
like this
No. of Upper case characters :  4
No. of Lower case Characters :  33
</code></pre>
",https://stackoverflow.com/questions/76023760/write-a-python-function-that-accepts-a-string-and-calculates-the-number-of-upper,False,
76023724,what will be the python function in django framework to take the attendance of customer in mess website according using there alloted number?,"<p>what will be the view.py function for my django project?</p>
",https://stackoverflow.com/questions/76023724/what-will-be-the-python-function-in-django-framework-to-take-the-attendance-of-c,False,
76023706,Need help in creating a CLI app with poetry,"<p>i've created a cli app in python using typer but instead of calling <code>py main.py</code> i want to call the program by using the command <code>starfish</code>. In past i've created and published cli apps using poetry packaging but this time i am facing issues.</p>
<p><strong>pyproject.toml</strong></p>
<pre class=""lang-ini prettyprint-override""><code>[tool.poetry]
name = &quot;starfish&quot;
version = &quot;0.1.0&quot;
description = &quot;A python based File Transfer Protocol(FTP) program.&quot;
authors = [&quot;777advait&quot;]
license = &quot;MIT License&quot;
readme = &quot;README.md&quot;

[tool.poetry.dependencies]
python = &quot;^3.9&quot;
rich = &quot;^13.3.4&quot;
questionary = &quot;^1.10.0&quot;
bcrypt = &quot;^4.0.1&quot;
typer = &quot;^0.7.0&quot;
scapy = &quot;^2.5.0&quot;

[tool.poetry.scripts]
starfish=&quot;starfish.main:run&quot;

[build-system]
requires = [&quot;poetry-core&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;
</code></pre>
<p>following image shows the issue that im facing
<a href=""https://i.stack.imgur.com/GoB5f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GoB5f.png"" alt=""issue"" /></a></p>
",https://stackoverflow.com/questions/76023706/need-help-in-creating-a-cli-app-with-poetry,False,
76023703,Pandas summation of neighboring rows (rolling sum),"<p>I would like to know is there any quick method to sum up the besides row of an dataframe. I can do that with python for loop but it's slow, so I would like to know is there any method to do the same thing while running faster.</p>
<p>Here's a code example of what I'm trying to do.</p>
<pre><code>import pandas as pd

dictionary = {
    'In':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
}

df = pd.DataFrame(dictionary)

forecast = 1
temp = []
for i in range(len(df[&quot;1&quot;])):
    temp = temp + [sum(df[&quot;In&quot;][i - forecast : i + forecast + 1])]

df[&quot;Out&quot;] = temp

df[&quot;Out&quot;][:forecast] = None
df[&quot;Out&quot;][-forecast:] = None

print(df)


#Output:
    In   Out
0    1   NaN
1    2   6.0     #(1+2+3)
2    3   9.0     #(2+3+4)
3    4  12.0      ...
4    5  15.0
5    6  18.0
6    7  21.0
7    8  24.0
8    9  27.0
9   10  30.0
10  11  33.0
11  12  36.0
12  13  39.0
13  14  42.0
14  15   NaN
</code></pre>
",https://stackoverflow.com/questions/76023703/pandas-summation-of-neighboring-rows-rolling-sum,True,76023725
76023698,Python reading old script code that is no longer in script,"<p>I am in the testing and debugging phase of a pretty large program and am having an odd issue. Had an simple issue that was overlooked with the path to a file being incorrect. So fixed that in the code and made sure it was correct in all scripts. But everytime i run the script it comes back with the same error stating that on line 17 FileNotFoundError. well ive played with this to the point that there isnt a line 17 even in the errored out script. Talking to a buddy weve come to the conclussion that some how it must be cached or something because even removing all text from the script and saving it it still does this. I have deleted the <em>pycache</em> stuff in the project directory but not sure where to go from this point.</p>
<p>Deleted <em>pycache</em> files. Didnt help.
Deleted all text in script and ran it. Shouldve come back with nothing but still came back with incorrect file or directory error.</p>
",https://stackoverflow.com/questions/76023698/python-reading-old-script-code-that-is-no-longer-in-script,False,
76023682,Implementing the State Pattern in Practice,"<p>I have a use case that involves a physical mailbox with packages.</p>
<p>My app should have three states:</p>
<ul>
<li>Unlock</li>
<li>Lock</li>
<li>Payment</li>
</ul>
<p>Anyone can perform the Unlock action if the mailbox is empty. However, if it's not empty, only the owner or the person who received the shared item can unlock it. Locking can be performed by both persons. Additionally, the other person should pay for the item, otherwise, they won't be able to use it. The item is free to use for the first hour, after which it becomes payable. We have 100 different mailboxes.</p>
<p>The initial implementation was done without using the state pattern, but some change requests propagated through all APIs.</p>
<p>I'm considering whether it would be wise to use the state/strategy pattern to address this.</p>
",https://stackoverflow.com/questions/76023682/implementing-the-state-pattern-in-practice,False,
76023652,Trouble with assigning colored categories on a broken bar chart,"<p>Why am I getting such extended final colors (green) parts to these bars?</p>
<p>Each GUI box should take two comma-separated integers for start and duration of that category. The numbers should get a little larger for each category, so that the colors don't overlap.   This can be entered in all the rows of the GUI, one pair of numbers in each box:  1,9 10,19 20,29 30,39 40,49. Any help would be greatly appreciated.</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
import tkinter as tk

# Define the data as a dictionary
data = {
    'x_values': [],
    'y_values': [(6.0, 2), (8.5, 2), (12.5, 2), (15.0, 2), (19.0, 2)]
}

# Define a list of colors and categories for the bars
colors = ('tab:red', 'tab:orange', 'tab:purple', 'tab:blue', 'tab:green')
categories = ('Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5')
jobs = ('Job1', 'Job2', 'Job3', 'Job4', 'Job5')

# Create a GUI interface to get user input for x_values
def get_x_values():
    global data
    x_values = []
    for i in range(len(jobs)):
        values_list = []
        for j in range(len(categories)): 
            x, y = x_values_entry[i][j].get().split(',')
            values_list.append((int(x), int(y)))
        x_values.append(values_list)
    data['x_values'] = x_values
    root.destroy()

root = tk.Tk()

# Create a grid of input boxes for the x_values
x_values_entry = []
for i in range(len(jobs)):
    row = []
    for j in range(len(categories)):
        if i == 0:
            label = tk.Label(root, text=categories[j])
            label.grid(row=i, column=j+1)
        if j == 0:
            label = tk.Label(root, text=jobs[i])
            label.grid(row=i+1, column=j)
        entry = tk.Entry(root, width=10)
        entry.grid(row=i+1, column=j+1)
        row.append(entry)
    x_values_entry.append(row)

# Add a button to submit the x_values and close the GUI
submit_button = tk.Button(root, text='Submit', command=get_x_values)
submit_button.grid(row=len(jobs)+1, columnspan=len(categories)+1)

# Run the GUI
root.mainloop()

# Define a list of colors and categories for the bars
colors = ('tab:red', 'tab:orange', 'tab:purple', 'tab:blue', 'tab:green')
categories = ('Category 1', 'Category 2', 'Category 3', 'Category 4', 'Category 5')

# Add the colors and categories to each row of the DataFrame
for i in range(len(data['x_values'])):
    data['facecolors'] = [colors] * len(data['x_values'])
    data['categories'] = [categories] * len(data['x_values'])

# Create a pandas DataFrame from the data
df = pd.DataFrame(data)

# Create a new figure and axis
fig, ax = plt.subplots(figsize=(10,6))

# Loop through each row of the DataFrame and plot the broken bar chart
for i, row in df.iterrows():                             #makes no dif 0 or i here^
    ax.broken_barh(row['x_values'], row['y_values'], facecolors=row['facecolors'])
    #ax.broken_barh(row['x_values'], row['y_values'], facecolors=row['facecolors'])

# Create legend entries with color rectangles and category labels
legend_entries = [Patch(facecolor=color, edgecolor='black', label=category) for color, category in zip(colors, categories)]

# Add the legend to the plot
ax.legend(handles=legend_entries, loc='upper right', ncol=5, bbox_to_anchor=(1.0, 1.00))

# Customize the axis labels and limits
ax.set_xlabel('Days')
ax.set_ylabel('Jobs')
ax.set_yticks([7, 9.5, 13.5, 16, 20], labels=['Job1', 'Job2', 'Job3','Job4', 'Job5'])
                                                    
                                                  
title = ax.set_title('Tasks and Crew-Days')
title.set_position([0.5, 1.0])               #set title at center
ax.set_ylim(5, 26)
ax.grid(True)

# Display the plot
plt.show()
</code></pre>
",https://stackoverflow.com/questions/76023652/trouble-with-assigning-colored-categories-on-a-broken-bar-chart,False,
76023648,How do I train the NN layers when the loss function is doing a derivative of the NN,"<p>To give you some context, I am training a neural network that learn a Hamiltonian. To do so, I must use a customized neural network like the following.</p>
<pre><code>class HNN(keras.Model):
  def __init__(self, input_dim=2, hidden_dim=200):
    super(HNN, self).__init__()
    self.dense1 = tf.keras.layers.Dense(hidden_dim, activation='tanh')
    self.dense2 = tf.keras.layers.Dense(hidden_dim, activation='tanh')
    self.dense3 = tf.keras.layers.Dense(1)
    O = np.zeros((input_dim//2, input_dim//2))
    I = np.identity(input_dim//2)
    M = np.concatenate([np.concatenate((O, I), axis=1), np.concatenate((-I, O), axis=1)], axis=0)
    self.M = tf.constant(M, dtype='double')

  def call(self, x):
    y = self.dense1(x)
    y = self.dense2(y)
    y = self.dense3(y)
    return y

  def forward(self, x):
    with tf.GradientTape() as tape:
        y = self.dense1(x)
        y = self.dense2(y)
        y = self.dense3(y)
    y = tape.gradient(y, x)
    y = self.M @ y
    return y
</code></pre>
<p>This neural network takes a (2, batch_size) input (canonical coordinates) anr return the the symplectic gradient of itself. To put it simply, this gradient is the same as the classical gradient but it is rotated 90 deg through M so that it points to where the hamiltonian is constant (Energy conserved). Now the thing is the Loss Function has this form</p>
<p><a href=""https://i.stack.imgur.com/8EZWQ.png"" rel=""nofollow noreferrer"">Equation for the Loss Function</a></p>
<p>Where in this case H == NN. So thats why 'forward' has that form with the tape gradient. Finally, my training function is this one</p>
<pre><code>def train_HNN(data, learning_rate = 1e-3, epochs = 200):
    model = HNN(input_dim=data[['q', 'p']].shape[1], hidden_dim=HIDDEN_DIM)
    loss_func = tf.keras.losses.MeanSquaredError()
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    for i in range(epochs):
        with tf.GradientTape() as t:
            t.watch(model.trainable_variables)
            predictions = model.forward(tf.Variable(tf.stack(data[['q', 'p']])))
            loss = loss_func(tf.Variable(tf.stack(data[['dq', 'dp']])), predictions)
        gradients = t.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        print (i, loss)
    return model
</code></pre>
<p>As you can see, my Loss function does not take explicitly the weigths as the thing it has to train.</p>
<p>My dataset is a pandas dataframe of the form</p>
<p>q       p         dqdt      dpdt<br />
float  float    float      float<br />
float  float    float      float<br />
float  float    float      float<br />
....</p>
<p>Now, the issue here is that I am getting the following warning.</p>
<pre><code>WARNING:tensorflow:Gradients do not exist for variables ['dense_8/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
</code></pre>
<p>However, it is working as expected
<a href=""https://i.stack.imgur.com/nhMZo.png"" rel=""nofollow noreferrer"">Some graphs that show the result</a> My neural network preserves energy as expected.</p>
<p>How can I fix this warning? Why is my code working anyway.</p>
",https://stackoverflow.com/questions/76023648/how-do-i-train-the-nn-layers-when-the-loss-function-is-doing-a-derivative-of-the,False,
76023629,anomaly detection for transactional data,"<p>i want to detect anomalous behaviour of transaction i have mixed type of data categorical columns like bank name , transaction method , user id , ip address etc and numerical column like transaction amount etc .
i want to detect anomalous activity across all these columns globally like a particular transaction was anomalous used a completely different bank .
as well at the user level like same user used different method of payment than usual .
currently i am using isolation forest but i cant get the reason for the anomaly</p>
",https://stackoverflow.com/questions/76023629/anomaly-detection-for-transactional-data,False,
76023622,PermissionError with smtplib python,"<p>I have been experimenting with smptlib and am trying to sen a email to somebody. Here is my code:</p>
<pre><code>import smtplib
my_email = &quot;sender_email@gmail.com&quot;
my_password = &quot;password1234&quot;
connection = smtplib.SMTP(&quot;smtp.gmail.com&quot;)
connection.starttls()
connection.login(user=my_email, password=my_password)
connection.sendmail(from_addr=my_email, to_addrs=&quot;receiver_email@gmail.com&quot;, msg=&quot;Hello&quot;)
connection.quit()
</code></pre>
<p>The sender's email, sender's password, and receiver's email are not valid, for privacy reasons. I do, however keep getting this error:</p>
<p><code>PermissionError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions</code></p>
<p>It may help you to know that I am using the latest version of PyCharm IDE and the latest version of smtplib. My question is: Why am I continually getting this error?</p>
<p>I tried fiddling around, and I think that the code:</p>
<p><code>connection = smtplib.SMTP(&quot;smpt.gmail.com&quot;)</code></p>
<p>was the problem. I think. I also have tried putting in examples and other people's code, but I get the same <code>PermissionError</code> as always.</p>
",https://stackoverflow.com/questions/76023622/permissionerror-with-smtplib-python,False,
76023629,anomaly detection for transactional data,"<p>i want to detect anomalous behaviour of transaction i have mixed type of data categorical columns like bank name , transaction method , user id , ip address etc and numerical column like transaction amount etc .
i want to detect anomalous activity across all these columns globally like a particular transaction was anomalous used a completely different bank .
as well at the user level like same user used different method of payment than usual .
currently i am using isolation forest but i cant get the reason for the anomaly</p>
",https://stackoverflow.com/questions/76023629/anomaly-detection-for-transactional-data,False,
76023622,PermissionError with smtplib python,"<p>I have been experimenting with smptlib and am trying to sen a email to somebody. Here is my code:</p>
<pre><code>import smtplib
my_email = &quot;sender_email@gmail.com&quot;
my_password = &quot;password1234&quot;
connection = smtplib.SMTP(&quot;smtp.gmail.com&quot;)
connection.starttls()
connection.login(user=my_email, password=my_password)
connection.sendmail(from_addr=my_email, to_addrs=&quot;receiver_email@gmail.com&quot;, msg=&quot;Hello&quot;)
connection.quit()
</code></pre>
<p>The sender's email, sender's password, and receiver's email are not valid, for privacy reasons. I do, however keep getting this error:</p>
<p><code>PermissionError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions</code></p>
<p>It may help you to know that I am using the latest version of PyCharm IDE and the latest version of smtplib. My question is: Why am I continually getting this error?</p>
<p>I tried fiddling around, and I think that the code:</p>
<p><code>connection = smtplib.SMTP(&quot;smpt.gmail.com&quot;)</code></p>
<p>was the problem. I think. I also have tried putting in examples and other people's code, but I get the same <code>PermissionError</code> as always.</p>
",https://stackoverflow.com/questions/76023622/permissionerror-with-smtplib-python,False,
76023613,how to Run Python application in interface graphique c#,"<p>how to Run Python application in interface graphique c#</p>
<p>how to Run Selenium Python application in interface graphique c# avec visual studio</p>
",https://stackoverflow.com/questions/76023613/how-to-run-python-application-in-interface-graphique-c,False,
76023605,print the state of all switched when pressin a button using customtkinter,"<p>I am trying to create an app with a pop-up with list of names and the user can select those that are on/off. When they click the &quot;Get Switches Status&quot; button, I want to print a list of the switches that were on.</p>
<p>In the code I wrote only seems to print the initial state of the switches and nothing happens when I click the button. Does anyone have an idea why?</p>
<p>Cheers in advance!</p>
<pre><code>import customtkinter as ctk

def get_switches_status(switches):
    # Get the status of the switches (i.e., which ones are on/off)
    status = [switch.get() for switch in switches]
    
    # Find the indices of the switches that are on
    on_indices = [i for i, val in enumerate(status) if val]
    
    # Print out the status of the switches
    if on_indices:
        print(f&quot;The switches at indices {on_indices} are ON.&quot;)
    else:
        print(&quot;No switches are ON.&quot;)
        
app = ctk.CTk()
switches_gui = []
for i in range(5):
    switch = ctk.CTkSwitch(app, text=f&quot;Switch {i+1}&quot;)
    switch.pack()
    switches_gui.append(switch)   

# Create a button to get the status of the switches
get_status_button = ctk.CTkButton(app, text=&quot;Get Switches Status&quot;, command=get_switches_status(switches = switches_gui))
get_status_button.pack()
# Run the CTk event loop
app.mainloop()



</code></pre>
<p>I tried the code above and didn't work. The button works if doesn't require an input and only ouputs something in the &quot;get_switch_status&quot; fucntion. Somethink like:</p>
<pre><code>def get_switches_status():
    print('test')
    
app = ctk.CTk()
get_status_button = ctk.CTkButton(app, text=&quot;Get Switches Status&quot;, command=get_switches_status)
get_status_button.pack()
app.mainloop()

</code></pre>
<p>but this defeats the point of what I am trying to do</p>
",https://stackoverflow.com/questions/76023605/print-the-state-of-all-switched-when-pressin-a-button-using-customtkinter,False,
76023599,How to Update data based on condition on numpy array,"<blockquote>
<p>A = np.array([[0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0.,
0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [16.919376, 8.319569, -179.40163, -15617.348, -32694.873,-3027.3804], [16.919376, 8.319569, -179.40163, -15617.348, -32694.873,-3027.3804], [18.220955, 8.855534, -229.50981, -17311.98, -43606.348, -3266.0186], [19.3022, 5.62994,-299.1388, -18348.877, -58953.84,  -2805.2612], [0.72668076,0.45267868,-2.6522827,-216.83502,-1031.2653,-109.199524],
[19.3022, 5.62994,-299.1388, -18348.877, -58953.84,  -2805.2612],<br />
[0.72668076,0.45267868,-2.6522827,-216.83502,-1031.2653,-109.199524],
[16.919376,8.319569,-179.40163, -15617.348, -32694.873, -3027.3804],
[0.72668076,0.45267868,-2.6522827,-216.83502,-1031.2653,-109.199524],
[16.919376, 8.319569, -179.40163, -15617.348, -32694.873,-3027.3804],
[18.220955, 8.855534, -229.50981, -17311.98, -43606.348, -3266.0186],
[0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0.,
0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [18.220955, 8.855534, -229.50981, -17311.98, -43606.348, -3266.0186],
[0., 0., 0., 0., 0., 0.], [18.220955, 8.855534, -229.50981, -17311.98,
-43606.348, -3266.0186], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [19.3022, 5.62994,-299.1388, -18348.877, -58953.84,  -2805.2612], [18.220955, 8.855534, -229.50981, -17311.98, -43606.348, -3266.0186], [0.72668076,0.45267868,-2.6522827,-216.83502,-1031.2653,-109.199524],
[16.919376, 8.319569, -179.40163, -15617.348, -32694.873,-3027.3804],
[18.220955, 8.855534, -229.50981, -17311.98, -43606.348, -3266.0186],
[0., 0., 0., 0., 0., 0.],
[0.72668076,0.45267868,-2.6522827,-216.83502,-1031.2653,-109.199524],
[0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0.,
0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [16.919376, 8.319569, -179.40163, -15617.348, -32694.873,-3027.3804],
[0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0.,
0., 0.], [18.220955, 8.855534, -229.50981, -17311.98, -43606.348,-3266.0186]])</p>
</blockquote>
<p>How check data each 5 index, so if each 5 index, 4 index value all zero and 1 index not zero  or 3 index all zero value and 2 index is not zero value. then  I want change the non zero value to zero value?</p>
<p>my data:</p>
<pre><code>A = np.array([    
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [16.919376, 8.319569, -179.40163, -15617.348, -32694.873, -3027.3804],
    [16.919376, 8.319569, -179.40163, -15617.348, -32694.873, -3027.3804],
    [18.220955, 8.855534, -229.50981, -17311.98, -43606.348,  -3266.0186],
    [19.3022, 5.62994,-299.1388, -18348.877, -58953.84,  -2805.2612],
    [0.72668076, 0.45267868, -2.6522827, -216.83502, -1031.2653,  -109.199524],
    [19.3022, 5.62994,-299.1388, -18348.877, -58953.84,  -2805.2612],  
    [0.72668076, 0.45267868, -2.6522827, -216.83502, -1031.2653,  -109.199524],
    [16.919376, 8.319569, -179.40163, -15617.348, -32694.873, -3027.3804],
    [0.72668076, 0.45267868, -2.6522827, -216.83502, -1031.2653,  -109.199524],
    [16.919376, 8.319569, -179.40163, -15617.348, -32694.873, -3027.3804],
    [18.220955, 8.855534, -229.50981, -17311.98, -43606.348,  -3266.0186],   
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [18.220955, 8.855534, -229.50981, -17311.98, -43606.348,  -3266.0186],
    [0., 0., 0., 0., 0., 0.],
    [18.220955, 8.855534, -229.50981, -17311.98, -43606.348,  -3266.0186],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [19.3022, 5.62994,-299.1388, -18348.877, -58953.84,  -2805.2612],
    [18.220955, 8.855534, -229.50981, -17311.98, -43606.348,  -3266.0186],
    [0.72668076, 0.45267868, -2.6522827, -216.83502, -1031.2653,  -109.199524],
    [16.919376, 8.319569, -179.40163, -15617.348, -32694.873, -3027.3804],
    [18.220955, 8.855534, -229.50981, -17311.98, -43606.348,  -3266.0186],
    [0., 0., 0., 0., 0., 0.],
    [0.72668076, 0.45267868, -2.6522827, -216.83502, -1031.2653,  -109.199524],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [16.919376, 8.319569, -179.40163, -15617.348, -32694.873, -3027.3804],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [0., 0., 0., 0., 0., 0.],
    [18.220955, 8.855534, -229.50981, -17311.98, -43606.348,  -3266.0186],
])
</code></pre>
<p>my expected results:</p>
<pre><code>A = np.array([[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[16.919376, 8.319569, -179.40163, -15617.348, -32694.873, -3027.3804],
[18.220955, 8.855534, -229.50981, -17311.98, -43606.348,  -3266.0186],
[19.3022, 5.62994,-299.1388, -18348.877, -58953.84,  -2805.2612],
[0.72668076, 0.45267868, -2.6522827, -216.83502, -1031.2653,  -109.199524],
[19.3022, 5.62994,-299.1388, -18348.877, -58953.84,  -2805.2612],
[0.72668076, 0.45267868, -2.6522827, -216.83502, -1031.2653,  -109.199524],
[16.919376, 8.319569, -179.40163, -15617.348, -32694.873, -3027.3804],
[0.72668076, 0.45267868, -2.6522827, -216.83502, -1031.2653,  -109.199524],
[16.919376, 8.319569, -179.40163, -15617.348, -32694.873, -3027.3804],
[18.220955, 8.855534, -229.50981, -17311.98, -43606.348,  -3266.0186],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[19.3022, 5.62994,-299.1388, -18348.877, -58953.84,  -2805.2612],
[18.220955, 8.855534, -229.50981, -17311.98, -43606.348,  -3266.0186],
[0.72668076, 0.45267868, -2.6522827, -216.83502, -1031.2653,  -109.199524],
[16.919376, 8.319569, -179.40163, -15617.348, -32694.873, -3027.3804],
[18.220955, 8.855534, -229.50981, -17311.98, -43606.348,  -3266.0186],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0., 0.]])
</code></pre>
<p>I've try this code but its not work:</p>
<pre><code># iterate over the rows in steps of 5
for i in range(0, A.shape[0], 5):
    row = A[i]
    num_zeros = np.count_nonzero(row == 0)
    num_nonzeros = np.count_nonzero(row != 0)
    # if the number of zeros is greater than the number of non-zeros, update the non-zero values to zero
    if num_zeros &gt; num_nonzeros:
        row[row != 0] = 0
</code></pre>
",https://stackoverflow.com/questions/76023599/how-to-update-data-based-on-condition-on-numpy-array,False,
76023593,Storing a value from ios-xe router command output in python,"<p>After making a connection to a cisco device using python, and executing a command I get a output as shown below,
Basically I want to get only one value, that the value for <code>Services</code> which is shown as <code>1</code> below and store it in a variable.</p>
<pre><code>#show ethernet cfm domain brief
Domain Name                              Index Level Services Archive(min)
EVC                                          2     4        1     100
</code></pre>
<p>I'm not sure how to parse this, I tried converting to a list, but the list became very huge something like this and I dont know if the same will work on another cisco ios-xe device.</p>
<pre><code>['Domain', 'Name', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Index', 'Level', 'Services', 'Archive(min)\r\nEVC', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '2', '', '', '', '', '4', '', '', '', '', '', '', '', '1', '', '', '', '', '100']
</code></pre>
<p>Is there any other reliable way to get the value of <code>services</code> which is <code>1</code> ?</p>
",https://stackoverflow.com/questions/76023593/storing-a-value-from-ios-xe-router-command-output-in-python,False,
76023590,Requesting the fritz-box login page via python-requests timeouts inside a docker container,"<p>I have a simple python script which requests the html-content of my fritz-box login page:</p>
<pre><code>import requests

r = requests.get('http://192.168.178.1')
print(r.text)
</code></pre>
<p>If run that code on my computer it works as expected. But if i create a simple docker container and try to run it, the request timeouts:</p>
<pre><code>FROM python:alpine3.17

WORKDIR /usr/src/testcase

RUN pip install --upgrade pip

COPY ./requirements.txt .

RUN pip install -r requirements.txt

COPY . .

ENTRYPOINT [&quot;tail&quot;, &quot;-f&quot;, &quot;/dev/null&quot;]
</code></pre>
<p>I can request other network devices in my network and I get the html-content, just the request for the fritz-box times out. Pinging the fritz-box also works. What could be the reason?</p>
",https://stackoverflow.com/questions/76023590/requesting-the-fritz-box-login-page-via-python-requests-timeouts-inside-a-docker,False,
76023575,How do I edit an already existing diagram in matplotlib (python)?,"<p>I already have an existing figure that I have created in matplotlib. I have called <code>plt.show()</code>on it already. What I want to do is to be able to add another line on it without deleting the first figure. Here is my code already:</p>
<pre><code>import matplotlib.pyplot as plt
a=complex(5,6)
b=complex(2,2)
c=complex(7,-2)


def plot_vector(start, end):
    plt.plot([start.real, end.real], [start.imag, end.imag])
    plt.ylabel('i')
    plt.xlabel('r')
    plt.title('My first graph!')
    plt.ylim(-8,8)
    plt.xlim(-8,8)
    plt.show()


plot_vector(b,c)
#here the figure shows
#after the figure shows, I need to have some code run that asks the user for input
#then, I want the figure to be updated with:
plot_vector(a,c)
</code></pre>
",https://stackoverflow.com/questions/76023575/how-do-i-edit-an-already-existing-diagram-in-matplotlib-python,True,
76023573,Matplotlib Text in Table not centered,"<p>I tried several methods but can't figure out how to properly center the text in the columns and tighten all of it. Help would be much appreciated.</p>
<pre><code>
plt.close()
colors = []
for _, row in mail_decision_table.iterrows():
    colors_in_column = [&quot;aliceblue&quot;, &quot;lightgrey&quot;]
    if row[&quot;Decision&quot;]==&quot;SELL&quot;:
        colors_in_column[1] = &quot;lightcoral&quot;
    else:
        colors_in_column[1] = &quot;lightgreen&quot;
    colors.append(colors_in_column)

fig, ax = plt.subplots()
ax.axis('tight')
ax.axis('off')



the_table = ax.table(cellText = mail_decision_table.values,
    colLabels = mail_decision_table.columns,
    loc='center',
    rowLoc='center',
    colColours = [&quot;grey&quot;,&quot;grey&quot;],
    cellColours=colors)


for key, cell in the_table.get_celld().items():
    cell.set_linewidth(0.5)
    cell.set_linestyle(&quot;dashed&quot;)
    cell.set_edgecolor(&quot;grey&quot;)
    cell._loc = 'center'
    if key[0] ==0 :
        cell._text.set_color('white')

plt.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/Ou1Tb.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ou1Tb.jpg"" alt=""enter image description here"" /></a></p>
",https://stackoverflow.com/questions/76023573/matplotlib-text-in-table-not-centered,True,76023659
76023564,HTTP/HTTPS Proxy Server,"<p>I am making my own proxy server implementation in python that can serve all http/https requests of browser using socket programming. Here is my code:</p>
<pre><code>from socket import error as SocketError
import requests
import urllib3
import os.path
import socket
import random
import errno
import os
import sys
import ssl

def MainServerController():

    Proxy()

def Proxy():
    port_addr = ('localhost', 12345)

    context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)
    context.load_cert_chain(certfile='certificate.pem', keyfile='private_key.pem')

    # Create a server socket, bind it to a port and start listening
    tcpSerSock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    tcpSerSock.bind(port_addr)

    
    tcpSerSock.listen(40)

    while True:

        # Start receiving data from the client
        print(f'\nReady to serve at {port_addr}')
        tcpCliSock, addr = tcpSerSock.accept()

        
        tcpCliSock = context.wrap_socket(tcpCliSock)
        
        print('Received a connection from:', addr)
        message = tcpCliSock.recv(40960).decode('utf-8', 'ignore')

        if message == '':
            continue

        print(f'\'{message}\'')

        # Extract the filename from the given message
        if message.split(' ')[1].startswith('http') or message.split(' ')[1].startswith('https'):
            filename = message.split(' ')[1].split('//')[1]
        else:
            filename = message.split(':')[0].split(' ')[1]

        try:
            response = requests.get(f&quot;http://{filename}&quot;, verify=False)
        except:
            print(f&quot;Not found {filename}&quot;)
            continue

        
        try:
            tcpCliSock.send(response.content + b&quot;\r\n&quot;)
        except SocketError as e:
            if e.errno != errno.ECONNRESET:
                print(f'Connection Reset By Peer! {filename} ')
            continue
    tcpSerSock.shutdown(socket.SHUT_RDWR)
    tcpCliSock.close()
MainServerController()
</code></pre>
<p>The above code is perfectly running for all requests except for https if I am commenting this line:</p>
<pre><code>tcpCliSock = context.wrap_socket(tcpCliSock)
</code></pre>
<p>And giving not secure connection page on browser for https requests but by using this line neither http nor https request is served and throwing error like :</p>
<pre><code>File &quot;C:\Users\DELL\Desktop\RSA\https.py&quot;, line 36, in Proxy
    tcpCliSock = context.wrap_socket(tcpCliSock)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python311\Lib\ssl.py&quot;, line 517, in wrap_socket
    return self.sslsocket_class._create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python311\Lib\ssl.py&quot;, line 1066, in _create
    self._sslobj = self._context._wrap_socket(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ssl.SSLError: Cannot create a client socket with a PROTOCOL_TLS_SERVER context (_ssl.c:795)
</code></pre>
<p>The key and certificate is generated by :</p>
<pre><code>openssl req -new -x509 -days 365 -nodes -out certificate.pem -keyout private_key.pem
</code></pre>
<p>Can somebody help me in this?</p>
",https://stackoverflow.com/questions/76023564/http-https-proxy-server,False,
76023559,Python selenium waiting to download file completely,"<p>I am writing code to download an Android APK file. It worked. However, I  faced trouble in the process of waiting for the file to download completely. I want to download many files, and each file has a different size, so I can't set the <code>time.sleep(time)</code> in general.</p>
<p>I try to combine a <code>Thread</code> and a <code>While loop</code>, and it worked. But, in the cut-off internet case, my code is still in the infinite while loop because at that time, the file is not downloaded completed and it is like that <code>.crdownload</code>.
Please help me with the issue.</p>
<pre><code>def waitingDownload():
   path = &quot;C:\\Users\\ASUS\\anaconda3\\phd_implement\\download_apk\\tmp\\*.apk&quot;
   files = glob.glob(path)
   if(len(files)!=0):
       return True
   else:
       return False

#Setup web driver
chromedriver = 'C:\\Users\\ASUS\\anaconda3\\phd_implement\\chromedriver.exe'
os.environ[&quot;webdriver.chrome.driver&quot;] = chromedriver
chrome_options = Options()
chrome_options.add_argument(&quot;--disable-blink-features=AutomationControlled&quot;)
chrome_options.add_argument(&quot;--disable-popup-blocking&quot;)
prefs = {'profile.default_content_setting_values.automatic_downloads': 1,&quot;download.default_directory&quot; : &quot;C:\\Users\\ASUS\\anaconda3\\phd_implement\\download_apk\\tmp\\&quot;}
chrome_options.add_experimental_option(&quot;prefs&quot;, prefs)
web = webdriver.Chrome(options=chrome_options)
web.maximize_window()
# Download
url=&quot;https://download.apkcombo.com/com.avast.android.secure.browser/Avast%20Secure%20Browser_7.5.2_apkcombo.com.apk?ecp=Y29tLmF2YXN0LmFuZHJvaWQuc2VjdXJlLmJyb3dzZXIvNy41LjIvNDYzMC5jZWZhYTAzZTYwOTY3NGY0Y2Y2YWRiMTk1N2VkMzhlODMzYTExNGQzLmFwaw==&amp;iat=1681553964&amp;sig=4cc693ff4b5dfe4adea4030b65b8e454&amp;size=198815248&amp;from=cf&amp;version=latest&amp;lang=en&amp;fp=1dce2e74bf37995f5efd03cd5877afb5&amp;ip=130.25.102.251&quot;
def task(url):
   web.get(url)
# create a thread
thread= Thread(target=task(url))
# run the thread
thread.start()
# wait for the thread to finish
print('Waiting for the download file...')
while True:
    if(waitingDownload()):
        break
thread.join()
print('Download completed')
web.close()
</code></pre>
",https://stackoverflow.com/questions/76023559/python-selenium-waiting-to-download-file-completely,True,76025443
76023530,All other transforms disappear after broadcasting my custom odom to base_link transform (ROS2 Foxy),"<p>I have made a node to broadcast the odometry transform and publish the odometry topic.
Before I run this new node, everything looks good. However, when I run my odometry node, and broadcast the tf odom to base_link, all other transforms to base_link disappear.</p>
<p>I have searched the internet a lot but I can't find how to fix this issue.</p>
<p>Here is what I have in my node (simplified for posting here):</p>
<pre><code>#!/usr/bin/env python3

import rclpy
from rclpy.node import Node
import traceback

from geometry_msgs.msg import TransformStamped
from sensor_msgs.msg import JointState
from nav_msgs.msg import Odometry
from tf2_ros import TransformBroadcaster


class CalculatedNode(Node):

    def __init__(self):
        super().__init__('calculated_node')
        # Create publisher
        self.calculated_node = self.create_publisher(Odometry, '/odom', 10)
        self.odom_tf_broadcaster = TransformBroadcaster(self)

        # Subscribe to node
        self.subscription = self.create_subscription(
            JointState,
            '/joint_states',
            self.update_odom,
            1)

    def update_odom(self, msg):
        try:
            # DO THE ODOM CALCULATIONS (using data from the /joint_states topic)
            # [...]
            # Broadcast the transform
            odom_trans = TransformStamped()
            self.odom_tf_broadcaster.sendTransform(odom_trans)
            # Publish Odom
            self.calculated_node.publish(Odometry())
        except Exception as e: 
            traceback.print_exc()
            print(self.get_clock().now())
            # print(e)
        

def main(args=None):
    rclpy.init(args=args)
    calculate_odom = CalculatedNode()

    try:
        rclpy.spin(calculate_odom)
    except KeyboardInterrupt:
        pass

    calculate_odom.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
</code></pre>
",https://stackoverflow.com/questions/76023530/all-other-transforms-disappear-after-broadcasting-my-custom-odom-to-base-link-tr,False,
76023508,Pandas does not separate columns of imported csv file,"<p>I want to import a csv file as a dataframe using pandas. The file's structure looks as provided in the screenshot.
(<a href=""https://i.stack.imgur.com/N91d7.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/N91d7.png</a>)
However, for some reason, using</p>
<pre><code>df = pd.read_csv(&quot;Test.csv&quot;, delimiter = ',')
</code></pre>
<p>does not work. The resulting dataframe contains all content in one single column.</p>
<p>How can I separate the columns correctly? Thanks in advance.</p>
<p>I already played around with different options for the &quot;read_csv&quot; prompt, however, I did not yet find a solution.</p>
",https://stackoverflow.com/questions/76023508/pandas-does-not-separate-columns-of-imported-csv-file,True,76023713
76023492,"Python game not working correctly, directions failing","<p>I tried to put the code in correctly, i have to make a game and its supposed to give a room when you put in a direction because you're walking through a house but whenever i put a direction it just gives invalid six times, i dont know where i went wrong, i thought if i brought else in line with the other if maybe that would work but i got nothing, the game starts in the kitchen and it's supposed to ask for a direction i put in south and its supposed to say dining room but i just get the invaild
im brand new to python and help would be much appreciated</p>
<pre><code># main menu
def game_instructions():
    print('Welcome to the murder mystery game')
    print('Collect all 6 items to escape the house')
    print('Move commands: Go North, Go South, Go East, Go West')
    print('Add to Inventory: get item name')
    print('You are in the Kitchen')


rooms = {
    'Kitchen': {'South': 'Dining Room', 'East': 'Front Closet', 'Item': 'Knife'},
    'Front Closet': {'West': 'Kitchen', 'Item': 'Coat'},
    'Dining Room': {'West': 'Living Room', 'East': 'Bathroom', 'North': 'Kitchen', 'South': 'Laundry Room',
                    'Item': 'Brick'},
    'Living Room': {'East': 'Dining Room', 'Item': 'Shoes'},
    'Bathroom': {'West': 'Dining Room', 'North': 'Bedroom'},
    'Laundry Room': {'North': 'Dining Room', 'East': 'Game Room', 'Item': 'Road Flare'},
    'Bedroom': {'South': 'Bathroom', 'Villain': 'Killer'},
    'Game Room': {'west': 'Laundry Room', 'Item': 'Phone'}
}
starting_room = 'Kitchen'
current_room = ['rooms']


def game_run(room):
    directions = ['North', 'South', 'East', 'West', 'Exit']
    print('\nYou are currently in the ()'.format(current_room))


move = input('Where to next?\n')

if current_room == 'Bedroom' and len(inventory) &lt; 6:
    print('You have been killed')
if current_room == 'Bedroom' and len(inventory) &gt; 6:
    print('You have escaped the house')

if current_room == 'Kitchen':
    if input('South'):
        print('You are in the Dining Room')
    elif input('East'):
        print('you are in the Front Closet')
else:
    print('Invalid Direction')

if current_room == 'Front Closet':
    if input('West'):
        print('you are in the kitchen')
else:
    print('Invalid Direction')

if current_room == 'Dining Room':
    if input('West'):
        print('you are in the Living Room')
    elif input('East'):
        print('Bathroom')
    elif input('North'):
        print('Kitchen')
    elif input('South'):
        print('Laundry Room')
else:
    print('Invalid Direction')

if current_room == 'Living Room':
    if input('East'):
        print('Dining room')
else:
    print('Invalid Direction')

if current_room == 'Bathroom':
    if input('West'):
        print('Dining Room')
    elif input('North'):
        print('Bedroom')
else:
    print('Invalid Direction')

if current_room == 'Laundry room':
    if input('North'):
        print('Dining Room')
    elif input('East'):
        print('Game Room')
else:
    print('Invalid Direction')

if current_room == 'Bedroom':
    if input('South'):
        print('Bathroom')
else:
    print('Invalid Direction')

if current_room == 'Game Room':
    if input('West'):
        print('Laundry Room')
else:
    print('Invalid Direction')

</code></pre>
",https://stackoverflow.com/questions/76023492/python-game-not-working-correctly-directions-failing,False,
76023490,How to print python list comprehension values (Extended Iterable Unpacking),"<pre><code>class Triangle(Polygon):
    # Initializing the number of sides of the triangle to 3 by 
    # calling the __init__ method of the Polygon class
    def __init__(self):
        Polygon.__init__(self,3)    
  
      def __init__(self, no_of_sides):
        self.n = no_of_sides
        self.sides = [0 for i in range(no_of_sides)]

    def inputSides(self):
       self.sides = [float(input(&quot;Enter side &quot;+str(i+1)+&quot; : &quot;)) for i in range(self.n)]

    def findArea(self):
       a, b, c = self.sides
</code></pre>
<p><code>a, b, c = self.sides</code> - what does this line mean?</p>
<p>How will assign list value to variable?</p>
",https://stackoverflow.com/questions/76023490/how-to-print-python-list-comprehension-values-extended-iterable-unpacking,False,
76023474,"why is DF[&#39;date&#39;] = [re.findall(&quot;(-)(-)&quot;, i) for i in DF[&#39;date&#39;]] not working?","<p>I have a problem with the extraction
Given is a column Date, where I want to extract the months
for example: 2022-05-12 --&gt; 05</p>
<p>How can I programm this?</p>
<pre><code>import pandas as pd
import re
DF = pd.DataFrame({
    &quot;item_id&quot;: [1, 2, 3, 4],
    &quot;costumer&quot;: [&quot;CG&quot;, &quot;MD&quot;, &quot;CC&quot;, &quot;CZ&quot;],
    &quot;Betrag&quot;: [150, 12, 78, 56],
    &quot;games&quot;: [[&quot;Minecraft&quot;, &quot;Uno&quot;], [&quot;WOW&quot;, &quot;Minecraft&quot;], [&quot;WOW&quot;, &quot;Minecraft&quot;], [&quot;WOW&quot;, &quot;Minecraft&quot;, &quot;The last of us&quot;]], 
    &quot;date&quot;: [&quot;2022-11-18&quot;, &quot;2022-09-12&quot;, &quot;2022-08-26&quot;, &quot;2022-05-12&quot;]
})

DF['date'] = [re.findall(&quot;(-)(-)&quot;, i) for i in DF['date']]
DF

</code></pre>
<p>Expected Output: each row in date:
11
09
08
05</p>
",https://stackoverflow.com/questions/76023474/why-is-dfdate-re-findall-i-for-i-in-dfdate-not-working,False,
76023451,How to improve this code to click all xpath elements with selenium python,"<p>How to improve this code to click all XPath elements, because there are some elements with the same position 'str(i)' which are not considered and not clicked. Also, when returning to the previous page, the position 'str(i)' changes. I am using Selenium WebDriver for chrome.</p>
<p>This string:</p>
<pre><code>/html/body/div[1]/div/div[1]/div/div/div[2]/div/div[2]/div[1]/div[1]/div/div/div/div[' + str(i) + ']/div/div/div[1]/div[1]/h3/a
</code></pre>
<p>references an element with the structure:</p>
<pre><code>&lt;a href=&quot;/dashboard?id=RQ&quot;&gt;RQ&lt;/a&gt;
</code></pre>
<p>Code</p>
<pre><code>i = 1
while True:
    try:
        elements = driver.find_elements(By.XPATH, '/html/body/div[1]/div/div[1]/div/div/div[2]/div/div[2]/div[1]/div[1]/div/div/div/div[' + str(i) + ']/div/div/div[1]/div[1]/h3/a')

        if not elements:
            break

        for element in elements:
            element.click()
            time.sleep(5)
            WebDriverWait(driver, 5).until(EC.url_changes('url'))
            driver.back()

            time.sleep(2)

        i += 1

    except NoSuchElementException:
        break
</code></pre>
",https://stackoverflow.com/questions/76023451/how-to-improve-this-code-to-click-all-xpath-elements-with-selenium-python,False,
76023439,"Tetris game, not getting fulfiled lines removed correctly","<p>iam new to python and iam making tetris game, but my fulfiled lines are not removed correctly. When i fulfil just one or two lines in bottom, it works fine, but when i fulfil maybe 5 or more at once, it gets broken and lines are not removed anymore.</p>
<p>This my function function:</p>
<pre><code>def check_line(shapes):
        my_arr = []
        count_dict = {}
        finished_arr = []

        for shape in shapes:
            for rect in shape:
                
                row = int(rect.y)
                my_arr.append(row)
        
        for y in my_arr:
            if y not in count_dict:
                count_dict[y] = 1
            else:
                count_dict[y] += 1

        
        most_common_nums = [k for k, v in count_dict.items() if v == 10]
        
        for one in most_common_nums:
            
            finished_arr.append(one)
        print(count_dict)
        
            
                
        return finished_arr
</code></pre>
<p>Iam printing count_dict and when are a lot of lines at once getting removed, it is printing {765: 11, 720: 12, 675: 9, 630: 8, 585: 6, 540: 2, 45: 2, 90: 2}.. i dont know how it counts 12 or 11 rectangles in one line.</p>
<p>And this is my implementation to loop:</p>
<pre><code>if event.type == MOVE_SHAPE:
                
                
                
                
                collides_with_grid = any(shape.y &gt;= (H*TILE) - TILE for shape in shapes[shape_num])
                collides_with_shape = False
                
                for rect in shapes[shape_num]:
                    for i in range(shape_num):
                        for rect2 in shapes[i]:
                        
                            if collides_shapes(rect, rect2):
                                collides_with_shape = True

                
                if collides_with_grid or collides_with_shape:
                    
                    if check_game_over(shapes):
                        
                        
                        game_over = True
                        if game_over:
                            current_speed = 0

                        else:
                            current_speed = speed

                        font_over = pygame.font.SysFont(&quot;calibri&quot;, 80, bold=True)
        
                        text2 = font_over.render(&quot;GAME OVER&quot;, True, &quot;white&quot;)
                        text2_rect = text2.get_rect()
                        text2_rect.midtop =  220, 350
                        
                            
                        
                    
                    fulfilled_lines = check_line(shapes)
                    current_score += score_plus(len(fulfilled_lines))
                    text_score = font.render(f&quot;SCORE: {current_score}&quot;, True, &quot;white&quot;)
                    
                    
                    

                    
                    if len(fulfilled_lines) &gt; 0:
                        if music:
                            music_coll.play()

                        
                        
                        for shape in shapes:
                            new_shapes = []
                            for rect in shape:
                                if rect.y not in fulfilled_lines:
                                    new_shapes.append(rect)

                            shape.clear()
                            shape.extend(new_shapes)
                        

                        
                            
                                    
                    
                        for shape in shapes:
                            for rect in shape:
                                for line in fulfilled_lines:
                                    if rect.y &lt; line:
                                        rect.y += (len(fulfilled_lines) * TILE)


                    add_shape()
                    
                    shape_num += 1  
                        
                    
                    for shape in shapes[shape_num]:
                        grid.append(shape)       
                    

                else:
                    for shape in shapes[shape_num]:
                        shape.y += TILE
</code></pre>
<p>Thanks for help</p>
",https://stackoverflow.com/questions/76023439/tetris-game-not-getting-fulfiled-lines-removed-correctly,False,
76023433,Plotly subplots with multiple loops,"<p>I am trying to make plotly subplots using three loops. My problem is that the plots figures are not showing in 2x3 grid. this is my code:</p>
<pre><code>plot_row = 2
plot_col = 3
fig = make_subplots(rows = plot_row, cols = plot_col,specs = [[{'type':'box'},{'type':'box'},{'type':'box'}],[{'type':'box'},{'type':'box'},{'type':'box'}]])

x = 0
for i in range(1,plot_row+1):
    for j in range(1,plot_col+1):
        for col in num_cat_col:
            fig.add_trace(go.Box(x=df[col],y=df['Price']),row = i,col=j)
            x = x+1
 fig.show()
</code></pre>
<p>I have tried to change the position of fig.show multiple times but still unable to get the results.
Can someone please help me.
Thank you</p>
",https://stackoverflow.com/questions/76023433/plotly-subplots-with-multiple-loops,False,
76023394,"Python reportlab, the problem of not positioning the side title at the beginning of the PDF page it is on","<pre><code>#-----PDF SECTION----------------------------------------------------------------------------
from reportlab.lib.pagesizes import letter, mm
from reportlab.lib.units import inch
from reportlab.pdfgen import canvas
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib import colors
from reportlab.platypus import Paragraph, SimpleDocTemplate, Spacer, Image, PageBreak
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont

if &quot;roboto-bold&quot; or &quot;roboto-regular&quot; not in [fam.lower() for fam in pdfmetrics.getRegisteredFontNames()]:
    pdfmetrics.registerFont(TTFont(&quot;Roboto-Bold&quot;, &quot;fonts\Roboto\Roboto-Bold.ttf&quot;))
    pdfmetrics.registerFont(TTFont(&quot;Roboto-Regular&quot;, &quot;fonts\Roboto\Roboto-Regular.ttf&quot;))
else:
    print(&quot;Roboto-Bold/Roboto-Regular fonts have already been registered.&quot;)


styles = getSampleStyleSheet()

# footer style
footer_style = ParagraphStyle(
    name='footer',
    parent=styles['Normal'],
    fontName='Helvetica',
    fontSize=8,  # Yazı boyutunu küçülttük
    textColor=colors.lightgrey,
    alignment=0,  # Sola hizalama için değeri 0 olarak ayarladık
)

# footer text
footer_text = 'Created by HepsiBurada Price Tracker App'
footer = Paragraph(footer_text, footer_style)

# header style
header_style = ParagraphStyle(
    name='header',
    parent=styles['Normal'],
    fontName='Helvetica-Oblique',
    fontSize=10,
    textColor=colors.black,
    alignment=1,
)

# cover page style
cover_style = ParagraphStyle(
    name='cover',
    parent=styles['Normal'],
    fontName='Roboto-Bold',
    textColor=colors.black,
    alignment=1
)


# side header style

side_header_style = ParagraphStyle(
    name='side_header',
    parent=styles['Normal'],
    fontName='Roboto-Bold',
    fontSize=18,
    textColor=colors.black,
    leftIndent=-63, 
    topPadding=0,
    spaceBefore=-20,
    spaceAfter=15 
)



# result style
result_style = ParagraphStyle(
    name='result',
    parent=styles['Normal'],
    fontName='Roboto-Regular',
    fontSize=12,
    textColor=colors.black,
    leftIndent=-63,
    spaceAfter=14
   
)




def generate_pdf(pdf_file, content, footer_style, header_style, cover_style, result_style, side_header_style):
   
    doc = SimpleDocTemplate(pdf_file, pagesize=(215.9 * mm, 279.4 * mm))

   


    # cover page
    cover_text = '&lt;font size=&quot;30&quot; color=&quot;black&quot;&gt;Pınar Süt 1lt Analysis Results&lt;/font&gt; &lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;'

    cover = Paragraph(cover_text, cover_style)

    # İstediğiniz görseli ekleyin
    image = Image('cover_template_for_pdf/cover_template.jpg', 7 * inch, 6.5 * inch)

    # page content
    content = [cover, Spacer(1, 0.5 * inch), image, PageBreak()] + content 



    def page_number(canvas, doc):
        page_num = canvas.getPageNumber()
        canvas.setFillColorRGB(0.2, 0.2, 0.2)
        text = &quot;{}&quot;.format(page_num)
        canvas.setFont(&quot;Helvetica&quot;, 10)  # Sayfa numarası yazı boyutu
        canvas.drawCentredString(doc.width / 2 + doc.leftMargin, 0.5 * inch, text)
        
        
        canvas.setFont(&quot;Helvetica&quot;, 8)  # Alt bilgi yazı boyutu
        canvas.setFillColorRGB(191/255, 87/255, 0)  # Turuncu rengi ayarla
        footer_width = canvas.stringWidth(footer_text, &quot;Helvetica&quot;, 8)
        x = doc.leftMargin - 20 * mm  # Sola yasla ve sol taraftan 10 mm boşluk bırak
        canvas.drawString(x, 0.3 * inch, footer_text)  

        
    
        
    def onFirstPage(canvas, doc):
        canvas.saveState()
        page_number(canvas, doc)
        # gray line
        canvas.setStrokeColor(colors.orange)
        canvas.setLineWidth(1)
        canvas.rect(0, 0.75 * inch, doc.pagesize[0], 1, fill=True, stroke=False)
        canvas.restoreState()

    def onLaterPages(canvas, doc):
        canvas.saveState()
        page_number(canvas, doc)
        # gray line
        canvas.setStrokeColor(colors.orange)
        canvas.setLineWidth(1)
        canvas.rect(0, 0.75 * inch, doc.pagesize[0], 1, fill=True, stroke=False)
        canvas.restoreState()

    
     
        
    doc.build(content, onFirstPage=onFirstPage, onLaterPages=onLaterPages)
    return pdf_file



subheading1=&quot;1. Summary information&quot;
Searched_product=f&quot;Searched product: {search_term}&quot;
Seller_offering_the_lowest_price_for_the_searched_product=f&quot;Seller offering the lowest price for the searched product: {product['seller']}&quot;
Link_of_the_searched_product=f&quot;Link of the searched product: {product['url'][0]}&quot;
Duration_of_tracking_the_product=f&quot;Duration of tracking the product:{tracking_days}&quot;

subheading2=&quot;2. Graphics&quot;    
    
# Add the content to the existing PDF
content_list  = [Paragraph(subheading1, side_header_style), 
Paragraph(Searched_product,result_style),
Paragraph(Seller_offering_the_lowest_price_for_the_searched_product,result_style),
Paragraph(Link_of_the_searched_product,result_style),
Paragraph(Duration_of_tracking_the_product,result_style),
PageBreak(),
Paragraph(subheading2, side_header_style)]
           
#Image(lowest_highest_prices_filename, width=500, height=250)
   
generate_pdf
(&quot;ornek.pdf&quot;,content_list,footer_style,header_style,cover_style,result_style,side_header_style)

</code></pre>
<p>Hello friends, I cannot move the side headers ( Summary information and Graphics) to the top of the page. Actually, I use two parameters like topPadding=0, spaceBefore=-20 in the side header style, but it doesn't work. A thick gray line appears at the junction of the two pages. I thought the problem might be with it, but I couldn't eliminate it either. How Can I fix this?</p>
<p><strong>The Problem:</strong></p>
<p><a href=""https://i.stack.imgur.com/1XVf1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1XVf1.png"" alt=""enter image description here"" /></a></p>
",https://stackoverflow.com/questions/76023394/python-reportlab-the-problem-of-not-positioning-the-side-title-at-the-beginning,True,76024517
76023358,I am trying to create a Maze game in Pygame but its not working,"<pre><code># Import the Pygame library
import pygame
import random

# Initialize Pygame
pygame.init()

# Set up the display
WIDTH = 800
HEIGHT = 600
SCREEN = pygame.display.set_mode((WIDTH, HEIGHT))
pygame.display.set_caption(&quot;Maze&quot;)

# Define colors
WHITE = (255, 255, 255)
BLACK = (0, 0, 0)
RED = (255, 0, 0)
GREEN = (0, 255, 0)
BLUE = (0, 0, 255)

# Define the size of each cell in the maze
CELL_SIZE = 20

# Define the number of cells in the maze
NUM_ROWS = int(HEIGHT / CELL_SIZE)
NUM_COLS = int(WIDTH / CELL_SIZE)

# Define a 2D array to store the maze
maze = [[0 for i in range(NUM_COLS)] for j in range(NUM_ROWS)]

# Define a function to generate the maze using a depth-first search algorithm
def generate_maze():
    # Start at the top-left corner
    stack = [(0, 0)]
    
    # Loop until the stack is empty
    while stack:
        # Pop the current cell off the stack
        current = stack.pop()
        
        # Mark the current cell as visited
        maze[current[1]][current[0]] = 1
        
        # Get the neighbors of the current cell
        neighbors = []
        if current[0] &gt; 0:
            neighbors.append((current[0] - 1, current[1]))
        if current[0] &lt; NUM_COLS - 1:
            neighbors.append((current[0] + 1, current[1]))
        if current[1] &gt; 0:
            neighbors.append((current[0], current[1] - 1))
        if current[1] &lt; NUM_ROWS - 1:
            neighbors.append((current[0], current[1] + 1))
        
        # Shuffle the neighbors
        random.shuffle(neighbors)
        
        # Loop through the neighbors
        for neighbor in neighbors:
            # If the neighbor has not been visited
            if maze[neighbor[1]][neighbor[0]] == 0:
                # Mark the neighbor as visited
                maze[neighbor[1]][neighbor[0]] = 1
                
                # Remove the wall between the current cell and the neighbor
                if neighbor[0] &lt; current[0]:
                    maze[current[1]][current[0] - 1] = 1
                elif neighbor[0] &gt; current[0]:
                    maze[current[1]][current[0] + 1] = 1
                elif neighbor[1] &lt; current[1]:
                    maze[current[1] - 1][current[0]] = 1
                else:
                    maze[current[1] + 1][current[0]] = 1
                
                # Add the neighbor to the stack
                stack.append(neighbor)

# Define a function to draw the maze
def draw_maze():
    for i in range(NUM_ROWS):
        for j in range(NUM_COLS):
            x = j * CELL_SIZE
            y = i * CELL_SIZE
            
            # Draw the walls of the cell
            if maze[i][j] == 0:
                pygame.draw.line(SCREEN, BLACK, (x, y), (x + CELL_SIZE, y))
                pygame.draw.line(SCREEN, BLACK, (x + CELL_SIZE, y), (x + CELL_SIZE, y + CELL_SIZE))
                pygame.draw.line(SCREEN, BLACK, (x + CELL_SIZE, y + CELL_SIZE), (x, y + CELL_SIZE))
                pygame.draw.line(SCREEN, BLACK, (x, y + CELL_SIZE), (x, y))

# Define the player's starting position
player_row = 0
player_col = 0

# Define a function to draw the player
def draw_player(row, col):
    x = col * CELL_SIZE
    y = row * CELL_SIZE
    pygame.draw.rect(SCREEN, RED, (x, y, CELL_SIZE, CELL_SIZE))

# Generate the maze
generate_maze()

# Draw the maze
draw_maze()

# Game loop
player_row = 0
player_col = 0
running = True
while running:
    # Handle events
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            running = False
        elif event.type == pygame.KEYDOWN:
            if event.key == pygame.K_UP:
                player_row -= 1
            elif event.key == pygame.K_DOWN:
                player_row += 1
            elif event.key == pygame.K_LEFT:
                player_col -= 1
            elif event.key == pygame.K_RIGHT:
                player_col += 1
    
    # Clear the screen
    SCREEN.fill(WHITE)
    
    # Draw the maze and player
    draw_maze()
    draw_player(player_row, player_col)
    
    # Update the display
    pygame.display.update()

# Quit Pygame
pygame.quit()

</code></pre>
<p>I am using Repl.it and it's just outputting a blank white screen with a red dot in the Screen. I've tried troubleshooting it and rewriting and stuff and its not working. Its really frustrating please help me. I also used Chat GPT to try and trouble shoot it and it did not help at all. It's incredible how useless it can be.</p>
",https://stackoverflow.com/questions/76023358/i-am-trying-to-create-a-maze-game-in-pygame-but-its-not-working,False,
76023357,"IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed sklearn confusion matrix","<p>I encounter Index error when I use confusion matrix from sklearn library. I have 2 arrays: y_train and y_train_pred. Their shapes are 1 dimension (1000,) and there were 3 classes: 0, 1, and 2. However, when I use the following code to get confusion matrix, I get</p>
<blockquote>
<p>IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed</p>
</blockquote>
<pre><code>from sklearn.metrics import confusion_matrix
cm_train = confusion_matrix(y_train, y_train_pred)
</code></pre>
<p>How to solve the issue?</p>
",https://stackoverflow.com/questions/76023357/indexerror-too-many-indices-for-array-array-is-1-dimensional-but-2-were-index,False,
76023355,How to extract all pointers with all forms from xml file using minidom and python (i used srcml to parse from c to xml file),"<p>Thanks in advance ----</p>
<p>Ivtried the normal way using name and operator tags
But sometimes it works and majority of time doesnt</p>
",https://stackoverflow.com/questions/76023355/how-to-extract-all-pointers-with-all-forms-from-xml-file-using-minidom-and-pytho,False,
76023332,"Calculate precision ,recall using timm in pytorch","<p>I want Calculate precision ,recall using <strong>timm in pytorch</strong>, I have target and prediction array.
How can I do this using <strong>&quot;timm.utilis&quot;</strong>?</p>
<p>I want to calculate precision, recall , AUC</p>
",https://stackoverflow.com/questions/76023332/calculate-precision-recall-using-timm-in-pytorch,False,
76023324,Beautifulsoup Findall() returns empty list [],"<p>Im learning Web Scraping, I've writen a code to get the <code>&lt;li&gt;</code> list of connectors:</p>
<pre><code>from bs4 import BeautifulSoup
import requests


html_text = requests.get('https://www.workato.com/integrations/salesforce').text

soup = BeautifulSoup(html_text,'lxml')

apps = soup.find_all('span', {'class':'adapter-list__item-name'})

print(soup)
</code></pre>
<p>but I just got the empty value</p>
<pre><code>[]
</code></pre>
<p>What should I do to improve my code?</p>
",https://stackoverflow.com/questions/76023324/beautifulsoup-findall-returns-empty-list,False,
76023269,How to apply a rotation to a node in trimesh,"<p>I have a <code>.glb</code> glTF file that specifies a character.
The character has 75 nodes, and no animation track.</p>
<p>I wrote the following code to render the character, and now I would like to apply some rotation to specific nodes before I render it.</p>
<p>My rendering code is:</p>
<pre class=""lang-py prettyprint-override""><code>import math

import pyrender
import numpy as np
from PIL import Image
import trimesh


def rotation_x(angle_degrees):
    angle = angle_degrees * math.pi / 180
    return np.array([
        [1, 0, 0, 0],
        [0, math.cos(angle), -math.sin(angle), 0],
        [0, math.sin(angle), math.cos(angle), 0],
        [0, 0, 0, 1],
    ])


class Renderer:
    def __init__(self, character_path: str):
        self.mesh = trimesh.load(character_path)

        # Just trying some random rotations
        self.rotate(&quot;mixamorig:LeftUpLeg&quot;, np.array([0.707, 1.0, 0.0, 0.707]))
        self.rotate(&quot;mixamorig:RightHand&quot;, np.array([0.707, 5.0, 0.0, 0.707]))
        self.rotate(&quot;mixamorig:LeftHand&quot;, np.array([0.707, 2.0, 0.0, 0.707]))
        self.rotate(&quot;mixamorig:Spine&quot;, np.array([1.707, 2.0, 0.0, 0.707]))

        self.scene = self.make_scene()
        self.renderer = pyrender.OffscreenRenderer(viewport_width=1024, viewport_height=1024)

    def rotate(self, node_name: str, quaternion: np.ndarray):
        raise NotImplementedError()

    def make_scene(self):
        # Create a pyrender scene
        scene = pyrender.Scene()

        # Add the mesh to the scene
        for geometry in self.mesh.geometry.values():
            node = pyrender.Mesh.from_trimesh(geometry, smooth=True)
            scene.add(node)

        # Set up a camera with an orthographic projection
        camera = pyrender.PerspectiveCamera(yfov=float(np.radians(54)), aspectRatio=1)
        camera_pose = np.array([
            [1, 0, 0, 0],
            [0, 1, 0, 3.5],  # Move the camera 1 meter up in the y-axis
            [0, 0, 1, 4],
            [0, 0, 0, 1],
        ]) @ rotation_x(-20)
        scene.add(camera, pose=camera_pose)

        # Set up a directional light
        light = pyrender.DirectionalLight(color=np.ones(3), intensity=3.0)
        light_pose = np.array([
            [1, 0, 0, 0],
            [0, 1, 0, 5],
            [0, 0, 1, 3],
            [0, 0, 0, 1],
        ])
        scene.add(light, pose=light_pose)
        return scene


    def __call__(self):
        color, depth = self.renderer.render(self.scene)
        return color, depth
</code></pre>
<p>I have tried many solutions, but to no avail.
This solution returns no errors, but also does not perform the rotation.</p>
<pre class=""lang-py prettyprint-override""><code>    def rotate(self, node_name: str, quaternion: np.ndarray):
        rotation_matrix = trimesh.transformations.quaternion_matrix(quaternion)
        self.mesh.graph[node_name][0].setflags(write=1)
        self.mesh.graph[node_name][0][:] = rotation_matrix
</code></pre>
<p><strong>How can I control the rotation of a specific node before rendering it?</strong></p>
<p>For testing purposes, the character I am using is:
<a href=""https://firebasestorage.googleapis.com/v0/b/sign-mt-assets/o/3d%2Fcharacter.glb?alt=media"" rel=""nofollow noreferrer"">https://firebasestorage.googleapis.com/v0/b/sign-mt-assets/o/3d%2Fcharacter.glb?alt=media</a></p>
",https://stackoverflow.com/questions/76023269/how-to-apply-a-rotation-to-a-node-in-trimesh,False,
76023252,Can&#39;t use pyinstaller to convert my py file to exe,"<p>I have already installed pyinstaller
I am trying to use pyinstaller in cmd but I receive error:</p>
<p>command:</p>
<pre><code>pyinstaller --onefile --windowed --icon=mc.ico mcservmng.py
</code></pre>
<p>error :</p>
<pre><code>pyinstaller is not recognized as internal or external command
</code></pre>
",https://stackoverflow.com/questions/76023252/cant-use-pyinstaller-to-convert-my-py-file-to-exe,False,
76023246,inside nested loops appending list to a new list gives unexpected result,"<p>I have tried to write my error issue in reproducible manner for platform to be seen and guided. I cannot see my logic gap why this error happens.
I have a inner loop which brings new elements while scraping and appends it to list named <code>list_inner</code>. Then in outer loop list named <code>list_outer</code> appends that new list. But final result gives amount of members right, but elements of list list_outer are same, the last list element of list <code>list_inner</code>. How can this happen? If it will be one elemented list I will understand.</p>
<pre><code>import random
list_inner=[]
list_outer=[]
for i in range(5):
    for r in range(random.randint(1,10)):
        list_inner.append(r)
        print(r)
    list_outer.append(list_inner)
    print(list_outer)
print(list_outer)
</code></pre>
<p>I am sharing for two results, as giving idea what is in real and what I was expecting. I got this result:</p>
<pre><code>0
1
2
3
[[0, 1, 2, 3]]
0
1
2
3
4
[[0, 1, 2, 3, 0, 1, 2, 3, 4], [0, 1, 2, 3, 0, 1, 2, 3, 4]]
</code></pre>
<p>But I was expecting this result:</p>
<pre><code>[[0,1,2,3],[0,1,2,3,4]]
</code></pre>
",https://stackoverflow.com/questions/76023246/inside-nested-loops-appending-list-to-a-new-list-gives-unexpected-result,True,76023293
76023235,Cannot reinitialise DataTable in Django,"<p>I'm confused on how can I infuse my json format from django to Datatable , what I tried below using  loop in the script</p>
<p><code>{% for json in company %}</code> <code>&lt;script&gt;..&lt;/script&gt;</code>  <code>{% endfor% }</code></p>
<p>but It persist error  <code>DataTables warning: table id=DataTables_Table_0 - Cannot reinitialise DataTable.</code></p>
<p>Is there's any wrong with my implementation, Am I correct? to fetch all data from views.py json to datatable , is there anyway?</p>
<p><strong>Javascript</strong></p>
<pre><code>{% block footer_scripts %}
{% for row in company %}
&lt;script&gt;

$(function () {
  var dt_basic_table = $('.datatables-basic'),
    dt_complex_header_table = $('.dt-complex-header'),
    dt_row_grouping_table = $('.dt-row-grouping'),
    dt_multilingual_table = $('.dt-multilingual'),
    dt_basic;

  // DataTable with buttons
  // --------------------------------------------------------------------

  if (dt_basic_table.length) {
    dt_basic = dt_basic_table.DataTable({
      ajax: assetsPath + 'json/table-datatable_example.json',  //I want to infused here the json format from views.py 
      columns: [
        { data: '' },
        { data: 'id' },
        { data: 'id' },
        { data: 'full_name' },
        { data: 'email' },
        { data: 'start_date' },
        { data: 'salary' },
        { data: 'status' },
        
        { data: '' }
      ],
      columnDefs: [
        {
          // For Responsive
          className: 'control',
          orderable: false,
          searchable: false,
          responsivePriority: 2,
          targets: 0,
          render: function (data, type, full, meta) {
            return '';
          }
        },
        {
          // For Checkboxes
          targets: 1,
          orderable: false,
          searchable: false,
          responsivePriority: 3,
          checkboxes: true,
          render: function () {
            return '&lt;input type=&quot;checkbox&quot; class=&quot;dt-checkboxes form-check-input&quot;&gt;';
          },
          checkboxes: {
            selectAllRender: '&lt;input type=&quot;checkbox&quot; class=&quot;form-check-input&quot;&gt;'
          }
        },
        {
          targets: 2,
          searchable: false,
          visible: false
        },
        {
          // Avatar image/badge, Name and post
          targets: 3,
          responsivePriority: 4,
          render: function (data, type, full, meta) {
            var $user_img = full['avatar'],
              $name = full['full_name'],
              $post = full['post'];
            if ($user_img) {
              // For Avatar image
              var $output =
                '&lt;img src=&quot;' + assetsPath + 'img/avatars/' + $user_img + '&quot; alt=&quot;Avatar&quot; class=&quot;rounded-circle&quot;&gt;';
            } else {
              // For Avatar badge
              var stateNum = Math.floor(Math.random() * 6);
              var states = ['success', 'danger', 'warning', 'info', 'primary', 'secondary'];
              var $state = states[stateNum],
                $name = full['full_name'],
                $initials = $name.match(/\b\w/g) || [];
              $initials = (($initials.shift() || '') + ($initials.pop() || '')).toUpperCase();
              $output = '&lt;span class=&quot;avatar-initial rounded-circle bg-label-' + $state + '&quot;&gt;' + $initials + '&lt;/span&gt;';
            }
            // Creates full output for row
            var $row_output =
              '&lt;div class=&quot;d-flex justify-content-start align-items-center user-name&quot;&gt;' +
              '&lt;div class=&quot;avatar-wrapper&quot;&gt;' +
              '&lt;div class=&quot;avatar me-2&quot;&gt;' +
              $output +
              '&lt;/div&gt;' +
              '&lt;/div&gt;' +
              '&lt;div class=&quot;d-flex flex-column&quot;&gt;' +
              '&lt;span class=&quot;emp_name text-truncate&quot;&gt;' +
              $name +
              '&lt;/span&gt;' +
              '&lt;small class=&quot;emp_post text-truncate text-muted&quot;&gt;' +
              $post +
              '&lt;/small&gt;' +
              '&lt;/div&gt;' +
              '&lt;/div&gt;';
            return $row_output;
          }
        },
        {
          responsivePriority: 1,
          targets: 4
        },
        {
          // Label
          targets: -2,
          render: function (data, type, full, meta) {
            var $status_number = full['status'];
            var $status = {
              1: { title: 'Current', class: 'bg-label-primary' },
              2: { title: 'Professional', class: ' bg-label-success' },
              3: { title: 'Rejected', class: ' bg-label-danger' },
              4: { title: 'Resigned', class: ' bg-label-warning' },
              5: { title: 'Applied', class: ' bg-label-info' }
            };
            if (typeof $status[$status_number] === 'undefined') {
              return data;
            }
            return (
              '&lt;span class=&quot;badge ' + $status[$status_number].class + '&quot;&gt;' + $status[$status_number].title + '&lt;/span&gt;'
            );
          }
        },
      
      ],
  });
    $('div.head-label').html('&lt;h5 class=&quot;card-title mb-0&quot;&gt;Admin&lt;/h5&gt;');
  }
});


&lt;/script&gt;
{% endfor %}
{% endblock footer_scripts %}
</code></pre>
<p><strong>This is my Views.py</strong> I used json to convert data to jason format and fetch it inside datatable.</p>
<pre><code>from main.models import ( Company,Unit )
import json 
def company(request):
    qs_json = serializers.serialize('json', Company.objects.filter().order_by('name'))
    context = {
        'company' : qs_json,
    }
    return render(request, 'admin/company.html', context)
</code></pre>
<p><strong>Updated:Table headers</strong>
my tbody is on my script so should i used loop there?</p>
<pre><code>   &lt;div class=&quot;card&quot;&gt;
        &lt;div class=&quot;card-datatable table-responsive pt-0&quot;&gt;
        &lt;table class=&quot;datatables-basic table&quot;&gt;
            &lt;thead&gt;
            &lt;tr&gt;
                &lt;th&gt;&lt;/th&gt;
                &lt;th&gt;&lt;/th&gt;
                &lt;th&gt;id&lt;/th&gt;
                &lt;th&gt;Name&lt;/th&gt;
                &lt;th&gt;Code&lt;/th&gt;
                &lt;th&gt;Address&lt;/th&gt;
                &lt;th&gt;Remarks&lt;/th&gt;
                &lt;th&gt;Status&lt;/th&gt;
                &lt;th&gt;Action&lt;/th&gt;
            &lt;/tr&gt;
            &lt;/thead&gt;
        &lt;/table&gt;
        &lt;/div&gt;
    &lt;/div&gt;
</code></pre>
",https://stackoverflow.com/questions/76023235/cannot-reinitialise-datatable-in-django,False,
76023127,Unable to display continuous output from a python script to HTML view in django,"<p>I am trying to display the continuous output from a python script on the HTML page but the output is being displayed once the entire execution of the script is completed. I have used subprocess to run the python script(test.py) and when I execute the python script independently I could see the output line by line but the same is not working when I use it along with websockets. Can anyone please help me fix the problem?</p>
<p>Here is my code:</p>
<p><strong>consumers.py</strong></p>
<pre class=""lang-py prettyprint-override""><code>import os
import subprocess
from channels.generic.websocket import AsyncWebsocketConsumer


class MyAsyncWebsocketConsumer(AsyncWebsocketConsumer):

    async def connect(self):
        print(&quot;Websocket connected&quot;)
        print(&quot;Channel name...&quot;, self.channel_name)
        print(&quot;Channel layer...&quot;, self.channel_layer)
        await self.channel_layer.group_add('programmers', self.channel_name)
        await self.accept()

    async def receive(self, text_data=None, bytes_data=None):
        print(&quot;Message received from the client&quot;, text_data)
        current_dir = os.getcwd()
        script = current_dir + '/' + 'test.py'
        try:
            p = subprocess.Popen(['python', script], stdout=subprocess.PIPE, bufsize=3)
            for line in iter(p.stdout.readline, b''):
                line = line.decode(&quot;utf-8&quot;).strip()
                await self.send(text_data=line)
        except subprocess.CalledProcessError as e:
            print(e)

    async def disconnect(self, code):
        print(&quot;Websocket disconnected&quot;, code)
</code></pre>
<p><strong>index.html</strong></p>
<pre class=""lang-py prettyprint-override""><code>&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;
&lt;head&gt;
   &lt;meta charset=&quot;UTF-8&quot;&gt;
   &lt;title&gt;Index&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
   &lt;h3&gt;Count Page&lt;/h3&gt;
   &lt;textarea id=&quot;chat-log&quot; cols=&quot;100&quot; rows=&quot;20&quot;&gt;

  &lt;/textarea&gt;&lt;br&gt;

   &lt;input type=&quot;button&quot; value=&quot;Submit&quot; id=&quot;script-run-submit&quot;&gt;


   &lt;script&gt;
       var ws = new WebSocket('ws://127.0.0.1:8000/ws/awsc/')

        ws.onopen = function() {
            console.log(&quot;Websocket connection opened&quot;)
        }

        ws.onmessage = function(event) {
            console.log(&quot;Message received from the server&quot;, event)
            data = event['data']
            document.querySelector('#chat-log').value += (data + '\n')
        }

        ws.onerror = function(event) {
            console.log(&quot;Websocket error occurred&quot;, event)
        }

         ws.onclose = function(event) {
            console.log(&quot;Websocket connection close&quot;, event)
        }

        document.getElementById('script-run-submit').onclick = function (event) {
               console.log(&quot;This function is executed&quot;)
               ws.send(JSON.stringify({ 'msg' : 'Trigger the script' }))
       }

   &lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p><strong>Expectation</strong></p>
<p>The script output should be displayed line by line in the HTML page</p>
<p><strong>Actual output</strong></p>
<p>All the output of the script is displayed at once in the HTML page</p>
",https://stackoverflow.com/questions/76023127/unable-to-display-continuous-output-from-a-python-script-to-html-view-in-django,False,
76023125,Import Error then I try to import tfds. Using Tensorflow-Macos,"<p>I want to test out Tensorflow on my Macbook Air (2020), but when I try to import Tensorflow-Datasets I get some Errors. I used miniconda to create an environment and to use Tensorflow for macOS.</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Cell In[2], line 3
      1 import tensorflow as tf
      2 # Brining in tensorflow datasets for fashion mnist 
----&gt; 3 import tensorflow_datasets as tfds
      4 # Bringing in matplotlib for viz stuff
      5 from matplotlib import pyplot as plt

File ~/miniconda/envs/env-tf2/lib/python3.10/site-packages/tensorflow_datasets/__init__.py:43
     41 _TIMESTAMP_IMPORT_STARTS = time.time()
     42 from absl import logging
---&gt; 43 import tensorflow_datasets.core.logging as _tfds_logging
     44 from tensorflow_datasets.core.logging import call_metadata as _call_metadata
     46 _metadata = _call_metadata.CallMetadata()

File ~/miniconda/envs/env-tf2/lib/python3.10/site-packages/tensorflow_datasets/core/__init__.py:22
     18 # Allow to use `tfds.core.Path` in dataset implementation which seems more
     19 # natural than having to import a third party module.
     20 from etils.epath import Path
---&gt; 22 from tensorflow_datasets.core import community
     23 from tensorflow_datasets.core.dataset_builder import BeamBasedBuilder
     24 from tensorflow_datasets.core.dataset_builder import BuilderConfig

File ~/miniconda/envs/env-tf2/lib/python3.10/site-packages/tensorflow_datasets/core/community/__init__.py:18
      1 # coding=utf-8
      2 # Copyright 2023 The TensorFlow Datasets Authors.
      3 #
   (...)
     13 # See the License for the specific language governing permissions and
     14 # limitations under the License.
     16 &quot;&quot;&quot;Community dataset API.&quot;&quot;&quot;
---&gt; 18 from tensorflow_datasets.core.community.huggingface_wrapper import mock_builtin_to_use_gfile
     19 from tensorflow_datasets.core.community.huggingface_wrapper import mock_huggingface_import
     20 from tensorflow_datasets.core.community.load import builder_cls_from_module

File ~/miniconda/envs/env-tf2/lib/python3.10/site-packages/tensorflow_datasets/core/community/huggingface_wrapper.py:31
     28 from unittest import mock
     30 from etils import epath
---&gt; 31 from tensorflow_datasets.core import dataset_builder
     32 from tensorflow_datasets.core import dataset_info
     33 from tensorflow_datasets.core import download

File ~/miniconda/envs/env-tf2/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:34
     32 from etils import epath
     33 from tensorflow_datasets.core import constants
---&gt; 34 from tensorflow_datasets.core import dataset_info
     35 from tensorflow_datasets.core import dataset_metadata
     36 from tensorflow_datasets.core import decode

File ~/miniconda/envs/env-tf2/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_info.py:47
     45 from etils import epath
     46 from tensorflow_datasets.core import constants
---&gt; 47 from tensorflow_datasets.core import file_adapters
     48 from tensorflow_datasets.core import lazy_imports_lib
     49 from tensorflow_datasets.core import naming

File ~/miniconda/envs/env-tf2/lib/python3.10/site-packages/tensorflow_datasets/core/file_adapters.py:29
     26 from tensorflow_datasets.core.utils import type_utils
     27 from tensorflow_datasets.core.utils.lazy_imports_utils import tensorflow as tf
---&gt; 29 from array_record.python import array_record_module
     31 ExamplePositions = List[Any]
     34 class FileFormat(enum.Enum):

ImportError: dlopen(/Users/lars/miniconda/envs/env-tf2/lib/python3.10/site-packages/array_record/python/array_record_module.so, 0x0002): tried: '/Users/lars/miniconda/envs/env-tf2/lib/python3.10/site-packages/array_record/python/array_record_module.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/lars/miniconda/envs/env-tf2/lib/python3.10/site-packages/array_record/python/array_record_module.so' (no such file), '/Users/lars/miniconda/envs/env-tf2/lib/python3.10/site-packages/array_record/python/array_record_module.so' (not a mach-o file)
</code></pre>
<p>I use Python 3.10.9 and following versions:</p>
<p>Tensorflow-Macos: 2.9.1
Tensorflow-Metal: 0.5.1
Tensorflow-Datasets: 4.9.0</p>
",https://stackoverflow.com/questions/76023125/import-error-then-i-try-to-import-tfds-using-tensorflow-macos,False,
76023117,Scrapy in python - save within the parse function -thread safe?,"<p>I'm using scrapy to download pages. I would like to save all the downloaded pages in a single file. I have the following code for the constructor and parse:</p>
<pre><code>    def __init__(self):
        self.time = time_utils.get_current_time_hr()
        self.folder = f&quot;{ROOT_DIR}/data/tickers/scrapy/{self.time}/&quot;
        os.makedirs(self.folder, exist_ok=True)
        filename = self.folder + &quot;bigfile.txt&quot;
        self.f = open(filename, 'w')

    def parse(self, response):
        buffer = list()
        buffer.append(response.body.decode(&quot;utf-8&quot;) )
        self.f.write(&quot;&quot;.join(buffer))
        self.f.flush()
</code></pre>
<p>Is there a possibility that different html pages will be mixed in the big_file.txt I'm writing?</p>
",https://stackoverflow.com/questions/76023117/scrapy-in-python-save-within-the-parse-function-thread-safe,True,
76023097,"Getting 1327, &#39;Undeclared variable: 30&#39; error","<p>Getting sql error in python flask:</p>
<blockquote>
<p>MySQLdb.OperationalError: (1327, 'Undeclared variable: 0')</p>
</blockquote>
<p>My code:</p>
<pre><code>n = 30
val = 'products'
sql = &quot;&quot;&quot;SELECT * FROM `%s` limit `%s`&quot;&quot;&quot; % (val, n)
cur.execute(sql)
products = cur.fetchall()
</code></pre>
<p>query is working fine when executed in database admin:
<code>SELECT * FROM products limit 0, 30;</code></p>
",https://stackoverflow.com/questions/76023097/getting-1327-undeclared-variable-30-error,False,
76023089,How to connect DigitalOcean Function to MySQL?,"<p>I have created a simple Python Function (section Functions and NOT Apps). The function needs to access the MySQL database (the database is also created on DigitalOcean). When running the function I get an error: &quot;2023-04-15T13:55:42.402632917Z stderr: The function exceeded its time limits of 6000 milliseconds. Logs might be missing.&quot;</p>
<p>The function is very basic, so 6 sec should more than enough to run successfuly. When it is trying to connect to DB the timeout occurs somehow.</p>
<p>If I comment out the lines which connect to DB than the app runs just fine without errors.</p>
",https://stackoverflow.com/questions/76023089/how-to-connect-digitalocean-function-to-mysql,False,
76023085,Native messaging not working from host to chrome extension,"<p>I am working on a chrome extension native messaging with the host on Python. The communication from extension to python works perfectly fine. But when I am trying to send the message from the host to the extension, it is not working. I am getting no errors. Even I have enabled logging for the browser. I have tested on Linux and Mac with the same issue. Below is my code. Any help would be appreciated.</p>
<p><strong>background.js</strong></p>
<pre><code>const port = chrome.runtime.connectNative(&quot;com.google.chrome.uniphore&quot;);
port.onDisconnect.addListener((p) =&gt; console.log(chrome.runtime.lastError));

port.postMessage(&quot;ping&quot;)

port.onMessage.addListener(function (msg) {
  console.log('Received' + msg);
  return false;
});

chrome.runtime.onInstalled.addListener((reason) =&gt; {
  console.log(reason);
});
</code></pre>
<p><strong>main.py</strong></p>
<pre><code>import sys
import json
import struct



def getMessage():
    try:
        rawLength = sys.stdin.buffer.read(4)
        if len(rawLength) == 0:
            sys.exit(0)
        messageLength = struct.unpack('@I', rawLength)[0]
        message = sys.stdin.buffer.read(messageLength).decode('utf-8')
        return json.loads(message)
    except Exception as x:
        print(&quot;Error&quot;, x)

# Send an encoded message to stdout
def sendMessage(encodedMessage):
    try:
        sys.stdout.buffer.write(encodedMessage[&quot;length&quot;])
        sys.stdout.buffer.write(encodedMessage[&quot;content&quot;])
        sys.stdout.flush()
    except Exception as x:
        print(&quot;Error&quot;, x)

def encodeMessage(messageContent):
    # https://docs.python.org/3/library/json.html#basic-usage
    # To get the most compact JSON representation, you should specify 
    # (',', ':') to eliminate whitespace.
    # We want the most compact representation because the browser rejects
    # messages that exceed 1 MB.
    encoded_content = json.dumps(messageContent).encode(&quot;utf-8&quot;)
    encoded_length = struct.pack(&quot;@I&quot;, len(encoded_content))
    return {&quot;length&quot;: encoded_length, &quot;content&quot;: encoded_content}

while True:
    receivedMessage = getMessage()
    if receivedMessage == &quot;ping&quot;:
        sendMessage(encodeMessage(&quot;pong&quot;))
</code></pre>
<p><strong>run.sh</strong></p>
<pre><code>#!/bin/sh
python3 main.py &gt;&gt; hello 2&gt;&gt; err_file
</code></pre>
<p><strong>com.google.chrome.echo.json</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;com.google.chrome.echo&quot;,
    &quot;description&quot;: &quot;echo Application&quot;,
    &quot;path&quot;: &quot;/Applications/native/run.sh&quot;,
    &quot;type&quot;: &quot;stdio&quot;,
    &quot;allowed_origins&quot;: [
      &quot;chrome-extension://bmfbcejdknlknpncfpeloejonjoledha/&quot;
    ]
}
</code></pre>
",https://stackoverflow.com/questions/76023085/native-messaging-not-working-from-host-to-chrome-extension,False,
76023066,A simple multiplication code embedded in html with pyscript,"<p>I want to create a html web page that runs a python code via pyscript to multiply two given integers as follows:</p>
<ul>
<li>The integers &quot;a&quot; and &quot;b&quot; are given by an html form</li>
<li>The output is printed as a*b in the &quot;Output&quot; area of the html page</li>
</ul>
<p>Any suggestion would be preciated.</p>
<p>Here is what I did:</p>
<p>`</p>


<p>Multiply</p>





<pre><code>&lt;form onsubmit=&quot;return false&quot;&gt;
    &lt;label for=&quot;a&quot;&gt;First number&lt;/label&gt;&lt;br&gt;
    &lt;input type=&quot;integer&quot; id=&quot;a&quot; name=&quot;name&quot; value=&quot;&quot;&gt;&lt;br&gt;

    &lt;label for=&quot;b&quot;&gt;Second number&lt;/label&gt;&lt;br&gt;
    &lt;input type=&quot;integer&quot; id=&quot;b&quot; name=&quot;name&quot; value=&quot;&quot;&gt;&lt;br&gt;

    &lt;input pys-onClick=&quot;sub&quot; type=&quot;submit&quot; id=&quot;btn-form&quot; value=&quot;submit&quot;&gt;


&lt;/form&gt; 

&lt;p&gt;Output:&lt;/p&gt;
&lt;p id = 'output'&gt;&lt;/p&gt;
&lt;py-script&gt;

    def sub(*args,**kwargs):
        a = {Element('a').value}
        b = {Element('b').value}
        result_place = Element('output')
        result_place.write( a * b )

&lt;/py-script&gt;
</code></pre>

` 
",https://stackoverflow.com/questions/76023066/a-simple-multiplication-code-embedded-in-html-with-pyscript,True,
76023060,Polars Expression for Aggregation of Sums Across Variable Length Groups,"<p>This is a two part question. Firstly how to a achieve a polars expression such that:</p>
<pre><code>import datetime
import polars as pl
pl.Config.set_tbl_rows(100)

df = pl.DataFrame(
    {
        &quot;id&quot;: [&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;D&quot;, &quot;D&quot;],
        &quot;value&quot;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
        &quot;category&quot;: [&quot;X&quot;, &quot;X&quot;, &quot;X&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;X&quot;, &quot;X&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;Y&quot;],
    }
)

expr = ...alias(&quot;feature&quot;)
df = df.with_column(expr)
print(df)
</code></pre>
<p>yields</p>
<pre><code>shape: (12, 4)
┌─────┬───────┬─────────┬──────────────────────┐
│ id  ┆ value ┆ category┆ feature              │ 
│ --- ┆ ---   ┆ ---     ┆ ---                  │
│ str ┆ i64   ┆ str     ┆ f64                  │
╞═════╪═══════╪═════════╪══════════════════════╡
│ A   ┆ 1     ┆ X       ┆ null                 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ A   ┆ 2     ┆ X       ┆ null                 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ A   ┆ 3     ┆ X       ┆ null                 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ A   ┆ 4     ┆ X       ┆ null                 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ B   ┆ 5     ┆ Y       ┆ null                 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ B   ┆ 6     ┆ Y       ┆ null                 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ B   ┆ 7     ┆ Y       ┆ null                 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ C   ┆ 8     ┆ X       ┆ 2.7                  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ C   ┆ 9     ┆ X       ┆ 2.7                  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ C   ┆ 10    ┆ X       ┆ 2.7                  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ D   ┆ 11    ┆ Y       ┆ 1.27777777           │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ D   ┆ 12    ┆ Y       ┆ 1.27777777           │
└─────┴───────┴─────────┴──────────────────────┘
</code></pre>
<p>such that <code>feature</code> is the ratio of the sums of the current id to the previous id in that particular category.
that is:</p>
<pre><code>2.7 = (8 + 9 + 10) / (1 + 2 + 3 + 4)
2.7 = (sum of value id &quot;C&quot;) / (sum of value in previously occurring group in category `X`. That is, `id == &quot;A&quot;`)
</code></pre>
<p>And then the second part:</p>
<p>I'd like to generalise this to rather than a ratio against the previous group's sum, to a ratio against an <code>ewm_mean</code> of the sum of all previous groups.</p>
<p>For the first part, I was able to achieve the result when the length of unique <code>id</code>s was constant. In this case you can shift by this lenght after a sum over <code>id</code> and <code>category</code> window expression. I cannot fathom how to achieve the result with variable length groups for either the first part or the second part.</p>
<p>I appreciate any response, so thank you in advance.</p>
",https://stackoverflow.com/questions/76023060/polars-expression-for-aggregation-of-sums-across-variable-length-groups,True,
76023025,How to sort dataset by frequency of genders in countries?,"<p>I am working on a project where I've been given a 'Customer Segmentation' csv and need to come up with some insights to derive from it using fairly basic Python techniques.</p>
<p>The csv data is grouped into columns covering the first name, last name, title, gender, city, country, country code, job industry, job title, and language of 3,740 customers.</p>
<p>One task I wanted to perform on this is producing an output of what the 3-5 most common genders are for various countries and jobs. (e.g. are some professions more female-dominated, do some countries have more prominent non-binary populations, etc.)</p>
<p>I'm really pretty new to Python and would greatly appreciate if someone can please steer me in the right direction as to how one would go about doing this as I don't even know what to search for to tackle this at present. Thanks</p>
",https://stackoverflow.com/questions/76023025/how-to-sort-dataset-by-frequency-of-genders-in-countries,False,
76022992,How would I check if a mechanical component is in its home position compared to a number of training images (Ok condition),"<p>Using openCV and python. What computer vision technique would be best to check whether a mechanical component is in its home position and located correctly when compared to a trained set of good condition images?</p>
<p>I've not yet tried any code as I'm looking for the best option to invest time into.</p>
",https://stackoverflow.com/questions/76022992/how-would-i-check-if-a-mechanical-component-is-in-its-home-position-compared-to,False,
76022988,ImportError: attempted relative import beyond top-level package,"<p>I am using django framework, Here i have created the first_app application in django, i have created a simple view which will return hello world http response,
I am trying to map this url in the urls.py, but it throws import_error by saying that &quot; attempted relative import beyond top-level package&quot;, i have attached the project folder structure below for reference,
<a href=""https://i.stack.imgur.com/i2aNp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i2aNp.png"" alt=""folder strcuture"" /></a></p>
<p><a href=""https://i.stack.imgur.com/VTV4n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VTV4n.png"" alt=""error message"" /></a></p>
<p>How do i need import that view in urls.py file which are in above attached folder structure.</p>
<p>I am using this pattern &quot;<strong>from ..first_app import views</strong>&quot; which throws error like attached format</p>
",https://stackoverflow.com/questions/76022988/importerror-attempted-relative-import-beyond-top-level-package,False,
76022985,Web Scraping by BeautifulSoup if the data shown only if I click &quot;show details&quot;,"<p>I am trying to scrape data from selling cars website, when I enter the website I see a table of cars (type, price, year), but if I want to know more details about the car I have to click on the car and then it shows more details. How can i scrape data from the those details without Selenium?</p>
<pre><code>import headers
import requests
from bs4 import BeautifulSoup

page_num = 1
url = f&quot;https://www.example.com/vehicles/cars?page={page_num}&quot;

req = requests.get(url, headers=headers.get_headers()).text

soup = BeautifulSoup(req,&quot;html.parser&quot;)

def decide_row(soup):
    rows = soup.find_all('div',class_=&quot;feeditem table&quot;)
    return rows

def decide_details(rows):
    for car in rows:
        car_kilometrage = car.find('div',id='accordion_wide_0')
        print(car_kilometrage)

decide_details(decide_row(soup))
</code></pre>
",https://stackoverflow.com/questions/76022985/web-scraping-by-beautifulsoup-if-the-data-shown-only-if-i-click-show-details,True,76023647
76022975,is there a magic method for the mapping unpack operator?,"<p>Is it possible to modify the behavior of the mapping unpacking operator with a special method?</p>
<pre><code>**my_dict
</code></pre>
",https://stackoverflow.com/questions/76022975/is-there-a-magic-method-for-the-mapping-unpack-operator,False,
76022937,Use get function instead of if statement to assign values to keys in a dictionary,"<p>I was asked to replace the if statements in the following function histogram(s) with <code>d.get()</code>, which assigns a key a corresponding value.</p>
<pre><code>def histogram(s):
    d = dict()
    for c in s:
        if c not in d:
            d[c] = 1
        else:
            d[c] += 1
    return d
print(histogram('happy'))
</code></pre>
<p>This is what I tried:</p>
<pre><code>def ordered_items(s):
    d = dict()
    for c in s:  
        d[c] = 1
        d.get(c, 0)
    return d
print(ordered_items('happy'))
</code></pre>
<p>I was expecting it to take each character in s and assign it the value 1 and that's what happened, but what I need the program to do is give each key the correspond value that counts how many times the character appears in the word. I'm having trouble assigning the characters to keys in the dictionary and incrementing the value for every time another of the same character appears. Any suggestions?</p>
",https://stackoverflow.com/questions/76022937/use-get-function-instead-of-if-statement-to-assign-values-to-keys-in-a-dictionar,True,
76022924,Python function not working properly with f&#39;{col}&#39;,"<p>EDIT SUMMARY
So I have: p1 dataframe with the prices and the returns of n-financial instrument of the last 5 years:</p>
<pre><code>       Date         a_price   b_price   a_ret    b_ret
  0  2018-04-13      6.335     5.114    0.0047    0.01
         .
         .
         .
1272 2023-04-13      11.525     5.708    0.0039   -0.04
</code></pre>
<p>Then I have another dataframe seed, which contains only one date and the Standard deviations of the same n-instruments</p>
<pre><code>      Date         a_SD    b_SD
 63  2023-04-13   0.019   0.017
</code></pre>
<p>In p1 I need to create n-new columns EWMA as follows:
for the oldest date in p1 (2018-04-13), a_EWMA = X*(seed[a_SD]**2) + (1-X) *(p1[a_Ret]*<em>2)
for any other date a_EWMA = X</em>(p1.loc[i-1,a_EWMA]**2) + (1-X) *(p1[a_Ret]**2).</p>
<p>The desired output is p1 as follows:</p>
<pre><code>  Date            a_price   b_price   a_ret    b_ret   a_EWMA   b_EWMA
 0   2018-04-13    6.335     5.114    0.0047    0.01     ...     ...
         .
         .
         .
1272 2023-04-13   11.525     5.708    0.0039   -0.04     ...     ...
</code></pre>
<p>Since I need to automatize, I'm trying to use f'{col} to match the 2 dataframes but I think I'm not doing it correctly.
I'm doing it with the below function but it gives me all None results and worst it's creating too many columns while it should create only n-more (2 in this example)</p>
<pre><code>def EWMA(x):
    for i in p1.index:
        if p1.loc[i,'Date'] == min_Date and '_SD' in seed.columns:
            return math.sqrt((lam*(seed[[f'{col}_SD'for col in seed.columns]])**2) + (1-lam) * (p1.loc[i,[f'{col}_Ret' for col in p1.columns]]**2))
        elif p1.loc[i,'Date'] != min_Date and '_SD' in seed.columns:
            return math.sqrt(lam*((p1.loc[i-1,[f'{col}_EWMA'for col in p1.columns]])**2)+(1-lam)*(p1.loc[i,[f'{col}_Ret'for col in p1.columns]]**2)) if f'_Ret' in p1.columns:
    p1[[f'{col}_EWMA'for col in p1.columns]] = p1.apply(EWMA)
</code></pre>
",https://stackoverflow.com/questions/76022924/python-function-not-working-properly-with-fcol,False,
76022890,Why are my deep learning models giving unreasonably high accuracy on test data?,"<p>I'm trying to do sarcasm detection on Twitter data to replicate the results mentioned in this <a href=""https://aclanthology.org/2020.wnut-1.2.pdf"" rel=""nofollow noreferrer"">paper</a>. Binary classification problem. For that I used a separate set of <strong>unlabeled</strong> tweets to create the embedding matrix using Word2Vec model. Before doing that I preprocessed the <strong>unlabeled</strong> data and removed the rare words as mentioned in the paper. Code is as follows:</p>
<pre><code>model = Word2Vec(df_hing_eng['tweet_text'], vector_size=300, window=10, hs=0, negative = 1)
embedding_size = model.wv.vectors.shape[1]
</code></pre>
<p>Next I fit a tokenizer on this <strong>unlabeled</strong> data:</p>
<pre><code>tok = Tokenizer()
tok.fit_on_texts(df_hing_eng['tweet_text'])
vocab_size = len(tok.word_index) + 1
</code></pre>
<p>Next, I created the embedding matrix as follows:</p>
<pre><code>word_vec_dict={}
for word in vocab:
    word_vec_dict[word]=model.wv.get_vector(word)

embed_matrix=np.zeros(shape=(vocab_size,embedding_size))
for word,i in tok.word_index.items():
    embed_vector=word_vec_dict.get(word)
    if embed_vector is not None:  
        embed_matrix[i]=embed_vector 
</code></pre>
<p>Now, I'm using a separate set of <strong>labeled</strong> tweets to be used as training and test data (for the DL models). I used the same preprocessing steps as the <code>unlabeled</code> data and removed the same rare words we found in the <code>unlabeled</code> data. Now I find the maximum length of all tweets in the <strong>labeled</strong> data.</p>
<pre><code>maxi = -1
for row in df_labeled.loc[:,'tweet_text']:
    if len(row)&gt;maxi:
        maxi = len(row)
</code></pre>
<p>After that I used the tokenizer, that I fit on the <strong>unlabeled</strong> data, to create the word indices for the <strong>labeled</strong> data as follows:</p>
<pre><code>encoded_tweets = tok.texts_to_sequences(df_labeled['tweet_text'])
</code></pre>
<p>Now I padded the <strong>labeled</strong> data to the length of the maximum tweets among the <strong>labeled</strong> data.</p>
<pre><code>padded_tweets = pad_sequences(encoded_tweets, maxlen=maxi, padding='post')
</code></pre>
<p>Finally, I split the <strong>labeled</strong> data into training and test data as follows,</p>
<pre><code>x_train,x_test,y_train,y_test=train_test_split(padded_tweets, df_labeled['is_sarcastic'], test_size=0.10, random_state=42)
</code></pre>
<p><strong>Is there any data leakage anywhere from training to test data or any other problem?</strong> Almost all of my DL models are giving more than 90% accuracy contrary to the original paper which reported a maximum of 75% accuracy. The codes for DL models were written by the authors of the papers. I used the same parameters as they mentioned.</p>
<p>The tokenizer was actually fit on a completely different <strong>unlabeled</strong> data that is absolutely separate from (<strong>labeled</strong>) training and test data.</p>
<p><strong>Edit</strong></p>
<p>Further analyses of the datasets reveal the followings:</p>
<ol>
<li>There are huge overlaps between <strong>unlabeled</strong> embedding data and <strong>labeled</strong> data.</li>
<li>There are many duplicate rows in both <strong>labeled</strong> and <strong>unlabeled</strong> data.</li>
</ol>
<p>I shared the code of an LSTM below. These codes originally written by the authors of the paper.</p>
<pre><code>#define callbacks
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)
callbacks_list = [early_stopping]



 #this one---&gt;LSTM

    lstm_out1 = 150
    
    model = Sequential()
    model.add(Embedding(vocab_size, embed_dim,
              weights=[embed_matrix], input_length=max_tweet_len, trainable=False))
    model.add(LSTM(lstm_out1, dropout=0.2, recurrent_dropout=0.2))
    
    model.add(Dense(64, activation='relu'))
    
    model.add(Dense(1, activation='sigmoid'))
    
    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
    
    model.compile(loss='binary_crossentropy',
                      optimizer=adam,
                      metrics=['accuracy'])
    
    model.summary()
</code></pre>
",https://stackoverflow.com/questions/76022890/why-are-my-deep-learning-models-giving-unreasonably-high-accuracy-on-test-data,False,
76022862,Summing rows of a table in a for loop,"<p>I'm trying to create a function that takes a table and returns a list containing the sum of each row.
E.g.  <code>func([[0.2,0.1],[-0.2,0.1],[0.2,-0.1],[-0.2,-0.1]])</code> returns <code>[0.3, -0.1, 0.1, -0.3]</code>.</p>
<p>I know that the table is a 2D nested list, that each row contains only numbers and that each row is the same length.</p>
<p>I've got this code, but it seems to just return the last number in the list. Something is broken with the accumulators but I can't figure out what I'm doing wrong here.</p>
<pre class=""lang-py prettyprint-override""><code>num_rows = len(table)
num_columns = len(table[0])
result = []

for m in range(num_columns):
    row = []
    for n in range(num_rows):
        row.append(table[n][m])
        row = sum(row)

result.append(row)

return result
</code></pre>
",https://stackoverflow.com/questions/76022862/summing-rows-of-a-table-in-a-for-loop,False,
76022838,I am trying to incorporate several python dataframes of similar format into an sqllite table,"<p>I am reading historical ticker data into a dataframe. The dataframe has two columns - date, with daily dates. This is also the index column. The second column is named after the company ticker and has closing prices.</p>
<p>The dataframe (df_1) would look like this:</p>
<p><a href=""https://i.stack.imgur.com/FNDHY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FNDHY.png"" alt=""enter image description here"" /></a>
What I want to do is write this dataframe into an sqlite table.</p>
<p>I then want to add to this table, bringing in data from another dataframe (df_2)</p>
<p><a href=""https://i.stack.imgur.com/d6Yxs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d6Yxs.png"" alt=""enter image description here"" /></a></p>
<p>Ideally, I would like the sqlite table to have one 'date' column as the primary key and then several ticker columns. Where prices aren't available I am happy to get null values.</p>
<p>I have over 80k of dataframes, and I am reading them in one at a time. So I am keen on hearing about a more efficient solution. I am sure there is, I am relatively inexperienced. I have tried concat but doesn't seem the most efficient way to do it.</p>
<p>I have also tried adding a column and updating the SQLite table. However, it changes the values of previous tickers to null. I suspect it has something to do with the changes in the primary key</p>
",https://stackoverflow.com/questions/76022838/i-am-trying-to-incorporate-several-python-dataframes-of-similar-format-into-an-s,False,
76022822,How can i download this video on colab?,"<p>website generates a token then video starts playing. Now Matter what i do its not working. IF Someone can help me.</p>
<p><a href=""https://i.stack.imgur.com/fgqfP.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>i tried wget!</p>
<pre class=""lang-py prettyprint-override""><code>import requests

# Replace the URL with the actual URL of the mp4 video file you want to download
url = 'https://example.com/video-file.mp4'

# Send a GET request to the URL to retrieve the mp4 video file content
response = requests.get(url)

# Replace 'video-file.mp4' with the desired name and extension of the output file
output_file_name = 'video-file.mp4'

# Save the retrieved mp4 video file content to the specified output file
with open(output_file_name, 'wb') as f:
    f.write(response.content)
</code></pre>
",https://stackoverflow.com/questions/76022822/how-can-i-download-this-video-on-colab,False,
76022821,Python API/Lib for Datomic,"<p>does anyone know if there is a python implementation/API/Library or anything for Datomic.
Or any way to interact with a Datomic Database from Python ?</p>
<p>I've tried looking on the official Datomic Docs but nothing is explained. I would prefer to avoid Presto.</p>
",https://stackoverflow.com/questions/76022821/python-api-lib-for-datomic,False,
76022817,How to get rid of black lines caused of blending images?,"<p>I recently worked on a project that required blending multiple images together. I successfully completed all the necessary steps and successfully blended the images. However, I noticed that when I blended the images, black lines appeared on the borders of each piece of the final image, indicating where each image was placed. You can see an example of the final image with these artifacts in the following link:</p>
<p><a href=""https://i.stack.imgur.com/FQBg8.jpg"" rel=""nofollow noreferrer"">final image</a></p>
<p>These artifacts are noticeable even with a quick glance at the image.</p>
<p>This is my code for blending the images:</p>
<pre><code>def normalize_piece(img):
    return cv2.normalize(img.astype('float'), None, 0.0, 1.0, cv2.NORM_MINMAX)


def blend_pieces(piece1, piece2, transformation_matrix, width, height):
    piece1_norm = normalize_piece(piece1)
    piece2_norm = normalize_piece(piece2)

    piece1_warped = cv2.warpPerspective(src=piece1_norm, M=transformation_matrix, dsize=(width, height))

    black_rgb_pixel = np.zeros(3)
    mask_left = np.all(piece1_warped != black_rgb_pixel, axis=-1)
    mask_right = np.all(piece2_norm != black_rgb_pixel, axis=-1)

    mask_overlap = mask_left &amp; mask_right
    mask_left_only = mask_left &amp; ~mask_right
    mask_right_only = mask_right &amp; ~mask_left

    piece1_warped[mask_left_only] = piece1_warped[mask_left_only]
    piece1_warped[mask_right_only] = piece2_norm[mask_right_only]
    piece1_warped[mask_overlap] = (piece1_warped[mask_overlap] + piece2_norm[mask_overlap]) / 2

    return piece1_warped
</code></pre>
<h1>Edit</h1>
<p>I have these 3 parts of the final image for example and I want to merge them together:</p>
<p><a href=""https://i.stack.imgur.com/M0O9A.jpg"" rel=""nofollow noreferrer"">part 1</a>, <a href=""https://i.stack.imgur.com/p4lW3.jpg"" rel=""nofollow noreferrer"">part 2</a>, <a href=""https://i.stack.imgur.com/Qii16.jpg"" rel=""nofollow noreferrer"">part 3</a></p>
<p>I need to blend part 1 with part 2. And then, I need to blend the blended part with part 3.</p>
",https://stackoverflow.com/questions/76022817/how-to-get-rid-of-black-lines-caused-of-blending-images,True,76044703
76022812,Converting CSV file to Python Dictionary,"<p>I have an csv file as shown below called &quot;Data.csv&quot;.
<a href=""https://i.stack.imgur.com/0adGv.png"" rel=""nofollow noreferrer"">Data.csv</a>
I am trying to convert it to python so that it goes into a dictionary but after trying to figure it out for 2 hours i decided to put them into individual lists.
Ive tried MULTIPLE different ways to do this. The Name column imports in perfectly fine and i get an output as expected as-
['Royal Dutch Shell plc', 'HSBC Holdings plc', 'BP plc', 'AstraZeneca plc', 'BHP Group plc'........</p>
<p>However as soon as i try doing it for the P/E ratio it throws up an &quot;SyntaxError: can't assign to operator&quot;. So far ive tried reading the csv and iterating it through, tried .tolist(), even tried list(data.Name), and even tried using a cursor to go through each individual piece of data and putting it in to a variable ALL to the same result. Name goes perfectly smoothly and as soon as it gets to P/E Ratio or any number based cells it blows up.</p>
<p>Below are some examples of codes ive tried-</p>
<pre><code>import csv
import pandas as pd
from pandas import *
import numpy as np

data=[&quot;Name&quot;,&quot;P/E Ratio&quot;,&quot;P/E/G Ratio&quot;,&quot;Earnings Per Share&quot;,&quot;BETA&quot;,&quot;Score&quot;]
for i in range (0,len(data)):
    cursor=(pd.read_csv(&quot;Data.csv&quot;, usecols=[(data[i])]))

data=read_csv(&quot;Data.csv&quot;)
Name=data[&quot;Name&quot;].tolist()
P/E_Ratio=data[&quot;P/E Ratio&quot;].tolist()
P/E/G_Ratio=data[&quot;P/E/G Ratio&quot;].tolist()
Earnings_Per_Share=data[&quot;Earnings_Per_Share&quot;].tolist()
BETA=data[&quot;BETA&quot;].tolist()
Score=data[&quot;Score&quot;].tolist()
print(Name,P/E_Ratio,P/E/G_Ratio,Earnings_Per_Share,BETA,Score)

data = pandas.read_csv(&quot;Data.csv&quot;,header=0)
Name=list(data.Name)
print(Name)
P/E_Ratio=list(data.P/ERatio)
</code></pre>
",https://stackoverflow.com/questions/76022812/converting-csv-file-to-python-dictionary,True,76022873
76022811,pandas_merge asof throws &quot;ValueError: left keys must be sorted&quot; even though the DataFrame was sorted,"<p>There are two dataframes df1 and df2 with common columns 'id' and 'time'. I would like to do left merge and for each id and time in df1, find the row with closest time for the same id from df2 and merge it to df1. Tried the code below but there is an error. How to resolve it?</p>
<pre><code>df1 = pd.DataFrame({'id': [1, 1, 2, 3, 4], 'datetime': ['2022-01-01 01:00:00', '2022-01-01 02:00:00', '2021-01-02 01:00:00', '2022-01-03 01:00:00', '2022-01-04 01:00:00'], 'val1': [10, 20, 30, 40, 50]})

df2 = pd.DataFrame({'id': [1, 1, 2, 3, 4, 5],
                'datetime': ['2022-01-01 03:00:00', '2022-01-01 04:00:00', '2022-01-02 02:00:00', '2022-01-03 03:00:00', '2022-01-04 02:00:00', '2022-01-04 03:00:00'],
                'val2': [100, 200, 300, 400, 500, 600]})
# convert datetime columns to datetime dtype
df1['datetime'] = pd.to_datetime(df1['datetime'])
df2['datetime'] = pd.to_datetime(df2['datetime'])

# sort dataframes by id and datetime
df1 = df1.sort_values(['id', 'datetime'])
df2 = df2.sort_values(['id', 'datetime'])

merged_df = pd.merge_asof(df1, df2, on='datetime', by='id', direction='nearest')

ValueError: left keys must be sorted
</code></pre>
",https://stackoverflow.com/questions/76022811/pandas-merge-asof-throws-valueerror-left-keys-must-be-sorted-even-though-the,True,76022878
76022795,How can I read an image path that is stored in a variable?,"<p>I store a path in a variable like this</p>
<pre class=""lang-py prettyprint-override""><code>photo_path = &quot;C:\Users\THIS PC\OneDrive\Documents\ATE\abc.jpg&quot;
</code></pre>
<p>I want to open it <code>photo_read = cv2.imread(r&quot;photo_path&quot;)</code>
The <code>r&quot;photo_path&quot;</code> doesn't work. Is there any way that I can do that? Many thanks.</p>
",https://stackoverflow.com/questions/76022795/how-can-i-read-an-image-path-that-is-stored-in-a-variable,True,
76022794,Is it logical to convert more than 50 type string value into integer in a column?,"<p>I've been working on a relatively big dataset. I could change most of the string columns into integer with numbers since they weren't too diverse (max 7 type of string). There are two string columns left and they have more than 50 type string. Is it logical to change them with numbers? Or do you have any suggestions?</p>
<p><a href=""https://i.stack.imgur.com/euuFl.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I want to have all of my datasets columns be int or float.</p>
",https://stackoverflow.com/questions/76022794/is-it-logical-to-convert-more-than-50-type-string-value-into-integer-in-a-column,False,
76022787,How to make requests from another flask route using the flask oauth2 client in authlib,"<p>I am using the oauth2 flask client in the authlib library. Following the documentation I am able to run the <code>oauth.my_server.authorize_access_token()</code> method inside the <code>/authorize</code> route, which sets the <code>oauth.my_server.token</code> to the token value.</p>
<p>But I am now having problems making a get request to the remote server from another route. I can see that after redirecting from the <code>/authorize</code> route, the token attribute becomes <code>None</code>.</p>
<pre class=""lang-py prettyprint-override""><code>from flask import url_for, render_template

@auth.route('/login')
def login():
    redirect_uri = url_for('authorize', _external=True)
    return oauth.my_server.authorize_redirect(redirect_uri)

@auth.route('/authorize')
def authorize():
    token = oauth.my_server.authorize_access_token()
    print(token == oauth.my_server.token)  # &lt;-- True
    return redirect(url_for('other.some_other_route'))
</code></pre>
<p>So on return we get the token and store it in the oauth client object, but then the token is gone after redirecting to the next route:</p>
<pre class=""lang-py prettyprint-override""><code>@other.route('/some_other_route', methods=['GET'])
def some_other_route():
    print(&quot;Token after redirect: &quot;, oauth.my_server.token)  # &lt;-- None
    oauth.my_server.get('resource')  # Gives error
    return &quot;&quot;
</code></pre>
<p>The error given when trying <code>oauth.my_server.get('resource')</code> is:
<code>authlib.integrations.base_client.errors.MissingTokenError: missing_token</code>. I think this is expected behaviour, but I can't see an example in the docs for how to make requests from another route. Should I store it in the session object (or a cache?) and then just reassign it manually with <code>oauth.my_server.token = session.get('token')</code>?</p>
",https://stackoverflow.com/questions/76022787/how-to-make-requests-from-another-flask-route-using-the-flask-oauth2-client-in-a,False,
76022783,Text in table cell in PyQt6 not showing on first insert,"<h1>Conditions</h1>
<p>There is a table <a href=""https://github.com/polnikov/air-system/blob/c31e06e3ee2aecbe33fde47258a7a6e905024892/app.py#L249"" rel=""nofollow noreferrer""><code>sputnik_table</code></a>. When entered in cell 1, cell 2 should display the calculated value. Handler function <a href=""https://github.com/polnikov/air-system/blob/c31e06e3ee2aecbe33fde47258a7a6e905024892/app.py#L618"" rel=""nofollow noreferrer""><code>calculate_klapan_pressure_loss</code></a>.</p>
<p><strong>sputnik_table</strong></p>
<pre><code>def create_sputnik_table_box(self) -&gt; object:
        _box = QGroupBox(CONSTANTS.SPUTNIK_TABLE.TITLE)
        style = self.box_style
        _box.setStyleSheet(style)
        _box.setFixedHeight(298)

        _layout = QVBoxLayout()

        self.sputnik_table = QTableWidget(5, 15)
        _table = self.sputnik_table
        _table.verticalHeader().setVisible(False)
        _table.horizontalHeader().setStretchLastSection(True)
        _table.horizontalHeader().setStyleSheet(&quot;QHeaderView::section { background-color: #F3F3F3 }&quot;)
        _table.setStyleSheet(&quot;QTableWidget { border: 2px solid grey; }&quot;)
        _table.setHorizontalHeaderLabels(CONSTANTS.SPUTNIK_TABLE.HEADER)
        _table.setObjectName(CONSTANTS.SPUTNIK_TABLE.NAME)

        num_rows = _table.rowCount()
        num_cols = _table.columnCount()

        # устанавливаем виджет QTableWidgetItem для всех ячеек таблицы и отключаем редактирование
        for row in range(num_rows):
            for col in range(num_cols):
                _table.setItem(row, col, QTableWidgetItem())
                _table.item(row, col).setTextAlignment(Qt.AlignmentFlag.AlignCenter)
                editable = (
                    (0, 1),
                    (1, 1),
                    (1, 2),
                    (1, 3),
                    (1, 4),
                    (1, 11),
                    (3, 1),
                    (3, 2),
                    (3, 3),
                    (3, 4),
                    (3, 11),
                )
                if (row, col) not in editable:
                    _table.item(row, col).setFlags(Qt.ItemFlag.ItemIsEnabled)
                else:
                    _table.item(row, col).setBackground(QColor(229, 255, 204))

        for row in range(num_rows):
            _table.setRowHeight(row, 36)
            match row:
                case 2 | 4:
                    _table.setSpan(row, 0, 1, 13)
                case 0:
                    _table.setSpan(row, 2, 1, 11)
                case 1 | 3:
                    _table.setSpan(row, 14, 2, 1)

        for col in range(num_cols):
            match col:
                case 0:
                    _table.setColumnWidth(col, 80)
                case 1:
                    _table.setColumnWidth(col, 80)
                case 3 | 4:
                    _table.setColumnWidth(col, 110)
                case _:
                    _table.setColumnWidth(col, 72)

        # установка заливки для редактируемых столбцов
        _table.item(0, 1).setBackground(QColor(229, 255, 204))

        # заполняем таблицу
        _table.item(0, 0).setText(CONSTANTS.SPUTNIK_TABLE.KLAPAN_LABEL)
        _table.item(0, 1).setToolTip(CONSTANTS.SPUTNIK_TABLE.KLAPAN_FLOW_TOOLTIP)
        _table.item(1, 0).setText(CONSTANTS.SPUTNIK_TABLE.SECTOR_1)
        _table.item(3, 0).setText(CONSTANTS.SPUTNIK_TABLE.SECTOR_2)

        # размещаем радиокнопки
        for row in (1, 3):
            widget = QWidget()
            layout = QHBoxLayout(widget)
            layout.setAlignment(Qt.AlignmentFlag.AlignCenter)
            radio_button = QRadioButton(widget)
            layout.addWidget(radio_button)
            widget.setLayout(layout)
            _table.setCellWidget(row, 14, widget)

        self.radio_button1 = _table.cellWidget(1, 14).findChild(QRadioButton)
        self.radio_button2 = _table.cellWidget(3, 14).findChild(QRadioButton)
        radio_button1, radio_button2 = self.radio_button1, self.radio_button2
        radio_button1.setChecked(True)

        radio_button1.setToolTip(CONSTANTS.SPUTNIK_TABLE.RADIO_TOOLTIP_1)
        radio_button2.setToolTip(CONSTANTS.SPUTNIK_TABLE.RADIO_TOOLTIP_2)

        # обработчики
        radio_button1.clicked.connect(self.set_sputnik_airflow_in_main_table_by_radiobutton_1)
        radio_button2.clicked.connect(self.set_sputnik_airflow_in_main_table_by_radiobutton_2)

        radio_button2.clicked.connect(self.uncheck_radio_button_1)
        radio_button1.clicked.connect(self.uncheck_radio_button_2)

        radio_button1.clicked.connect(self.calculate_kms_by_radiobutton_1)
        radio_button2.clicked.connect(self.calculate_kms_by_radiobutton_2)
        radio_button1.clicked.connect(self.calculate_branch_pressure_by_radiobutton_1)
        radio_button2.clicked.connect(self.calculate_branch_pressure_by_radiobutton_2)
        radio_button1.clicked.connect(self.calculate_full_pressure_by_radiobutton_1)
        radio_button2.clicked.connect(self.calculate_full_pressure_by_radiobutton_2)

        _table.cellChanged.connect(self.set_sputnik_airflow_in_main_table)
        _table.cellChanged.connect(self.set_deflector_pressure_in_main_table)

        _table.cellChanged.connect(self.calculate_klapan_pressure_loss)
        _table.cellChanged.connect(self.calculate_air_velocity)
        _table.cellChanged.connect(self.calculate_diameter)
        _table.cellChanged.connect(self.calculate_dynamic)
        _table.cellChanged.connect(self.calculate_specific_pressure_loss)
        _table.cellChanged.connect(self.calculate_m)
        _table.cellChanged.connect(self.calculate_linear_pressure_loss)
        _table.cellChanged.connect(self.calculate_local_pressure_loss)
        _table.cellChanged.connect(self.calculate_sputnik_full_pressure_loss)
        _table.cellChanged.connect(self.calculate_kms)
        _table.cellChanged.connect(self.calculate_result_sputnik_pressure)
        _table.cellChanged.connect(self.calculate_branch_pressure)
        _table.cellChanged.connect(self.calculate_full_pressure)

        _table.itemChanged.connect(self.validate_input_data_in_tables)

        _layout.addWidget(_table)
        _box.setLayout(_layout)
        return _box
</code></pre>
<p><strong>calculate_klapan_pressure_loss</strong></p>
<pre><code>def calculate_klapan_pressure_loss(self, row=False, column=False) -&gt; None:
        sender = self.sender()
        if isinstance(sender, QComboBox):
            klapan_widget_value = CONSTANTS.INIT_DATA.KLAPAN_ITEMS.get(row)
            hand_klapan_flow = self.klapan_input.text()
            current_klapan_flow = self.sputnik_table.item(0, 1).text()
            self._calculate_klapan_pressure_loss(klapan_widget_value, hand_klapan_flow, current_klapan_flow)

        elif isinstance(sender, QLineEdit):
            klapan_widget_value = CONSTANTS.INIT_DATA.KLAPAN_ITEMS.get(self.klapan_widget.currentText())
            hand_klapan_flow = row
            current_klapan_flow = self.sputnik_table.item(0, 1).text()
            self._calculate_klapan_pressure_loss(klapan_widget_value, hand_klapan_flow, current_klapan_flow)

        elif isinstance(sender, QTableWidget):
            if row == 0 and column == 1:
                klapan_widget_value = CONSTANTS.INIT_DATA.KLAPAN_ITEMS.get(self.klapan_widget.currentText())
                hand_klapan_flow = self.klapan_input.text()
                current_klapan_flow = self.sputnik_table.item(0, 1).text()
                self._calculate_klapan_pressure_loss(klapan_widget_value, hand_klapan_flow, current_klapan_flow)


    def _calculate_klapan_pressure_loss(self, klapan_widget_value, hand_klapan_flow, current_klapan_flow) -&gt; None:
        table = self.sputnik_table
        if hand_klapan_flow or klapan_widget_value == '--':
            klapan_flow = hand_klapan_flow
        else:
            klapan_flow = klapan_widget_value

        if all([current_klapan_flow, klapan_flow]):
            current_klapan_flow = int(current_klapan_flow)
            klapan_flow = int(klapan_flow)
            pressure_loss = 10 * pow(current_klapan_flow / klapan_flow, 2)
            pressure_loss = &quot;{:.2f}&quot;.format(round(pressure_loss, 2))
            table.item(0, 13).setText(pressure_loss)
            table.item(0, 13).setBackground(QColor(153, 255, 255))
        else:
            table.item(0, 13).setText('')
            table.item(0, 13).setBackground(QColor(255, 255, 255))
</code></pre>
<p>table picture:<br/>
<img src=""https://i.stack.imgur.com/nif7i.png"" alt=""table picture"" /></p>
<p><a href=""https://github.com/polnikov/air-system/blob/main/app.py"" rel=""nofollow noreferrer""><strong>Full project file</strong></a></p>
<h1>Problem</h1>
<p>After starting the program, if you enter any number in cell 1, cell 2 is processed by the function, but does not display text (color fill is applied). Then, if you enter a number again, but different from the original one, then the text appears in the cell and all subsequent entries in cell 1 are processed as needed.</p>
<p><strong>This behavior happened &quot;suddenly&quot;</strong></p>
<ol>
<li>If you change the cell in the handler function, for example, to the next one (on the left), then the text is inserted from the first time.</li>
<li>If you set the text in this cell when creating a table, it is also displayed immediately without problems.</li>
</ol>
",https://stackoverflow.com/questions/76022783/text-in-table-cell-in-pyqt6-not-showing-on-first-insert,False,
76022762,Is it okay to collect input tuples using a loop by adding to an outer tuple instead of appending to a list?,"<p>For my program, I am inputting a 2 dimensional array that is <strong>read a lot of times later in it but never changed</strong>. I know that tuples are faster if I'm only reading values.</p>
<p>Here's the code to input by using lists:</p>
<pre class=""lang-py prettyprint-override""><code>n = 3 # sample value
grid = []
for i in range(n):
    grid.append(tuple(map(int, input().split())))
</code></pre>
<p>And here's the code to input by using tuples:</p>
<pre class=""lang-py prettyprint-override""><code>n = 3 # sample value
grid = ()
for i in range(n):
    grid += (tuple(map(int, input().split())),)
</code></pre>
<p>Note: The input is provided in this format:</p>
<pre><code>1 0 0
0 0 0
0 0 1
</code></pre>
<p>which results in the list <code>[(1, 0, 0), (0, 0, 0), (0, 0, 1)]</code>, respectively the tuple <code>((1, 0, 0), (0, 0, 0), (0, 0, 1))</code>.</p>
<p>From my testing the total time of the program was higher when using tuples. Is there even a better way to input tuples?</p>
<p>Here's the whole code:</p>
<pre class=""lang-py prettyprint-override""><code>n, m = list(map(int, input().split()))
grid = []
for i in range(n):
    grid.append(tuple(map(int, input().split())))

def t2nh(st):
    t = len(grid) + len(grid[0])
    for i in range(len(grid)):
        for j in range(len(grid[i])):
            if grid[i][j] == 1:
                t = min(abs(i+1 - st[0]) + abs(j+1 - st[1]), t)
    return t

t = 0
for i in range(len(grid)):
    for j in range(len(grid[i])):
        t += t2nh([i+1, j+1])

print(t)
</code></pre>
",https://stackoverflow.com/questions/76022762/is-it-okay-to-collect-input-tuples-using-a-loop-by-adding-to-an-outer-tuple-inst,True,76022888
76022736,"How to get first record group by non relational join query, Django?","<p><strong>Model:</strong></p>
<pre><code>class AD(model.Model):
    serial              = models.CharField(unique=True, max_length=50)
    po_number           = models.IntegerField(blank=True, null=True)

class IDA(model.Model):
    serial              = models.CharField(unique=True, max_length=50)

class POR(models.Model):
    po_number               = models.IntegerField(unique=True)
    is_closed_po            = models.BooleanField(default=True)
</code></pre>
<p><strong>Raw Query:</strong></p>
<pre><code>Select MIN(a.serial) as first_value, a.po_number
From AD a
Join IDA i ON a.serial=i.serial
Join POR p ON a.po_number=p.po_number
Where p.is_closed_po=0
Group By a.po_number
</code></pre>
<p><strong>What I'm trying to do in Django:</strong></p>
<pre><code>AD.objects.extra(
    select={'is_closed_po': 'por.is_closed_po'},
    tables=['por', 'ida'],
    where=['ad.po_number = por.po_number',
           'ad.serial = ida.serial',
           'is_closed_po = 0'
           ],
).values('serial', 'po_number')
</code></pre>
<p>How can I achieve this in Django? Any help would be much appreciated.</p>
",https://stackoverflow.com/questions/76022736/how-to-get-first-record-group-by-non-relational-join-query-django,False,
76022730,What is the correct way to add a custom module to the AutoPyTorch pipeline?,"<p>I need to add a custom module to my search space when using the <a href=""https://github.com/automl/Auto-PyTorch"" rel=""nofollow noreferrer"">Auto-PyTorch</a> library. Specifically, I would like to add a custom backbone while using TabularClassificationPipeline.</p>
<p>It is pretty clear to me which methods and attributes my custom module should have, but I don't understand how to pass it to the pipeline itself.</p>
<p>Does anyone have any experience with this?</p>
<p>*I tried reading their official docs, however, there is no such example there.</p>
",https://stackoverflow.com/questions/76022730/what-is-the-correct-way-to-add-a-custom-module-to-the-autopytorch-pipeline,False,
76022694,Python tkinter global variables use mainloop(),"<p>I'm a newbie teaching myself tkinter.
May I ask why the first picture of the Pycharm Python console in the picture below does not use mainloop(), then execute the first picture.</p>
<p>Next, when the second picture is being executed,added mainloop() this time, during execution, but why can mainloop() make the r=s.Tk() of the first picture displayed?
He is the old variable of r, how to display it together?
<img src=""https://i.stack.imgur.com/5PsVg.png"" alt=""image 1"" /></p>
<p><img src=""https://i.stack.imgur.com/BTHrE.png"" alt=""image 2"" /></p>
<p>I tested Spyder's IPython console, by the way, it's the same.
I want to know why.</p>
",https://stackoverflow.com/questions/76022694/python-tkinter-global-variables-use-mainloop,True,
76022687,How can I fix this error? positional argument follows keyword argument,"<p>Hi my code have an error from model.summary</p>
<p>How do I fix this error ?</p>
<p>coding:</p>
<pre><code>from tensorflow.keras.applications import InceptionResNetV2
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import InputLayer
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import Model 
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping


googleNet_model = InceptionResNetV2(include_top=False,  weights='imagenet', input_shape=input_shape)
googleNet_model.trainable = True
model = Sequential()

model.add(googleNet_model)
model.add(GlobalAveragePooling2D())
model.add(Dense(2, activation='softmax'))

model.compile (loss=tf.keras.losses.BinaryCrossentropy,
              tf.keras.optimizers.legacy.Adam (learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),  
              metrics=['accuracy']
model.summary()

</code></pre>
<p>Error:</p>
<pre><code> model.summary()
    ^
SyntaxError: positional argument follows keyword argument

</code></pre>
<p>How do I avoid this error? I'm making a silly mistake somewhere but not sure how to fix it.</p>
",https://stackoverflow.com/questions/76022687/how-can-i-fix-this-error-positional-argument-follows-keyword-argument,False,
76022678,"TF dataset sample_from_datasets is not working when used iteratively, is this a bug in tf ds?","<p>I have a dataset with each class having sub folders. I want to balance all the way from sub folders to main classes. I created a dataset for each subfolder and created balanced dataset for each class using sample_from_datasets. Then I created balanced dataset using above balanced class datasets to form final balanced dataset. But its not working as expected.</p>
<pre><code>    &quot;&quot;&quot;method to create a subdirectory balanced dataset&quot;&quot;&quot;
import os
import random
import cv2
import tensorflow as tf
import matplotlib.pyplot as plt


def get_class_dir_dict(root_directory, classes):
    class_dir_dict = {}
    folder_paths = []
    for dirpath, dirnames, filenames in os.walk(root_directory):
        if not dirnames:  # Only collect the folder paths without subdirectories
            folder_paths.append(dirpath)
            for class_name in classes:
                if class_name in dirpath.split(os.sep):
                    if class_dir_dict.get(class_name) is None:
                        class_dir_dict[class_name] = [dirpath]
                    else:
                        class_dir_dict[class_name].append(dirpath)
    for class_dir_dict_key in class_dir_dict.keys():
        print(class_dir_dict_key, len(class_dir_dict[class_dir_dict_key]))
    return class_dir_dict


def process_path(file_path, label):
    img = tf.io.read_file(file_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, [224, 224])
    img = img / 255.0
    return img, label


def create_dataset_from_folder(filenames, labels):
    filename_dataset = tf.data.Dataset.from_tensor_slices(filenames)
    label_dataset = tf.data.Dataset.from_tensor_slices(labels)

    dataset = tf.data.Dataset.zip((filename_dataset, label_dataset))

    dataset = dataset.map(process_path)

    return dataset


def get_balanced_dataset(root_directory, classes, min_count=100, undersample=False):
    class_dir_dict = get_class_dir_dict(root_directory, classes)
    # create dataset of each subdirectory
    main_dataset = []
    for class_dir_dict_key in class_dir_dict.keys():
        print(&quot;\n&quot;, class_dir_dict_key)
        one_class_datasets = []
        for dirpath in class_dir_dict[class_dir_dict_key]:
            print(dirpath)
            filenames = os.listdir(dirpath)
            img_names = []
            for filename in filenames:
                if filename.endswith(&quot;.jpg&quot;) or filename.endswith(&quot;.png&quot;):
                    filename = os.path.join(dirpath, filename)
                    img_names.append(filename)
            print(len(img_names))
            # create tf dataset of filenames and labels
            label = classes.index(class_dir_dict_key)
            labels = [label] * len(img_names)
            ds = create_dataset_from_folder(img_names, labels)
            if len(img_names) &lt; min_count:
                ds = ds.repeat()
            if undersample:
                ds = ds.shuffle(min_count)
                ds = ds.take(min_count)
            one_class_datasets.append(ds)
        sample_dataset = tf.data.Dataset.sample_from_datasets(
            one_class_datasets,
            weights=[1 / len(one_class_datasets)] * len(one_class_datasets),
        )
        main_dataset.append(sample_dataset)
    final_ds = tf.data.Dataset.sample_from_datasets(
        main_dataset, weights=[1 / len(main_dataset)] * len(main_dataset)
    )
    return final_ds


if __name__ == &quot;__main__&quot;:
    # Your dataset root directory
    root_directory = &quot;DB&quot;

    # Choose your classes!
    classes = [&quot;male&quot;, &quot;female&quot;]
    # classes = [&quot;kid&quot;, &quot;middle&quot;, &quot;old&quot;]

    final_ds = get_balanced_dataset(
        root_directory, classes, min_count=10, undersample=True
    )

    # Test the dataset by displaying the first image and label
    print(final_ds.element_spec)
    counts = {class_name: 0 for class_name in classes}
    for image, label in final_ds.take(30):
        # cv2 imshow
        image = image.numpy()
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        cv2.imshow(&quot;image&quot;, image)
        print(label.numpy())
        cv2.waitKey(300)
        counts[classes[label.numpy()]] += 1
    print(counts)
</code></pre>
<p>Results in</p>
<pre><code>0
1
0
0
1
0
0
0
0
1
0
1
0
0
0
0
0
0
0
0
0
0
</code></pre>
",https://stackoverflow.com/questions/76022678/tf-dataset-sample-from-datasets-is-not-working-when-used-iteratively-is-this-a,False,
76022661,CS50 Web Project 2: How to save the starting bid,"<p>I am working on Project 2 for CS50 Web Programming course (an eBay-like e-commerce auction site in Django) and I would like to save the starting bid of an auction in order that compare it with later auctions.</p>
<p>Please see my code below:</p>
<p><strong>models.py</strong></p>
<pre><code>from django.contrib.auth.models import AbstractUser
from django.db import models

class User(AbstractUser):
    pass

class Category(models.Model):
    categoryName = models.CharField(max_length=20)

    def __str__(self):
        return self.categoryName
    
class Listing(models.Model):
    title = models.CharField(max_length=30)             
    description = models.CharField(max_length=100)                            
    imageUrl = models.CharField(max_length=500)         
    isActive = models.BooleanField(default=True)        
    price = models.IntegerField(default=0)              
    startingBid = models.IntegerField(default=0)        
    owner = models.ForeignKey(User, on_delete=models.CASCADE, blank=True, null=True, related_name=&quot;user&quot;) 
    category = models.ForeignKey(Category, on_delete=models.CASCADE, blank=True, null=True, related_name=&quot;category&quot;) 
    watchlist = models.ManyToManyField(User, blank=True, null=True, related_name=&quot;listingWatchlist&quot;)

class Bid(models.Model):
    bid = models.IntegerField(default=0) 
    user = models.ForeignKey(User, on_delete=models.CASCADE, blank=True, null=True, related_name=&quot;userBid&quot;)    
    listing = models.ForeignKey(Listing, on_delete=models.CASCADE, blank=True, null=True, related_name=&quot;bidListing&quot;)           

class Comments(models.Model):
    author = models.ForeignKey(User, on_delete=models.CASCADE, blank=True, null=True, related_name=&quot;userComment&quot;)
    listing = models.ForeignKey(Listing, on_delete=models.CASCADE, blank=True, null=True, related_name=&quot;listingComment&quot;)
    comment = models.CharField(max_length=200)       

    def __str__(self):
        return f&quot;{self.author} comment on {self.listing}&quot;
</code></pre>
<p><strong>Function for adding a bid (into views.py)</strong></p>
<pre><code>def addNewBid(request, id):

    currentUser = request.user
    listingData = Listing.objects.get(pk=id)

    isListWatchList = request.user in listingData.watchlist.all()

    allComments = Comments.objects.filter(listing=listingData)

    newBid = request.POST['newBid']

    isOwner = request.user.username == listingData.owner.username

    # The bid must be at least as large as the starting bid, and must be greater than any other bids that have been placed (if any). 
    # If the bid doesn’t meet those criteria, the user should be presented with an error.
    #
    if int(newBid) &gt;= listingData.price.bid:
        updateBid = Bid(user = currentUser, bid = int(newBid))
        updateBid.save()
        listingData.price = updateBid
        listingData.save()

        return render(request, &quot;auctions/listing.html&quot;, {
            &quot;listing&quot;: listingData,
            &quot;message&quot;: &quot;Bid succesful: Current price was updated!&quot;,
            &quot;update&quot;: True,
            &quot;isListingWatchList&quot;: isListWatchList,
            &quot;allComments&quot;: allComments,
            &quot;isOwner&quot;: isOwner,
        })
    
    else:
        return render(request, &quot;auctions/listing.html&quot;, {
            &quot;listing&quot;: listingData,
            &quot;message&quot;: &quot;Bid failed!&quot;,
            &quot;updated&quot;: False,
            &quot;isListingWatchList&quot;: isListWatchList,
            &quot;allComments&quot;: allComments,
            &quot;isOwner&quot;: isOwner,
        })
</code></pre>
<p><strong>listing.html</strong></p>
<pre><code>{% extends &quot;auctions/layout.html&quot; %}

{% block body %}

     {% if message %}
        {% if updated %}
             &lt;div class=&quot;alert alert-success&quot; role=&quot;alert&quot;&gt;
                {{ message }}
            &lt;/div&gt;
        {% else %}
             &lt;div class=&quot;alert alert-warning&quot; role=&quot;alert&quot;&gt;
                {{ message }}
            &lt;/div&gt;
        {% endif %}
    {% endif %}

    {% if not listing.isActive and user == listing.price.user %}
        &lt;div class=&quot;alert alert-success&quot; role=&quot;alert&quot;&gt;
            Wow! You won the auction !
        &lt;/div&gt;
    {% endif %}

    &lt;h2&gt;{{ listing.title }}&lt;/h2&gt;
    &lt;br&gt;
    &lt;p&gt;{{ listing.description }}&lt;/p&gt;
    &lt;h5&gt;Current price: ${{ listing.price.bid }}&lt;/h5&gt;
    &lt;p&gt;Owner: {{ listing.owner }}&lt;/p&gt;
    
    {% if user.is_authenticated %}
        &lt;form action=&quot;{% url 'addNewBid' id=listing.id %}&quot; method=&quot;POST&quot;&gt;
            {% csrf_token %}
            &lt;div class=&quot;form-group&quot;&gt;
                &lt;label for=&quot;comment&quot;&gt;New Bid:&lt;/label&gt;
                &lt;input type=&quot;number&quot; min=&quot;0&quot; name=&quot;newBid&quot; placeholder=&quot;&quot;&gt;
                &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary&quot;&gt;Add&lt;/button&gt;
            &lt;/div&gt;            
        &lt;/form&gt;

        {% if isOwner and listing.isActive %}
            &lt;form action=&quot;{% url 'closeAuction' id=listing.id %}&quot; method=&quot;POST&quot;&gt;
                {% csrf_token %}
                &lt;div class=&quot;col-md-12 text-center&quot;&gt;
                    &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary&quot;&gt;Close Auction&lt;/button&gt;
                &lt;/div&gt;
            &lt;/form&gt;
        {% endif %}
    {% endif %}

    &lt;img src=&quot;{{ listing.imageUrl }}&quot; alt=&quot;{{ listing.title }}&quot; height=&quot;400px&quot;&gt;
    &lt;h2&gt;Comments&lt;/h2&gt;    
    &lt;ul class=&quot;list-group&quot;&gt;
        {% for i in allComments %}
            &lt;li class=&quot;list-group-item&quot;&gt;{{ i.comment }}
                &lt;br&gt;
                &lt;p&gt;Author: {{ i.author }}&lt;/p&gt;
            &lt;/li&gt;
        {% endfor %}
    &lt;/ul&gt;
    &lt;br&gt;

    {% if user.is_authenticated %}
        &lt;form action=&quot;{% url 'addNewComment' id=listing.id %}&quot; method=&quot;POST&quot;&gt;
            {% csrf_token %}
            &lt;div class=&quot;form-group&quot;&gt;
                &lt;label for=&quot;comment&quot;&gt;New Comment:&lt;/label&gt;
                &lt;input type=&quot;text&quot; name=&quot;newComment&quot; placeholder=&quot;&quot;&gt;
                &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary&quot;&gt;Add&lt;/button&gt;
            &lt;/div&gt;            
        &lt;/form&gt;
    {% endif %}

    {% if user.is_authenticated %}
        {% if isListingWatchList %}        
            &lt;form action=&quot;{% url 'removeWatchList' id=listing.id %}&quot; method=&quot;POST&quot;&gt;
                {% csrf_token %}
                &lt;div class=&quot;col-md-12 text-center&quot;&gt;
                    &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary&quot;&gt;Remove from Watchlist&lt;/button&gt;
                &lt;/div&gt;             
            &lt;/form&gt;
        {% else %}                
            &lt;form action=&quot;{% url 'addWatchList' id=listing.id %}&quot; method=&quot;POST&quot;&gt;
                {% csrf_token %}
                &lt;div class=&quot;col-md-12 text-center&quot;&gt;
                    &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary&quot;&gt;Add to Watchlist&lt;/button&gt;
                &lt;/div&gt;               
            &lt;/form&gt;
        {% endif %}
    {% endif %}

{% endblock %}
</code></pre>
<p>I would appreciate your support,</p>
<p>Javier</p>
",https://stackoverflow.com/questions/76022661/cs50-web-project-2-how-to-save-the-starting-bid,False,
76022646,Importance of understanding the code for implementing object detection algorithms from scratch,"<p>I have started learning object detection recently and have come across many algorithms like Faster RCNN, YOLO, SSD, etc.
I want to implement them into my project and get a hands-on experience with these algorithm.
Should I attempt on learning and understanding the programs which implement these algorithms from scratch?
Or should I just use an already implemented code and train the model on my dataset?
I wish to be a Deep Learning Researcher, so I wish to prepare accordingly. Could someone please guide me through this process?</p>
<p>I have watched youtube videos and read multiple articles on the implementation of these algorithms. I do want to understand deep learning from the very basics, so wonder if understanding these courses will be helpful or not.
I just want to get some idea from experienced people working with deep learning about whether they think understanding the codes and implementing these algorithms from scratch is a good idea and will help me in the long run.7</p>
",https://stackoverflow.com/questions/76022646/importance-of-understanding-the-code-for-implementing-object-detection-algorithm,False,
76022634,using os to find python version,"<p>When Ever I try to run this code it shows Error</p>
<pre class=""lang-py prettyprint-override""><code>import os 

file_names = os.listdir(&quot;D:/python/Pandas &amp; Matplotlib&quot;)
print(list(file_names))

py_ver =os.system(&quot;cmd/k 'python --version'&quot;)

print(file_names)
print(py_ver)
</code></pre>
<p>Error</p>
<pre><code>[python' is not recognized as an internal or external command,
operable program or batch file.]
</code></pre>
<p>I want to get the Python version installed on my pc using os.</p>
",https://stackoverflow.com/questions/76022634/using-os-to-find-python-version,False,
76022577,I need to write text from a file to tkinter canvas - the error is bad screen distance,"<p>I need to write a few lines from a file to the canvas. the error is bad screen distance. Can someone please help me?</p>
<pre class=""lang-py prettyprint-override""><code>f = open(“mytus_fakt.txt”, “r”, encoding = “utf-8”)
for i,row in enumerate(f):
    x = row.split(“\t”)
canvas.create_text(x,y-750, text = x[0])
</code></pre>
<p>It should write the first couple of words (until tabulator) on tkinter canvas, but it is not working the error is bad screen distance.
How can the data from the file be drawn on the canvas?</p>
",https://stackoverflow.com/questions/76022577/i-need-to-write-text-from-a-file-to-tkinter-canvas-the-error-is-bad-screen-dis,False,
76022561,How do I include and link &quot;matplotlibcpp.h&quot; into my c++ program?,"<p>I have a C++ program that I want to plot data in, I came across <a href=""https://matplotlib-cpp.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">Matplotlibcpp</a>. The steps for including and linking the library to my C++ program can be seen <a href=""https://matplotlib-cpp.readthedocs.io/en/latest/compiling.html#compiling"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I started by changing my console compile command from this (works fine):</p>
<pre><code>g++ -g -O -Wall ./src/fft.cpp ./src/linked_list.cpp ./src/main.cpp ./src/tim5.cpp 
./src/tcpip_client.cpp ./src/plotter.cpp -o ./bin/LiDAR_API
</code></pre>
<p>to this:</p>
<pre><code>g++ -g -O -Wall ./src/fft.cpp ./src/linked_list.cpp ./src/main.cpp ./src/tim5.cpp 
./src/tcpip_client.cpp ./src/plotter.cpp -o ./bin/LiDAR_API -DWITHOUT_NUMPY - 
I/usr/include/python3.11 -lpython3.11
</code></pre>
<p>But I keep getting the following error during compilation:</p>
<pre><code>./src/../inc/plotter.h:6:10: fatal error: matplotlibcpp.h: No such file or directory
6 | #include &quot;matplotlibcpp.h&quot;
</code></pre>
<p>All my other code works just fine, it's only importing and linking the <strong>matpolotlibcpp</strong> libraries that are the issue. I assume it has something to do with the way I am trying to compile everything.</p>
<p>I am using Fedora and have also installed Python using my package manager including python-dev (v3.11).</p>
",https://stackoverflow.com/questions/76022561/how-do-i-include-and-link-matplotlibcpp-h-into-my-c-program,False,
76022555,"Django Value Error: Cannot Assign Queryset must be instance, ManyToMany Relationship","<p>I have built a model,OpeningDays, to manage a ManyToMany relationship (opening_days field in my BookingManagement model), since I wanted to add some additional data to each instance. I am using the 'through' approach as you can see from my models.py. Within my form, I have a CheckboxSelectMultiple, and for each day which is checked I want to create a model instance. However, I am getting the following error message:</p>
<p>ValueError at /listings/createlisting/openingdays/b8s43t
Cannot assign &quot;&lt;QuerySet [&lt;DaysOfWeek: Monday&gt;, &lt;DaysOfWeek: Tuesday&gt;, &lt;DaysOfWeek: Wednesday&gt;]&gt;&quot;: &quot;OpeningDays.daysofweek&quot; must be a &quot;DaysOfWeek&quot; instance.</p>
<p>The exception is being raise on this line of my code:</p>
<pre><code> if form.is_valid(): 
</code></pre>
<p>In order to try and solve this I have attempted to iterate through the queryset in my view with a for loop. However this hasn't worked either and I get the same error message. Any ideas how I can fix this?</p>
<p>My models.py:</p>
<pre><code>class DaysOfWeek(models.Model):
    day = models.CharField(max_length=13)

    def __str__(self):
        return str(self.day)


class BookingManagement(models.Model):


    listing = models.ForeignKey(Listing, verbose_name=&quot;Listing&quot;, on_delete=models.CASCADE)
    opening_days = models.ManyToManyField(DaysOfWeek, verbose_name=&quot;Opening Days&quot;, through=&quot;OpeningDays&quot;)
    booking_slots_per_day = models.IntegerField(verbose_name=&quot;Available booking slots per day&quot;, blank=True)
    max_bookings_per_time_slot = models.PositiveIntegerField(verbose_name=&quot;Maximum bookings per time slot&quot;, blank=False)
    standard_booking_duration = models.PositiveIntegerField(verbose_name=&quot;Standard booking duration in minutes&quot;, blank=False)
    min_booking_duration = models.PositiveIntegerField(verbose_name=&quot;Minimum booking duration in minutes&quot;, blank=True)
    max_booking_duration = models.PositiveIntegerField(verbose_name=&quot;Maximum booking duration in minutes&quot;, blank=True)
    booking_duration_increment = models.PositiveIntegerField(verbose_name=&quot;Booking duration increment&quot;, blank=True)
    min_people_per_booking = models.PositiveSmallIntegerField(verbose_name=&quot;Minimum number of people per booking&quot;, blank=False)
    max_people_per_booking = models.PositiveSmallIntegerField(verbose_name=&quot;Maximum number of people per booking&quot;, blank=False)
    turnaround_time = models.PositiveSmallIntegerField(verbose_name=&quot;Turn around time between activities&quot;, blank=True)
    time_slot_frequency = models.PositiveSmallIntegerField(verbose_name=&quot;Time slot frequency&quot;, blank=False)
    max_people_per_time_slot = models.PositiveSmallIntegerField(verbose_name=&quot;Maximum number of people who can book per time slot&quot;, blank=False)
    max_people_per_day = models.PositiveSmallIntegerField(verbose_name=&quot;Maximum number of people who can book per day&quot;, blank=True)
    min_adults_per_booking = models.PositiveSmallIntegerField(verbose_name=&quot;Minimum number of adults per booking&quot;, blank=True)
    max_children_per_adult = models.PositiveSmallIntegerField(verbose_name=&quot;Maximum number of children per adult&quot;, blank=True)
    min_age = models.PositiveSmallIntegerField(verbose_name=&quot;Minimum age&quot;, blank=False)
    max_age = models.PositiveSmallIntegerField(verbose_name=&quot;Maximum age&quot;, blank=True)
    child_max_age = models.PositiveSmallIntegerField(verbose_name=&quot;Maximum age for child&quot;, blank=True)


class OpeningDays(models.Model):
    bookingmanagement = models.ForeignKey(BookingManagement, on_delete=models.CASCADE)
    daysofweek = models.ForeignKey(DaysOfWeek, on_delete=models.CASCADE, null=True)
    opening_time = models.TimeField(verbose_name=&quot;Opening Time&quot;, blank=True, null=True)
    closing_time = models.TimeField(verbose_name=&quot;Closing Time&quot;, blank=True, null=True)

    class Meta:
        unique_together = [['bookingmanagement', 'daysofweek']]
</code></pre>
<p>My forms.py:</p>
<pre><code>class OpeningDaysForm(forms.ModelForm):
    &quot;&quot;&quot;A form to complete the opening days of a listing. &quot;&quot;&quot;

    daysofweek = forms.ModelMultipleChoiceField(queryset=DaysOfWeek.objects.all(), widget=forms.CheckboxSelectMultiple(attrs={'name': 'daysofweek', 'id': 'daysofweek'}))

    class Meta:

        model = OpeningDays
        fields = ['daysofweek']
</code></pre>
<p>My views.py:</p>
<pre><code>class OpeningDaysView(NextUrlMixin, View):
    form_class = OpeningDaysForm
    template_name = 'listings/createlistingopeningdays.html'
    default_next = '/'
    

    def get(self, request, *args, **kwargs):
        form = self.form_class()
        context = {
            'form': form
        }
        return render(request, self.template_name, context=context)

    def post(self, request, *args, **kwargs):
        form = self.form_class(request.POST)
        listing = get_object_or_404(Listing, slug=self.kwargs['slug'])
        bookingmanagement = get_object_or_404(BookingManagement, listing=listing)
        if form.is_valid():
            open_days = request.POST.getlist('daysofweek')
            for day in open_days:
                opendays = OpeningDays(
                    bookingmanagement = bookingmanagement,
                    daysofweek = day
                )
                opendays.save()

            slug = listing.slug
            return redirect(reverse('listings:create_opening_hours', kwargs={'slug': slug}))
            # return redirect(self.get_next_url())

        else:
            print(&quot;Form Validation Error&quot;)
            context = {
                'form': form
            }
            return render(request, self.template_name, context)
</code></pre>
<p>And my html template:</p>
<pre><code>{% extends &quot;base.html&quot; %}
{% block content %}

    &lt;h1&gt;Opening Days&lt;/h1&gt;

    &lt;div class=&quot;container&quot;&gt;
        &lt;form method=&quot;POST&quot; element=&quot;multipart/form-data&quot;&gt;
            {% csrf_token %}

            &lt;div class=&quot;mb-3&quot;&gt;
                &lt;label for=&quot;title&quot;&gt;Opening Days&lt;/label&gt;
                {{form.daysofweek}}
            &lt;/div&gt;

            &lt;div class='mb-5'&gt;
                &lt;button type='submit' class='btn btn-primary'&gt;Next&lt;/button&gt;
            &lt;/div&gt;

        &lt;/form&gt;
    &lt;/div&gt;
{% endblock %}
</code></pre>
",https://stackoverflow.com/questions/76022555/django-value-error-cannot-assign-queryset-must-be-instance-manytomany-relation,True,76022806
76022548,Tensorflow for language mode,"<blockquote>
<p>ValueError: Output tensors of a Functional model must be the output of a TensorFlow <code>Layer</code> (thus holding past layer metadata)</p>
<p>encoder-decoder method for training eng-mar language translation model</p>
</blockquote>
",https://stackoverflow.com/questions/76022548/tensorflow-for-language-mode,False,
76022519,What is the flow of the statement in this python code?,"<p>It's a simple start and stop code where If a user enter start in the command it will display car started and if a user enters a stop command then it will display car stopped.
The code is changed a bit to include conditions where if the users enters start or stop in the shell then it will display the result accordingly but if the user enters twice start or stop then it will display car is already started or car is already stopped.</p>
<p>Here is the code -</p>
<pre><code>started = False
while True:
    command=input(&quot;&gt;&quot;).lower()
    if command == &quot;start&quot;:
        if started:
            print(&quot;Car is already started&quot;)
        else:
            started = True
            print(&quot;Car Started..&quot;)

    elif command == &quot;stop&quot;:
        if not started:
            print(&quot;car already stopped&quot;)
        else:
            started = False
            print(&quot;car stopped...&quot;)

    elif command==&quot;help&quot;:
        print(&quot;&quot;&quot;start- to start car
stop-to stop car
quit-to quit app
        &quot;&quot;&quot;)
</code></pre>
<p>Kindly explain how is the flow of the statement is done in this code.
I am not able to understand the Boolean part here.
If the user enters start or stop then which if or else block gets executed.</p>
",https://stackoverflow.com/questions/76022519/what-is-the-flow-of-the-statement-in-this-python-code,False,
76022512,How to add an edit and delete function in my python app to edit or delete an entire excel row (not just the values but the row itself)?,"<p>Here is my edited/stripped code that contains only the functions that needed solution:</p>
<pre><code>def on_treeview_click(event):
    item = treeview.focus()
    values = treeview.item(item)['values']
    name_entry.delete(0, 'end')
    account_entry.delete(0, 'end')
    payment_entry.delete(0, 'end')
    if values:
        name_entry.insert(0, values[0])
        account_entry.insert(0, values[3])
        payment_entry.insert(0, values[2])

def update_data():
    name = name_entry.get()
    account = account_entry.get()
    payment = payment_entry.get()

    if name and account and payment:
        # Calculate the balance
        try:
            balance = float(account) - float(payment)
        except ValueError:
            balance = &quot;&quot;

        # Insert or update row in Excel sheet
        path = &quot;accounts.xlsx&quot;
        workbook = openpyxl.load_workbook(path)
        sheet = workbook.active
        row_values = [name, account, payment, balance]

        # Check if the name already exists in the sheet
        name_exists = False
        for row in sheet.iter_rows(min_row=2, values_only=True):
            if row[0] == name:
                row[1] = account
                row[2] = payment
                row[3] = balance
                name_exists = True
                break

        if not name_exists:
            sheet.append(row_values)

        workbook.save(path)

        # Insert row into treeview
        treeview.insert('', tk.END, values=row_values)

        # Clear the values
        name_entry.delete(0, &quot;end&quot;)
        name_entry.insert(0, &quot;Name&quot;)
        account_entry.delete(0, &quot;end&quot;)
        account_entry.insert(0, &quot;Account&quot;)
        payment_entry.delete(0, &quot;end&quot;)
        payment_entry.insert(0, &quot;Payment&quot;)

def delete_data():
    # Insert or update row in Excel sheet
    path = &quot;accounts.xlsx&quot;
    workbook = openpyxl.load_workbook(path)
    sheet = workbook.active

    # get the selected items from the treeview
    selection = treeview.selection()
    for item in selection:
        # extract the row number from the treeview item tag
        tags = treeview.item(treeview.focus(), 'tags')
        if tags:
            row = int(treeview.item(treeview.focus(), 'tags')[0])
            # delete the row from the sheet
            sheet.delete_rows(row)
            # delete the item from the treeview
            treeview.delete(item)

    # save the changes to the workbook
    workbook.save('accounts.xlsx')

button = ttk.Button(widgets_frame, text=&quot;Save Data&quot;, command=save_data)
button.grid(row=4, column=0, padx=5, pady=5, sticky=&quot;nsew&quot;)

button = ttk.Button(widgets_frame, text=&quot;Update Data&quot;, command=save_data)
button.grid(row=5, column=0, padx=5, pady=5, sticky=&quot;nsew&quot;)

button = ttk.Button(widgets_frame, text=&quot;Refresh Data&quot;, command=refresh_data)
button.grid(row=6, column=0, padx=5, pady=5, sticky=&quot;nsew&quot;)

button = ttk.Button(widgets_frame, text=&quot;Delete Data&quot;, command=delete_data)
button.grid(row=7, column=0, padx=5, pady=5, sticky=&quot;nsew&quot;)

cols = (&quot;Student Name&quot;, &quot;Account&quot;, &quot;Payment&quot;, &quot;New Balance&quot;)
treeview = ttk.Treeview(treeFrame, show=&quot;headings&quot;,
                        yscrollcommand=treeScroll.set, columns=cols, height=13)
treeview.column(&quot;Student Name&quot;, width=200)
treeview.column(&quot;Account&quot;, width=150)
treeview.column(&quot;Payment&quot;, width=150)
treeview.column(&quot;New Balance&quot;, width=150)
treeview.pack(fill=&quot;both&quot;, expand=True)
treeview.bind(&quot;&lt;ButtonRelease-1&gt;&quot;, on_treeview_click)
treeScroll.config(command=treeview.yview)

load_data()

# Set the weight of the rows and columns to 1, so they expand to fill the available space
root.grid_rowconfigure(0, weight=1)
root.grid_columnconfigure(1, weight=1)

root.mainloop()
</code></pre>
<p>I am not really a programmer I just scrapped codes from anywhere and trying to build an app. This one would be useful in our small school.</p>
<p>As you can see in my code, I already have a functions called update_data() and delete_data(), however, when i click the button &quot;Delete Data&quot; nothing happens and when I click the &quot;Update Data&quot; it creates a new data in a new row.</p>
<p>What I want to accomplish is that in my</p>
<pre><code>onclick_treeview_click(event)
</code></pre>
<p>the treeview item being focused/selected/clicked; the entire row from which it is taken can be edited or deleted thru the update_data() and delete_data() functions.</p>
",https://stackoverflow.com/questions/76022512/how-to-add-an-edit-and-delete-function-in-my-python-app-to-edit-or-delete-an-ent,False,
76022507,Read and Interpolate data from NetCDF file throws shape mismatch error,"<p>I have this netcdf file with data having latitude, longitude and temperature. The temperature has the dimension 103x61 in terms of latitude and longitude. I want to resize this 103x61 grid to 70x70 by applying nearest interpolation. Here is my code:</p>
<pre><code>import numpy as np
from netCDF4 import Dataset
from scipy.interpolate import griddata

# load data from netCDF file
nc_file = Dataset('data.nc', 'r')
lat = nc_file.variables['lat'][:]
lon = nc_file.variables['lon'][:]
temp = nc_file.variables['temp'][:]

# create new grid
new_lat = np.linspace(lat.min(), lat.max(), 80)
new_lon = np.linspace(lon.min(), lon.max(), 80)
new_lat, new_lon = np.meshgrid(new_lat, new_lon)

# flatten lat, lon, and temp arrays
lat_flat = lat.flatten()
lon_flat = lon.flatten()
temp_flat = temp.flatten()

# interpolate to new grid
new_temp = griddata((lat_flat, lon_flat), temp_flat, (new_lat, new_lon), method='linear')

# save new_temp to npy file
np.save('new_temp.npy', new_temp)
</code></pre>
<p>But this produces the following error:</p>
<blockquote>
<p>ValueError: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (103,) and arg 1 with shape (61,).</p>
</blockquote>
<p>I know there is a mismatch between height and width but I can not afford to clip the data from any side to fit into square grid and then interpolate it. Is there a way to get around this error?</p>
",https://stackoverflow.com/questions/76022507/read-and-interpolate-data-from-netcdf-file-throws-shape-mismatch-error,False,
76022501,How can I capture raw messages that are coming out to OpenAI API?,"<p>I'm using LangChain to build prompts that are later sent to the OpenAI API. I've built an agent, but it's behaving a bit differently than I expected. It looks like it's missing some of my instructions that I included in the prompt. Specifically: it seems to not remember past messages. I'm looking for a way to debug it.</p>
<p>Here's more or less how I'm building the agent. I'm not including everything; it's just for general information on what I'm using.</p>
<pre class=""lang-py prettyprint-override""><code>memory = ConversationBufferMemory(memory_key=&quot;chat_history&quot;, return_messages=True)
llm = ChatOpenAI(temperature=0)
agent_chain = initialize_agent(tools, llm, agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)
agent_chain.run(input=&quot;hi, I am Bob&quot;)
</code></pre>
<p>As far as I know, LangChain itself adds some parts of prompts. For example, this particular agent I use add some information about how the answer should be structured. But this is not transparent to the end user, the final prompt is being constructed inside the library. Even in verbose mode I can't see every part of the sent prompt.</p>
<p>Is there any way I could retrieve complete, raw messages sent to the OpenAI API and responses from it?</p>
",https://stackoverflow.com/questions/76022501/how-can-i-capture-raw-messages-that-are-coming-out-to-openai-api,True,
76022469,How to kill a child process and not leave behind a zombie?,"<p>I am keyboard manager script that listens for keys and starts process in the background. This is done mainly for the sake of convenience. Let's say I press the hotkey that lunches the application below. The application lunches as a child process of the parent (the keyboard manager script). Sometimes, I get a problem with the process and I need to kill it so that I can run a new one. However, if I kill the child process, it doesn't go away and remains a zombie process of the parent. I would like to know how to kill the child process and have it go away (i.e., stop being a zombie process).</p>
<pre class=""lang-py prettyprint-override""><code>if &quot;jupyter-lab&quot; not in (p.name() for p in psutil.process_iter()):
    cmd = [ JUPYTER_PATH, &quot;lab&quot;, &quot;--notebook-dir=&quot; + NOTEBOOK_PATH]
    proc = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        start_new_session=True,
    )
</code></pre>
",https://stackoverflow.com/questions/76022469/how-to-kill-a-child-process-and-not-leave-behind-a-zombie,False,
76022463,"Pycuda implementation is really slow compared to CPU, multiple kernel calls","<p>I'm developing a particle filter where I have J particles for each timestep. The code loops through the timesteps and every loop I'm parallelizing three steps of computation. The problem for me is that this implementation is really slow. I think it is because of sending data between CPU and GPU, but I have no idea how to fix it.</p>
<p>Do you have any tips?</p>
<p>Thanks in advance!</p>
<p>The code of the main loop, where I initialize the pycuda.GPUArrays:</p>
<pre><code>def bootstrap(X1, X2, y, dt, L, r_sigma2):
    X1_gpu = gpuarray.to_gpu(X1)
    X2_gpu = gpuarray.to_gpu(X2)
    y_gpu = gpuarray.to_gpu(y)

    L_gpu = gpuarray.to_gpu(L)

    for n in range(1, N):
        logl_gpu = gpuarray.empty(X1[:, 0].shape, dtype=np.float32)
        weights_gpu = gpuarray.empty(logl_gpu.shape, dtype=np.float32)
        x1_prev = gpuarray.to_gpu(X1[:, n-1])
        x2_prev = gpuarray.to_gpu(X2[:, n-1])
        y_n = y_gpu[:,n]

        X_prev_gpu = gpuarray.stack((x1_prev, x2_prev), axis=1)
        e = np.random.randn(J, 2)
        e = e.astype('float32')

        X_new_gpu = gpuarray.empty(X_prev_gpu.shape, np.float32)
        pendulum_model(X_prev_gpu, X_new_gpu, dt, L_gpu, e)
        #X1_gpu[:, n] = X_new_gpu[:, 0]
        #X2_gpu[:, n] = X_new_gpu[:, 1]
        
        GPU_log_likelihood(y_n, X_new_gpu[:, 0], logl_gpu, r_sigma2)
        compute_normalized_weights(weights_gpu, logl_gpu)
        weights = weights_gpu.get()
        #resample_systematic_gpu(weights, X1)
        resample_idx = resample_systematic(weights)
        X1_new = X_new_gpu[:, 0].get()
        X2_new = X_new_gpu[:, 1].get()

      
        X1_new = X1_new[resample_idx]
        X2_new = X2_new[resample_idx]
        
        X1[:,n] = X1_new
        X2[:,n] = X2_new


        #X1_gpu[:, n] = X1_gpu[resample_idx,n]
        #X2_gpu[:, n] = X2_gpu[resample_idx,n]

    
    x_new = X_new_gpu.get()
    return X1, X2, x_new



X1, X2, x_new = bootstrap(X1, X2, y, dt, L, r_sigma2)
</code></pre>
<p>I have tried various different methods and finding examples online. The pycuda documentation wasn't much help and I haven't found any other good resources.</p>
<p>I also tried to send the data using cuda.InOut() and cuda.In() methods, but it was as slow as gpuarrays</p>
",https://stackoverflow.com/questions/76022463/pycuda-implementation-is-really-slow-compared-to-cpu-multiple-kernel-calls,False,
76022457,How to fix this error? ModuleNotFoundError: No module named &#39;pygsp.graphs.nngraph&#39;,"<p>I am not able to import SphereHealpix from pygsp
from pygsp.graphs.nngraphs.spherehealpix import SphereHealpix
(Ref: <a href=""https://github.com/deepsphere/deepsphere-pytorch/blob/master/deepsphere/utils/laplacian_funcs.py"" rel=""nofollow noreferrer"">https://github.com/deepsphere/deepsphere-pytorch/blob/master/deepsphere/utils/laplacian_funcs.py</a>)</p>
<p>the latest pygsp 0.5.1 version doesnt have these attributes</p>
",https://stackoverflow.com/questions/76022457/how-to-fix-this-error-modulenotfounderror-no-module-named-pygsp-graphs-nngrap,False,
76022455,Transparent background Tkinter,"<p>I try to set à simple text with Tkinter on my back ground but the white box is a little trash. I try solution that I found (wm.attributs(&quot;-transparentcolor&quot;, &quot;randomcolor&quot;) but all do disappear the canva background but also my global background.</p>
<pre><code>from tkinter import *


root = Tk()
root.geometry(&quot;600x300&quot;)
root.resizable(False, False)
root.title('Aveugle EDT')


class Button():
    def Button_format(self, position_X, position_Y):
        self.format = Frame(root)
        self.format.place(x=position_X, y=position_Y)
        self.Text_written = StringVar()

    def Button_visual(self, font_color, text_color, border_size, font_text):    
        self.Variable_texte = Entry(self.format, textvariable = self.Text_written, 
                                    background = font_color, bd = border_size, 
                                    fg = text_color, font = font_text)
        self.Variable_texte.pack(fill='x', expand=True)
        self.Variable_texte.focus()
        
    def Window_visual(self, FD_image):
        self.Background_image = PhotoImage(file = FD_image)
        self.background_label = Label(root, image = self.Background_image)
        self.background_label.place(x=0, y=0, relwidth=1, relheight=1)
        
    def Text_name(self):
        self.canvas = Canvas(root, width=40, height=16)
        self.canvas.create_text(20, 10, text = &quot;Test&quot;, font=('Helvetica 10 bold'))
        self.canvas.place(x=10, y= 1)
    

Bouton_test = Button()


Bouton_test.Window_visual(FD_image = &quot;FD.png&quot;)

Bouton_test.Button_format(position_X = 10, position_Y = 23)
Bouton_test.Button_visual(&quot;#F9F8ED&quot;, &quot;black&quot;, 3, &quot;Georgia&quot;)
Bouton_test.Text_name()

root.mainloop()
</code></pre>
<p>Using the wm.attributs of tkinter but doesnt work proprety</p>
<p>and when I use canvas he don't use all the image like this[Screen of test whit canvas (image = 1000x1000, root = 600x300]</p>
",https://stackoverflow.com/questions/76022455/transparent-background-tkinter,False,
76022445,"TensorFlow: Input to reshape is a tensor with 2099200 values, but the requested shape requires a multiple of 31","<p>I'm reviewing <a href=""https://github.com/Karlosse/PassGAN"" rel=""nofollow noreferrer"">PassGAN</a> project based on TensorFlow and, when I generate samples by the command:</p>
<pre><code>python sample.py --input-dir pretrained --checkpoint pretrained/checkpoints/195000.ckpt --output gen_passwords.txt --batch-size 1024 --num-samples 1000000
</code></pre>
<p>I get an error containing the following statement:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 2099200 values, but the requested shape requires a multiple of 31
    [[{{node Reshape_1}}]]
</code></pre>
<p>This error is triggered when, on <a href=""https://github.com/Karlosse/PassGAN/blob/master/sample.py"" rel=""nofollow noreferrer"">sample.py</a> the generation of samples is run by <code>samples = session.run(fake_inputs)</code> where <code>fake_inputs = models.Generator(args.batch_size, args.seq_length, args.layer_dim, len(charmap))</code>. `models.Generator() is defined in <a href=""https://github.com/Karlosse/PassGAN/blob/master/models.py"" rel=""nofollow noreferrer"">models.py</a>.</p>
<p>The <code>31</code> value in the error is given by the value of <code>len(charmap)</code>. In this case, <code>2099200</code> must be a multiple of <code>32</code> so I input <code>len(charmap)+1</code> as argument in <code>models.Generator()</code>.</p>
<p>If I run it again by the same command above, I get now the following error:</p>
<pre><code>INVALID_ARGUMENT: Input to reshape is a tensor with 2099200 values, but the requested shape has 327680
</code></pre>
<p>At this point, if I change the <code>batch_size</code>, both of the input to reshape and the requested shape will change.</p>
<p>How can I fix this issue related to the input to reshape and the requested shape in order to be equal?</p>
",https://stackoverflow.com/questions/76022445/tensorflow-input-to-reshape-is-a-tensor-with-2099200-values-but-the-requested,False,
76022427,Regex: How to use list elements in regex,"<p>I am building a regex query (in Python 3.7) to search for some pattern in a string.
The aim of this is to separate the date from the rest of the query.</p>
<p>The date can take multiple formats, especially for the month.
I would like to integrate list elements in my query, to make it more robust.£
In the example below, I can get date stuff with multiple formats like &quot;oct. 2019 / febr, 2018 / 1997 / 16 January 2012, 08/09/84, 08/09/1964, etc...&quot;. Some format are stranges, but I have to handle this.</p>
<p>An example of want I want:</p>
<pre class=""lang-py prettyprint-override""><code># Python 3.7
import re 


text = &quot;&quot;&quot;
    Some text with some oct date oct. 2017
    Here is a quote from 2018
    Now we can talk about some articles published the 27 January 2017
    This example is the critic's point this is the end. 2019.
&quot;&quot;&quot;

dt_elements = [&quot;oct,&quot;, &quot;oct.&quot;, &quot;jan.&quot;] # And all the stuff I can think about

for line in text.split(&quot;\n&quot;):
    if re.search(r&quot;[a-zA-Zéû]{3}. \d{4}&quot;, line):
        data = re.search(r&quot;[a-zA-Zéû]{3}. \d{4}&quot;, line).group()[0]  # How to integrate dt_elements  here?!
        # This does not works

# Take care about critic example below!!!
</code></pre>
<p>That's it :)</p>
<p>PS:
For the other date format like %D/%M/%Y or %D Janvier %Y I have already some queries which are working, I am looking for a solution to my problem above</p>
",https://stackoverflow.com/questions/76022427/regex-how-to-use-list-elements-in-regex,False,
76022412,Python program for creating countdown timer with features pause and resume using tkinter,"<p>Need display screen with buttons resume and pause.</p>
<p>Pause and resume button should work properly.</p>
<p>Should be able to provide user input and should specify time in hour,minute and second.</p>
",https://stackoverflow.com/questions/76022412/python-program-for-creating-countdown-timer-with-features-pause-and-resume-using,False,
76022408,Scrape table from html,"<p>How to scrape the table from this html with pandas and bs4 then get the first table as the main column and second table and convert it into an csv with python?</p>
<pre><code>&lt;table background-color=&quot;#00CED1&quot; border=&quot;1&quot;&gt;
&lt;tr&gt;&lt;th&gt;STT&lt;/th&gt;&lt;th&gt;Há» vÃ  tÃªn&lt;/th&gt;&lt;th&gt;Sá» bÃ¡o danh&lt;/th&gt;&lt;th&gt;ChuyÃªn&lt;/th&gt;&amp;gt;&lt;th&gt;Äm ToÃ¡n chung&lt;/th&gt;&lt;th&gt;Äm 
VÄn chung&lt;/th&gt;&lt;th&gt;Äm ChuyÃªn&lt;/th&gt;&lt;th&gt;Tá»ng Äiá»m&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;ThÃ¡i BÃ¡ Viá»t An&lt;/td&gt;&lt;td&gt;CO.0001&lt;/td&gt;&lt;td&gt;ToÃ¡n&lt;/td&gt;&lt;td&gt;8.75&lt;/td&gt;&lt;td&gt;4.75&lt;/td&gt;&lt;td&gt;7.5&lt;/td&gt;&lt;td&gt;28.5&lt;/t
d&gt;&lt;/tr&gt;
&lt;/table&gt;
</code></pre>
",https://stackoverflow.com/questions/76022408/scrape-table-from-html,True,76022698
76022378,Numpy set all values to np.nan after index in multi-dimensional array,"<p>I have two numpy arrays - arr1 and arr2. arr2 contains index values for arr1. The shape of arr1 is (100, 8, 96, 192) and the shape of arr2 is (8, 96, 192). What I would like do is set all of the values in arr1 to np.nan after the index values in arr2.</p>
<p>For context, arr1 is time x model x lat x lon and all the indexes values in arr2 correspond to a point in time in the arr1 array. I would like to set the arr1 values at after the point in time in arr2 to np.nan.</p>
<p>Sample Data</p>
<pre><code>arr1 = np.random.rand(*(100, 8, 96, 192))
arr2 = np.random.randint(low=0, high=80,size=(8, 96, 192))
</code></pre>
<pre><code>in: print(arr1)

out: array([[[[0.61718651, 0.24426295, 0.9165573 , ..., 0.24155022,
          0.22327592, 0.9533857 ],
         [0.21922781, 0.87948651, 0.926359  , ..., 0.64281931,
         ...,
         [0.09342961, 0.29533331, 0.11398662, ..., 0.36239606,
          0.40228814, 0.87284515]]]])
</code></pre>
<pre><code>in: print(arr2)

out: array([[[22,  5, 64, ...,  0, 37,  6],
        [71, 48, 33, ...,  8, 38, 32],
        [15, 41, 61, ..., 56, 32, 48],
        ...,
        ...,
        [66, 31, 32, ...,  0, 10,  6],
        [ 9, 28, 72, ..., 71, 29, 34],
        [65, 22, 50, ..., 58, 49, 35]]])
</code></pre>
<p>For reference I have previously asked this question which had some similarities.
<a href=""https://stackoverflow.com/questions/75954978/numpy-multi-dimensional-index"">Numpy multi-dimensional index</a></p>
<p>Based upon this, I tried</p>
<pre><code>arr1 = np.random.rand(100, 8, 96, 192)
arr2 = np.random.randint(low=0, high=80, size=(8, 96, 192))
I, J, K = np.indices((8, 96, 192), sparse=True)
out = arr1[arr2:, I, J, K]


TypeError: only integer scalar arrays can be converted to a scalar index
</code></pre>
<p>Also, perhaps similar to this in concept, but for much higher dimensional arrays
<a href=""https://stackoverflow.com/questions/30019605/set-values-in-numpy-array-to-nan-by-index"">Set values in numpy array to NaN by index</a></p>
",https://stackoverflow.com/questions/76022378/numpy-set-all-values-to-np-nan-after-index-in-multi-dimensional-array,True,76022589
76022365,Why does the jacobian of the metric tensor give zero?,"<p>I am trying to compute the derivatives of the <a href=""https://en.wikipedia.org/wiki/Metric_tensor"" rel=""nofollow noreferrer"">metric tensor</a> given as follows:</p>
<p><img src=""https://quicklatex.com/cache3/e6/ql_b24e8838c420fed42f5ae46a5346c9e6_l3.png"" alt=""Metric tensor in spherical coordinates"" /></p>
<p>As part of this, I am using PyTorch to compute the jacobian of the metric. Here is my code so far:</p>
<pre class=""lang-py prettyprint-override""><code># initial coordinates
r0, theta0, phi0 = (3., torch.pi/2, 0.1)
coord = torch.tensor([r0, theta0, phi0], requires_grad=True)
print(&quot;r, theta, phi coordinates: &quot;,coord.data)

def metric_tensor(coords):
    r = coords[0]
    theta = coords[1]
    phi = coords[2]
    return torch.tensor([
        [1., 0., 0.],
        [0., r ** 2, 0.],
        [0., 0., r ** 2 * torch.sin(theta) ** 2]
    ])

jacobian = torch.autograd.functional.jacobian(metric_tensor, coord, create_graph=True)
</code></pre>
<p>For reasons I don't understand, the jacobian always returns zero, even though the derivatives of the metric shouldn't all be zero. Could anyone point me to what the issue may be?</p>
",https://stackoverflow.com/questions/76022365/why-does-the-jacobian-of-the-metric-tensor-give-zero,True,76023498
76022359,Initiating a dependent QCombobox with PyQt5 in python,"<p>I am learning to work with PyQt5 and want to add a dependent QComboBox.
The items of the Combobox should be a list of the files in the directory.
The function get_files() reads the files in the directory and returns them as a list.</p>
<p>However the QCombobox only shows these items if I run the script in debug mode.
Why is this happening?</p>
<p>Here is the code:</p>
<pre><code>from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtWidgets import QMessageBox
import time

class Ui_MainWindow(object):
    def setupUi(self, MainWindow):
        MainWindow.setObjectName(&quot;MainWindow&quot;)
        MainWindow.resize(700, 681)
        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName(&quot;centralwidget&quot;)
        MainWindow.setCentralWidget(self.centralwidget)

self.B_SOFT = QtWidgets.QComboBox(self.centralwidget)
        self.B_SOFT.setGeometry(QtCore.QRect(130, 150, 360, 50))
        self.B_SOFT.setObjectName(&quot;B_SOFT&quot;)

        xlsx,SOFTS = get_files()
        [self.B_SOFT.addItem(x) for x in SOFTS]

def get_files():
    import os; import numpy as np
    entries = os.listdir()
    msk = ['.xls' in x for x in entries]
    xlsx = [x for i,x in enumerate(entries) if msk[i]]
    names = [x.replace('EPB_SOFTWARE__','').replace('.xlsx','') for x in xlsx]
    return xlsx,names

if __name__ == &quot;__main__&quot;:
    import sys
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = Ui_MainWindow()
    ui.setupUi(MainWindow)
    MainWindow.show()
    sys.exit(app.exec_())
</code></pre>
",https://stackoverflow.com/questions/76022359/initiating-a-dependent-qcombobox-with-pyqt5-in-python,False,
76022357,time data &#39;2023-04-15T16:22:03.721461+05:00&#39; does not match format &quot;%Y-%m-%dT%H:%M:%S.%f+%Z&#39;&quot;,"<p>I get my data from drf as json and ther my datetime looks like:</p>
<pre><code>              `'2023-04-15T16:22:03.721461+05:00'`
</code></pre>
<p>but i want show datetime in template like:</p>
<pre><code>                `'15-04-2023 16:22'` 
</code></pre>
<p>so i use tamplatetag:</p>
<pre><code># app/templatetags/event_extras.py
@register.filter(expects_localtime=True)
def parse_iso(value):
    return datetime.datetime.strptime(value, &quot;%Y-%m-%dT%H:%M:%S.%f+%Z'&quot;)
</code></pre>
<p>and html like:</p>
<pre><code>{% load event_extras %}

&lt;th&gt;{{r.datetime|parse_iso}|date:'d/m/Y H:M'}&lt;/th&gt;
</code></pre>
<p>but got this error:
<code>time data '2023-04-15T16:22:03.721461+05:00' does not match format &quot;%Y-%m-%dT%H:%M:%S.%f+%Z'&quot;</code></p>
",https://stackoverflow.com/questions/76022357/time-data-2023-04-15t162203-7214610500-does-not-match-format-y-m-dth,True,76022414
76022351,Python lists always assign reference not value,"<p>I want to copy list values to another list because I modify the original one and use the new one. Bu t whatever I try it will alwasy assign reference not value.</p>
<pre><code>class Chromosome:

    def __init__(self):
        self.matrix = [[0 for x in range(LENGTH)] for y in range(LENGTH)]

class Population:

    def __init__(self, start):
        self.start = start
        self.chromosomes = []

    def crossover(self):
        for index in range(0, POPULATION_SIZE - 1, 2):
           swap_point = random.randint(7, 10)
           matrix1 = self.chromosomes[index].matrix
           matrix2 = self.chromosomes[index + 1].matrix
           self.chromosomes[index].crossover(matrix2, swap_point)
           self.chromosomes[index + 1].crossover(matrix1, swap_point)
</code></pre>
<p>The following lines is that ı wan to assign values not reference</p>
<p><code>matrix1 = self.chromosomes[index].matrix[:]</code> and
<code>matrix2 = self.chromosomes[index + 1].matrix[:]</code></p>
<p>I tried followings</p>
<p><code>list(self.chromosomes[index + 1].matrix)</code></p>
<p><code>self.chromosomes[index + 1].matrix[:]</code></p>
<p><code>self.chromosomes[index + 1].matrix.copy()</code></p>
<p>How can I get values of the chromosome to matrix1 and matrix2?</p>
",https://stackoverflow.com/questions/76022351/python-lists-always-assign-reference-not-value,True,76022387
76022329,discord.py @silent message,"<p>I have a small discord bot and want to create a <code>!luck</code> command. If you use it the result should be like <code>@wumpus won!</code>. Now, to prevent ping spam there's a possibility to &quot;turn off&quot; pings, just add @silence to your message. Basically it works perfectly, when I add @silent before my message, the user doesn't gets pinged (message looks like this after sending: <code>@wumpus this is a silent mention</code>). But if I want to do that with my discord bot, the <code>@silent</code> before the message is just like normel text.</p>
<p>My code:</p>
<pre class=""lang-py prettyprint-override""><code>await ctx.message.reply(f&quot;@silent &lt;@{choice(members)}&gt; won!&quot;)
</code></pre>
<p>The result is <code>@silent @wumpus won!</code></p>
<p>How can I use the <code>@silent</code> feature in discord.py?</p>
",https://stackoverflow.com/questions/76022329/discord-py-silent-message,True,76022447
76022321,Mean dice coefficient small and dice coefficients for 2 classes increase while dice coefficient for other 2 classes decrease,"<p>I have to segment images with 5 classes. The images have dimension (112,192,160,3) and masks have dimension (112,192,160,5). Both are numpy arrays of dtype 'float 32'.<br />
The model used is a simple U-Net:</p>
<pre><code>from keras.models import Model
from keras.layers import Input, Conv3D, MaxPooling3D, concatenate, Conv3DTranspose, BatchNormalization, Dropout, Lambda
from keras.optimizers import Adam
from keras.metrics import MeanIoU

kernel_initializer =  'he_uniform' #Try others if you want


################################################################
def simple_unet_model(IMG_DEPTH, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS, num_classes):
#Build the model
    inputs = Input((IMG_DEPTH, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))
    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if we normalize our inputs beforehand
    s = inputs

    #Contraction path
    c1 = Conv3D(16, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(s)
    # c1 = Dropout(0.1)(c1)
    c1 = BatchNormalization()(c1) 
    c1 = Conv3D(16, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(c1)
    p1 = MaxPooling3D((2, 2, 2))(c1)
    
    c2 = Conv3D(32, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(p1)
    # c2 = Dropout(0.1)(c2)
    c2 = BatchNormalization()(c2) 
    c2 = Conv3D(32, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(c2)
    p2 = MaxPooling3D((2, 2, 2))(c2)
     
    c3 = Conv3D(64, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(p2)
    # c3 = Dropout(0.2)(c3)
    c3 = BatchNormalization()(c3) 
    c3 = Conv3D(64, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(c3)
    p3 = MaxPooling3D((2, 2, 2))(c3)
     
    c4 = Conv3D(128, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(p3)
    # c4 = Dropout(0.2)(c4)
    c4 = BatchNormalization()(c4) 
    c4 = Conv3D(128, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(c4)
    p4 = MaxPooling3D(pool_size=(2, 2, 2))(c4)
     
    c5 = Conv3D(256, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(p4)
    # c5 = Dropout(0.3)(c5)
    c5 = BatchNormalization()(c5) 
    c5 = Conv3D(256, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(c5)
    
    #Expansive path 
    u6 = Conv3DTranspose(128, (2, 2, 2), strides=(2, 2, 2), padding='same')(c5)
    u6 = concatenate([u6, c4])
    c6 = Conv3D(128, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(u6)
    # c6 = Dropout(0.2)(c6)
    c6 = BatchNormalization()(c6) 
    c6 = Conv3D(128, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(c6)
     
    u7 = Conv3DTranspose(64, (2, 2, 2), strides=(2, 2, 2), padding='same')(c6)
    u7 = concatenate([u7, c3])
    c7 = Conv3D(64, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(u7)
    # c7 = Dropout(0.2)(c7)
    c7 = BatchNormalization()(c7) 
    c7 = Conv3D(64, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(c7)
     
    u8 = Conv3DTranspose(32, (2, 2, 2), strides=(2, 2, 2), padding='same')(c7)
    u8 = concatenate([u8, c2])
    c8 = Conv3D(32, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(u8)
    # c8 = Dropout(0.1)(c8)
    c8 = BatchNormalization()(c8) 
    c8 = Conv3D(32, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(c8)
     
    u9 = Conv3DTranspose(16, (2, 2, 2), strides=(2, 2, 2), padding='same')(c8)
    u9 = concatenate([u9, c1])
    c9 = Conv3D(16, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(u9)
    # c9 = Dropout(0.1)(c9)
    c9 = BatchNormalization()(c9) 
    c9 = Conv3D(16, (3, 3, 3), activation='relu', kernel_initializer=kernel_initializer, padding='same')(c9)
     
    outputs = Conv3D(num_classes, (1, 1, 1), activation='softmax')(c9)
     
    model = Model(inputs=[inputs], outputs=[outputs])
    #compile model outside of this function to make it flexible. 
    model.summary()
    
    return model

model = simple_unet_model(112, 192, 160, 3, 5)
print(model.input_shape)
print(model.output_shape)
</code></pre>
<p>The dice coefficients and some other metrics are defined as reported below:</p>
<pre><code># dice loss as defined above for 3 classes
def dice_coef(y_true, y_pred, smooth=1.0):
    class_num = 5
    for i in range(class_num):
        y_true_f = K.flatten(y_true[:,:,:,:,i])
        y_pred_f = K.flatten(y_pred[:,:,:,:,i])
        intersection = K.sum(y_true_f * y_pred_f)
        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))
   
        if i == 0:
            total_loss = loss
        else:
            total_loss = total_loss + loss
    total_loss = total_loss / class_num

    return total_loss

def dice_coef_loss(y_true, y_pred):
    return 1-dice_coef(y_true, y_pred,smooth=1.0)

def iou(y_true, y_pred):
    intersect = K.sum(y_pred*y_true)
    union = K.sum(y_pred) + K.sum(y_true) - intersect
    iou = K.mean(intersect/union)
    return iou
 
# define per class evaluation of dice coef
# inspired by https://github.com/keras-team/keras/issues/9395

def dice_coef_wholet(y_true, y_pred, epsilon=1e-6):
  intersection = K.sum(K.abs(y_true[:,:,:,:,4] * y_pred[:,:,:,:,4]))
  return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,:,4])) + K.sum(K.square(y_pred[:,:,:,:,4])) + epsilon)

def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):
  intersection = K.sum(K.abs(y_true[:,:,:,:,1] * y_pred[:,:,:,:,1]))
  return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,:,1])) + epsilon)

def dice_coef_edema(y_true, y_pred, epsilon=1e-6):
    intersection = K.sum(K.abs(y_true[:,:,:,:,2] * y_pred[:,:,:,:,2]))
    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,:,2])) + epsilon)


def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):
    intersection = K.sum(K.abs(y_true[:,:,:,:,3] * y_pred[:,:,:,:,3]))
    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,:,3])) + epsilon)


# Computing Precision 
def precision(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision

    
# Computing Sensitivity      
def sensitivity(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    return true_positives / (possible_positives + K.epsilon())


# Computing Specificity
def specificity(y_true, y_pred):
    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))
    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))
    return true_negatives / (possible_negatives + K.epsilon())
</code></pre>
<p>Then I train the model:
csv_logger = CSVLogger('training.log', separator=',', append=False)</p>
<pre><code>callbacks = [
      keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=2, min_lr=0.000001, verbose=1)]
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy',dice_coef,dice_coef_wholet,dice_coef_edema,dice_coef_enhancing,dice_coef_necrotic,iou, precision,sensitivity,specificity])
history =  model.fit(training_generator,
                     epochs=40,
                     steps_per_epoch=len(train_ids),
                     callbacks= callbacks,
                     validation_data = valid_generator,
                     verbose=1)  
</code></pre>
<p>The problem is that during training the mean dice coefficient (dice coef) remains low and the dice coefficients of 2 classes increases while the dice coefficient of the other 2 classes decreases.</p>
<pre><code>Epoch 1/40
250/250 [==============================] - 353s 1s/step - loss: 0.3813 - accuracy: 0.9063 - dice_coef: 0.2799 - dice_coef_wholet: 0.4881 - dice_coef_edema: 0.2904 - dice_coef_enhancing: 0.0865 - dice_coef_necrotic: 0.0853 - iou: 0.7194 - precision: 0.9418 - sensitivity: 0.8342 - specificity: 0.9936 - val_loss: 0.4633 - val_accuracy: 0.9679 - val_dice_coef: 0.2247 - val_dice_coef_wholet: 0.1407 - val_dice_coef_edema: 0.0626 - val_dice_coef_enhancing: 0.0287 - val_dice_coef_necrotic: 0.0225 - val_iou: 0.8495 - val_precision: 0.9743 - val_sensitivity: 0.9449 - val_specificity: 0.9936 - lr: 1.0000e-04
Epoch 2/40
250/250 [==============================] - 347s 1s/step - loss: 0.1663 - accuracy: 0.9704 - dice_coef: 0.3804 - dice_coef_wholet: 0.7223 - dice_coef_edema: 0.4817 - dice_coef_enhancing: 0.0817 - dice_coef_necrotic: 0.0428 - iou: 0.9112 - precision: 0.9921 - sensitivity: 0.9557 - specificity: 0.9980 - val_loss: 1.0876 - val_accuracy: 0.9509 - val_dice_coef: 0.2734 - val_dice_coef_wholet: 0.3303 - val_dice_coef_edema: 0.1461 - val_dice_coef_enhancing: 0.0291 - val_dice_coef_necrotic: 0.0034 - val_iou: 0.8840 - val_precision: 0.9649 - val_sensitivity: 0.9346 - val_specificity: 0.9913 - lr: 1.0000e-04
Epoch 3/40
250/250 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 0.9695 - dice_coef: 0.3764 - dice_coef_wholet: 0.7491 - dice_coef_edema: 0.4737 - dice_coef_enhancing: 0.0350 - dice_coef_necrotic: 0.0025 - iou: 0.9093 - precision: 0.9909 - sensitivity: 0.9579 - specificity: 0.9977
Epoch 3: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.
250/250 [==============================] - 347s 1s/step - loss: 0.2974 - accuracy: 0.9695 - dice_coef: 0.3764 - dice_coef_wholet: 0.7491 - dice_coef_edema: 0.4737 - dice_coef_enhancing: 0.0350 - dice_coef_necrotic: 0.0025 - iou: 0.9093 - precision: 0.9909 - sensitivity: 0.9579 - specificity: 0.9977 - val_loss: 5.1600 - val_accuracy: 0.9379 - val_dice_coef: 0.2809 - val_dice_coef_wholet: 0.3353 - val_dice_coef_edema: 0.1948 - val_dice_coef_enhancing: 0.0185 - val_dice_coef_necrotic: 0.0036 - val_iou: 0.8782 - val_precision: 0.9499 - val_sensitivity: 0.9229 - val_specificity: 0.9875 - lr: 1.0000e-04
Epoch 4/40
250/250 [==============================] - 348s 1s/step - loss: 0.3830 - accuracy: 0.9688 - dice_coef: 0.3855 - dice_coef_wholet: 0.7839 - dice_coef_edema: 0.4798 - dice_coef_enhancing: 0.0185 - dice_coef_necrotic: 0.0033 - iou: 0.9152 - precision: 0.9922 - sensitivity: 0.9585 - specificity: 0.9981 - val_loss: 6.4940 - val_accuracy: 0.9360 - val_dice_coef: 0.2840 - val_dice_coef_wholet: 0.3801 - val_dice_coef_edema: 0.1773 - val_dice_coef_enhancing: 0.0171 - val_dice_coef_necrotic: 0.0050 - val_iou: 0.8717 - val_precision: 0.9506 - val_sensitivity: 0.9230 - val_specificity: 0.9877 - lr: 2.0000e-05
Epoch 5/40
250/250 [==============================] - ETA: 0s - loss: 0.4289 - accuracy: 0.9690 - dice_coef: 0.3886 - dice_coef_wholet: 0.7989 - dice_coef_edema: 0.4644 - dice_coef_enhancing: 0.0146 - dice_coef_necrotic: 0.0031 - iou: 0.9185 - precision: 0.9928 - sensitivity: 0.9600 - specificity: 0.9982
Epoch 5: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.
250/250 [==============================] - 346s 1s/step - loss: 0.4289 - accuracy: 0.9690 - dice_coef: 0.3886 - dice_coef_wholet: 0.7989 - dice_coef_edema: 0.4644 - dice_coef_enhancing: 0.0146 - dice_coef_necrotic: 0.0031 - iou: 0.9185 - precision: 0.9928 - sensitivity: 0.9600 - specificity: 0.9982 - val_loss: 6.2250 - val_accuracy: 0.9460 - val_dice_coef: 0.2787 - val_dice_coef_wholet: 0.3363 - val_dice_coef_edema: 0.1720 - val_dice_coef_enhancing: 0.0137 - val_dice_coef_necrotic: 0.0043 - val_iou: 0.8846 - val_precision: 0.9583 - val_sensitivity: 0.9311 - val_specificity: 0.9895 - lr: 2.0000e-05
Epoch 6/40
250/250 [==============================] - 347s 1s/step - loss: 0.4298 - accuracy: 0.9698 - dice_coef: 0.3913 - dice_coef_wholet: 0.8036 - dice_coef_edema: 0.4716 - dice_coef_enhancing: 0.0141 - dice_coef_necrotic: 0.0031 - iou: 0.9223 - precision: 0.9931 - sensitivity: 0.9614 - specificity: 0.9983 - val_loss: 6.3233 - val_accuracy: 0.9440 - val_dice_coef: 0.2842 - val_dice_coef_wholet: 0.3698 - val_dice_coef_edema: 0.1794 - val_dice_coef_enhancing: 0.0172 - val_dice_coef_necrotic: 0.0061 - val_iou: 0.8797 - val_precision: 0.9574 - val_sensitivity: 0.9299 - val_specificity: 0.9893 - lr: 4.0000e-06
Epoch 7/40
250/250 [==============================] - ETA: 0s - loss: 0.4510 - accuracy: 0.9693 - dice_coef: 0.3919 - dice_coef_wholet: 0.8133 - dice_coef_edema: 0.4625 - dice_coef_enhancing: 0.0126 - dice_coef_necrotic: 0.0028 - iou: 0.9224 - precision: 0.9932 - sensitivity: 0.9615 - specificity: 0.9983
Epoch 7: ReduceLROnPlateau reducing learning rate to 1e-06.
250/250 [==============================] - 348s 1s/step - loss: 0.4510 - accuracy: 0.9693 - dice_coef: 0.3919 - dice_coef_wholet: 0.8133 - dice_coef_edema: 0.4625 - dice_coef_enhancing: 0.0126 - dice_coef_necrotic: 0.0028 - iou: 0.9224 - precision: 0.9932 - sensitivity: 0.9615 - specificity: 0.9983 - val_loss: 7.8702 - val_accuracy: 0.9399 - val_dice_coef: 0.2892 - val_dice_coef_wholet: 0.3933 - val_dice_coef_edema: 0.1858 - val_dice_coef_enhancing: 0.0151 - val_dice_coef_necrotic: 0.0055 - val_iou: 0.8757 - val_precision: 0.9543 - val_sensitivity: 0.9267 - val_specificity: 0.9886 - lr: 4.0000e-06
Epoch 8/40
250/250 [==============================] - 347s 1s/step - loss: 0.4494 - accuracy: 0.9696 - dice_coef: 0.3928 - dice_coef_wholet: 0.8132 - dice_coef_edema: 0.4625 - dice_coef_enhancing: 0.0123 - dice_coef_necrotic: 0.0026 - iou: 0.9233 - precision: 0.9932 - sensitivity: 0.9617 - specificity: 0.9983 - val_loss: 8.6596 - val_accuracy: 0.9376 - val_dice_coef: 0.2898 - val_dice_coef_wholet: 0.3982 - val_dice_coef_edema: 0.1889 - val_dice_coef_enhancing: 0.0152 - val_dice_coef_necrotic: 0.0055 - val_iou: 0.8719 - val_precision: 0.9523 - val_sensitivity: 0.9246 - val_specificity: 0.9881 - lr: 1.0000e-06
Epoch 9/40
250/250 [==============================] - 348s 1s/step - loss: 0.4543 - accuracy: 0.9694 - dice_coef: 0.3928 - dice_coef_wholet: 0.8140 - dice_coef_edema: 0.4616 - dice_coef_enhancing: 0.0120 - dice_coef_necrotic: 0.0026 - iou: 0.9232 - precision: 0.9932 - sensitivity: 0.9617 - specificity: 0.9983 - val_loss: 8.1100 - val_accuracy: 0.9393 - val_dice_coef: 0.2896 - val_dice_coef_wholet: 0.3970 - val_dice_coef_edema: 0.1878 - val_dice_coef_enhancing: 0.0149 - val_dice_coef_necrotic: 0.0053 - val_iou: 0.8739 - val_precision: 0.9538 - val_sensitivity: 0.9262 - val_specificity: 0.9885 - lr: 1.0000e-06
Epoch 10/40
250/250 [==============================] - 349s 1s/step - loss: 0.4596 - accuracy: 0.9693 - dice_coef: 0.3924 - dice_coef_wholet: 0.8142 - dice_coef_edema: 0.4608 - dice_coef_enhancing: 0.0118 - dice_coef_necrotic: 0.0026 - iou: 0.9228 - precision: 0.9932 - sensitivity: 0.9615 - specificity: 0.9983 - val_loss: 7.0463 - val_accuracy: 0.9433 - val_dice_coef: 0.2869 - val_dice_coef_wholet: 0.3811 - val_dice_coef_edema: 0.1877 - val_dice_coef_enhancing: 0.0154 - val_dice_coef_necrotic: 0.0055 - val_iou: 0.8777 - val_precision: 0.9569 - val_sensitivity: 0.9293 - val_specificity: 0.9892 - lr: 1.0000e-06
Epoch 11/40
250/250 [==============================] - 348s 1s/step - loss: 0.4577 - accuracy: 0.9694 - dice_coef: 0.3929 - dice_coef_wholet: 0.8153 - dice_coef_edema: 0.4588 - dice_coef_enhancing: 0.0118 - dice_coef_necrotic: 0.0025 - iou: 0.9234 - precision: 0.9932 - sensitivity: 0.9617 - specificity: 0.9983 - val_loss: 6.8756 - val_accuracy: 0.9433 - val_dice_coef: 0.2866 - val_dice_coef_wholet: 0.3834 - val_dice_coef_edema: 0.1821 - val_dice_coef_enhancing: 0.0149 - val_dice_coef_necrotic: 0.0054 - val_iou: 0.8779 - val_precision: 0.9571 - val_sensitivity: 0.9295 - val_specificity: 0.9892 - lr: 1.0000e-06
Epoch 12/40
250/250 [==============================] - 350s 1s/step - loss: 0.4621 - accuracy: 0.9692 - dice_coef: 0.3927 - dice_coef_wholet: 0.8158 - dice_coef_edema: 0.4579 - dice_coef_enhancing: 0.0116 - dice_coef_necrotic: 0.0025 - iou: 0.9233 - precision: 0.9932 - sensitivity: 0.9616 - specificity: 0.9983 - val_loss: 7.1805 - val_accuracy: 0.9427 - val_dice_coef: 0.2867 - val_dice_coef_wholet: 0.3847 - val_dice_coef_edema: 0.1828 - val_dice_coef_enhancing: 0.0150 - val_dice_coef_necrotic: 0.0055 - val_iou: 0.8766 - val_precision: 0.9565 - val_sensitivity: 0.9289 - val_specificity: 0.9891 - lr: 1.0000e-06
Epoch 13/40
250/250 [==============================] - 350s 1s/step - loss: 0.4615 - accuracy: 0.9694 - dice_coef: 0.3931 - dice_coef_wholet: 0.8156 - dice_coef_edema: 0.4588 - dice_coef_enhancing: 0.0115 - dice_coef_necrotic: 0.0025 - iou: 0.9236 - precision: 0.9932 - sensitivity: 0.9617 - specificity: 0.9983 - val_loss: 8.3051 - val_accuracy: 0.9394 - val_dice_coef: 0.2905 - val_dice_coef_wholet: 0.3989 - val_dice_coef_edema: 0.1913 - val_dice_coef_enhancing: 0.0147 - val_dice_coef_necrotic: 0.0051 - val_iou: 0.8739 - val_precision: 0.9538 - val_sensitivity: 0.9262 - val_specificity: 0.9884 - lr: 1.0000e-06
Epoch 14/40
250/250 [==============================] - 351s 1s/step - loss: 0.4658 - accuracy: 0.9693 - dice_coef: 0.3927 - dice_coef_wholet: 0.8156 - dice_coef_edema: 0.4581 - dice_coef_enhancing: 0.0113 - dice_coef_necrotic: 0.0024 - iou: 0.9231 - precision: 0.9932 - sensitivity: 0.9615 - specificity: 0.9983 - val_loss: 7.5375 - val_accuracy: 0.9421 - val_dice_coef: 0.2881 - val_dice_coef_wholet: 0.3925 - val_dice_coef_edema: 0.1821 - val_dice_coef_enhancing: 0.0152 - val_dice_coef_necrotic: 0.0054 - val_iou: 0.8762 - val_precision: 0.9562 - val_sensitivity: 0.9286 - val_specificity: 0.9890 - lr: 1.0000e-06
Epoch 15/40
250/250 [==============================] - 351s 1s/step - loss: 0.4732 - accuracy: 0.9691 - dice_coef: 0.3929 - dice_coef_wholet: 0.8176 - dice_coef_edema: 0.4559 - dice_coef_enhancing: 0.0109 - dice_coef_necrotic: 0.0023 - iou: 0.9232 - precision: 0.9932 - sensitivity: 0.9615 - specificity: 0.9983 - val_loss: 8.2374 - val_accuracy: 0.9400 - val_dice_coef: 0.2898 - val_dice_coef_wholet: 0.3953 - val_dice_coef_edema: 0.1943 - val_dice_coef_enhancing: 0.0146 - val_dice_coef_necrotic: 0.0050 - val_iou: 0.8737 - val_precision: 0.9541 - val_sensitivity: 0.9265 - val_specificity: 0.9885 - lr: 1.0000e-06
Epoch 16/40
250/250 [==============================] - 353s 1s/step - loss: 0.4715 - accuracy: 0.9692 - dice_coef: 0.3925 - dice_coef_wholet: 0.8147 - dice_coef_edema: 0.4591 - dice_coef_enhancing: 0.0109 - dice_coef_necrotic: 0.0023 - iou: 0.9227 - precision: 0.9932 - sensitivity: 0.9614 - specificity: 0.9983 - val_loss: 8.8130 - val_accuracy: 0.9386 - val_dice_coef: 0.2896 - val_dice_coef_wholet: 0.4025 - val_dice_coef_edema: 0.1847 - val_dice_coef_enhancing: 0.0160 - val_dice_coef_necrotic: 0.0055 - val_iou: 0.8715 - val_precision: 0.9534 - val_sensitivity: 0.9255 - val_specificity: 0.9883 - lr: 1.0000e-06
Epoch 17/40
250/250 [==============================] - 352s 1s/step - loss: 0.4717 - accuracy: 0.9693 - dice_coef: 0.3934 - dice_coef_wholet: 0.8172 - dice_coef_edema: 0.4564 - dice_coef_enhancing: 0.0108 - dice_coef_necrotic: 0.0023 - iou: 0.9238 - precision: 0.9932 - sensitivity: 0.9616 - specificity: 0.9983 - val_loss: 8.3258 - val_accuracy: 0.9402 - val_dice_coef: 0.2896 - val_dice_coef_wholet: 0.3980 - val_dice_coef_edema: 0.1881 - val_dice_coef_enhancing: 0.0149 - val_dice_coef_necrotic: 0.0051 - val_iou: 0.8737 - val_precision: 0.9545 - val_sensitivity: 0.9269 - val_specificity: 0.9886 - lr: 1.0000e-06
Epoch 18/40
250/250 [==============================] - 352s 1s/step - loss: 0.4763 - accuracy: 0.9692 - dice_coef: 0.3926 - dice_coef_wholet: 0.8159 - dice_coef_edema: 0.4569 - dice_coef_enhancing: 0.0106 - dice_coef_necrotic: 0.0022 - iou: 0.9228 - precision: 0.9932 - sensitivity: 0.9614 - specificity: 0.9983 - val_loss: 8.5508 - val_accuracy: 0.9399 - val_dice_coef: 0.2897 - val_dice_coef_wholet: 0.3970 - val_dice_coef_edema: 0.1899 - val_dice_coef_enhancing: 0.0150 - val_dice_coef_necrotic: 0.0050 - val_iou: 0.8733 - val_precision: 0.9541 - val_sensitivity: 0.9265 - val_specificity: 0.9885 - lr: 1.0000e-06
Epoch 19/40
250/250 [==============================] - 352s 1s/step - loss: 0.4783 - accuracy: 0.9692 - dice_coef: 0.3929 - dice_coef_wholet: 0.8159 - dice_coef_edema: 0.4578 - dice_coef_enhancing: 0.0104 - dice_coef_necrotic: 0.0022 - iou: 0.9230 - precision: 0.9932 - sensitivity: 0.9614 - specificity: 0.9983 - val_loss: 9.3853 - val_accuracy: 0.9368 - val_dice_coef: 0.2918 - val_dice_coef_wholet: 0.4100 - val_dice_coef_edema: 0.1906 - val_dice_coef_enhancing: 0.0156 - val_dice_coef_necrotic: 0.0051 - val_iou: 0.8700 - val_precision: 0.9518 - val_sensitivity: 0.9241 - val_specificity: 0.9879 - lr: 1.0000e-06
Epoch 20/40
250/250 [==============================] - 350s 1s/step - loss: 0.4813 - accuracy: 0.9693 - dice_coef: 0.3932 - dice_coef_wholet: 0.8151 - dice_coef_edema: 0.4601 - dice_coef_enhancing: 0.0103 - dice_coef_necrotic: 0.0022 - iou: 0.9232 - precision: 0.9931 - sensitivity: 0.9614 - specificity: 0.9983 - val_loss: 7.4871 - val_accuracy: 0.9430 - val_dice_coef: 0.2884 - val_dice_coef_wholet: 0.3869 - val_dice_coef_edema: 0.1937 - val_dice_coef_enhancing: 0.0143 - val_dice_coef_necrotic: 0.0050 - val_iou: 0.8763 - val_precision: 0.9564 - val_sensitivity: 0.9290 - val_specificity: 0.9891 - lr: 1.0000e-06
Epoch 21/40
250/250 [==============================] - 352s 1s/step - loss: 0.4830 - accuracy: 0.9691 - dice_coef: 0.3927 - dice_coef_wholet: 0.8171 - dice_coef_edema: 0.4543 - dice_coef_enhancing: 0.0101 - dice_coef_necrotic: 0.0021 - iou: 0.9228 - precision: 0.9932 - sensitivity: 0.9613 - specificity: 0.9983 - val_loss: 8.6420 - val_accuracy: 0.9402 - val_dice_coef: 0.2895 - val_dice_coef_wholet: 0.3953 - val_dice_coef_edema: 0.1915 - val_dice_coef_enhancing: 0.0149 - val_dice_coef_necrotic: 0.0049 - val_iou: 0.8731 - val_precision: 0.9543 - val_sensitivity: 0.9267 - val_specificity: 0.9886 - lr: 1.0000e-06
Epoch 22/40
250/250 [==============================] - 353s 1s/step - loss: 0.4878 - accuracy: 0.9691 - dice_coef: 0.3926 - dice_coef_wholet: 0.8159 - dice_coef_edema: 0.4574 - dice_coef_enhancing: 0.0099 - dice_coef_necrotic: 0.0021 - iou: 0.9226 - precision: 0.9931 - sensitivity: 0.9613 - specificity: 0.9983 - val_loss: 8.8685 - val_accuracy: 0.9392 - val_dice_coef: 0.2905 - val_dice_coef_wholet: 0.4003 - val_dice_coef_edema: 0.1955 - val_dice_coef_enhancing: 0.0148 - val_dice_coef_necrotic: 0.0050 - val_iou: 0.8716 - val_precision: 0.9534 - val_sensitivity: 0.9259 - val_specificity: 0.9883 - lr: 1.0000e-06
Epoch 23/40
250/250 [==============================] - 351s 1s/step - loss: 0.4866 - accuracy: 0.9692 - dice_coef: 0.3927 - dice_coef_wholet: 0.8153 - dice_coef_edema: 0.4580 - dice_coef_enhancing: 0.0099 - dice_coef_necrotic: 0.0020 - iou: 0.9225 - precision: 0.9931 - sensitivity: 0.9612 - specificity: 0.9983 - val_loss: 7.2780 - val_accuracy: 0.9444 - val_dice_coef: 0.2869 - val_dice_coef_wholet: 0.3849 - val_dice_coef_edema: 0.1850 - val_dice_coef_enhancing: 0.0152 - val_dice_coef_necrotic: 0.0054 - val_iou: 0.8775 - val_precision: 0.9578 - val_sensitivity: 0.9304 - val_specificity: 0.9894 - lr: 1.0000e-06
</code></pre>
",https://stackoverflow.com/questions/76022321/mean-dice-coefficient-small-and-dice-coefficients-for-2-classes-increase-while-d,False,
76022304,How to extract NDVI values from shape file,"<p>I am doing a machine learning project in which I am making a crop classifier using NDVI values from sentinel hub API, the problem I am facing is that when i called api for 100000 coordinates it said that limit has reached for google api.</p>
<p>Now i have researched more and have seen that 50000 is the limit of API, then I discovered that NDVIs can also be extracted from shapefiles and as I have shape files present for cities, I wanted to know is there any possibility that NDVI can be extracted from shape files. If some one can provide a tutorial or tell about the steps to do it, it would be very helpful</p>
",https://stackoverflow.com/questions/76022304/how-to-extract-ndvi-values-from-shape-file,False,
76022291,Get list of latest installed python packages,"<p>I am trying to get an ordered list of the lastest installed packages in Python. I am using pip (I cannot use -conda or terminal). The instruction <code>pip list</code> returns all the packages I have ever installed. Since I installed a package which damaged the older ones, I need the list of the latest installed packages in order to uninstall only them and reinstall them later. In simple words, I need to do a rollback. How can I do it?</p>
",https://stackoverflow.com/questions/76022291/get-list-of-latest-installed-python-packages,True,76022385
76022286,Why is my raise to power code running without doing anything?,"<pre><code>def raise_to_power(base_num, pow_num):
    result = 1
    i = 1
    base_num = float(input(&quot;Your base number= &quot;))
    pow_num = float(input(&quot;Your Power number = &quot;))
    if pow_num == 0:
        print(result)
    else:
        while i &lt;= pow_num:
            result = result * base_num
            i += 1
    print(result)
</code></pre>
<p>When I run the code it's not even asking for an input, any ideas why?</p>
<p>I played with the indents changed the positioning of the code but that only furthers my problem. I don't know why it's not even asking for the input.</p>
",https://stackoverflow.com/questions/76022286/why-is-my-raise-to-power-code-running-without-doing-anything,True,76022342
76022261,How to solve task with regex,"<p>Task:
The input is a string consisting of characters A, B, C.</p>
<p>Find the maximal number of &quot;A*A&quot; or &quot;C*C&quot; sub-strings, that are consecutive.</p>
<p>Where the * sign stands for any possible character.</p>
<p>Take into account overlaps.</p>
<hr />
<p>INPUT: <a href=""https://kompege.ru/files/sJDwcyfWx.txt"" rel=""nofollow noreferrer"">https://kompege.ru/files/sJDwcyfWx.txt</a></p>
<p>OUTPUT: 21</p>
<p>LANGUAGE: PYTHON</p>
<p>Solution without RegEX (Correct):</p>
<pre><code>with open(&quot;./data/24_4546.txt&quot;) as f:
    s = f.readline().replace('\n', '')

c = m = 0
for j in range(3):
    for i in range(j, len(s) - 2, 3):
        if s[i] + s[i + 2] == &quot;AA&quot; or s[i] + s[i + 2] == &quot;CC&quot;:
            c += 1
        else:
            m = max(m, c)
            c = 0

m = max(m, c)
print(m)
</code></pre>
<hr />
<p>My attempts with regex (not work):</p>
<pre><code>import re
with open(&quot;./data/24_4546.txt&quot;) as f:
    s = f.readline().replace('\n', '')

print(len(max(re.sub(r&quot;[^*]&quot;, &quot; &quot;, re.sub(r&quot;(?=A[A-C]A)|(?=C[A-C]C)&quot;, &quot;*&quot;, s)).split(), key=len)))
</code></pre>
",https://stackoverflow.com/questions/76022261/how-to-solve-task-with-regex,True,76023450
76022261,How to solve task with regex,"<p>Task:
The input is a string consisting of characters A, B, C.</p>
<p>Find the maximal number of &quot;A*A&quot; or &quot;C*C&quot; sub-strings, that are consecutive.</p>
<p>Where the * sign stands for any possible character.</p>
<p>Take into account overlaps.</p>
<hr />
<p>INPUT: <a href=""https://kompege.ru/files/sJDwcyfWx.txt"" rel=""nofollow noreferrer"">https://kompege.ru/files/sJDwcyfWx.txt</a></p>
<p>OUTPUT: 21</p>
<p>LANGUAGE: PYTHON</p>
<p>Solution without RegEX (Correct):</p>
<pre><code>with open(&quot;./data/24_4546.txt&quot;) as f:
    s = f.readline().replace('\n', '')

c = m = 0
for j in range(3):
    for i in range(j, len(s) - 2, 3):
        if s[i] + s[i + 2] == &quot;AA&quot; or s[i] + s[i + 2] == &quot;CC&quot;:
            c += 1
        else:
            m = max(m, c)
            c = 0

m = max(m, c)
print(m)
</code></pre>
<hr />
<p>My attempts with regex (not work):</p>
<pre><code>import re
with open(&quot;./data/24_4546.txt&quot;) as f:
    s = f.readline().replace('\n', '')

print(len(max(re.sub(r&quot;[^*]&quot;, &quot; &quot;, re.sub(r&quot;(?=A[A-C]A)|(?=C[A-C]C)&quot;, &quot;*&quot;, s)).split(), key=len)))
</code></pre>
",https://stackoverflow.com/questions/76022261/how-to-solve-task-with-regex,True,76023450
76022258,Errors when using VOSK for real-time speech recognition (python),"<p>I am trying to install the VOSK library for speech recognition, I also installed a trained model and unpacked it in .../vosk/vosk-model-ru-0.42.. But I have errors during the launch of the model, I don't understand what it wants from me<a href=""https://i.stack.imgur.com/mceVY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mceVY.jpg"" alt=""enter image description here"" /></a></p>
<p>I didn't find a solution in google</p>
",https://stackoverflow.com/questions/76022258/errors-when-using-vosk-for-real-time-speech-recognition-python,False,
76022237,working examples of js.amcharts in python,"<p>I came across this <a href=""https://pypi.org/project/js.amcharts"" rel=""nofollow noreferrer"">link</a> for using js.amcharts in python, but couldn't find any working examples of the same. I did try googling but no luck. Can someone please help me on this? I like the 3d charts in amcharts and thus want to generate a PDF in python and embed the charts in it. Please help.</p>
",https://stackoverflow.com/questions/76022237/working-examples-of-js-amcharts-in-python,False,
76022234,Heroku application issues: connection timeout when using heroku redis plan above mini,"<p>I am experiencing a timeout issue when attempting to connect to my Redis server on Heroku. I have configured my application to use TLS for the connection, as shown in the following Python code:</p>
<pre><code>python
Copy code
import os
from urllib.parse import urlparse
import redis

url = urlparse(os.environ.get(&quot;REDIS_URL&quot;))
r = redis.Redis(host=url.hostname, port=url.port, password=(redacted)), ssl=True, ssl_cert_reqs=None)```

When I run this code on Heroku, I encounter a timeout error. However, when I test the same code locally by setting the REDIS_URL environment variable to my Redis server's URL, the connection is successful and I can perform basic Redis operations without any issues.

Here is how I was handling the TLS connection to the redis server on my code:

```os.environ[&quot;REDIS_URL&quot;] = &quot;redisURL&quot;

def backend_url_with_tls(redis_url):
url = urlparse(redis_url)
query = &quot;ssl_cert_reqs=CERT_NONE&quot;
return urlunparse((url.scheme, url.netloc, url.path, url.params, query, url.fragment))

redis_url = os.environ.get(&quot;REDIS_URL&quot;)
backend_url = backend_url_with_tls(redis_url)

app = Celery('chatbot_tasks', broker=redis_url, backend=backend_url)```

I had to downgrade to the mini plan because I can't find a solution to this issue.
</code></pre>
",https://stackoverflow.com/questions/76022234/heroku-application-issues-connection-timeout-when-using-heroku-redis-plan-above,False,
76022213,partially initialized module &#39;openai&#39; has no attribute &#39;Completion&#39; (most likely due to a circular import) my code is giving this error,"<p>There is no issue with my code and I have installed OpenAI, but it's not working. It says that openaiCompletioncreate is not installed ㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤ</p>
",https://stackoverflow.com/questions/76022213/partially-initialized-module-openai-has-no-attribute-completion-most-likely,False,
76022203,unable to convert Unix time to normal format in a DataFrame in python,"<p>I have a DataFrame with one single column named &quot;time&quot; (int64 type) which has Unix time format which I want to convert it to normal time format (%Y %M %D %H %M %S), but I just keep getting error.</p>
<p>here is my code:</p>
<pre><code>df_time[&quot;time&quot;] = pd.to_datetime(df_time[&quot;time&quot;], unit='s')
</code></pre>
<p>and I received this error:</p>
<blockquote>
<p>OutOfBoundsDatetime: cannot convert input with unit 's'</p>
</blockquote>
<p>Then I tried:</p>
<pre><code>df_time[&quot;time&quot;] = pd.to_datetime(df_time[&quot;time&quot;], format='%Y %M %D %H %M %S', errors='coerce')
</code></pre>
<p>and now I just get NaT for all values.</p>
<p>I have manually checked the values to make sure they are all in a correct Unix format and they are fine.
for example :</p>
<pre><code>0   1681560550701
1   1681560545790
2   1681560543705
3   1681560541624
4   1681560540980
5   1681560520533
6   1681560520533
</code></pre>
<p>I would really appreciate if you can help me through this.</p>
",https://stackoverflow.com/questions/76022203/unable-to-convert-unix-time-to-normal-format-in-a-dataframe-in-python,False,
76022183,python dataframe rolling on month-end dates: rolling average of the lowest 1% values,"<p>For each month-end date and each &quot;PERMNO&quot; (company identifier), I'd like to compute the rolling average of the lowest 1% values in the past 252 days from a dataframe column named &quot;RET&quot;</p>
<p>To compute it, I did:</p>
<pre><code>def get_es(x):
    return (x&lt;=x.iloc[0:249].quantile(0.01)).mean()
df['es_1pct'] = df.groupby(['PERMNO'])['RET'].rolling(window = 250, min_periods=200).apply(get_es).reset_index()['RET']
</code></pre>
<p>However, the results are completely wrong because it returns the whole data sample's average of the observations that are below the 1st percentile, and it's not rolling at all.</p>
<p>(p.s., I learned the code from <a href=""https://stackoverflow.com/questions/71217160/calculate-the-average-of-the-lowest-n-percentile"">Calculate the average of the&#160;lowest n&#160;percentile</a>, but they don't do rolling.)</p>
<p>Could someone please help out on how to roll just on the month-end dates to speed up the computation; and more importantly, how to properly compute the rolling average of the lowest 1st percentile?</p>
<p>Much appreciated!</p>
<p>Best,
Darcy</p>
",https://stackoverflow.com/questions/76022183/python-dataframe-rolling-on-month-end-dates-rolling-average-of-the-lowest-1-va,False,
76022167,Return a reference to unpacked dictionary in Python,"<p>There is a common way to pass arguments in Python like this:</p>
<pre><code>def foo(a1, a2):
    print(a1, a2)
    
def my_dict():
    return {'a1': 4, 'a2':5}

foo(**my_dict())
</code></pre>
<p>...</p>
<pre><code>4 5
</code></pre>
<p>But now I want to move the asterisks inside my_dict function:</p>
<pre><code>def foo(a1, a2):
    print(a1, a2)
    
def my_dict():
    return **{'a1': 4, 'a2':5}

foo(my_dict())
</code></pre>
<p>...</p>
<pre><code>SyntaxError: invalid syntax
</code></pre>
<p>Is there a way to achieve such behavior in Python? I am dealing with third-party code and can modify my_dict function, but can't modify foo(my_dict()) call.</p>
",https://stackoverflow.com/questions/76022167/return-a-reference-to-unpacked-dictionary-in-python,False,
76022112,Amazon pinpoint sms adding own phone number to send sms,"<p>I hope you all are having a great day. I was checking amazon pinpoint sms and it worked fine on test messaging part.</p>
<p>I have two questions though.</p>
<ol>
<li>Using my own phone number for sending and receiving messages. The normal way the pinpoint service is this to request a phone number. Is it possible to setup my own phone number as a service that does the messaging and receiving part?</li>
</ol>
<p>In the case of if 1 is not possible:</p>
<ol start=""2"">
<li>The amount of numbers I can buy from the phone services are limited. The cheapest one is USA and I am normally ok with buying it but this limits the number of messages I can send to it since I am not from USA and that type of message will cost much more than the normal messaging(SMS).</li>
</ol>
<p>I hope my question is clear, please comment on whichever part on the question should be updated.</p>
<p>P.S I am from Turkey so using another number from Turkey through Amazon would be nice too.</p>
<p>Thanks :)</p>
",https://stackoverflow.com/questions/76022112/amazon-pinpoint-sms-adding-own-phone-number-to-send-sms,False,
76022101,Python cx_Oracle Error - cx_AttributeError: &#39;function&#39; object has no attribute &#39;,"<p>I have a requirement in which I have to extract data from multiple oracle tables to one excel file - one sheet per tbale.
I create one config file and extracts.py files as below.</p>
<p>I am getting error as below.</p>
<p>And code for file - Oracle_Extracts.py is as below</p>
<pre><code>#!/usr/bin/env python
import cx_Oracle
import pandas as pd
import yaml 
import xlsxwriter

# Load config.yaml file
with open(&quot;config.yaml&quot;, &quot;r&quot;) as f:
    config = yaml.safe_load(f)

# Get Oracle connection details
OracleuserID = config[&quot;Oracle&quot;][&quot;username&quot;]
Oraclepassword = config[&quot;Oracle&quot;][&quot;password&quot;]
Oracledsn = config[&quot;Oracle&quot;][&quot;dsn&quot;]


def printheader():
    print('\n################### Oracle Extracts Automation Framework #########################')

def Oracle_connection(username,password,dsn):
    global conn
    global cur
    print('\n####################### Establishing Connections to Oracle DB ##############################')
    try:
        conn = cx_Oracle.connect(user=OracleuserID, password=Oraclepassword,
                               dsn=Oracledsn,
                               encoding=&quot;UTF-8&quot;)
        print('\nConnected to Oracle DB')
        cur=conn.cursor()

    except Exception as err:
        # logger.error('Error while connecting to Oracle {}'.format(err))
        print('\nError while connecting to Oracle')
        print(err)

printheader()

# Set the output file path
output_path = 'D:\Automation\Table_Export.xlsx'

# Create the connection to the Oracle database
Oracle_connection(OracleuserID,Oraclepassword,Oracledsn)

# Define the list of tables to retrieve data from
tables = ['OFFICER', 'CUSTOMER', 'PRODUCT']

# Export data from each table to a separate sheet in an Excel file
with pd.ExcelWriter(output_path) as writer:
    #for table in tables:
        # Define the SQL query to retrieve data from the table
        query = f&quot;SELECT * FROM OFFICER ORDER BY INS_DTTM&quot;

        # Use pandas to read the data from the Oracle database into a DataFrame
        dataframe = pd.read_sql(query, Oracle_connection)

        # Write the DataFrame to a new sheet in the Excel file
        dataframe.to_excel(writer, sheet_name=table, index=False)

# Close the database connection
print('\n####################### Closing Connections to Oracle DB ##############################')
cur.close()
conn.close()
print('\nDisconnected from Oracle DB')
</code></pre>
<p>And Config file - config.yaml is as below.</p>
<pre><code># Configuration file for Oracle EXtracts

#################### Oracle-- ODS
Oracle:

  dsn: localhost:1521/XEPDB1
  username: SALES
  password: Admin
</code></pre>
<p>Can anybody please help me with this ?</p>
<p>Thanks,
Mahesh</p>
<p>Error I am getting is as below.</p>
<pre><code>PS D:\Automation&gt; python .\Oracle_Extracts.py
Traceback (most recent call last):
  File &quot;D:\Automation\Oracle_Extracts.py&quot;, line 6, in &lt;module&gt;
    from sqlalchemy import create_engine
ModuleNotFoundError: No module named 'sqlalchemy'
PS D:\Automation&gt; python .\Oracle_Extracts.py

################### Oracle Extracts Automation Framework #########################

####################### Establishing Connections to Oracle DB ##############################

Connected to Oracle DB
D:\Automation\Oracle_Extracts.py:57: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
  dataframe = pd.read_sql(query, Oracle_connection)
Traceback (most recent call last):
  File &quot;D:\Automation\Oracle_Extracts.py&quot;, line 57, in &lt;module&gt;
    dataframe = pd.read_sql(query, Oracle_connection)
  File &quot;C:\Users\Mahes\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\io\sql.py&quot;, line 633, in read_sql
    return pandas_sql.read_query(
  File &quot;C:\Users\Mahes\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\io\sql.py&quot;, line 2264, in read_query
    cursor = self.execute(sql, params)
  File &quot;C:\Users\Mahes\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\io\sql.py&quot;, line 2198, in execute
    cur = self.con.cursor()
AttributeError: 'function' object has no attribute 'cursor'
</code></pre>
",https://stackoverflow.com/questions/76022101/python-cx-oracle-error-cx-attributeerror-function-object-has-no-attribute,False,
76022058,What could be causing &quot;SSLError: EOF occurred in violation of protocol&quot; when using requests.get() to access YouTube videos in python?,"<p>I am very new to programming and the computer world in general. I am trying to send a request to <a href=""http://www.youtube.com"" rel=""nofollow noreferrer"">www.youtube.com</a> using the requests module. When I type its address in my browser, everything works just fine and I can access YouTube without any problems. However, when I try to access YouTube with the following Python code:</p>
<pre><code>import requests
requests.get(&quot;https://www.youtube.com&quot;)
</code></pre>
<p>I encounter the following errors:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\connectionpool.py&quot;, line 700, in urlopen
    self._prepare_proxy(conn)
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\connectionpool.py&quot;, line 996, in _prepare_proxy
    conn.connect()
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\connection.py&quot;, line 369, in connect
    self.sock = conn = self._connect_tls_proxy(hostname, conn)
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\connection.py&quot;, line 504, in _connect_tls_proxy
    socket = ssl_wrap_socket(
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\util\ssl_.py&quot;, line 453, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\util\ssl_.py&quot;, line 495, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock)
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\ssl.py&quot;, line 500, in wrap_socket
    return self.sslsocket_class._create(
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\ssl.py&quot;, line 1040, in _create    self.do_handshake()
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\ssl.py&quot;, line 1309, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1129)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\adapters.py&quot;, line 489, in send
    resp = conn.urlopen(
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\connectionpool.py&quot;, line 787, in urlopen
    retries = retries.increment(
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\urllib3\util\retry.py&quot;, line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.youtube.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;c:\Users\BeBit\Desktop\Python WorkPlace\MyProjectsandOnlyMines\YouTubeDownload.py&quot;, line 7, in &lt;module&gt;
    requests.get(&quot;https://www.youtube.com&quot;)
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\api.py&quot;, line 73, in get
    return request(&quot;get&quot;, url, params=params, **kwargs)
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\api.py&quot;, line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\sessions.py&quot;, line 587, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\sessions.py&quot;, line 701, in send
    r = adapter.send(request, **kwargs)
  File &quot;C:\Users\BeBit\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\adapters.py&quot;, line 563, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='www.youtube.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol 
(_ssl.c:1129)')))
</code></pre>
<p>I'm using a vless configuration in v2rayN if this information can help.
can someone tell me what should I do?</p>
<p>&quot;I expected to get &lt;Response [200]&gt; but I encountered a lot of errors.&quot;</p>
",https://stackoverflow.com/questions/76022058/what-could-be-causing-sslerror-eof-occurred-in-violation-of-protocol-when-usi,False,
76022054,Multiple python version in one project,"<p>I have a project which is running by Python 3.10
I want to add new_script.py to my project (by import), but Problem is this new_script.py only works with Python 3.7</p>
<p>How can I handle this new_script.py to run by python 3.7 while Main.py is running by Python 3.10?</p>
<pre><code>Main.py (running with 3.10)
--- import new_script (should run with 3.7)
</code></pre>
<p>I searched a lot, but I didn't find any solution for running two version of python at same time in one project.</p>
",https://stackoverflow.com/questions/76022054/multiple-python-version-in-one-project,True,
76022035,How to solve the recursion error in my video gray scale converter,"<p>I am working on a desktop app which can convert a video to gray scale video using Kivy and OpenCV.</p>
<p>But after using the <code>pyinstaller</code> with this below command</p>
<pre><code>pyinstaller --name VideoCartoonizer --windowed --onefile main.py
</code></pre>
<p>I got an error showing</p>
<blockquote>
<p>Failed to execute script 'main' due to unhandled exception: maximum recursion depth exceeded</p>
</blockquote>
<p>Below is my code:</p>
<pre><code>import cv2

import numpy as np

from moviepy.editor import VideoFileClip

from moviepy.editor import VideoFileClip, concatenate_videoclips, VideoClip

from kivy.app import App

from kivy.uix.boxlayout import BoxLayout

from kivy.uix.label import Label

from kivy.uix.textinput import TextInput

from kivy.uix.button import Button

from kivy.uix.filechooser import FileChooserListView

from kivy.uix.popup import Popup

class VideoCartoonizer(App):

    def build(self):

        # create the GUI

        self.layout = BoxLayout(orientation='vertical')

        # select input file button

        self.select_input_button = Button(text='Select Input File', size_hint_y=0.1)

        self.select_input_button.bind(on_press=self.select_input_file)

        self.layout.add_widget(self.select_input_button)

        # input file path label

        self.layout.add_widget(Label(text='Or enter the input video file path:', size_hint_y=0.1))

        # input file path text input

        self.input_file = TextInput(size_hint_y=0.1)

        self.layout.add_widget(self.input_file)

        # convert button

        self.convert_button = Button(text='Convert to cartoon', size_hint_y=0.1)

        self.convert_button.bind(on_press=self.convert_video)

        self.layout.add_widget(self.convert_button)

        # output file path label

        self.layout.add_widget(Label(text='Output file path:', size_hint_y=0.1))

        # output file path text input

        self.output_file = TextInput(size_hint_y=0.1, readonly=True)

        self.layout.add_widget(self.output_file)

        return self.layout

    def select_input_file(self, *args):

        # create the file chooser

        file_chooser = FileChooserListView(filters=['*.mp4'])

        # create the popup

        popup = Popup(title='Select Input File', content=file_chooser, size_hint=(0.9, 0.9))




# set the callback for when a file is selected

        def on_selection(instance,selected_file):

            self.input_file.text = selected_file[0]

            popup.dismiss()

        file_chooser.bind(selection=on_selection)

        # open the popup

        popup.open()

    def convert_video(self, *args):

    # get the input file path

        input_file = self.input_file.text

    # define the video processing function

        def process_image(image):

        # resize the image while maintaining aspect ratio

            aspect_ratio = image.shape[1] / image.shape[0]

            target_width = 640

            target_height = int(target_width / aspect_ratio)

            image = cv2.resize(image, (target_width, target_height)) 

        # convert to grayscale

            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

            return gray

    # process the input video and save the output

        output_file = input_file.replace(&quot;.mp4&quot;, &quot;_cartoon.mp4&quot;)

        input_video = VideoFileClip(input_file)

    # create a queue to keep track of the frames that need to be processed

        frame_queue = list(input_video.iter_frames())

    # create an output video clip with the same parameters as the input video

        output_video = None

        for i, frame in enumerate(input_video.iter_frames()):

            if output_video is None:

                output_video = VideoClip(

                    lambda t: process_image(frame_queue.pop(0)),

                    duration=input_video.duration

            )

            else:

                output_video = concatenate_videoclips([

                    output_video,

                    VideoClip(

                        lambda t: process_image(frame_queue.pop(0)),

                        duration=input_video.duration

                    )

                ])

    # write the output video

        output_video.write_videofile(output_file, codec='mpeg4')
</code></pre>
<p>Error:</p>
<pre><code> File &quot;logging\__init__.py&quot;, line 1084, in emit

AttributeError: 'NoneType' object has no attribute 'write'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File &quot;logging\__init__.py&quot;, line 1084, in emit

AttributeError: 'NoneType' object has no attribute 'write'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File &quot;logging\__init__.py&quot;, line 1084, in emit

AttributeError: 'NoneType' object has no attribute 'write'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File &quot;logging\__init__.py&quot;, line 1084, in emit

AttributeError: 'NoneType' object has no attribute 'write'


File &quot;logging\__init__.py&quot;, line 1657, in callHandlers

  File &quot;logging\__init__.py&quot;, line 950, in handle

  File &quot;logging\__init__.py&quot;, line 1089, in emit

  File &quot;logging\__init__.py&quot;, line 1002, in handleError

  File &quot;kivy\logger.py&quot;, line 338, in write

  File &quot;logging\__init__.py&quot;, line 1454, in warning

  File &quot;logging\__init__.py&quot;, line 1585, in _log

  File &quot;logging\__init__.py&quot;, line 1595, in handle

  File &quot;logging\__init__.py&quot;, line 1657, in callHandlers

  File &quot;logging\__init__.py&quot;, line 950, in handle

  File &quot;logging\__init__.py&quot;, line 1089, in emit

  File &quot;logging\__init__.py&quot;, line 1002, in handleError

  File &quot;kivy\logger.py&quot;, line 338, in write

  File &quot;logging\__init__.py&quot;, line 1454, in warning

  File &quot;logging\__init__.py&quot;, line 1585, in _log

  File &quot;logging\__init__.py&quot;, line 1595, in handle

  File &quot;logging\__init__.py&quot;, line 1657, in callHandlers

  File &quot;logging\__init__.py&quot;, line 950, in handle

  File &quot;logging\__init__.py&quot;, line 1081, in emit

  File &quot;logging\__init__.py&quot;, line 925, in format

  File &quot;kivy\logger.py&quot;, line 291, in format

  File &quot;copy.py&quot;, line 172, in deepcopy

  File &quot;copy.py&quot;, line 270, in _reconstruct

  File &quot;copy.py&quot;, line 146, in deepcopy

  File &quot;copy.py&quot;, line 230, in _deepcopy_dict

  File &quot;copy.py&quot;, line 146, in deepcopy

  File &quot;copy.py&quot;, line 210, in _deepcopy_tuple

RecursionError: maximum recursion depth exceeded
</code></pre>
<p>Want to make my video gray scaling work as a exe file, as it is working fine in any text editor</p>
",https://stackoverflow.com/questions/76022035/how-to-solve-the-recursion-error-in-my-video-gray-scale-converter,True,
76022019,Why am I getting error all arrays must be of the same length?,"<p>I was hoping that this would print a six-bar broken bar chart with 5 colored categories from GUI entries, but I get an error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/lib/python3.10/idlelib/run.py&quot;, line 578, in runcode
    exec(code, self.locals)
  File &quot;/home/jerry/Python_Code/AA Python II Submissions/my_script.py&quot;, line 64, in &lt;module&gt;
    df = pd.DataFrame(data)
  File &quot;/home/jerry/.local/lib/python3.10/site-packages/pandas/core/frame.py&quot;, line 708, in __init__
    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)
  File &quot;/home/jerry/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py&quot;, line 481, in dict_to_mgr
    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)
  File &quot;/home/jerry/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py&quot;, line 115, in arrays_to_mgr
    index = _extract_index(arrays)
  File &quot;/home/jerry/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py&quot;, line 655, in _extract_index
    raise ValueError(&quot;All arrays must be of the same length&quot;)
ValueError: All arrays must be of the same length
</code></pre>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
import tkinter as tk

# Define the data as a dictionary
data = {
    'x_values': [],
    'y_values': [(6.0, 2), (8.5, 2), (12.5, 2), (15.0, 2), (19.0, 2), (21.5, 2)]
}

# Define a list of colors and categories for the bars
colors = ('tab:red', 'tab:orange', 'tab:purple', 'tab:blue', 'tab:green')
categories = ('Category 1', 'Category 2', 'Category 3', 'Category 4', 'Category 5')

# Create a GUI interface to get user input for x_values
def get_x_values():
    global data
    x_values = []
    for i in range(len(categories)):
        values_list = []
        for j in range(5):
            x, y = x_values_entry[i][j].get().split(',')
            values_list.append((int(x), int(y)))
        x_values.append(values_list)
    data['x_values'] = x_values
    root.destroy()

root = tk.Tk()

# Create a grid of input boxes for the x_values
x_values_entry = []
for i in range(len(categories)):
    label = tk.Label(root, text=categories[i])
    label.grid(row=i, column=0)
    entry1 = tk.Entry(root, width=10)
    entry1.grid(row=i, column=1)
    entry2 = tk.Entry(root, width=10)
    entry2.grid(row=i, column=2)
    entry3 = tk.Entry(root, width=10)
    entry3.grid(row=i, column=3)
    entry4 = tk.Entry(root, width=10)
    entry4.grid(row=i, column=4)
    entry5 = tk.Entry(root, width=10)
    entry5.grid(row=i, column=5)
    x_values_entry.append([entry1, entry2, entry3, entry4, entry5])

# Add a button to submit the x_values and close the GUI
submit_button = tk.Button(root, text='Submit', command=get_x_values)
submit_button.grid(row=len(categories), columnspan=6)

# Run the GUI
root.mainloop()

# Add the colors and categories to each row of the DataFrame
for i in range(len(data['x_values'])):
    data['facecolors'] = [colors] * len(data['x_values'])
    data['categories'] = [categories] * len(data['x_values'])

# Create a pandas DataFrame from the data
df = pd.DataFrame(data)

# Create a new figure and axis
fig, ax = plt.subplots(figsize=(10,6))

# Loop through each row of the DataFrame and plot the broken bar chart
for i, row in df.iterrows():
    ax.broken_barh(row['x_values'], row['y_values'], facecolors=row['facecolors'])

# Create legend entries with color rectangles and category labels
legend_entries = [Patch(facecolor=color, edgecolor='black', label=category) for color, category in zip(colors, categories)]

# Add the legend to the plot
ax.legend(handles=legend_entries, loc='upper right', ncol=5, bbox_to_anchor=(1.0, 1.00))


# Customize the axis labels and limits
ax.set_xlabel('Days')
ax.set_ylabel('Jobs')
ax.set_yticks([7, 9.5, 13.5, 16, 20, 22.5], labels=['#3-actual', '#3-budget',
                                                    '#2-actual', '#2-budget',
                                                    '#1-actual', '#1-budget'])
title = ax.set_title('Tasks and Crew-Days')
title.set_position([0.5, 1.0])               #set title at center
ax.set_ylim(5, 26)
ax.grid(True)

# Display the plot
plt.show()
</code></pre>
<p>Each GUI box should take two comma-separated integers for start and duration of that category. The numbers should get a little larger for each category, so that the colors don't overlap. Any help would be greatly appreciated.</p>
<p><a href=""https://i.stack.imgur.com/fBz5A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fBz5A.png"" alt=""Example:"" /></a></p>
",https://stackoverflow.com/questions/76022019/why-am-i-getting-error-all-arrays-must-be-of-the-same-length,False,
76021996,Efficiently post-changing the object of iteration,"<p>I have a <code>stack</code> - an array of arrays of <code>elements</code> (initially, of the form <code>[[element]]</code>), a <code>queue</code> - an array of <code>element</code> and a function <code>func</code> mapping pair <code>(element1, element2)</code> to <code>resulting_element</code>. I'm trying to process <code>stack</code> and <code>queue</code> as follows</p>
<pre><code>for new_element in queue:                                   #first loop
    stack_copy = [list(array) for array in stack]
    stack[0].append(new_element)
    for (index, array) in enumerate(stack_copy):            #second loop
        for element in array:                               #third loop
            resulting_element = func(element, new_element)
            try:
                stack[i+1].append(resulting_element)
            except:
                stack.append(list())
                stack[i+1].append(resulting_element)
</code></pre>
<p>So basically I want <code>new_element</code> to be inserted into <code>stack[0]</code> and<code>resulting_element</code> to be inserted into <code>stack[index+1]</code> (does not matter on which position) but only on next iteration of the first loop. The problem is that as we iterating inner loops, we obtain multiple <code>resulting_elements</code>. I tried that to create a temporary variable <code>stack_copy</code>, so that we iterated on &quot;fixed&quot; object but this seems to be very inefficient. I'm looking for the way to make this procedure work faster. I was thinking to some way to rewrite stack as a generator, which could be modified while it being iterated.
Essentially, the content of two inner loops should remain unchanged.</p>
",https://stackoverflow.com/questions/76021996/efficiently-post-changing-the-object-of-iteration,False,
76021971,Simple discord.py mp3 bot not playing music,"<p>Discord bot dosent play anykind of music, neither mine nor other bots codes work he just dosent play music. idk maybe i am just stupid BUT IT DOSENT WORK WHY</p>
<pre><code>import discord
import time
import os
from discord.ext import commands

BOT_TOKEN = &quot;&quot; # put token here

intentss = discord.Intents.default()
intentss.message_content = True
intentss.voice_states = True

bot = commands.Bot(command_prefix=&quot;;;&quot;, intents = intentss)

OPUS_LIBS = ['libopus-0.x86.dll', 'libopus-0.x64.dll', 'libopus-0.dll', 'libopus.so.0', 'libopus.0.dylib']

@bot.command()
async def join(ctx):
    channelVC = ctx.author.voice.channel
    await channelVC.connect()

@bot.command()
async def leave(ctx):
    await ctx.voice_client.disconnect()

@bot.command()
async def play(ctx):
    voice = ctx.guild.voice_client
    mloc = 'C:/Users/Lukas/Desktop/Bot Bethoven/Youtube/test.mp3'
    voice.play(discord.FFmpegPCMAudio(executable = &quot;C:/ffmpeg/bin/ffmpeg.exe&quot;, source = mloc))


bot.run(BOT_TOKEN)
</code></pre>
<p>it just give error codes:</p>
<pre><code>discord.ext.commands.errors.CommandInvokeError: Command raised an exception: ClientException: Not connected to voice.
INFO     discord.player ffmpeg process 22896 has not terminated. Waiting to terminate...
INFO     discord.player ffmpeg process 22896 should have terminated with a return code of 1.
</code></pre>
<p>intresting thing when i put join and play in the same command it dosent show not connected to voice error, actually it dosent show anything and neither plays mp3 file</p>
<p>i tryed reinstalling: PyNaCl, FFmpeg, Visual Code, updating python.
nothing helps. this problem started after i did 1 month break from coding bot, before that it worked fine.</p>
<p>i'm thinking problem is something with my pc, maybe path or something dosent work(because it worked month ago and i didnt do anything to code), i tryed checking but everything seems normal,</p>
",https://stackoverflow.com/questions/76021971/simple-discord-py-mp3-bot-not-playing-music,True,76024867
76021962,Why is the loss error increasing? Error backpropagation algorithm,"<p>I implemented it according to the formula of error backpropagation method. Is there any problem</p>
<pre><code>import numpy as np
# 定义神经网络的参数
# 输入层3个，隐藏层4个，输出层1个
W2 = np.random.randn(4, 3)  # 第二层权重矩阵
B2 = np.random.randn(4, 1)  # 第二层的偏置
W3 = np.random.randn(1, 4)  # 第三层权重矩阵
B3 = np.random.randn(1, 1)


# 定义激活函数sigmoid函数
def sigmoid(X):
    return 1 / (1 + np.exp(-X))


def sigmoid_derivative(X):
    return sigmoid(X) * (1 - sigmoid(X))


# 定义神经网络的前向传播函数
def forward(X):
    # 第2层
    Z2 = np.dot(W2, X) + np.tile(B2, (1, X.shape[1]))
    A2 = sigmoid(Z2)
    # 第3层
    Z3 = np.dot(W3, A2) + np.tile(B3, (1, X.shape[1]))
    A3 = sigmoid(Z3)
    return A3


# 定义损失值
def loss(y, y_hat):
    m = len(y)
    return np.sum((y - y_hat) ** 2) / (2 * m)


# 定义损失函数的偏导数
def loss_derivative(y, y_hat):
    return y - y_hat


# 定义反向传播函数
def backward(X, y, y_hat):
    Z2 = np.dot(W2, X)  # m * 100
    A2 = sigmoid(Z2)  # m * 100
    Z3 = np.dot(W3, A2)  # k * 100

    # 第3层, J(a)*g(z3)
    delta3 = (y - y_hat) * sigmoid_derivative(Z3)  # k * 100
    # 第2层, (δ2=(W3.T)*δ3)**g(z2)
    delta2 = np.dot(W3.T, delta3) * sigmoid_derivative(Z2)  # m * 100

    # 第3层, dw3=δ3*(a2.T), db3=δ3
    dW3 = np.dot(delta3, A2.T)
    db3 = np.sum(delta3, axis=1, keepdims=True)

    # 第2层, dw2=δ2*(a1.T), db2=δ2
    dW2 = np.dot(delta2, X.T)
    db2 = np.sum(delta2, axis=1, keepdims=True)

    return dW3, dW2, db3, db2


# 定义sigmoid函数的导数
def sigmoid_grad(x):
    return sigmoid(x) * (1 - sigmoid(x))


# 训练数据
X = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
y = np.array([[0], [0], [1], [1], [1]])

# 定义学习率
learning_rate = 0.1

# 训练神经网络
iteratorNum = 10 * 100
J_history = np.zeros(iteratorNum)
for i in range(iteratorNum):
    # 前向传播
    y_hat = forward(X.T)
    # 计算损失函数
    J_history[i] = loss(y, y_hat)
    # 反向传播
    dW3, dW2, dB3, dB2 = backward(X.T, y.T, y_hat)
    # 更新参数
    W3 -= learning_rate * dW3
    B3 -= learning_rate * dB3
    W2 -= learning_rate * dW2
    B2 -= learning_rate * dB2

print(J_history)
case1 = np.array([[0, 0, 0], [1, 1, 0]])
print(forward(case1.T))

</code></pre>
<p>the end loss is 0.99999444. the error loss because biger. what is the reason</p>
<p>[0.99999401 0.99999403 0.99999404 0.99999406 0.99999408 0.99999409
0.99999411 0.99999413 0.99999414 0.99999416 0.99999417 0.99999419
0.99999421 0.99999422 0.99999424 0.99999425 0.99999427 0.99999428
0.9999943  0.99999432 0.99999433 0.99999435 0.99999436 0.99999438
0.99999439 0.99999441 0.99999442 0.99999444]</p>
<p>the res is [[0.9999975  0.99999866]]</p>
",https://stackoverflow.com/questions/76021962/why-is-the-loss-error-increasing-error-backpropagation-algorithm,False,
76021933,Python Recursion Limit (infinite loop),"<p>asyncio&amp;aiohttp webrequest while true loop</p>
<blockquote>
<p>RecursionError: maximum recursion depth exceeded while calling a Python object</p>
</blockquote>
<p>Code working but after 4,5 minutes recursion limit error</p>
<pre><code>import aiohttp
import asyncio

async def main():
  async with aiohttp.ClientSession() as resp:
      async with resp.patch(f&quot;https://mywebsite.com&quot;) as resp:
        if resp.status == 200:
          exit()
        else:
          print(f' &gt; {resp.status} ')
          await main()


while True:
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
</code></pre>
<p>How to disable recursion limit or how to make really infinite loop??</p>
",https://stackoverflow.com/questions/76021933/python-recursion-limit-infinite-loop,False,
76021929,How can I display photos stored on Amazon S3 with Subquery code of Django?,"<p>I deployed a Web App made by Django to AWS and stored media files to Amazon S3. But when I use Subquery code I wrote below, I can not get the media files and correct urls of Amazon S3.</p>
<p>What should I change my code?</p>
<pre><code>user = get_object_or_404(MyUser, pk=5)
subqs = ItemPhoto.objects.filter(item=OuterRef('item')).filter(priority=1) 
queryset = Order.objects.prefetch_related(
    Prefetch(
        'orderdetail_orderid', 
        queryset=OrderDetail.objects.select_related('order', 'item').annotate(
                item_photos=Subquery(subqs.values('photo')),
        ),
        to_attr='oor',
    ),
).filter(user=user).order_by('-created_at')

photos1 = [item.item_photos for order in queryset for item in order.oor]
#[sh/Gold Ship/76e91e2d048147c9939d099224e09554.jpg, sh/Epiphaneia/9476dcb430754049b16c62949ecd3839.jpg]

photos2 = [item.item_photos.photo for order in queryset for item in order.oor]
# Traceback (most recent call last):
#   File &quot;&lt;console&gt;&quot;, line 1, in &lt;module&gt;
#   File &quot;&lt;console&gt;&quot;, line 1, in &lt;listcomp&gt;
# AttributeError: 'str' object has no attribute 'photo'

all_photos = ItemPhoto.objects.all()
photos3 = [p.photo for p in all_photos]
#[&lt;ImageFieldFile: sh/Gold Ship/76e91e2d048147c9939d099224e09554.jpg&gt;, &lt;ImageFieldFile: sh/Epiphaneia/9476dcb430754049b16c62949ecd3839.jpg&gt;]

photos4 = [p.photo.url for p in all_photos] # templates can display photos with these urls
#['https://xxxxx.s3.xxxxx.amazonaws.com/media/sh/Gold%20Ship/76e91e2d048147c9939d099224e09554.jpg, 'https://xxxxx.s3.xxxxx.amazonaws.com/media/sh/Epiphaneia/9476dcb430754049b16c62949ecd3839.jpg']
</code></pre>
<p>template.html</p>
<pre><code>{% for order in orders %}
    {% for item in order.oor %}
        &lt;img src=&quot;{{ item.item_photos }}&quot;&gt;
        # src=&quot;sh/Gold Ship/76e91e2d048147c9939d099224e09554.jpg&quot; &lt;- I can not get any photo with this type of url

        &lt;img src=&quot;{{ item.item_photos.photo }}&quot;&gt;
        # src=&quot;&quot; &lt;- None

        &lt;img src=&quot;{{ item.item_photos.photo.url }}&quot;&gt;
        # src=&quot;&quot; &lt;- None

    {% endfor %}
{% endfor %}
</code></pre>
<p>models.py</p>
<pre><code>def get_itemimage_path(instance, filename): 
    prefix = 'sh{}{}{}'.format(os.sep, instance.item, os.sep) 
    name = str(uuid.uuid4()).replace('-', '')
    extension = filename.split('.')[-1]
    return '{}{}.{}'.format(prefix, name, extension)

class ItemPhoto(models.Model): 
    item = models.ForeignKey(Item, on_delete=models.CASCADE,
        related_name=&quot;item_photo&quot;, verbose_name=&quot;item_id&quot;,)
    photo = models.ImageField(&quot;item_photo&quot;, blank=True, 
        upload_to=get_itemimage_path)
    priority = models.IntegerField(&quot;priority&quot;,)
</code></pre>
<p>settings.py</p>
<pre><code>MEDIA_ROOT = os.path.join(BASE_DIR, &quot;media&quot;)
if USING_AWS:
    MEDIA_URL = &quot;https://%s/media/&quot; % (AWS_S3_CUSTOM_DOMAIN)
else:
    MEDIA_URL = &quot;/media/&quot;
DEFAULT_FILE_STORAGE = &quot;storages.backends.s3boto3.S3Boto3Storage&quot;
</code></pre>
<p>Python 3
Django 4.1</p>
",https://stackoverflow.com/questions/76021929/how-can-i-display-photos-stored-on-amazon-s3-with-subquery-code-of-django,False,
76021927,Python Discord bot for Midjourney,"<p>I found a Py <a href=""https://medium.com/@neonforge/how-to-automate-midjourney-image-generation-with-python-and-gui-automation-ac9ca5f747ae"" rel=""nofollow noreferrer"">Script on a Site</a>, but it's not workin always.
The error suggests that the bot is freezing somewhere but I'm not sure why.
<a href=""https://medium.com/@neonforge/how-to-create-a-discord-bot-to-download-midjourney-images-automatically-python-step-by-step-guide-3e76d3282871"" rel=""nofollow noreferrer"">The Download Script</a> is not working aswell. It's not completly broken but only picks up a few images. No Errors, it just stops saving the images to the output folder.</p>
<p>the error</p>
<pre><code>WARNING  discord.gateway Shard ID None heartbeat blocked for more than 60 seconds.
Loop thread traceback (most recent call last):
  File &quot;D:\dc_bot\main.py&quot;, line 94, in &lt;module&gt;
    client.run(discord_token)
  File &quot;C:\Users\arko7\AppData\Local\Programs\Python\Python311\Lib\site-packages\discord\client.py&quot;, line 860, in run
    asyncio.run(runner())
  File &quot;C:\Users\arko7\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py&quot;, line 190, in run
    return runner.run(main)
  File &quot;C:\Users\arko7\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py&quot;, line 118, in run
    return self._loop.run_until_complete(task)
  File &quot;C:\Users\arko7\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py&quot;, line 640, in run_until_complete
    self.run_forever()
  File &quot;C:\Users\arko7\AppData\Local\Programs\Python\Python311\Lib\asyncio\windows_events.py&quot;, line 321, in run_forever
    super().run_forever()
  File &quot;C:\Users\arko7\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py&quot;, line 607, in run_forever
    self._run_once()
  File &quot;C:\Users\arko7\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py&quot;, line 1922, in _run_once
    handle._run()
  File &quot;C:\Users\arko7\AppData\Local\Programs\Python\Python311\Lib\asyncio\events.py&quot;, line 80, in _run
    self._context.run(self._callback, *self._args)
  File &quot;C:\Users\arko7\AppData\Local\Programs\Python\Python311\Lib\site-packages\discord\client.py&quot;, line 441, in _run_event
    await coro(*args, **kwargs)
  File &quot;D:\dc_bot\main.py&quot;, line 63, in on_message
    while prompt_counter &lt; len(prompts):
</code></pre>
<p>The original Code by Michael King</p>
<pre><code>import time
import discord
from discord.ext import commands
from dotenv import load_dotenv
import pyautogui as pg


discord_token = &quot;YOUR_DISCORD_TOKEN&quot;

# Using readlines()
prompt_file = open('prompts.txt', 'r')
prompts = prompt_file.readlines()

prompt_counter = 0

load_dotenv()
client = commands.Bot(command_prefix=&quot;*&quot;, intents=discord.Intents.all())


@client.event
async def on_ready():
    print(&quot;Bot connected&quot;)

@client.event
async def on_message(message):
    global prompt_counter

    msg = message.content
    print(message)

    while prompt_counter &lt; len(prompts):
        # Start Automation by typing &quot;automation&quot; in the discord channel
        if msg == 'automation':
            time.sleep(3)
            pg.press('tab')
            for i in range(1):
                time.sleep(3)
                pg.write('/imagine')
                time.sleep(5)
                pg.press('tab')
                pg.write(prompts[prompt_counter])
                time.sleep(3)
                pg.press('enter')
                time.sleep(5)
                prompt_counter += 1

        # continue Automation as soon Midjourney bot sends a message with attachment.
        for attachment in message.attachments:
            time.sleep(3)
            pg.write('/imagine')
            time.sleep(5)
            pg.press('tab')
            pg.write(prompts[prompt_counter])
            time.sleep(3)
            pg.press('enter')
            time.sleep(5)
            prompt_counter += 1

    # Stop Automation once all prompts are completed
    quit()

client.run(discord_token)
</code></pre>
<p>I tried to implement some changes with chatgpt but it did not work, here is the modified code:</p>
<pre><code>from promts.animals import write_prompts_to_file_animals
from promts.rooms import write_prompts_to_file_rooms
from promts.country_architecture import write_prompts_to_file_country_architecture

import time
import discord
from discord.ext import commands
from dotenv import load_dotenv
import pyautogui as pg
import os
from dotenv import load_dotenv
import asyncio

load_dotenv()
discord_token = os.getenv('DISCORD_TOKEN')

n_promts = 3

# Write the promts in the promts.txt


file_path = os.path.join(os.getcwd(), 'prompts.txt')
with open(file_path, 'w') as f:
    f.write('')
    print(&quot;Prompts deleted. Starting fresh!&quot;)

#write_prompts_to_file_animals(n_promts)
#write_prompts_to_file_rooms(n_promts)
write_prompts_to_file_country_architecture(n_promts)


# Using readlines()
prompt_file = open('prompts.txt', 'r')
# 'promts/radom_animals_niji', 'promts/radom_animals', 'promts/ramdom_rooms', 'promts/random_country_architecture'
prompts = prompt_file.readlines()

prompt_counter = 0


load_dotenv()
client = commands.Bot(command_prefix=&quot;*&quot;, intents=discord.Intents.all())


@client.event
async def on_ready():
    print(&quot;Bot connected&quot;)


@client.event
async def on_message(message):
    global prompt_counter

    msg = message.content
    print(message)

    while prompt_counter &lt; len(prompts):
        # code for automation
        prompt_counter = 0  # reset prompt counter after all prompts have been completed

        # Start Automation by typing &quot;automation&quot; in the discord channel
        if msg == 'automation':
            await asyncio.sleep(3)  # wait for 3 seconds without blocking the event loop
            pg.press('tab')
            for i in range(1):
                await asyncio.sleep(3)
                pg.write('/imagine')
                await asyncio.sleep(5)
                pg.press('tab')
                pg.write(prompts[prompt_counter])
                await asyncio.sleep(3)
                pg.press('enter')
                await asyncio.sleep(5)
                prompt_counter += 1

        # continue Automation as soon Midjourney bot sends a message with attachment.
        for attachment in message.attachments:
            await asyncio.sleep(3)
            pg.write('/imagine')
            await asyncio.sleep(5)
            pg.press('tab')
            pg.write(prompts[prompt_counter])
            await asyncio.sleep(3)
            pg.press('enter')
            await asyncio.sleep(5)
            prompt_counter += 1

    # Stop Automation once all prompts are completed
    return()

client.run(discord_token)
</code></pre>
<p>the download script to download the generated images</p>
<pre><code>import discord
from discord.ext import commands
import requests
from dotenv import load_dotenv
from PIL import Image
import os

discord_token = &quot;PASTE_YOUR_DISCORD_BOT_TOKEN&quot;

load_dotenv()
client = commands.Bot(command_prefix=&quot;*&quot;, intents=discord.Intents.all())
directory = os.getcwd()
print(directory)

def split_image(image_file):
    with Image.open(image_file) as im:
        # Get the width and height of the original image
        width, height = im.size
        # Calculate the middle points along the horizontal and vertical axes
        mid_x = width // 2
        mid_y = height // 2
        # Split the image into four equal parts
        top_left = im.crop((0, 0, mid_x, mid_y))
        top_right = im.crop((mid_x, 0, width, mid_y))
        bottom_left = im.crop((0, mid_y, mid_x, height))
        bottom_right = im.crop((mid_x, mid_y, width, height))
        return top_left, top_right, bottom_left, bottom_right

async def download_image(url, filename):
    response = requests.get(url)
    if response.status_code == 200:

        # Define the input and output folder paths
        input_folder = &quot;input&quot;
        output_folder = &quot;output&quot;

        # Check if the output folder exists, and create it if necessary
        if not os.path.exists(output_folder):
            os.makedirs(output_folder)

        # Check if the input folder exists, and create it if necessary
        if not os.path.exists(input_folder):
            os.makedirs(input_folder)
        with open(f&quot;{directory}/{input_folder}/{filename}&quot;, &quot;wb&quot;) as f:
            f.write(response.content)
        print(f&quot;Image downloaded: {filename}&quot;)
        input_file = os.path.join(input_folder, filename)

        if &quot;UPSCALED_&quot; not in filename:
            file_prefix = os.path.splitext(filename)[0]
            # Split the image
            top_left, top_right, bottom_left, bottom_right = split_image(input_file)
            # Save the output images with dynamic names in the output folder
            top_left.save(os.path.join(output_folder, file_prefix + &quot;_top_left.jpg&quot;))
            top_right.save(os.path.join(output_folder, file_prefix + &quot;_top_right.jpg&quot;))
            bottom_left.save(os.path.join(output_folder, file_prefix + &quot;_bottom_left.jpg&quot;))
            bottom_right.save(os.path.join(output_folder, file_prefix + &quot;_bottom_right.jpg&quot;))
        else:
            os.rename(f&quot;{directory}/{input_folder}/{filename}&quot;, f&quot;{directory}/{output_folder}/{filename}&quot;)
        # Delete the input file
        os.remove(f&quot;{directory}/{input_folder}/{filename}&quot;)

@client.event
async def on_ready():
    print(&quot;Bot connected&quot;)

@client.event
async def on_message(message):
    print(message.content)
    for attachment in message.attachments:
        if &quot;Upscaled by&quot; in message.content:
            file_prefix = 'UPSCALED_'
        else:
            file_prefix = ''
        if attachment.filename.lower().endswith((&quot;.png&quot;, &quot;.jpg&quot;, &quot;.jpeg&quot;, &quot;.gif&quot;)):
            await download_image(attachment.url, f&quot;{file_prefix}{attachment.filename}&quot;)


    # use Discord message to download images from a channel history, example: &quot;history:50&quot;
    if message.content.startswith(&quot;history:&quot;):
        download_qty = int(message.content.split(&quot;:&quot;)[1])
        channel = message.channel
        async for msg in channel.history(limit=download_qty):
            for attachment in msg.attachments:
                if &quot;Upscaled by&quot; in message.content:
                    file_prefix = 'UPSCALED_'
                else:
                    file_prefix = ''
                if attachment.filename.lower().endswith((&quot;.png&quot;, &quot;.jpg&quot;, &quot;.jpeg&quot;, &quot;.gif&quot;)):
                    try:
                        await download_image(attachment.url, f&quot;{file_prefix}{attachment.filename}&quot;)
                    except:
                        time.sleep(10)
                        continue

client.run(discord_token)
</code></pre>
",https://stackoverflow.com/questions/76021927/python-discord-bot-for-midjourney,False,
76021918,Python linter with variables in conditional code block,"<p>Consider the following code:</p>
<pre><code>testvar: bool = True
if testvar:
    foo: str = ''

# Statements that do not affect value of testvar

if testvar:
    bar: str = foo
</code></pre>
<p>Any linter I've tried will complain that <code>foo</code> is possibly unbound, although it obviously must be bound. Is this by design or omission, or am I wrong and it somehow could be unbound?</p>
<p>Also, what is the recommended way to avoid the warning?</p>
",https://stackoverflow.com/questions/76021918/python-linter-with-variables-in-conditional-code-block,False,
76021914,how to commit a pandas dataframe to JSON/utf-8 back to gitlab so it&#39;s read as a CSV,"<p>I can successfully read in a CSV like this using pandas and python-gitlab:</p>
<pre><code>    filename = &quot;file.csv&quot;
    f = project.files.get(file_path=filename, ref='master')
    data = pd.read_csv(StringIO(str(f.decode(),'utf-8')), sep=',', header=None, names=[&quot;direction&quot;, &quot;width&quot;, &quot;height&quot;])
</code></pre>
<p>but I can't get the right structure back into json that is then read as a CSV by gitlab.</p>
<p>an example csv on gitlab file looks like this so you can see the structure</p>
<pre><code>import csv, requests
from io import StringIO

url = &quot;https://gitlab.com/datasets_a/mpg-data/-/raw/master/mpg.csv&quot;
# Reading URL data
s = requests.get(url).content
# Decoding &amp; formatting
data=csv.reader(StringIO(s.decode('utf-8')))
#Printing the first 5 rows
for row in list(data)[:5]:
    print(row)
</code></pre>
<p>and I can't just seem to convert the dataframe to json - here is my attempt!</p>
<pre><code>def make_payload_commit(project, Time, filename, data):
        # payload
        data = {
                'branch': 'main',
                'commit_message': Time,
                'actions': [
                {
                        'action': 'update',
                        'file_path': filename,
                        'content': dataframe.to_json(),
                }
                ]
        }
        commit = project.commits.create(data)
</code></pre>
<p>gitlab doesn't register this as a CSV.</p>
<p>Would appreciate any help, if anyone knows how to do this!</p>
",https://stackoverflow.com/questions/76021914/how-to-commit-a-pandas-dataframe-to-json-utf-8-back-to-gitlab-so-its-read-as-a,True,76022902
76021906,How to call GDALDEMProcessing from the GDAL python wrapper?,"<p>I'm trying to calculate aspect and hillshade in python using GDAL. I have installed GDAL and GDAL.Native packages via NuGet but I could not find the GDALDEMProcessing calls (<a href=""https://gdal.org/programs/gdaldem.html"" rel=""nofollow noreferrer"">https://gdal.org/programs/gdaldem.html</a>) for calculating aspect and hillshade of DEM file.</p>
<p>How to calculate aspect and hillshade in python using GDAL?</p>
",https://stackoverflow.com/questions/76021906/how-to-call-gdaldemprocessing-from-the-gdal-python-wrapper,False,
76021892,Read each csv file with filename and store it in Redshift table using AWS Glue job,"<p>This code is giving a path error. I am trying to read the filename of each file present in an s3 bucket and then:</p>
<ul>
<li>Loop through these files using the list of filenames</li>
<li>Read each file and match the column counts with a target table present in Redshift</li>
<li>If the column counts match then load the table. If not, go in exception.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import sys
import logging
from datetime import datetime,timedelta,date
from pyspark.sql.functions import input_file_name
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
import boto3
import pandas as pd
import json
from botocore.exceptions import ClientError

if __name__ == &quot;__main__&quot;:
   
    args = getResolvedOptions(sys.argv, ['TempDir','JOB_NAME','secret_name'])
    secret_name = args['secret_name']
    temp_dir = args['TempDir']
   
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
   
    # getting redshift credentials
    session = boto3.session.Session()
    client = session.client(
        service_name='secretsmanager',
        region_name='us-east-1'
    )
     
    try:
        get_secret_value_response = client.get_secret_value(
            SecretId=secret_name
        )
    except ClientError as e:
        print(&quot;catch error:&quot;,e)
    else:
        # Decrypts secret using the associated KMS CMK.
        # Depending on whether the secret is a string or binary, one of these fields will be populated.
        if 'SecretString' in get_secret_value_response:
            secret = json.loads(get_secret_value_response['SecretString'])
           
     # Username and password
    redshiftUserID = secret.get('username')
    redshiftPassword = secret.get('password')
   
    print(&quot;STARTED &quot; + str(datetime.now()))
   
    #jdbc url for connection;
    jdbc_url = &quot;jdbc:redshift://rcm-analytics-dev2.cvxngrljq9pn.us-east-1.redshift.amazonaws.com:5439/rcmanalyticsdev2&quot;
   
# Read CSV Files
Client_files = glueContext.create_dynamic_frame.from_options(
    connection_type=&quot;s3&quot;,
    format=&quot;csv&quot;,
    connection_options={
        &quot;paths&quot;: [&quot;s3://rcma-chc-dev-analytics-cloud/merge/input&quot;],'recurse':True
    },
    format_options={'withHeader': True},transformation_ctx = &quot;Client_files&quot;
)

print(&quot;S3 FILES ARE READ &quot; + str(datetime.now()))
#extracting all the file names:
file_names=Client_files.toDF().withColumn(&quot;Filename&quot;,input_file_name()).select(&quot;Filename&quot;).distinct()

print(&quot;all files are:&quot;,file_names.show())
#MAKE LIST OF FILENAMES:
file_list=list(file_names)
#print EACH FILES FROM THE LIST:
for x in range(len(file_list)):
    print(file_list[x])

#PERFORM ETL JOB ON EACH FILE;
    
for obj in file_list:
    #each_clientfiles=spark.read.format(&quot;csv&quot;).option(&quot;header&quot;,&quot;True&quot;).option(&quot;path&quot;,&quot;obj&quot;).load()
    #each_clientfiles=each_clientfiles.toDF()
    
    each_clientfiles=glueContext.create_dynamic_frame.from_options(
    connection_type=&quot;s3&quot;,
    format=&quot;csv&quot;,
    connection_options={
        &quot;paths&quot;: [&quot;obj&quot;],'recurse':True
    },
    format_options={'withHeader': True},transformation_ctx = &quot;each_clientfiles&quot;
    )
    
# Convert the DataFrame back to DynamicFrame
    #dynamicframe1 = DynamicFrame.fromDF(each_clientfiles, glueContext, &quot;dynamicframe1&quot;)
#mapping files
    applymapping1 = ApplyMapping.apply(frame = each_clientfiles, mappings = [(&quot;org_code&quot;, &quot;string&quot;, &quot;org_code&quot;, &quot;string&quot;),(&quot;facility_code&quot;, &quot;string&quot;, &quot;facility_code&quot;, &quot;string&quot;),(&quot;facility&quot;,&quot;string&quot;,&quot;facility&quot;,&quot;string&quot;),(&quot;review_uuid&quot;, &quot;string&quot;, &quot;review_uuid&quot;, &quot;string&quot;),(&quot;create_date&quot;, &quot;timestamp&quot;, &quot;create_date&quot;, &quot;timestamp&quot;),(&quot;create_user&quot;, &quot;string&quot;, &quot;create_user&quot;, &quot;string&quot;),(&quot;presentation_time&quot;, &quot;timestamp&quot;, &quot;presentation_time&quot;, &quot;timestamp&quot;), (&quot;patient_ID&quot;, &quot;string&quot;, &quot;patient_ID&quot;, &quot;string&quot;),(&quot;encounter_ID&quot;, &quot;string&quot;, &quot;encounter_ID&quot;, &quot;string&quot;),(&quot;subset_id&quot;, &quot;string&quot;, &quot;subset_id&quot;, &quot;string&quot;),(&quot;subset_description&quot;, &quot;string&quot;, &quot;subset_description&quot;, &quot;string&quot;),(&quot;subset_data_version&quot;, &quot;string&quot;, &quot;subset_data_version&quot;, &quot;string&quot;),(&quot;episode_day&quot;,&quot;string&quot;,&quot;episode_day&quot;,&quot;string&quot;),(&quot;review_status_last&quot;, &quot;string&quot;, &quot;review_status_last&quot;, &quot;string&quot;), (&quot;phase_last&quot;, &quot;string&quot;, &quot;phase_last&quot;, &quot;string&quot;),(&quot;criteria_status_first&quot;, &quot;string&quot;, &quot;criteria_status_first&quot;, &quot;string&quot;),(&quot;criteria_status_last&quot;, &quot;string&quot;, &quot;criteria_status_last&quot;, &quot;string&quot;),(&quot;cp_count_checked_first&quot;, &quot;int&quot;, &quot;cp_count_checked_first&quot;, &quot;int&quot;),(&quot;cp_count_checked_last&quot;, &quot;int&quot;, &quot;cp_count_checked_last&quot;, &quot;int&quot;),(&quot;cp_count_w_clin_data&quot;, &quot;int&quot;, &quot;cp_count_w_clin_data&quot;, &quot;int&quot;), (&quot;cp_id&quot;, &quot;string&quot;, &quot;cp_id&quot;, &quot;string&quot;),(&quot;choice_first&quot;, &quot;string&quot;, &quot;choice_first&quot;, &quot;string&quot;),(&quot;choice_last&quot;, &quot;string&quot;, &quot;choice_last&quot;, &quot;string&quot;),(&quot;source&quot;, &quot;string&quot;, &quot;source&quot;, &quot;string&quot;),(&quot;source_seq&quot;, &quot;int&quot;, &quot;source_seq&quot;, &quot;int&quot;),(&quot;group_seq&quot;, &quot;int&quot;, &quot;group_seq&quot;, &quot;int&quot;),(&quot;data_group&quot;, &quot;string&quot;, &quot;data_group&quot;, &quot;string&quot;),(&quot;description&quot;, &quot;string&quot;, &quot;description&quot;, &quot;string&quot;), (&quot;type&quot;, &quot;string&quot;, &quot;type&quot;, &quot;string&quot;),(&quot;value&quot;, &quot;string&quot;, &quot;value&quot;, &quot;string&quot;)], transformation_ctx = &quot;applymapping1&quot;)

         #JDBC CONNECTION INPUTS
    conn_options = {
             &quot;url&quot;: &quot;jdbc:redshift://rcm-analytics-dev2.cvxngrljq9pn.us-east-1.redshift.amazonaws.com:5439/rcmanalyticsdev2&quot;,
             &quot;database&quot;:&quot;rcmanalyticsdev2&quot;,
             &quot;dbtable&quot;: &quot;ODS_EPREMIS.CSV_MERGE_INTERQUAL&quot;,
             &quot;user&quot;: redshiftUserID,
             &quot;password&quot;: redshiftPassword,
             &quot;redshiftTmpDir&quot;: args[&quot;TempDir&quot;]
             }
             
    target_frame = glueContext.create_dynamic_frame_from_options(&quot;redshift&quot;, conn_options)

    frame = each_clientfiles.toDF()
    try:
        frame= frame.toDF(*[field.name for field in target_frame.schema().fields]) # note, number of columns must match!
        applymapping1 = DynamicFrame.fromDF(frame, glueContext, &quot;final&quot;)

#DATAFRAME INSERTION IN THE REDSHIFT TABLE
        redshift_write = glueContext.write_dynamic_frame.from_jdbc_conf(
            frame=applymapping1,
            catalog_connection = &quot;acuity-redshift&quot;,
            redshift_tmp_dir =&quot;s3://rcma-chc-dev-analytics-cloud/merge/temp/*&quot;,
            connection_options=conn_options,transformation_ctx = &quot;redshift_write&quot;
         )
    except Exception as e:
        print(&quot;Caught Exception&quot;,e)
        #print(&quot;Print file name:&quot;,obj)
    finally:    
        print(&quot;Glue job finished &quot;, str(datetime.now()))
job.commit()
</code></pre>
",https://stackoverflow.com/questions/76021892/read-each-csv-file-with-filename-and-store-it-in-redshift-table-using-aws-glue-j,False,
76021878,is there a method to resize a python framebuf?,"<p>I am trying to resize a python framebuf - upscale it. Is this possible?</p>
<p>I tried looking for answers online but couldn't find it.</p>
",https://stackoverflow.com/questions/76021878/is-there-a-method-to-resize-a-python-framebuf,False,
76021870,"this is a python telegram bot code, it should do a poll, like the one in the photo","<p>this is the code of a telegram bot in python, it should make polls, as in the photo, and should not be anonymous when someone wrote the command '/calling&lt;here time{20:00 or 16:00, etc.}&gt;',<a href=""https://i.stack.imgur.com/veBPs.jpg"" rel=""nofollow noreferrer"">enter image description here</a>c.}&gt;'</p>
<pre><code>import telebot
import config
import time
import math
from datetime import datetime
from aiogram import Bot, Dispatcher , executor, types


bot = telebot.TeleBot(config.TOKEN)

@bot.message_handler(commands=[&quot;сalling&quot;])
def сalling(message):
    mess = f&quot;&lt;b&gt;{&quot;message&quot;}&lt;/b&gt;&quot;
    bot.send_message(message.chat.id,mess,parse_mode=&quot;html&quot;)

bot.polling(none_stop = True)
</code></pre>
",https://stackoverflow.com/questions/76021870/this-is-a-python-telegram-bot-code-it-should-do-a-poll-like-the-one-in-the-pho,False,
76021859,showing identation fault even though it is idented,"<pre><code>def gen_frames():

cap = cv2.VideoCapture(0)

while True:
    success, frame = cap.read()

    if not success:
        break

    ret, buffer = cv2.imencode('.jpg', frame)

    frame = buffer.tobytes()

    yield (b'--frame\r\n'
           b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n')

    if request.method == 'POST':
    msg = request.POST.get('msg')
    if msg:
        save_path = os.path.join(save_folder, &quot;captured_image.jpg&quot;)
        cv2.imwrite(save_path, frame)
        print(request.headers)
        return render(request,&quot;account/captured.html&quot;,{})


cap.release()
</code></pre>
<p>It keeps on saying TabError: inconsistent use of tabs and spaces in indentation</p>
<p>The identation fault is at if request.method == 'POST':</p>
",https://stackoverflow.com/questions/76021859/showing-identation-fault-even-though-it-is-idented,False,
76021857,403 error when scraping a URL that works on Firefox without cookies nor javascript,"<p>I have a URL that works on Firefox set to block all cookies and with JavaScript turned off, and yet when I scrape it on Python with <code>urllib</code>, I get <code>HTTP Error 403: Forbidden</code>. I use the same user-agent as Firefox, and here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>import urllib
import urllib.request

USER_AGENT_KEY = &quot;User-Agent&quot;
USER_AGENT_VALUE = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/111.0'

def get_page(url)
    req = urllib.request.Request(url)
    req.add_header(USER_AGENT_KEY, USER_AGENT_VALUE)

    # Empty SSL context, only for public websites, don't use this for banks or anything with a sign-in!
    response = urllib.request.urlopen(req, context = ssl.SSLContext(), timeout = TIMEOUT)

    data = response.read()
    html = data.decode('utf-8') 

    return html  # Returns &quot;HTTP Error 403: Forbidden&quot;
</code></pre>
<p>I don't know what mechanisms a site has to detect a user other than JavaScript, cookies, or user-agent. If relevant, one URL is <code>https://www.idealista.pt/comprar-casas/alcobaca/alcobaca-e-vestiaria/com-preco-max_260000,apenas-apartamentos,duplex/</code>.</p>
<p>How can this site detect the scraper?</p>
",https://stackoverflow.com/questions/76021857/403-error-when-scraping-a-url-that-works-on-firefox-without-cookies-nor-javascri,True,76022271
76021762,ResourceExhaustedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul],"<p>I am trying to read and load image segmentation dataset using colab. i am using colab gpu runtime. here is the code.</p>
<pre><code>class Dataset():
    
    def __init__(
        self,
        root_path: str,
        mode: str
    ):
        if mode == &quot;TRAIN&quot;:
            self.PATH = os.path.join(root_path, &quot;Train&quot;)
        elif mode == &quot;TEST&quot;:
            self.PATH = os.path.join(root_path, &quot;Test&quot;)
        
        self.images = os.listdir(self.PATH + &quot;/images&quot;)
        self.masks = os.listdir(self.PATH + &quot;/masks&quot;)
    
    def load_data(self):
        
        images = []
        masks = []
        
        for index in range(len(self.images)):            
            image = tf.io.read_file(os.path.join(self.PATH, &quot;images&quot;, self.images[index]))
            image = tf.image.decode_jpeg(image, channels=3)
            image = tf.image.convert_image_dtype(image, tf.float32)
            
            mask = tf.io.read_file(os.path.join(self.PATH, &quot;masks&quot;, self.images[index]))
            mask = tf.image.decode_jpeg(mask, channels=3)
            mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)

            height, width = image.shape[0], image.shape[1]
            height -=  height % 16
            width -= width % 16
            
            image = tf.image.resize(image, (height, width), method=&quot;nearest&quot;)
            mask = tf.image.resize(mask, (height, width), method=&quot;nearest&quot;)

            images.append(image)
            masks.append(mask)
    
        return tf.stack(images, axis=0), tf.stack(masks, axis=0)
</code></pre>
<pre><code>train_dataset = Dataset(root_path=PATH, mode=&quot;TRAIN&quot;)
train_images, train_masks = train_dataset.load_data()

test_dataset = Dataset(root_path=PATH, mode=&quot;TEST&quot;)
test_images, test_masks = test_dataset.load_data()
</code></pre>
<p>here is the error i got</p>
<pre><code>---------------------------------------------------------------------------
ResourceExhaustedError                    Traceback (most recent call last)
&lt;ipython-input-6-83bcb83ccdb2&gt; in &lt;cell line: 2&gt;()
      1 train_dataset = Dataset(root_path=PATH, mode=&quot;TRAIN&quot;)
----&gt; 2 train_images, train_masks = train_dataset.load_data()
      3 
      4 test_dataset = Dataset(root_path=PATH, mode=&quot;TEST&quot;)
      5 test_images, test_masks = test_dataset.load_data()

2 frames
/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   7260 def raise_from_not_ok_status(e, name):
   7261   e.message += (&quot; name: &quot; + name if name is not None else &quot;&quot;)
-&gt; 7262   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   7263 
   7264 

ResourceExhaustedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]
</code></pre>
<p>i am trying to read and load the image segmentation dataset using tensorflow. dataset details here ---&gt; <a href=""https://data.lib.vt.edu/articles/dataset/Concrete_Crack_Conglomerate_Dataset/16625056"" rel=""nofollow noreferrer"">https://data.lib.vt.edu/articles/dataset/Concrete_Crack_Conglomerate_Dataset/16625056</a></p>
",https://stackoverflow.com/questions/76021762/resourceexhaustederror-function-node-wrapped-mul-device-joblocalhost-rep,False,
76021704,How do I install the proxybroker2 repository?,"<p>Hey I wanted to install the <a href=""https://github.com/bluet/proxybroker2"" rel=""nofollow noreferrer"">git hub repository</a>, but I am not really sure about the installation process.</p>
<p>I am not really good with python and want to know what steps of the installation guide I need to follow. My OS is Ubuntu.</p>
<p>First I tried to install the latest development version:</p>
<pre><code> $ pip install -U git+https://github.com/bluet/proxybroker2.git.
</code></pre>
<p>At that point I didn't really know what to do there where several steps I tried.</p>
<p>Second I tried to install the docker image</p>
<blockquote>
<p>Use pre-built Docker image</p>
</blockquote>
<pre><code>$ docker pull bluet/proxybroker2
</code></pre>
<p>That didn't work</p>
<p>Then I installed  upx and objdump
upx
objdump (this tool is usually in the binutils package)</p>
<pre><code>$ sudo apt install -y upx-ucl binutils # On Ubuntu / Debian
</code></pre>
<p>I also tried to build the executable
Build</p>
<pre><code>pip install pyinstaller \
&amp;&amp; pip install . \
&amp;&amp; mkdir -p build \
&amp;&amp; cd build \
&amp;&amp; pyinstaller --onefile --name proxybroker --add-data &quot;../proxybroker/data:data&quot; --workpath ./tmp --distpath . --clean ../py2exe_entrypoint.py \
&amp;&amp; rm -rf tmp *.spec
</code></pre>
<p>I got an error message so that didnt work</p>
<p>The error message:</p>
<blockquote>
<p>ERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found</p>
</blockquote>
",https://stackoverflow.com/questions/76021704/how-do-i-install-the-proxybroker2-repository,False,
76021698,&quot;RuntimeError: mat1 and mat2 shapes cannot be multiplied&quot; Only when testing single images with the same input size. pytorch,"<p>I trained a pytorch model on datapoints of 64x64x3 and it did the training and evaluation fine.
when I tried to test the same model on live images I run into this error even though the input size <strong>is the same</strong>.</p>
<p>model:</p>
<pre><code>class liveClassifier(nn.Module):
  def __init__(self):
    super(liveClassifier, self).__init__()
    # input: 64x64x3
    self.conv01 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)
    # =&gt; 62x62x8
    self.conv02 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)
    # =&gt; 60x60x16
    self.pool01 = nn.MaxPool2d(kernel_size=2)
    # =&gt; 30x30x16
    self.conv03 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)
    # =&gt; 28x28x32
    self.pool02 = nn.MaxPool2d(kernel_size=2)
    # =&gt; 14x14x32
    self.conv04 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)
    # =&gt; 12x12x64
    self.pool03 = nn.MaxPool2d(kernel_size=2)
    # =&gt; 6x6x64
    self.linear01 = nn.Linear(6 * 6 * 64, 32)
    self.linear02 = nn.Linear(32, 16)
    self.linear03 = nn.Linear(16, 3)

  def forward(self, input):
    x = self.conv01(input)
    x = F.relu(x)
    
    x = self.conv02(x)
    x = F.relu(x)

    x = self.pool01(x)

    x = self.conv03(x)
    x = F.relu(x)

    x = self.pool02(x)

    x = self.conv04(x)
    x = F.relu(x)

    x = self.pool03(x)

    x = x.flatten(start_dim=1) # ??
    
    x = self.linear01(x)
    x = F.relu(x)

    x = self.linear02(x)
    x = F.relu(x)

    x = self.linear03(x)
    
    output = F.softmax(x, dim=1)
    return output
</code></pre>
<p>this code is the tarining that worked fine</p>
<pre><code>num_epochs = 30

train_loss_list = []
train_acc_list = []

validation_loss_list = []
validation_acc_list = []

for epoch in range(num_epochs):
  
  train_running_loss = 0.0
  train_acc = 0.0

  model = model.train()

  for i, (images, labels) in enumerate(train_loader):
        images = images.to(device)
        labels = labels.to(device)

        ## forward + backprop + loss
        logits = model(images)
        loss = criterion(logits, labels)
        optimizer.zero_grad()
        loss.backward()

        ## update model params
        optimizer.step()

        train_running_loss += loss.detach().item()
        train_acc += get_accuracy(logits, labels, BATCH_SIZE)
    

  
  model = model.eval()
  validation_running_loss = 0.0
  validation_acc = 0.0
  for i, (images, labels) in enumerate(validation_loader, 0):
    images = images.to(device)
    labels = labels.to(device)
    outputs = model(images)
    loss = criterion(outputs, labels)
    validation_running_loss += loss.detach().item()
    validation_acc += get_accuracy(outputs, labels, BATCH_SIZE)
  
  train_loss_list.append(train_running_loss / i)
  train_acc_list.append(train_acc / i)
  validation_loss_list.append(validation_running_loss / i)
  validation_acc_list.append(validation_acc / i)
  
  print('Epoch: %d | Train Loss: %.4f | Train Accuracy: %.2f' \
          %(epoch, train_running_loss / i, train_acc / i))
  print('Validation Loss: %.4f | Validation Accuracy: %.2f' \
          %(validation_running_loss / i, validation_acc / i))


model = model.eval()
</code></pre>
<p>this classification did not work due to this error</p>
<pre><code>print(type(frame))
frame = transform(Image.fromarray(frame)).float().to(device)
print(frame.shape) # torch.Size([3, 64, 64])
model.eval()
print(model(frame))

</code></pre>
<p>When I checked the data tensor shapes I got 64x64x3 in both cases, therefore I have no idea why one would work and the other won't.</p>
",https://stackoverflow.com/questions/76021698/runtimeerror-mat1-and-mat2-shapes-cannot-be-multiplied-only-when-testing-sing,True,76021890
76021696,Voxelization of an .stl file in Pyhton,"<p>I am trying to do a DataScience project and therefore need to feed an .stl file into a deep model.
I am imagining that a voxelized version of the triangular mesh would work best. Conditions for this voxel representation are that the object in the .stl file has to be filled so that the deep model can comprehend the volume of the object in 3D space.
So far I am able to convert the file to a binary numpy array that contains voxels of the surface of the object and even the surface has many empty spots. In general I am very unsatisfied with the results. Below is the code, which I was able to come up with.</p>
<pre><code>import open3d as o3d
import numpy as np

mesh = o3d.io.read_triangle_mesh(&quot;../Sample_Model/backpack_hook.stl&quot;)

mesh.scale(1 / np.max(mesh.get_max_bound() - mesh.get_min_bound()),
           center=mesh.get_center())

voxel_grid = o3d.geometry.VoxelGrid.create_from_triangle_mesh(mesh, voxel_size=0.05)
o3d.visualization.draw_geometries([voxel_grid])

voxels = voxel_grid.get_voxels()
indices = np.stack(list(vx.grid_index for vx in voxels))

a = np.zeros(shape=(np.amax(indices[:, 0]+1), np.amax(indices[:, 1]+1), np.amax(indices[:, 2]+1)))

for i in range(indices.shape[0]):
    coords = indices[i]
    a[coords[0], coords[1], coords[2]] = 1

import matplotlib.pyplot as plt

# change the last parameter to move through the z axis
plt.imshow(a[:, :, 3],interpolation='nearest')
plt.show()
</code></pre>
<p>I have read in past posts that this is quite a difficult problem and so I am not looking into implementing this myself but am hoping that there is a script outthere already doing what I require.</p>
",https://stackoverflow.com/questions/76021696/voxelization-of-an-stl-file-in-pyhton,False,
76021692,How to ensure that Python type checking tools correctly recognize &quot;type-conversion decorators&quot;?,"<p>Basically I am looking for a way to implement a Python decorator, which tries to automatically convert &quot;suitable&quot; argument to another type (from &quot;str&quot; to &quot;Palindrome&quot; in the example below). From the outside the function should look like it can be called with a &quot;suitable&quot; type, whereas on the inside the function does not have to deal with the type conversion logic and is ensured that the value was correctly converted previously (or an exception was raised). I am looking for a generic solution which correctly typechecks, e.g., in VSCode. I composed an example below &amp; am very happy to hear you inputs.</p>
<pre><code>from typing import Callable, ParamSpec, Self, TypeVar

P = ParamSpec(&quot;P&quot;)
R = TypeVar(&quot;R&quot;)


class Palindrome(str):
    def __new__(cls, value: str) -&gt; Self:
        if value == value[::-1]:
            return super().__new__(cls, value)
        raise ValueError(f&quot;{value!r} is not a palindrome.&quot;)

    def is_even(self) -&gt; bool:
        return len(self) % 2 == 0


def magic(func: Callable[P, R]) -&gt; Callable[P, R]:
    def wrapper(*args: P.args, **kwargs: P.kwargs) -&gt; R:
        new_args = []
        for arg, arg_type in zip(args, func.__annotations__.values()):
            if arg_type == Palindrome:
                arg = Palindrome(arg)
            new_args.append(arg)

        return func(*new_args, **kwargs)  # type: ignore
        # ^^^ would be nice to have if this would also typecheck

    return wrapper


def working_no_so_magic(func: Callable[[Palindrome, str], None]) -&gt; Callable[[str, str], None]:
    def wrapper(palindrome: str, text: str) -&gt; None:
        return func(Palindrome(palindrome), text)
    return wrapper


@magic
def do_something(palindrome: Palindrome, text: str) -&gt; None:
    print(type(palindrome), palindrome, palindrome.is_even())
    print(type(text), text)


do_something(Palindrome(&quot;aibohphobia&quot;), &quot;aibohphobia&quot;)
# Expected &amp; actual output:
# &gt;&gt; &lt;class '__main__.Palindrome'&gt; aibohphobia False
# &gt;&gt; &lt;class 'str'&gt; aibohphobia

do_something(&quot;aibohphobia&quot;, &quot;aibohphobia&quot;)
# ^^^ The above line should correctly typecheck!
#
# Expected &amp; actual output (with the @magic decorator):
# &gt;&gt; &lt;class '__main__.Palindrome'&gt; aibohphobia False
# &gt;&gt; &lt;class 'str'&gt; aibohphobia
# Actual output (without the @magic decorator applied):
# &gt;&gt; AttributeError: 'str' object has no attribute 'is_even'

do_something(Palindrome(&quot;not-a-palindrome&quot;), &quot;aibohphobia&quot;)
# Typechecks correctly, and correctly raises an ValueError.


do_something(&quot;not-a-palindrome&quot;, &quot;aibohphobia&quot;)
# Should typecheck correctly (although it would be amazing if we could catch that in the type system), 
# but does not using the current @magic implementation.
# Correctly raise an ValueError at runtime.
```python
</code></pre>
",https://stackoverflow.com/questions/76021692/how-to-ensure-that-python-type-checking-tools-correctly-recognize-type-conversi,False,
76021688,Pyomo Energy System Model: ValueError: No value for uninitialized NumericValue object,"<p>I'm currently trying to model an energy system model with V2G storage as my first go with pyomo. The aim of the model is to minimize costs while satisfying the demand for electricity. This can either be done through intermittent generation from solar, onshore wind and offshore wind or by discharging electricity from electric vehicles. I have attached a picture of the equations of the model. <a href=""https://i.stack.imgur.com/CN2t6.png"" rel=""nofollow noreferrer"">Model equations</a>
However, I'm experiencing some problems with my code and get the error code:</p>
<pre><code>289093 lines were written
GLPK Simplex Optimizer 5.0
52562 rows, 35044 columns, 140160 non-zeros
Preprocessing...
26279 rows, 26280 columns, 105113 non-zeros
Scaling...
 A: min|aij| =  2.860e-05  max|aij| =  1.107e+00  ratio =  3.872e+04
GM: min|aij| =  7.727e-03  max|aij| =  1.294e+02  ratio =  1.675e+04
EQ: min|aij| =  5.971e-05  max|aij| =  1.000e+00  ratio =  1.675e+04
Constructing initial basis...
Size of triangular part is 26279
      0: obj =   8.061601191e+09 inf =   1.219e+07 (23830)
   7587: obj =   5.758397626e+09 inf =   6.047e+06 (16900) 69
  14917: obj =   5.758397626e+09 inf =   3.688e+06 (11839) 29
LP HAS NO PRIMAL FEASIBLE SOLUTION
glp_simplex: unable to recover undefined or non-optimal solution
If you need actual output for non-optimal solution, use --nopresol
...
N_of = 0
ERROR: evaluating object as numeric value: V_discharge[0]
        (object: &lt;class 'pyomo.core.base.var._GeneralVarData'&gt;)
    No value for uninitialized NumericValue object V_discharge[0]
Output exceeds the size limit. Open the full output data in a text editor---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_15212\450560137.py in 
     80 print(&quot;N_on =&quot;, model.N_on())
     81 print(&quot;N_of =&quot;, model.N_of())
---&gt; 82 print(&quot;Z =&quot;, model.Z())
     83 

c:\Users\emok\Anaconda3\lib\site-packages\pyomo\core\base\expression.py in __call__(self, exception)
     59         if self.expr is None:
     60             return None
---&gt; 61         return self.expr(exception=exception)
     62 
     63     def is_named_expression_type(self):

c:\Users\emok\Anaconda3\lib\site-packages\pyomo\core\expr\base.py in __call__(self, exception)
    113 
    114         &quot;&quot;&quot;
--&gt; 115         return evaluate_expression(self, exception)
    116 
    117     def __str__(self):

c:\Users\emok\Anaconda3\lib\site-packages\pyomo\core\expr\visitor.py in evaluate_expression(exp, exception, constant)
   1240 
   1241     try:
...
pyomo\core\expr\numvalue.pyx in pyomo.core.expr.numvalue.value()

pyomo\core\expr\numvalue.pyx in pyomo.core.expr.numvalue.value()

ValueError: No value for uninitialized NumericValue object V_discharge[0]
</code></pre>
<p>When I initialize V_discharge I just get 0 for all my results. I have a feeling the trouble has to do with the dynamic aspect of my model as if it doesn't update the variable V (energy storage in the EVs) properly. Moreover, I have successfully solved the static model with no storage option.</p>
<p>Therefore, I have the following questions:</p>
<ol>
<li>Is my conclusion regard the source of the problem correct?</li>
<li>How do I solve this problem?</li>
<li>Are there any other issues in my code?</li>
</ol>
<p>All help would greatly be appreciated.</p>
<pre><code>import pandas as pd
from pyomo.environ import \*
from pyomo.opt import SolverFactory
from pyomo.core import Constraint

# Load data into a Pandas DataFrame
data = pd.read_csv(r'D:\data\data.csv')

# Create a Pyomo model
model = ConcreteModel()

# Define the sets
model.hours = Set(initialize=data.index, doc='Set of hours')

# Define the parameters
model.k_ipv = Param(default=40)
model.k_ion = Param(default=30)
model.k_iof = Param(default=46)
model.k_vv2g = Param(default=226.56)
model.eta_charge = Param (default=0.924)
model.eta_discharge = Param(default=0.903)
model.E_s = Param(default=160425)
model.alpha = Param(default=0.5)

#Define data
model.CF_pv = Param(model.hours, initialize=data['CF_pv'].to_dict())
model.CF_on = Param(model.hours, initialize=data['CF_on'].to_dict())
model.CF_of = Param(model.hours, initialize=data['CF_of'].to_dict())
model.pv2g = Param(model.hours, initialize=data['pv2g'].to_dict())
model.d_ev = Param(model.hours, initialize=data['d_ev'].to_dict())

# Define the variables
model.N_pv = Var(domain=NonNegativeReals, initialize=0)
model.N_on = Var(domain=NonNegativeReals, initialize=0)
model.N_of = Var(domain=NonNegativeReals, initialize=0)
model.V = Var(model.hours, domain=NonNegativeReals)
model.V_charge = Var(model.hours, domain=NonNegativeReals)
model.V_discharge = Var(model.hours, domain=NonNegativeReals)
model.C = Var(model.hours, domain=NonNegativeReals)

# Market balance constraints
model.d1 = Constraint(model.hours, rule=lambda model, h: model.CF_pv[h] * model.N_pv +
                                                   model.CF_on[h] * model.N_on +
                                                   model.CF_of[h] * model.N_of + model.V_discharge[h] == data['d'][h])

model.d2 = Constraint(model.hours, rule=lambda model, h: model.CF_pv[h] * model.N_pv +
                                                   model.CF_on[h] * model.N_on +
                                                   model.CF_of[h] * model.N_of == data['d'][h] + model.V_charge[h] + model.C[h])


#State of charge constraint
def s1_constraint(model,h):
    if h == model.hours.first():
        return model.V[h] == model.alpha*model.E_s / 2
    else: 
        return model.V[h] == model.V[h-1] + model.eta_charge * model.V_charge[h] - model.V_discharge[h]/model.eta_discharge - data['d_ev'][h]
model.s1 = Constraint(model.hours, rule = s1_constraint)

&quot;&quot;&quot;No free lunch&quot;&quot;&quot;
model.s2 = Constraint(rule=lambda model:  model.V[model.hours.first()] == model.V[model.hours.last()])

&quot;&quot;&quot;Maximum energy storage must be less or equal to capacity&quot;&quot;&quot;
model.s3 = Constraint(model.hours, rule=lambda model, h: model.V[h] &lt;= model.alpha*model.E_s)

# Charging and discharging constraints
&quot;&quot;&quot; Maximum discharge rate within a single hour &quot;&quot;&quot;
model.disc1 = Constraint(model.hours, rule=lambda model, h: model.V_discharge[h] &lt;= model.alpha*data['pv2g'][h])

&quot;&quot;&quot; Maximum charge rate within a single hour &quot;&quot;&quot;
model.char1 = Constraint(model.hours, rule=lambda model, h: model.V_charge[h] &lt;= model.alpha*data['pv2g'][h])

# Define objective function
def obj_expression(model):
    return model.k_ipv * model.N_pv + model.k_ion * model.N_on + model.k_iof * model.N_of + model.k_vv2g * sum(model.V_discharge[h] for h in model.hours)

model.Z = Objective(expr=obj_expression(model), sense=minimize) # Use the defined objective expression with the 'h' variable

# Solve the optimization problem
solver = SolverFactory('glpk')
#solver.solve(model)
results = solver.solve(model,tee=True)

# Access the solution
print(&quot;N_pv =&quot;, model.N_pv())
print(&quot;N_on =&quot;, model.N_on())
print(&quot;N_of =&quot;, model.N_of())
print(&quot;Z =&quot;, model.Z())
</code></pre>
",https://stackoverflow.com/questions/76021688/pyomo-energy-system-model-valueerror-no-value-for-uninitialized-numericvalue-o,True,76023828
76021665,Encounter error while using &#39;kaiser_fast&#39; in librosa.load,"<p>I want to load *.wav files. I am using the following code</p>
<pre><code> audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast', duration=20) 
</code></pre>
<p>However, I get the following error</p>
<pre><code>No module named 'resampy'
</code></pre>
<p>I tried to install resampy and it successfully installed. However, the error did not solve.</p>
<p>I am expecting to use kaise_fast and do not get an error.</p>
",https://stackoverflow.com/questions/76021665/encounter-error-while-using-kaiser-fast-in-librosa-load,False,
76021607,Is there a programatic way to maximise the values to the solution of a simultaneous equation,"<p>I am trying to write a computer program to calculate quantities for Chicken feed based on the % of protein in the feed</p>
<p>Here is a snippet with the example of how it works</p>
<pre><code>Assuming that the farmer wants to make feed with a 19% DCP [Digestible Crude Protein] 
The farmer may use: 

 - Whole maize (8.23 %DCP) 
 - Soya (45 % DCP) 
 - Omena (55 % DCP) 
 - Maize bran (7 % DCP) 



To make a 70 kg bag of feed for layers, a farmer would require the following ingredients:
    
    34 kg of whole maize
    12 kg of Soya
    8 kg of omena
    10 kg of maize bran


To get the weight of crude protein:

 A) Whole maize = 34 kg x 8.23 ÷100 = 2.80kg
 B) Soya bean = 12 kg x 45 ÷ 100 = 5.40 kg
 C) Omena = 8 kg x 55 ÷ 100 = 4.40 kg
 D) Maize bran = 10 kg x 7 ÷ 100 = 0.70 kg

(Total crude protein 13.30 kg)
</code></pre>
<p>Finally to get the total crude protein content of all these ingredients in a 70 kg bag, you take the total crude protein content of the combined ingredients, divide by 70 and multiply by 100 thus,</p>
<pre><code>(13.30 ÷ 70) x 100 = 19.0 %. 
</code></pre>
<p>Now there are infinite combinations of these 4 ingridents that would give a 19% crude protein. The general equations being</p>
<pre><code>A*0.082 + B*0.45 + C*0.55 + D*0.07 = 0.19 
----------------------------------
      A+B+C+D
</code></pre>
<p>Intuitively I think there could be a single solution to find a situation where values of A,B,C and D are as high as possible. So a solution where we use as much of each ingredient as possible by weight.</p>
<p>This is where I am not stuck as in the approach. What I thought of trying is starting all values with an arbitrary weight say 50kg. Then for each value of A,B,C and D adjust
only one value to get how much to adjust each one to get to a solution for that single ingredient. for example for A</p>
<pre><code> A = 0.19(B + C + D)-(B*0.45 + C*0.55 + D*0.07)
      --------------------------------------------
                 0.082-0.19
</code></pre>
<p>Then I substitute values of BCD with 50 to get the new weight of A then get the % increase or decrease of A vs. its previous value (of 50)</p>
<p>I repeat this for all values and see what the + increases or decreases were. For example I could get</p>
<p>A  -5%
B  +20%
C  +10%
D  -2%</p>
<p>I then normalise this by dividing by the smallest % increase. then finally use the final answers the increases/decreases I need to apply to those A,B,C and D values</p>
<p>This is an intuitive approach which could be wrong, I was wondering if anyone has any other possible solution, even machine learning related.</p>
",https://stackoverflow.com/questions/76021607/is-there-a-programatic-way-to-maximise-the-values-to-the-solution-of-a-simultane,False,
76021606,API endpoint parameters for annual and quarterly data: NASDAQ API,"<p>I'm trying to use the endpoint to filter between quarterly and annual financial data (balance sheets, income &amp; cash flow statements).</p>
<p>As of now the following endpoint: 'https://api.nasdaq.com/api/company/AAPL/financials?' only shows the annual data, but I also need to get the quarterly data too.</p>
<p>Does anyone know what would be <strong>the correct parameter to include in this endpoint?</strong> I've tried searching through the documents however this endpoint itself is not found anywhere but still works upto date (I was shown this endpoint by someone so I do not know the source of where it is found).</p>
<p>The documents in the NASDAQ data link do not provide the endpoint I'm using nor did their customer service help with identifying it. It is a valid endpoint though.</p>
<p>my python code below:</p>
<pre><code>import requests
import json

api = 'my_api_key'
headers = {
    'accept': 'application/json, text/plain, */*',
    'accept-encoding': 'gzip, deflate, br',
    'accept-language': 'en-US,en;q=0.9',
    'origin': 'https://www.nasdaq.com',
    'referer': 'https://www.nasdaq.com',
    'sec-fetch-mode': 'cors',
    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.79 Safari/537.36'
}

api_url = f'https://api.nasdaq.com/api/company/AAPL/financials?&amp;api_key={api}'

data = requests.get(api_url, headers=headers).json()


with open(&quot;nasdaq_financials.json&quot;, &quot;w&quot;) as f:
    json.dump(data, f, indent=4)
</code></pre>
",https://stackoverflow.com/questions/76021606/api-endpoint-parameters-for-annual-and-quarterly-data-nasdaq-api,False,
76021571,DRY - Code duplicated in recursive function: string replacer in dictionaries (python),"<p>I am building a json token replacer, that works like this:</p>
<p>input: <code>{&quot;greet&quot;: &quot;hello {{name}}&quot;}</code> and tokens <code>{&quot;name&quot;: &quot;Lorenzo&quot;}</code></p>
<p>output: <code>{&quot;greet&quot;: &quot;hello Lorenzo&quot;}</code></p>
<hr />
<p>As I want the function to work also with nested dicts/lists, I've used recursion to extend the functionality on more complex inputs such as:</p>
<p>input: <code>{&quot;a&quot;: {&quot;b&quot;: &quot;x{{ppp}}z&quot;}}</code> and tokens <code>{&quot;ppp&quot;: &quot;y&quot;})</code></p>
<p>output: <code>{&quot;a&quot;: {&quot;b&quot;: &quot;xyz&quot;}}</code></p>
<p>(other tests/example usages are available <a href=""https://github.com/lorenzua02/json-replacer/blob/main/test/tests.py#L27"" rel=""nofollow noreferrer"">here</a>)</p>
<hr />
<p>Even if the code works, I'm not satisfied with my current solution because I've violated the DRY principle.</p>
<p>I'm sure I can make my code cleaner: any ideas or suggestions? This is what I tried:</p>
<pre class=""lang-py prettyprint-override""><code>def __replace(input_str: str, pytokens: dict):
    for k, v in pytokens.items():
        input_str = re.sub(&quot;{{&quot; + k + &quot;}}&quot;, str(v), input_str)
    return input_str


def __json_replacer(pyinput, pytokens: dict):
    # TODO DRY: code is doubled, code this part again
    if isinstance(pyinput, str):
        return __replace(pyinput, pytokens)

    if isinstance(pyinput, list):
        for i in range(len(pyinput)):
            pyinput[i] = __json_replacer(pyinput[i], pytokens)
        return pyinput

    if not isinstance(pyinput, dict):
        return pyinput

    res = {}
    for k, v in pyinput.items():
        if isinstance(v, str):
            v = __replace(v, pytokens)

        if isinstance(v, list):
            for i in range(len(v)):
                v[i] = __json_replacer(v[i], pytokens)

        if isinstance(v, dict):
            v = __json_replacer(v, pytokens)

        res[k] = v

    return res
</code></pre>
<p>Note: the project is open source! If you provide a solution, feel free to open a PR and take the credits :) <a href=""https://github.com/lorenzua02/json-replacer"" rel=""nofollow noreferrer"">GitHub project link</a></p>
",https://stackoverflow.com/questions/76021571/dry-code-duplicated-in-recursive-function-string-replacer-in-dictionaries-py,True,76021642
76021525,Two variables in code getting mixed up. Variable2 being used when variable1 is required and called for,"<p><a href=""https://i.stack.imgur.com/530HZ.png"" rel=""nofollow noreferrer"">code, first case</a></p>
<p>I was trying to build a simple calculator. The problem arised when I tried to multiply a digit by 0. Then the variable &quot;div&quot; is used instead of &quot;mult&quot;. There's no problem when 0 is no1 and multiplied by no2 (0 * no2), only no1 * 0 doesn't work</p>
<p><a href=""https://i.stack.imgur.com/CWviQ.png"" rel=""nofollow noreferrer"">second case</a></p>
<p>I tried deleting the &quot;div&quot; variable. After this I was able to receive an answer to no1 * 0. Instead of using the div variable, I just put &quot;no1/no2&quot; inside the curly brackets.
Works as desired, but I don't get the underlying issue. Understanding it would be great for learning to code.</p>
<p>I'm not really sure what's happening because I'm really new to Python and coding.</p>
",https://stackoverflow.com/questions/76021525/two-variables-in-code-getting-mixed-up-variable2-being-used-when-variable1-is-r,False,
76021504,"AttributeError: &#39;NoneType&#39; object has no attribute &#39;data&#39;, Training error while fgsm attack","<p>I am trying to train a code, but I get this error. Here is my code:</p>
<pre><code>for i in range(num_iter):
            # Forward pass to get logits
            logits = self(images)

            # Calculate loss
            loss = self.criterion(logits, masks)

            # Zero gradients
            self.optim.zero_grad()

            # Backward pass to compute gradients
            loss.backward()

            images = images.detach()

            images = torch.tensor(images, dtype=torch.float32, requires_grad=True)

            # Get the gradients of the loss w.r.t. the input image
            data_grad = images.grad.data

            # Generate the perturbed image using FGSM
            perturbed_images = fgsm(images, epsilon, data_grad)

            # Re-classify the perturbed image
            logits_perturbed = self(perturbed_images)
</code></pre>
<p>code is just a slice of the entire code.</p>
<p>The error I see is:</p>
<pre class=""lang-py prettyprint-override""><code>File &quot;/home/xx/xx/xx.py&quot;, line 73, in training_step
    data_grad = images.grad.data
AttributeError: 'NoneType' object has no attribute 'data'
</code></pre>
<p>Trying to train the model, expected it to train well, but it does not work.</p>
",https://stackoverflow.com/questions/76021504/attributeerror-nonetype-object-has-no-attribute-data-training-error-while,False,
76021501,Render an image onto a pillow,"<p>I want to make a program that takes in an image and renders that image onto a pillow (cushion). How do I go about this?</p>
<p>I tried following online tutorials (using the python pillow library) but all I learnt was to change the dimensions of the image. I tried looking for repos that do similar things but had no luck</p>
",https://stackoverflow.com/questions/76021501/render-an-image-onto-a-pillow,False,
76021476,ModuleNotFoundError: No module named &#39;langchain.agents&#39;,"<p>Just intalled Lanchain. Been going through the first few steps of the getting started tutorial without a problem till I reach the Agents section.</p>
<pre class=""lang-py prettyprint-override""><code>from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.llms import OpenAI
</code></pre>
<pre class=""lang-bash prettyprint-override""><code>Traceback (most recent call last):
  File &quot;/langchain/agents.py&quot;, line 4, in &lt;module&gt;
    from langchain.agents import load_tools
ModuleNotFoundError: No module named 'langchain.agents'
</code></pre>
<p>I am running lanchain in a virtual environment in python 3.9. Installed langchain with the pip install command as shown in the docs.</p>
<p>Not sure why this module specifically could be missing</p>
",https://stackoverflow.com/questions/76021476/modulenotfounderror-no-module-named-langchain-agents,True,76021694
76021473,"visual studio is not able to show the Arabic language well, so it appears cut off","<p><a href=""https://i.stack.imgur.com/IueZu.jpg"" rel=""nofollow noreferrer"">enter image description here</a>
visual studio is not able to show the Arabic language well, so it appears cut off</p>
<p>visual studio is not able to show the Arabic language well, so it appears cut off</p>
",https://stackoverflow.com/questions/76021473/visual-studio-is-not-able-to-show-the-arabic-language-well-so-it-appears-cut-of,False,
76021462,pymongo.errors.ServerSelectionTimeoutError: SSL handshake failed: localhost:27017: [WinError 10054],"<p>I'm trying to connect to mongoclient via ssh tunnel. Here is the code</p>
<pre><code>import pymongo
from sshtunnel import SSHTunnelForwarder

with open(&quot;config.json&quot;) as f:
    config = json.load(f)

config_params = config[&quot;dev_db&quot;]

# SSH server credentials
SSH_HOST = config_params['host_name']
SSH_PORT = 22
SSH_USER = config_params['host_username']
SSH_KEY = config_params['private_key']

# MongoDB server credentials
MONGO_HOST = config_params[&quot;db_uri&quot;]
MONGO_PORT = 27017
params = config_params['params']

# Create an SSH tunnel
with SSHTunnelForwarder(
    (SSH_HOST, SSH_PORT),
    ssh_username=SSH_USER,
    ssh_pkey=SSH_KEY,
    remote_bind_address=(MONGO_HOST, MONGO_PORT),
    local_bind_address=('localhost', MONGO_PORT),
) as server:

    # Connect to MongoDB
    client = pymongo.MongoClient(
        'localhost',
        server.local_bind_port,
        username = params['username'],
        password=params['password'],
        tlsAllowInvalidCertificates=True,
        tls=True,
        tlsCAFile=params['tlsCAFile'],
        retryWrites=False
    )
    print(client)
    print(client.list_database_names())

</code></pre>
<p>Credentials are 100% valid. I can connect with them via mongo Atlas and everything works.
I also checked if pems are available from code and they really are.</p>
<p>Python 3.7 but I also tried 3.11. Pymongo 4.3.3 and sshtunnel 0.4. Sorry I spend on it may be 12 hours with the help of ChatGPT and we are both have no ideas :/</p>
",https://stackoverflow.com/questions/76021462/pymongo-errors-serverselectiontimeouterror-ssl-handshake-failed-localhost2701,True,76033903
76021451,Tkinter canvas not updating the loops,"<p>I want to make a program where a square goes from one side to another (in canvas) infinitely, but my program isn’t updating what is inside of the loops. I’m new to tkinter so sorry if this is some stupid mistake.
First, I had the mainloop at the end, but than it never got executed so the root was never displayed on the screen. I changed the program to this version, but now it’s like it stops executing at the point of mainloop: the loops don’t get executed and the square doesn’t move at all.
This is my code:</p>
<pre><code>import tkinter as tk

root = tk.Tk()
root.geometry(&quot;600x400&quot;)
canvas = tk.Canvas(root, width=600, height=400)
canvas.pack()
x = 300
y = 200
square = canvas.create_rectangle(x-10, y-10, x+10, y+10, fill=&quot;black&quot;)
root.mainloop()
for i in range(29):
    canvas.move(square, 10, 0)

while True:
    for j in range(58):
        canvas.move(square, -10, 0)
    for j in range(58):
        canvas.move(square, 10, 0)
</code></pre>
",https://stackoverflow.com/questions/76021451/tkinter-canvas-not-updating-the-loops,False,
76021448,"expected shape=(None, 100, 100, 3), found shape=(None, 100, 3)","<blockquote>
<p>Input 0 of layer &quot;sequential&quot; is incompatible with the layer: expected shape=(None, 100, 100, 3), found shape=(None, 100, 3)</p>
</blockquote>
<p>the image shape is
(100,100,3)
But in <code>model.predict</code> it show (None,100,3)
should be (None,100,100,3)</p>
<pre class=""lang-py prettyprint-override""><code>img = cv2.imread('/content/drive/MyDrive/cat.jpg')
img = cv2.resize(img,(100,100))
img = np.array(img)
</code></pre>
",https://stackoverflow.com/questions/76021448/expected-shape-none-100-100-3-found-shape-none-100-3,False,
76021421,Python Protocol for De-/Serialization with Shared Generic Types,"<p>I am writing a protocol that requires conforming classes to provide serializing/deserializing methods <code>to_config</code> and <code>from_config</code>.</p>
<p>This is my current approach. CV is the group of allowed types in the config.
The idea is to keep the serialization generic but enforce that whatever type is used for serialization equals the type used for serialization. In other words, the return type of implementation of <code>to_config</code> must be the type of the value argument for any implementation of <code>from_config</code>.</p>
<p>This is my approach (non-functional) below. I think the issue is that CVT and V are valid only within the context of their methods, but I want them to be valid on the Registrable class level.</p>
<pre><code>from typing import TypeVar, Protocol, Union, List, Dict
from typing_extensions import TypeAlias

CV: TypeAlias = Union[str, int, float, bool, List['CV'], Dict[str, 'CV']]
CVT = TypeVar('CVT', bound=CV)
V = TypeVar('V')

class Registrable(Protocol):
    
    @classmethod
    def from_config(cls: Type[V], value: CVT) -&gt; V:
       ...

    def to_config(self: V) -&gt; CVT:
        ...
</code></pre>
",https://stackoverflow.com/questions/76021421/python-protocol-for-de-serialization-with-shared-generic-types,False,
76021345,How can I change a column to a column from another dataframe based on a condition?,"<p>I have this data frame &quot;cus&quot; <a href=""https://i.stack.imgur.com/xPtvb.png"" rel=""nofollow noreferrer"">enter image description here</a> and need to replace column &quot;nationality_code&quot; with respective weight from this dataframe &quot;scores&quot; <a href=""https://i.stack.imgur.com/DNCn7.png"" rel=""nofollow noreferrer"">enter image description here</a>.</p>
<p>So &quot;nationality_code&quot; PT form &quot;cus&quot; represents 10 from &quot;scores&quot;. I need to replace all rows from &quot;cus&quot;. Can anyone help me please?</p>
<p>I'm just doing manually using this</p>
<pre><code>cus['nationality_code'].mask(cus['nationality_code'] == 'IE', sco.loc[sco['code'] == &quot;IE&quot;, 'risk_weight'].iloc[0], inplace=True)
</code></pre>
<p>But I need to type this code 100+ times because I have 100+ country codes to replace.</p>
",https://stackoverflow.com/questions/76021345/how-can-i-change-a-column-to-a-column-from-another-dataframe-based-on-a-conditio,False,
76021304,Why is this? just working on a python project,"<p>Here is my current code:</p>
<pre><code>while True:
  gname = input(&quot;Welcome to MoneyCalc. To start, please enter the name of the product/service: &quot;)
  coststr = &quot;What was the cost price of &quot; + gname + &quot;? &quot;
  cost = input(coststr)
  while True:
    isnum = cost.isnumeric()
    if isnum == &quot;False&quot;:
      print(&quot;Sorry, please try entering a number.&quot;)
</code></pre>
<p>I was expecting this to ask the question &quot;What was the cost price of x?&quot; until the user inputs a numeric value but instead I get the following output:</p>
<pre><code>Welcome to MoneyCalc. To start, please enter the name of the product/service: h
What was the cost price of h? hh
[Your interpreter session was terminated by signal SIGKILL.]
</code></pre>
",https://stackoverflow.com/questions/76021304/why-is-this-just-working-on-a-python-project,False,
76021302,Is Socket.IO connection state recovery supported by the Python server?,"<p>The Socket.IO docs describe <a href=""https://socket.io/docs/v4/connection-state-recovery"" rel=""nofollow noreferrer"">Connection state recovery</a> and how to implement it in JavaScript. It states, &quot;Connection state recovery must be enabled by the server&quot;. However, I cannot find any mention of connection state recovery in the <a href=""https://python-socketio.readthedocs.io/en/latest/server.html"" rel=""nofollow noreferrer"">Socket.IO Python server docs</a>.</p>
<p>I'm using this to initialize the server (see <a href=""https://python-socketio.readthedocs.io/en/latest/api.html#asyncserver-class"" rel=""nofollow noreferrer"">AsyncServer docs</a>):</p>
<pre><code>import socketio

sio_server = socketio.AsyncServer(
    async_mode='asgi',
    cors_allow_origins=[],  # allow ONLY FastAPI
)
</code></pre>
<p>In a simple test, I disconnected WiFi between my Python Socket.IO client and Python Socket.IO server. After 30 seconds, the client died with <code>socketio.exceptions.TimeoutError</code>. After reconnecting, the server, too, reported <code>socketio.exceptions.TimeoutError</code>.</p>
<p><strong>Is the connection state recovery feature implemented in the Python server?</strong> If so, where is the documentation? How do I enable it?</p>
",https://stackoverflow.com/questions/76021302/is-socket-io-connection-state-recovery-supported-by-the-python-server,False,
76021299,Is there a python function to compare two arbitrary misaligned non-smooth datasets?,"<p>I am modelling some fluid flows through anisotropic material. I'd like to measure the fit of my model. In the image, the black crosses mark experimental data, the grey dotted line marks a 'best guess' model made by tweaking four different parameters. Each dot is a calculation, and they don't quite line up with the crosses in time. Future models won't be linear either.</p>
<p><a href=""https://i.stack.imgur.com/HzaYE.png"" rel=""nofollow noreferrer"">image of matoplotlib scatter plot</a></p>
<p>What I need to do is, for each cross, iterate along the dots until I find the closest one, interpolate to the next one and find the squared distance between the X and the interpolation line. This feels like it should be a standard function, but I am unable to find it. any tips?</p>
<p>I've, googled, checked previous questions and  looked through scikit and pandas, but its a generic enough problem it is hard to pin down.</p>
",https://stackoverflow.com/questions/76021299/is-there-a-python-function-to-compare-two-arbitrary-misaligned-non-smooth-datase,False,
76021282,Django filter - filter a property of a model through a range of values,"<p>I have this model</p>
<pre><code>class Person(models.Model):
    name = models.CharField(max_length=150)
    date_birth = models.DateField()

</code></pre>
<p>and this property to calculate years and months of birth</p>
<pre><code>    @property
    def eta_annimesi(self):
        date2 = datetime.combine(self.date_birth , datetime.min.time())
        now  = datetime.now()                        
        duration = (now.year - date2.year) * 12 + now.month - date2.month
        years = duration // 12
        months = duration % 12
        if years &gt; 0:
            if months &gt; 0:
                msg_anni = years.__str__() + ' anni ' +months.__str__() +' mesi'
            else:
                msg_anni = years.__str__() + ' anni'
        else:
            if months &gt; 0:
                msg_anni = months.__str__() +' mesi'
            else:
                msg_anni = 'Appena nato'
        return msg_anni
</code></pre>
<p>I need to create filters on an html page that allow me to create a range between two integer values. (e.g. from 0 to 2 years)
If it were possible even months but it's not important.</p>
<p>I managed it by giving the possibility to enter the range of two dates but that's not what I need.</p>
<p>Ideas and suggestions?
Thanks in advance to anyone who wants to give me a hand.</p>
",https://stackoverflow.com/questions/76021282/django-filter-filter-a-property-of-a-model-through-a-range-of-values,False,
76021279,How to simulate keyboard and mouse input for android?,"<p>I am trying to simulate a keyboard and mouse input on android using a windows device (Both devices in question will be connected via a usb cable). I have searched online and the only related answers I could see were windows to windows connections. So,</p>
<p>Q1) Is it possible to make the windows device appear as an input device?</p>
<p>Q2) How should I go about doing it ? (Modules that I should use etc)</p>
<p>Q3) Will there be significant input lag ?</p>
<p>Q4) Will it require some sort of api on the receiving device?</p>
<p>I have looked into pynput, puautogui, win32.client, autotasker, AHK etc</p>
<p>TLDR: I am looking for a way to stream keyboard and mouse input to android</p>
",https://stackoverflow.com/questions/76021279/how-to-simulate-keyboard-and-mouse-input-for-android,False,
76021277,Scrapy. Handling Pagination,"<p>I'm using scrapy to collect data from <a href=""https://www.habermeyer.de/"" rel=""nofollow noreferrer"">habermeyer.de</a>. Although it's easy to iterate over categories and products, I can't find the right way to preserve pagination.
If we inspect the pagination mechanism in a web browser, we see that each time we press the button to <em>view more items</em>, we actually send a POST request with some form data, so it returns HTML with the new products. Moreover the required form data is injected into <strong>data-search-params</strong> attribute of the button, so it can be easily extracted and serialized into JSON.</p>
<p>Let's say we have a <a href=""https://www.habermeyer.de/spielwaren-habermeyer-ek-neuburgdonau/k/rennbahnen-rc-modellbau"" rel=""nofollow noreferrer"">category</a>. For the experiment, I copied the form data from the <em>Chrome's Developer Tools</em>, while interacting with the pagination manually, and pasted it into the script bellow, which I use in the <strong>scrapy shell</strong>:</p>
<p><a href=""https://postimg.cc/yDTrn4nr"" rel=""nofollow noreferrer""><img src=""https://i.postimg.cc/cJqyxd3Z/photo-5321041589130348210-w.jpg"" alt=""photo-5321041589130348210-w.jpg"" /></a></p>
<pre class=""lang-py prettyprint-override""><code>from scrapy.http import FormRequest


pagination_api_url = &quot;https://www.habermeyer.de/spielwaren-habermeyer-ek-neuburgdonau/search/navigationasn&quot;
form_data = {
  'factFinderSearchParameters': {
    'filters': [
      {
        'name': 'CategoryPath',
        'substring': False,
        'values': [{'exclude': False, 'type': 'or', 'value': ['Rennbahnen, RC &amp; Modellbau']}]
      }  
    ],
    'hitsPerPage': 24,
    'marketIds': ['400866330'],
    'page': 3,
    'query': '*'
  },
  'useAsn': '0'
}
headers = {
    &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded; charset=UTF-8&quot;,
    &quot;Origin&quot;: &quot;https://www.habermeyer.de&quot;,
    &quot;Referer&quot;: &quot;https://www.habermeyer.de/spielwaren-habermeyer-ek-neuburgdonau/k/rennbahnen-rc-modellbau&quot;,
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36&quot;,
}
r = FormRequest(pagination_api_url, formdata=form_data, headers=headers)
# fetch(r)
</code></pre>
<p><strong>Note</strong>: I had to convert the value of <code>useAsn</code> into str in order to avoid avoid <code>TypeError: to_bytes must receive a str or bytes object, got int</code>.</p>
<p>Though fetching the form request returns <em>HTTP 200</em>, the content of the returned HTML indicates that search returned no results.</p>
<p>As another experiment, I copied the encoded form data from the <em>Chrome's Developer Tools</em> and passed it into a simple POST request (see the code below). As a result I received the expected HTML output with the new products:</p>
<pre class=""lang-py prettyprint-override""><code>from scrapy import Request


encoded_form_data = &quot;factFinderSearchParameters=%7B%22filters%22%3A%5B%7B%22name%22%3A%22CategoryPath%22%2C%22substring%22%3Afalse%2C%22values%22%3A%5B%7B%22exclude%22%3Afalse%2C%22type%22%3A%22or%22%2C%22value%22%3A%5B%22Rennbahnen%2C+RC+%26+Modellbau%22%5D%7D%5D%7D%5D%2C%22hitsPerPage%22%3A24%2C%22marketIds%22%3A%5B%22400866330%22%5D%2C%22page%22%3A3%2C%22query%22%3A%22*%22%7D&amp;useAsn=0&quot;
r = Request(pagination_api_url, method=&quot;POST&quot;, body=encoded_form_data, headers=headers)
# fetch(r)
</code></pre>
<p>Encoding the initial form data represented as JSON, doesn't help as well, though the request returns HTTP 200:</p>
<pre class=""lang-py prettyprint-override""><code>from urllib.parse import urlencode


encoded_form_data = urlencode(form_data)
r = Request(pagination_api_url, method=&quot;POST&quot;, body=encoded_form_data, headers=headers)
# fetch(r)
</code></pre>
<p><strong>Python</strong> version: 3.10.6<br />
<strong>Scrapy</strong> version: 2.8.0</p>
",https://stackoverflow.com/questions/76021277/scrapy-handling-pagination,True,76024596
76021264,I&#39;m trying to build a model to know whether there would be an increase or drop in covid deaths in the UK in the future,"<p>I'm not getting my expected results from my machine learning model.</p>
<p>The below is my code:</p>
<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.arima.model import ARIMA

df = pd.read_csv(&quot;dataset.csv&quot;, parse_dates=['date'], index_col='date')

# extract the columns of interest
deaths = df['Cumulative_deaths']

# plot the time series
plt.plot(deaths)
plt.title('..Cumulative COVID Deaths in the UK')
plt.xlabel('Date')
plt.ylabel('Number of Deaths')
plt.show()

# split the data into train and test sets
train_size = int(len(deaths) * 0.8)
train_data = deaths[:train_size]
test_data = deaths[train_size:]

# fit an ARIMA model to the training data
model = ARIMA(train_data, order=(1,1,1))
model_fit = model.fit()

# make a forecast for the test data
forecast = model_fit.forecast(steps=len(test_data))

# plot the forecast
plt.plot(deaths, label='Actual')
plt.plot(test_data.index, forecast, label='Forecast')
plt.title('F Cumulative COVID Deaths in the UK')
plt.xlabel('Date')
plt.ylabel('Number of Deaths')
plt.legend()
plt.show()
</code></pre>
<p>This is the result for the forecast</p>
<p><a href=""https://i.stack.imgur.com/nFlx4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nFlx4.png"" alt=""enter image description here"" /></a></p>
<p>This is the result for the actual value</p>
<p><a href=""https://i.stack.imgur.com/eVvrM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eVvrM.png"" alt=""enter image description here"" /></a></p>
<p>The result doesn't makes sense to me and I'm not sure how to proceed further. I've tried googling but I'm not sure what's going on.</p>
<p>My dataset looks like this:</p>
<p><a href=""https://i.stack.imgur.com/6phti.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6phti.png"" alt=""enter image description here"" /></a></p>
<p>The error I'm getting is the following:</p>
<pre><code> UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.
  df = pd.read_csv(&quot;dataset.csv&quot;, parse_dates=['date'], index_col='date')
C:\Users\yhoss\.conda\envs\pythonProject1\lib\site-packages\statsmodels\tsa\base\tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\yhoss\.conda\envs\pythonProject1\lib\site-packages\statsmodels\tsa\base\tsa_model.py:471: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\yhoss\.conda\envs\pythonProject1\lib\site-packages\statsmodels\tsa\base\tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\yhoss\.conda\envs\pythonProject1\lib\site-packages\statsmodels\tsa\base\tsa_model.py:471: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\yhoss\.conda\envs\pythonProject1\lib\site-packages\statsmodels\tsa\base\tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\yhoss\.conda\envs\pythonProject1\lib\site-packages\statsmodels\tsa\base\tsa_model.py:471: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.
  self._init_dates(dates, freq)
C:\Users\yhoss\.conda\envs\pythonProject1\lib\site-packages\statsmodels\tsa\base\tsa_model.py:834: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.
  return get_prediction_index(
</code></pre>
",https://stackoverflow.com/questions/76021264/im-trying-to-build-a-model-to-know-whether-there-would-be-an-increase-or-drop-i,False,
76021251,Is there any way to create a command in discord.py-self for phonespoof?,"<p>That command will change the green circle of user to phone icon, please help me</p>
<p>I tried:
<a href=""https://github.com/passedout/DC-Mobile-Status/blob/main/Main.py"" rel=""nofollow noreferrer"">https://github.com/passedout/DC-Mobile-Status/blob/main/Main.py</a></p>
<p>but that didn't work, i completly dont know what to do.</p>
",https://stackoverflow.com/questions/76021251/is-there-any-way-to-create-a-command-in-discord-py-self-for-phonespoof,False,
76021222,How to use NSURLAuthenticationMethodDefault in Pyobjus,"<p>how i can use NSURLAuthenticationMethodDefault to connect website in</p>
<pre><code>import ios
url = 'https://exapmle.com'
web_view = IOSWebView().open(url)
</code></pre>
<p>i need to use NSURLAuthenticationMethodDefault p12 file to auth in <a href=""https://example.com"" rel=""nofollow noreferrer"">https://example.com</a> in the webview</p>
",https://stackoverflow.com/questions/76021222/how-to-use-nsurlauthenticationmethoddefault-in-pyobjus,False,
76021202,LightGBM Probabilities calibration with custom cross-entropy score and loss function,"<p>I am currently trying to perform <code>LightGBM</code> probabilities calibration with a custom cross-entropy score and loss function for a binary classification problem. My issue is related to the custom cross-entropy that leads to incompatibility with <code>CalibratedClassifierCV</code> where I got the following error:</p>
<pre><code>calibrated_model.fit(X, y): too many indices for an array: the array is 1-dimensional, but 2 were indexed.
</code></pre>
<p>I made my own calibration function with <a href=""https://gdmarmerola.github.io/probability-calibration/"" rel=""nofollow noreferrer"">this</a> tutorial. But I end up with a function that returns calibrated probabilities and not a 'full model' that is compatible with <code>mlflow</code>. I thus wonder how could I maintain my script in <code>sklearn</code> and keep something similar to the script below.</p>
<p>Thank you</p>
<pre><code>import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split  
from sklearn.datasets import load_breast_cancer
from sklearn.calibration import CalibratedClassifierCV

#  Cross-entropy score and loss function from : https://pub.towardsai.net/outline-a-smaller-class-with-the-custom-loss-function-94ff00359698
def first_grad_logreg_beta(predt,y_true, beta = 4):
    '''Compute the first derivative for custom logloss function'''
    return predt*(beta-beta*y_true+y_true)-y_true

def second_grad_logreg_beta(predt,y_true, beta = 4):
    '''Compute the second derivative for custom logloss function'''
    predt = np.array(predt)
    return (y_true+beta-beta*y_true)*predt*(1-predt)

def logregobj_beta(predt,y_true):
    beta = 4
    predt = np.array(predt)
    y_true = np.array(y_true)
    beta = np.array(beta)

    '''Custom logloss function update'''
    grad=first_grad_logreg_beta(predt,y_true, beta = beta)
    hess=second_grad_logreg_beta(predt,y_true, beta = beta)
    return grad,hess

def logreg_err_beta(predt,y_true, beta = 4):
    predt = np.array(predt)
    y_true = np.array(y_true)
    '''Custom evaluation metric that should be in line with custom loss function'''
    predt=np.clip(predt,10e-7,1-10e-7)
    loss_fn=y_true*np.log(predt)
    loss_fp=(1.0-y_true)*np.log(1.0-predt)
    return np.sum(-(loss_fn+beta*loss_fp))/len(y)

# Analysis---------------------------------------------------
X, y = load_breast_cancer(return_X_y=True)
clf = lgb.LGBMClassifier(objective = logregobj_beta)
clf.fit(X, y, eval_metric = logreg_err_beta)
calibrated_model = CalibratedClassifierCV(clf, method=&quot;sigmoid&quot;, cv = &quot;prefit&quot;)
calibrated_model.fit(X, y) # Issue !!!!
y_pred_calibrated = calibrated_model.predict_proba(X)[:,1]
</code></pre>
",https://stackoverflow.com/questions/76021202/lightgbm-probabilities-calibration-with-custom-cross-entropy-score-and-loss-func,False,
76021173,"why i get none output , if i use 3 as first_number and any number as second number","<pre><code>f = int(input(&quot;what is your first number? &quot;))
s = int(input(&quot;what is your second number? &quot;))
first_number = f / 2
second_number = round(s / 1)
def function(first_number, second_number):
  if first_number == float or second_number == int:
    return&quot;it is odd and even&quot;


print(function(first_number, second_number))
</code></pre>
<h1>Why i get none when first number is float and second number is int</h1>
",https://stackoverflow.com/questions/76021173/why-i-get-none-output-if-i-use-3-as-first-number-and-any-number-as-second-numb,False,
76021171,Numpy.ndarray contains elements of different type,"<p>It is known that numpy array elements must be of the same type. How to match this statement with the following ndarray.</p>
<pre><code>data = {'Courses' :&quot;pandas&quot;, 'Fees' : 20000, 'Duration' : &quot;30days&quot;}
series = pd.Series(data)
print (series)
</code></pre>
<p>Output
Courses     pandas
Fees         20000
Duration    30days
dtype: object</p>
<pre><code>print (series.values)
print(type(series.values))
</code></pre>
<p>['pandas' 20000 '30days']
&lt;class 'numpy.ndarray'&gt;</p>
<p>How to explain this result?</p>
<p>Of course I will not try to do some math operations with this ndarray</p>
",https://stackoverflow.com/questions/76021171/numpy-ndarray-contains-elements-of-different-type,False,
76021140,python coroutines do not run one by one?,"<p>I have a function which trigger events at certain frequency, and a coroutine task as action.  Below is my implementation (function <code>wrapper</code>), however it seems they run at the same time, what happened?</p>
<pre class=""lang-py prettyprint-override""><code>import time
import asyncio
start = time.time()

def main(callback):
    for i in range(5):
        time.sleep(.5)
        callback()

async def sub_task():
    print(&quot;sub_task&quot;, time.time() - start)


#-----------------------
# below is what i do 
def wrapper():
    asyncio.create_task(sub_task())

main(wrapper)

# output: why don't they run at 0.5 sec interval?
#sub_task 2.57704758644104
#sub_task 2.57704758644104
#sub_task 2.57704758644104
#sub_task 2.5780487060546875
#sub_task 2.5780487060546875
</code></pre>
<hr />
<p>I added some comments if above description might be misleading.
The code is tested in a clean jupyter notebook.
<a href=""https://i.stack.imgur.com/kRQbF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kRQbF.png"" alt=""enter image description here"" /></a></p>
",https://stackoverflow.com/questions/76021140/python-coroutines-do-not-run-one-by-one,True,
76021133,How to output error messages if split userinput is out of range?,"<p>I am building a text based game for a class. This isn't the game; I'm kind of testing the foundation separately. I'm aware of the misspellings and messiness but I'll still take advice on that as I tend to struggle with &quot;style&quot; a lot.</p>
<p>I feel like I'm right on the verge of solving this... There's a dictionary of areas and items and I want the player to have to type &quot;Go&quot; first for a direction or &quot;Get&quot; first if they want to retrieve an Item. This took me so long to figure out and yet once I finally got it I ran into other errors and checks and balances I can't quite seem to wrap my head around.</p>
<p>Right now what works is as follows</p>
<ul>
<li>player typing anything other than go/get, an error message occurs</li>
<li>player typing go/get then a direction or item that doesnt exist, an error message occurs</li>
<li>If the player tries to receive an Item that DOESN'T exist in a room that DOES contain an Item an error message occurs</li>
<li>if the player types <code>exit</code> the program ends</li>
</ul>
<p>This is almost perfect.</p>
<pre class=""lang-py prettyprint-override""><code>while action != 'exit':
    status()
    print(inventory)
    action = input('&gt; ').strip().title()
    splitaction = action.split()
    if splitaction[0] == 'Go'.strip().title():
        if splitaction[1] in rooms[current_room]:
            current_room = rooms[current_room][splitaction[1]]
            print(f'You went {splitaction[1]}')
        elif splitaction[1] not in rooms[current_room]:
            print('u cant do tht')


    if splitaction[0] == 'Get'.strip().title():
        item = rooms[current_room].get('Item')
        if splitaction[1] not in rooms[current_room].get('Item'):
            print('tht item no exists here')
        elif splitaction[1] in rooms[current_room].get('Item'):
            if item not in inventory:
                inventory.append(rooms[current_room].get('Item'))
                print(f'You got the {item}!')
            elif item in inventory:
                print('u alrdy got tht')
    elif action == 'exit'.strip().title():
        print(&quot;You've chosen to end the game, goodbye.&quot;)
        snooze(2)
        break
</code></pre>
<p>Right now my current problems</p>
<ul>
<li>If the player simply types Go/Get and nothing else I get the following error:</li>
</ul>
<pre><code>if splitaction[1] in rooms[current_room]:
       ~~~~~~~~~~~^^^
IndexError: list index out of range
</code></pre>
<ul>
<li>If the player tries to retrieve an item in a room that DOESN'T contain one I get the following error:</li>
</ul>
<pre><code>if splitaction[1] not in rooms[current_room].get('Item'):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: argument of type 'NoneType' is not iterable
</code></pre>
<p>I can't seem to wrap my head around how to fix and print error messages for these occassions? I wouldn't be surprised if something very simple is going over my head, my brain got fried from simply making the Go/Get 'kind of' work.</p>
<p>Thing's I've tried:</p>
<pre><code>if splitaction[1] not in rooms[current_room].get('Item'):
</code></pre>
<p>I expected to get error message since there's no item in that room but instead I get <code>NoneType not iterable</code>. I properly get the error message only if the room has an item but the one typed doesn't exist.</p>
<p>Also tried</p>
<pre><code>if len(splitaction) &lt;= 1
</code></pre>
<p>then print an error message so it could cover the player simply typing go/get but I still get the out of range error</p>
<p>Tried</p>
<pre><code>if splitaction[1] == none
</code></pre>
<p>for the same effect but nothing is working.</p>
<p>I tried to use a <code>try</code>/<code>except</code> but I got the same errors. I don't quite know how it works yet. I'm a complete beginner and we didn't go over that in class yet.</p>
<p>Update: Thanks to 0x01010 I was able to fix the out of range issue but I'm still struggling with the <code>NoneType</code> error:</p>
<pre><code>if splitaction[0] == 'Get'.strip().title():
    item = rooms[current_room].get('Item')
    if getelement(splitaction, 1) not in rooms[current_room]:
        print(&quot;That item doesn't exist&quot;)
    elif item in rooms[current_room].get(&quot;Item&quot;):
        if item not in inventory:
            inventory.append(rooms[current_room].get('Item'))
            print(f'You got the {item}!')
        elif item in inventory:
            print('You already have that item!')
</code></pre>
<p>Right now I get the &quot;Item doesn't exist&quot; even if the item does exist in the current room? If I change the the first nested if statement to <code>rooms[current_room].get('item')</code> it brings me back to the nonetype error if the player tries to receive an item in a room that doesn't have one. It works fine if the room does have an item. I'm almost at the final point where the game is at least playable even if not perfect but I don't want the program to crash simply because the player tries to receive an item in a room that doesn't have one.</p>
",https://stackoverflow.com/questions/76021133/how-to-output-error-messages-if-split-userinput-is-out-of-range,True,
76021110,Python socket.accept() exit directly,"<p>These days I'm learning socket coding for Python, and I'm learning the code of multi-thread socket coding. My code of server is beneath:</p>
<pre><code>import socket
import threading
class Server(object):
    def __init__(self):
        self.g_conn_pool = {}
        self.num = 0      
        self.address = ('', 8022)
        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.server_socket.bind(self.address)
        self.server_socket.listen(128)
    def accept_client(self):
        print(&quot;accept_client\n&quot;)
        while True:
            print('in')
            client_socket, info = self.server_socket.accept()
            print('client name: ' + client_socket.getsockname())
            print(client_socket, info)
            thread = threading.Thread(target=self.recv_msg, args=(client_socket, info))
            thread.daemon = True
            thread.start()
    def recv_msg(self, client, info)
        pass
    def start_new_thread(self):
        thread = threading.Thread(target=self.accept_client, args=())
        thread.daemon = True
        thread.start()
        print(&quot;start_new_thread&quot;)
multi_server = Server()
multi_server.start_new_thread()
</code></pre>
<p>When running the program, it seems that the program always exits at the line <code>client_socket, info = self.server_socket.accept()</code>, because the <code>'client name: '</code> is not printed. In my exception, the server should wait until some clients come to connect it after being started. However, I didn't have the opportunity to start my client program before the server program exiting quickly.</p>
<p>What problem causes this condition? Do I lack any codes or operations? Thanks for your help.</p>
",https://stackoverflow.com/questions/76021110/python-socket-accept-exit-directly,False,
76021108,Filling upper or under some contour curves which are related to different data,"<p>By using <code>ax.contour</code> I plotted three contour curves which are related to three different data and I would like to fill the under and/or upper space of these curves with a specific color. Could you please let me know how I can do this? Here is the code and you can find the regarded fig too.</p>
<pre><code>fig, ax = plt.subplots()

CS1 = ax.contour(X, Y, Z1, [2])
ax.clabel(CS1, inline=True, fontsize=10)

CS2 = ax.contour(X, Y, Z2, [11])
ax.clabel(CS2, inline=True, fontsize=10)

CS3 = ax.contour(X, Y, Z3, [580])
ax.clabel(CS3, inline=True, fontsize=10)


plt.show()
</code></pre>
<p>To clarify more, in fact, Z1, Z2 and Z3 are three different data and I plotted three different contour curves for them (as you can see in the fig), I want to fill upper the Z1 curve with red and under the Z2 and Z3 with blue and green respectively.</p>
<p><a href=""https://i.stack.imgur.com/05iAE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/05iAE.png"" alt=""Three contours for three different data named Z1,Z2 and Z3"" /></a></p>
<p>The filling methods in Matplotlip do not work here for these contour curves. Moreover, using for instance the method, <code>ax.contourf(X, Y, Z1, levels=[2, 3], colors='red', alpha=0.3)</code> does not work since it is useful when you have only one data (Z1), however, here I have three data (Z1,Z2 and Z3) with three contours.</p>
",https://stackoverflow.com/questions/76021108/filling-upper-or-under-some-contour-curves-which-are-related-to-different-data,False,
76021087,"Trading bot ERROR: mexc {&quot;code&quot;:10007,&quot;msg&quot;:&quot;symbol not support api&quot;}","<p>I have this python script and I use mexc API and I want to trade BTC/USDT but when I run the script I get this error ERROR: mexc {&quot;code&quot;:10007,&quot;msg&quot;:&quot;symbol not support api&quot;}</p>
<p>here is the script</p>
<pre><code>import ccxt
import time

# Trading pair and time frame
SYMBOL = 'BTC/USDT'
TIMEFRAME = '1m'
TRADE_SYMBOL = SYMBOL.replace('/', '')

# Trading amount and risk
TRADE_AMOUNT = 0.0025
RISK_PER_TRADE = 0.01
BALANCE = 250

# Exchange API keys
API_KEY = 'api key'
SECRET_KEY = 'secret key'
</code></pre>
<p>I found mexc symbol list and BTC/USDT was there so it should work but it doesn't</p>
",https://stackoverflow.com/questions/76021087/trading-bot-error-mexc-code10007-msgsymbol-not-support-api,False,
76021017,How to Normalise Column of Pandas DataFrame as Part of Preprocessing for Machine Learning?,"<p><strong>Context</strong></p>
<p>I am currently preprocessing my dataset for <code>Machine Learning</code> purposes. Now, I would like to <code>normalise</code> all numeric columns. I found a few solutions but none of them really mimics the behaviour I prefer.</p>
<p>My goal is to have normalised a column in the following way with the lowest value being converted to 0 and the highest to 1:</p>
<hr />
<p><strong>Code</strong></p>
<pre class=""lang-markdown prettyprint-override""><code>     column                  column_normalised
1    10                      0
2    30            -&gt;        1
2    20                      0.5
</code></pre>
<hr />
<p><strong>Question</strong></p>
<ul>
<li>How can I achieve this goal?</li>
<li>Would you also normalise numerically-encoded categorial features or leave them as it?</li>
</ul>
",https://stackoverflow.com/questions/76021017/how-to-normalise-column-of-pandas-dataframe-as-part-of-preprocessing-for-machine,True,
76020971,how collect the AD data,"<p>How do I collect the AD data I receive when I browse the edge Web page,especially the literal text.</p>
<p>I tried to modify the existing adblocking plugin but I don't know if it is feasible.
Hope to provide a feasible idea.Thank you!!!</p>
",https://stackoverflow.com/questions/76020971/how-collect-the-ad-data,False,
76020954,"How to fix the error: operands could not be broadcast together with shapes (100,) (2,)?","<p>I have to plot a diagram that does a curvefitting for the following formula:</p>
<pre><code>2*np.pi*np.sqrt((lx/g)*1+1/16*a**2)
</code></pre>
<p>Problem is that it gives me a matrix and thus the plotting doesn't work anymore if I interpret the following error correctly:</p>
<p>ValueError: operands could not be broadcast together with shapes (100,) (2,)</p>
<p>here is also the code that I have used so far:</p>
<pre><code>from scipy.optimize import curve_fit
import matplotlib.pyplot as plt
import numpy as np
##defining function of mathematical pendulum
my_data =genfromtxt('lundd_3.csv',delimiter=',',skip_header=1)
##defining my variables
lx=my_data[:,0].reshape(1,-1).flatten()
ty=my_data[:,1]
Xerr=my_data[:,2]
Yerr=my_data[:,3]
a=my_data[:,4]

def mathPen(lx,a,g):
##here must be the error because it somehow creates here the matrix but I don't know why??
    return 2*np.pi*np.sqrt((lx/g)*1+1/16*a**2)
Fit=curve_fit(mathPen, lx, ty)
fitParameter = Fit[0]
fehlerMatrix = Fit[1]


##defining variable again
g = fitParameter
##creating new values
xFunk = np.linspace(np.min(lx),np.max(lx),100)
print(xFunk)

yFunk = mathPen(xFunk,a,g)

plt.plot(xFunk,yFunk)
plt.plot(lx,ty,markers='x',linestyle = 'none',color='blue')
plt.show()
</code></pre>
<p>And now follows the errorcode and it shows me that the error must be in the function.
**
So is there a way to rewrite the function so I don't get a matrix or can I somehow reshape the values so that they can be plotted? **</p>
<p>I've tried to reshape the new xFunk Values with the np.reshape function and also have tried it with the flatten function.</p>
",https://stackoverflow.com/questions/76020954/how-to-fix-the-error-operands-could-not-be-broadcast-together-with-shapes-100,False,
76020948,Python class with generic and alternative constructor,"<p>Is there any way how to use <code>Generic</code> in class and at the same time define alternative constructor?</p>
<p>My code look like this</p>
<pre class=""lang-py prettyprint-override""><code>from __future__ import annotations
from typing import TypeVar, TypedDict, Generic, Type

import dataclasses


class Cfg(TypedDict):
    data: int
    foo: str


T = TypeVar(&quot;T&quot;)


@dataclasses.dataclass
class Foo(Generic[T]):
    
    data: T
    
    @classmethod
    def from_string(cls: Type[Foo[str]], text: str) -&gt; Foo[str]:
        return cls(text[1:-1])

    @classmethod
    def from_config(cls: Type[Foo[int]], data: Cfg) -&gt; Foo[int]:
        return cls(data[&quot;data&quot;])


a = Foo.from_string(&quot;sdffd&quot;)
b = Foo.from_config({&quot;data&quot;: 1, &quot;foo&quot;: &quot;sdfd&quot;})
</code></pre>
<p>But I get following error from mypy (strict mode is set for mypy)</p>
<pre><code>main.py:29: error: Invalid self argument &quot;Type[Foo[Any]]&quot; to attribute function &quot;from_string&quot; with type &quot;Callable[[Type[Foo[str]], str], Foo[str]]&quot;  [misc]
main.py:30: error: Invalid self argument &quot;Type[Foo[Any]]&quot; to attribute function &quot;from_config&quot; with type &quot;Callable[[Type[Foo[int]], Cfg], Foo[int]]&quot;  [misc]
</code></pre>
",https://stackoverflow.com/questions/76020948/python-class-with-generic-and-alternative-constructor,False,
76020926,python speech recognition showing error &quot;module recognition request failed: Internal Server Error&quot;,"<p>i was trying to conver voice into text but speech recognisition module throwing internal server error.
this is my code:-</p>
<pre><code>import speech_recognition as s 
sr=s.Recognizer()
print(&quot;i am your script and listening you...........................&quot;)
with s.Microphone() as m:
 audio=sr.listen(m)
 query=sr.recognize_google(audio,language='eng-in')
 print(query)
</code></pre>
<p>im getting error like:-
`i am your script and listening you...........................
Traceback (most recent call last):
File &quot;C:\Users\Believe Itsreal\AppData\Local\Programs\Python\Python310\lib\site-packages\speech_recognition_<em>init</em>_.py&quot;, line 894, in recognize_google
response = urlopen(request, timeout=self.operation_timeout)
File &quot;C:\Users\Believe Itsreal\AppData\Local\Programs\Python\Python310\lib\urllib\request.py&quot;, line 216, in urlopen
return opener.open(url, data, timeout)
File &quot;C:\Users\Believe Itsreal\AppData\Local\Programs\Python\Python310\lib\urllib\request.py&quot;, line 525, in open
response = meth(req, response)
File &quot;C:\Users\Believe Itsreal\AppData\Local\Programs\Python\Python310\lib\urllib\request.py&quot;, line 634, in http_response
response = self.parent.error(
File &quot;C:\Users\Believe Itsreal\AppData\Local\Programs\Python\Python310\lib\urllib\request.py&quot;, line 563, in error
return self._call_chain(*args)
File &quot;C:\Users\Believe Itsreal\AppData\Local\Programs\Python\Python310\lib\urllib\request.py&quot;, line 496, in _call_chain
result = func(*args)
File &quot;C:\Users\Believe Itsreal\AppData\Local\Programs\Python\Python310\lib\urllib\request.py&quot;, line 643, in http_error_default
raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 500: Internal Server Error</p>
<p>During handling of the above exception, another exception occurred:</p>
<p>Traceback (most recent call last):
File &quot;d:\coding\python_in_hole\python code\api.py&quot;, line 6, in 
query=sr.recognize_google(audio,language='eng-in')
File &quot;C:\Users\Believe Itsreal\AppData\Local\Programs\Python\Python310\lib\site-packages\speech_recognition_<em>init</em>_.py&quot;, line 896, in recognize_google
raise RequestError(&quot;recognition request failed: {}&quot;.format(e.reason))
speech_recognition.RequestError: recognition request failed: Internal Server Error
PS D:\coding\python_in_hole&gt; `</p>
<p>i tried to update the module still its not working one of my code on same module was working like a week before but now that code is also not working in this module</p>
",https://stackoverflow.com/questions/76020926/python-speech-recognition-showing-error-module-recognition-request-failed-inte,False,
76020895,get ESG scores from yesg,"<p>I am trying to get total esg scores from 2016-2021 for all companies in S&amp;P500, but not every ticker has esg scores and some of them does not have data for 2016-2021.</p>
<p>My current code look like this:</p>
<pre><code>import yesg
df = pd.DataFrame()
sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]
tickers = sp500['Symbol'].tolist()
for t in tickers:
  data=yesg.get_historic_esg(t)['Total-Score']
  df.append(data)
</code></pre>
<p>Please tell me how to get the data I need. Thanks in advance for your help.</p>
",https://stackoverflow.com/questions/76020895/get-esg-scores-from-yesg,True,76021072
76020879,How to find multiple subsets of one superset in python?,"<p>there is an algorithmic problem. I have several subsets of one super set (there can be n-th number of subsets, depending on the length of the super set). Here is an example:</p>
<pre><code>['/img0.jpg ']
['/img1.jpg']
['/img10.jpg']
['/img11.jpg' '/img9.jpg']
['/img12.jpg' '/img15.jpg']
['/img11.jpg' '/img13.jpg' '/img8.jpg' '/img9.jpg']
['/img14.jpg']
['/img15.jpg']
['/img16.jpg']
['/img14.jpg' '/img17.jpg']
['/img18.jpg']
['/img19.jpg' '/img22.jpg']
['/img2.jpg']
['/img19.jpg' '/img20.jpg' '/img21.jpg' '/img22.jpg']
['/img19.jpg' '/img21.jpg' '/img22.jpg']
['/img22.jpg']
['/img23.jpg']
['/img3.jpg']
['/img3.jpg' '/img4.jpg']
['/img0.jpg' '/img5.jpg']
['/img6.jpg']
['/img2.jpg' '/img7.jpg']
['/img11.jpg' '/img8.jpg' '/img9.jpg']
['/img9.jpg']
</code></pre>
<p>As a result , you should get:</p>
<pre><code>['/img1.jpg']
['/img10.jpg']
['/img11.jpg' '/img13.jpg' '/img8.jpg' '/img9.jpg']
['/img12.jpg' '/img15.jpg']
['/img14.jpg' '/img17.jpg']
['/img16.jpg']
['/img18.jpg']
['/img19.jpg' '/img20.jpg' '/img21.jpg' '/img22.jpg']
['/img2.jpg' '/img7.jpg']
['/img23.jpg']
['/img3.jpg' '/img4.jpg']
['/img0.jpg' '/img5.jpg']
['/img6.jpg']
</code></pre>
<p>Simply put, the task boils down to finding a super set, bypassing all subsets</p>
<p>Please tell me what are the ways and ideas for writing an algorithm?Can be solved without sets optimally?</p>
<p>I have the following code:</p>
<pre><code>dict_dir = {}

compare_keys = compare.keys() #dictionary with keys as input (n-th number of sets with super sets)

for idx, key1 in enumerate(compare_keys):
    array = []
    for key2 in compare_keys:
        if set(compare[key1]).issubset(set(compare[key2])):
            array.extend(list(compare[key2]))
            
    arr = list(set(array))
    
    dict_dir[f'dir{idx}'] = arr
</code></pre>
<p>The following result is obtained:</p>
<pre><code>[['/img0.jpg', '/img5.jpg'],
 ['/img1.jpg'],
 ['/img10.jpg'],
 ['/img11.jpg', '/img13.jpg', '/img8.jpg', '/img9.jpg'],
 ['/img12.jpg', '/img15.jpg'],
 ['/img11.jpg', '/img13.jpg', '/img8.jpg', '/img9.jpg'],
 ['/img14.jpg', '/img17.jpg'],
 ['/img12.jpg', '/img15.jpg'],
 ['/img16.jpg'],
 ['/img14.jpg', '/img17.jpg'],
 ['/img18.jpg'],
 ['/img19.jpg', '/img20.jpg', '/img21.jpg', '/img22.jpg'],
 ['/img2.jpg', '/img7.jpg'],
 ['/img19.jpg', '/img20.jpg', '/img21.jpg', '/img22.jpg'],
 ['/img19.jpg', '/img20.jpg', '/img21.jpg', '/img22.jpg'],
 ['/img19.jpg', '/img20.jpg', '/img21.jpg', '/img22.jpg'],
 ['/img23.jpg'],
 ['/img3.jpg', '/img4.jpg'],
 ['/img3.jpg', '/img4.jpg'],
 ['/img0.jpg', '/img5.jpg'],
 ['/img6.jpg'],
 ['/img2.jpg', '/img7.jpg'],
 ['/img11.jpg', '/img13.jpg', '/img8.jpg', '/img9.jpg'],
 ['/img11.jpg', '/img13.jpg', '/img8.jpg', '/img9.jpg']]
</code></pre>
<p>It works a little incorrectly and not optimally, I would like the work of such an algorithm to be minimally spent on memory and time.</p>
",https://stackoverflow.com/questions/76020879/how-to-find-multiple-subsets-of-one-superset-in-python,True,
76020838,"Find all possible sums of the combinations of sets of integers, efficiently","<p>I have an algorithm that finds the set of all unique sums of the combinations of k tuples drawn with replacement from of a list of tuples. Each tuple contains n positive integers, the order of these integers matters, and the sum of the tuples is defined as element-wise addition. e.g. (1, 2, 3) + (4, 5, 6) = (5, 7, 9)</p>
<p>Simple example for k=2 and n=3:</p>
<pre><code>input = [(1,0,0), (2,1,1), (3,3,2)]  
solution = [(1,0,0)+(2,1,1), (1,0,0)+(3,3,2), (2,1,1)+(3,3,2), (1,0,0)+(1,0,0), (2,1,1)+(2,1,1), (3,3,2)+(3,3,2)]  
solution = [(3,1,1), (4,3,2), (5,4,3), (2,0,0), (4,2,2), (6,6,4)]
</code></pre>
<p>In practice the integers in the tuples range from 0 to 50 (in some positions it may be a lot more constraint, like [0:2]), k goes up to 4 combinations, and the length of the tuples goes up to 5. The number of tuples to draw from goes up to a thousand.</p>
<p>The algorithm I currently have is an adaptation of an algorithm proposed in a <a href=""https://stackoverflow.com/questions/75800558/find-all-possible-sums-of-the-combinations-of-integers-from-a-set-efficiently"">related question</a>, it's more efficient then enumerating all combinations with itertools (if we're drawing 4 tuples out of 1000, there are billions of combinations, but the number of sums will be orders of magnitude less), but I don't see how to apply bitsets for example to this problem.</p>
<pre><code># example where length of tuples n = 3:
lst = []
for x in range(0,50,2):
    for y in range(0, 20, 1):
        for z in range(0, 3, 1):
            lst.append((x,y,z))

# this function works for any k and n
def unique_combination_sums(lst, k):
    n = len(lst[0])
    sums = {tuple(0 for _ in range(n))}  # initialize with tuple of zeros
    for _ in range(k):
        sums = {tuple(s[i]+x[i] for i in range(n)) for s in sums for x in lst}
    return sums

unique_combination_sums(lst, 4)
</code></pre>
",https://stackoverflow.com/questions/76020838/find-all-possible-sums-of-the-combinations-of-sets-of-integers-efficiently,True,
76020832,raise RuntimeError( RuntimeError: Model class django.contrib.contenttypes.models.ContentType,"<p>its my installed_apps</p>
<p>from pathlib import Path
from . import config
import django
django.setup()
import os
BASE_DIR = Path(<strong>file</strong>).resolve().parent.parent</p>
<h1>SECURITY WARNING: keep the secret key used in production secret!</h1>
<p>SECRET_KEY = 'django-insecure-3hs(_o73c=i#c!s(m4yr@c)4k-3=&amp;381r(y6+qy(8b=4p1@l)l'</p>
<p>DEBUG = True</p>
<p>ALLOWED_HOSTS = []</p>
<h1>Application definition</h1>
<p>INSTALLED_APPS = [
'rest_framework_simplejwt',
'django.contrib.admin',
'django.contrib.auth',
'django.contrib.contenttypes',
'django.contrib.sessions',
'django.contrib.messages',
'django.contrib.staticfiles',
'rest_framework',
'corsheaders',
'users',
'posts',
'settings',
]</p>
<p>File &quot;/Users/macbook_air/Desktop/ReactRoute/users/models.py&quot;, line 2, in 
from django.contrib.auth.models import AbstractUser,Group,Permission
File &quot;/Users/macbook_air/Desktop/ReactRoute/venv/lib/python3.11/site-packages/django/contrib/auth/models.py&quot;, line 5, in 
from django.contrib.contenttypes.models import ContentType
File &quot;/Users/macbook_air/Desktop/ReactRoute/venv/lib/python3.11/site-packages/django/contrib/contenttypes/models.py&quot;, line 139, in 
class ContentType(models.Model):
File &quot;/Users/macbook_air/Desktop/ReactRoute/venv/lib/python3.11/site-packages/django/db/models/base.py&quot;, line 134, in <strong>new</strong>
raise RuntimeError(
RuntimeError: Model class django.contrib.contenttypes.models.ContentType doesn't declare an explicit app_label and isn't in an application in INSTALLED_APPS.</p>
",https://stackoverflow.com/questions/76020832/raise-runtimeerror-runtimeerror-model-class-django-contrib-contenttypes-models,False,
76020824,Markers not updating on map in PyQt5 application with QTimer,"<p>I'm building a user interface, that put some random values in some graph and add some markers in a map randomly. This is the code:</p>
<pre><code>from PyQt5.QtWidgets import QApplication, QMainWindow, QWidget, QGridLayout, QHBoxLayout
from PyQt5.QtCore import QTimer
from classes.MapViewWidget import MapView
from classes.RawDataWidget import RawDataWidget
from classes.TemperatureGraphWidget import TemperatureGraph
from classes.ArduinoThread import ArduinoThread
import random


class MainWindow(QMainWindow):
    def __init__(self, *args, **kwargs):
        super(MainWindow, self).__init__(*args, **kwargs)
        self.setWindowTitle(&quot;Ground Control&quot;)

        # Create the widgets for the temperature and pressure graphs, the map, and the raw data
        self.map_view = MapView(self)
        self.temperature_graph = TemperatureGraph(self)
        self.pressure_graph = TemperatureGraph(self)
        self.raw_data_view = RawDataWidget(self)

        # Create a horizontal layout for the temperature and pressure graphs
        temperature_pressure_layout = QHBoxLayout()
        temperature_pressure_layout.addWidget(self.temperature_graph)
        temperature_pressure_layout.addWidget(self.pressure_graph)

        temperature_pressure_layout.setSpacing(0)  # Set spacing to zero

        # Create layout for all the widgets
        grid_layout = QGridLayout()

        # Every widget take up 1/2 of the raw
        grid_layout.setColumnStretch(0, 2)
        grid_layout.setColumnStretch(1, 2)

        grid_layout.addWidget(self.temperature_graph, 0, 0, 1, 1)
        grid_layout.addWidget(self.map_view, 0, 1, 1, 1)
        grid_layout.addWidget(self.pressure_graph, 1, 0, 1, 1)
        grid_layout.addWidget(self.raw_data_view, 1, 1, 1, 1)

        grid_layout.setSpacing(0)  # Set spacing to zero
        grid_layout.setContentsMargins(0, 0, 0, 0)  # Set margins to zero

        # Create a central widget and set the grid layout
        central_widget = QWidget()
        central_widget.setLayout(grid_layout)
        self.setCentralWidget(central_widget)

        # set timer of 1 sec
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_data)
        self.timer.start(1000)

    def update_data(self):

        latitude = random.uniform(12.0, 13.0)
        longitude = random.uniform(77.0, 78.0)
        self.map_view.add_marker(latitude, longitude)

        # update graph values randomly
        raw_text = &quot;&quot;
        for i in range(0, 10):
            rnd_num = random.randint(10, 24)
            raw_text += &quot;Temperature value: &quot; + \
                str(rnd_num) + &quot;°C --- Time: &quot; + str(i) + &quot;s\n&quot;
            self.temperature_graph.update(rnd_num, i)
            self.raw_data_view.set_data(raw_text)


if __name__ == &quot;__main__&quot;:
    app = QApplication([])
    window = MainWindow()
    window.show()
    app.exec_()
</code></pre>
<p>This is the code for the MapView class:</p>
<pre><code>from PyQt5.QtCore import QUrl
from PyQt5.QtWebEngineWidgets import QWebEngineView
from PyQt5.QtWebChannel import QWebChannel


class MapView(QWebEngineView):
    def __init__(self, parent=None):
        super(MapView, self).__init__(parent)
        self.web_channel = QWebChannel(self.page())
        self.page().setWebChannel(self.web_channel)

        # Load the HTML page
        self.load(QUrl.fromLocalFile(
            &quot;./ground_control/map/map.html&quot;))

        # Wait for the page to finish loading before initializing the map
        self.loadFinished.connect(self.initialize_map)

    def initialize_map(self):
        self.page().runJavaScript(&quot;initializeMap();&quot;)
    
    #add the marker to the map by calling js function
    def add_marker(self, latitude, longitude):
        self.page().loadFinished.connect(lambda: self.page().runJavaScript(
            f&quot;addMarker({latitude}, {longitude});&quot;))
</code></pre>
<p>The problem is that this code is not adding the markers to the map, however I tried changing the code in this way, replacing:</p>
<pre><code>        # set timer of 1 sec
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_data)
        self.timer.start(1000)
</code></pre>
<p>with:</p>
<pre><code>        self.update_data()
</code></pre>
<p>and it works fine, I don't understand why. What can I do to resolve the problem?</p>
",https://stackoverflow.com/questions/76020824/markers-not-updating-on-map-in-pyqt5-application-with-qtimer,True,76028926
76020786,"How to scale the y-axis in matplotlib, starting from zero","<p>I am trying to plot my image with a pixel indexing technique where only the white pixel is shown. The code can be seen below:</p>
<pre><code>img = cv2.imread('img.png')
img = cv2.resize(img, (180, 119))
arr0 = np.array(img)

height, width, _ = arr0.shape

x = range(width)
y = [height - np.argwhere(arr0[:, i, 0]==255).mean() for i in x]

plt.plot(y)
plt.show()
</code></pre>
<p>the result is:   [1]: <a href=""https://i.stack.imgur.com/5Bjnu.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/5Bjnu.png</a></p>
<p>but I want to make the y-axis start from zero not from 63 like in the image shown.</p>
<p>I've tried with <code>np.tan(height)</code> but the result is not what I expected. Any ideas?</p>
<p>UPDATE!! I SIMPLY KNOW HOW TO SET THE Y-AXIS TO ZERO!</p>
<pre><code>img = cv2.imread('img.png')
img = cv2.resize(img, (180, 119))
arr0 = np.array(img)

height, width, _ = arr0.shape

x = range(width)
y = [63.5 - np.argwhere(arr0[:, i, 0]==255).mean() for i in x] #Just change the height into the value from the start

plt.plot(y)
plt.show()
</code></pre>
<p>thanks all!!</p>
",https://stackoverflow.com/questions/76020786/how-to-scale-the-y-axis-in-matplotlib-starting-from-zero,False,
76020762,Pyparsing: how to match parentheses around comma_separated_list,"<p>I cannot figure out how to combine expressions with the <a href=""https://pyparsing-docs.readthedocs.io/en/latest/pyparsing.html#pyparsing.pyparsing_common.comma_separated_list"" rel=""nofollow noreferrer"">comma_separated_list</a> in order to match a list in parentheses. The following does not work, because the csv expression eats up the last parenthesis:</p>
<pre><code>import pyparsing as pp

rule = &quot;{&quot; + pp.common.comma_separated_list + &quot;}&quot;
rule.parse_string(&quot;{something}&quot;)

# &gt;&gt;&gt; ParseException: Expected '}', found end of text
</code></pre>
<p>The values in the list can contain parentheses as well so the rule must match only the outer ones. I guess [1:-1] solves it, but it's not the solution I'm looking for.</p>
",https://stackoverflow.com/questions/76020762/pyparsing-how-to-match-parentheses-around-comma-separated-list,True,
76020734,val_accuracy and val_loss not visible during model training even after adding validation data,"<p>I am training a ResNet 50 model to classify images as Covid infected or normal. While training, the only metrics that are shown are training accuracy and loss. The val_accuracy and val_loss are not visible in any epochs. I have passed the validation data but it's still not showing up.</p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)
print('The train Data Shape ', X_train.shape[0])
 
X_test, X_valid, y_test, y_valid = train_test_split(X_test,y_test, test_size = 0.5)
print('The validation Data Shape ', X_valid.shape[0])
print('The test Data Shape ', X_test.shape[0])

</code></pre>
<pre><code>lr = 1e-3
epochs = 50
bs = 8
optimizer = Adam(learning_rate = lr, decay= lr/epochs)
model.compile(optimizer, loss='binary_crossentropy', metrics=['accuracy'])

epochs = 100

#initialize the training data augmentation object
train_datagen = ImageDataGenerator(
        rotation_range=15,
        fill_mode =&quot;nearest&quot;)
metric = &quot;accuracy&quot;
 
checkpointer = ModelCheckpoint(filepath = &quot;./Model/CDX_Best_RestNet50.h5&quot;, save_best_only = True, monitor = metric , verbose=1)
start = time.time()
 
# let's get started !
 
history=model.fit(train_datagen.flow(X_train, y_train, batch_size = bs),
                            steps_per_epoch = len(X_train)//bs,
                            validation_data = (X_valid, y_valid),
                            validation_steps = len(X_valid)//bs,
                            epochs =epochs,
                            callbacks= [checkpointer])
 
end = time.time()
duration = end - start
print ('\n This Model took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration/60, epochs) )

</code></pre>
<p><strong>Also, this is the shape of X_train: The train Data Shape (224, 224, 3)</strong></p>
<p><strong>The validation metrics are visible only for the first epoch</strong>: <a href=""https://i.stack.imgur.com/Ua8sW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ua8sW.jpg"" alt=""Error"" /></a></p>
<hr />
",https://stackoverflow.com/questions/76020734/val-accuracy-and-val-loss-not-visible-during-model-training-even-after-adding-va,False,
76020713,Python 3 How to remove item from room after adding it to inventory?,"<p>I'm not sure what I did incorrectly, (still very much a beginner as far as coding goes), but the <code>del rooms[currm]['Item']</code> line doesn't actually delete the item. I followed a guide for most of this but the one I used didn't do a delete item bit. So any help will be appreciated. Using Python 3.</p>
<pre><code> item &amp; direction declaration
item = None
direction = 'direction'


def instructions():
    # print main menu and the commands
    print('\n' + ('\t' * 18) + f'''Evil Sorceress Text Adventure Game\n
                  Collect all 6 items to win the game, or be destroyed by the Evil Sorceress, Daeris.
                  Move commands: 'Go {direction}' (North, South, East, West)
                  Add to Inventory: Get {'Item'}\n\n''')


def prologue():  # story set up
    print('\t\tThe Evil Sorceress, Thalya, has taken over the castle of a neighboring kingdom for her Master:')
    print('\n' + ('\t' * 10) + '\033[1m' + 'The Ultimate Evil.' + '\033[0m\n')
    print('''\t\tIt’s up to you to defeat her and reclaim it for the forces of Good! You’ll enter via the Gatehouse.
        You must Get some Armor from the Guard Room for melee protection.
        An Amulet for magical protection from the Enchanting Room.
        A Spellbook from the Library to take down her magical shield.
        A Torch from the Courtyard to light the room.
        A Sword from the Armory to defeat her.
        And finally a Turkey Leg from the Great Hall because fighting evil on an empty stomach is never a good idea!'''
          '\n')


# room, item and direction assignments
rooms = {'Throne Room': {'South': 'Great Hall', 'Boss': 'Daeris'},
         'Great Hall': {'North': 'Throne Room', 'West': 'Library', 'South': 'Courtyard', 'East': 'Armory',
                        'Item': 'Turkey Leg'},
         'Library': {'East': 'Great Hall', 'Item': 'Spellbook'},
         'Courtyard': {'North': 'Great Hall', 'South': 'Gatehouse', 'East': 'Enchanting Room', 'Item': 'Torch'},
         'Armory': {'West': 'Great Hall', 'South': 'Enchanting Room', 'Item': 'Sword'},
         'Enchanting Room': {'North': 'Armory', 'West': 'Courtyard', 'Item': 'Amulet'},
         'Gatehouse': {'North': 'Courtyard', 'East': 'Guard House', },
         'Guard House': {'West': 'Gatehouse', 'Item': 'Armor'},
         }

inv = []  # assigns inventory variable
vowels = ['a', 'e', 'i', 'o', 'u']  # List of vowels
currm = 'Gatehouse'  # Assigns starting room as the Gatehouse
pin = ''  # Returns results of previous input

# prints prologue &amp; instructions
prologue()
input('\n Press any key to continue\n\n')
instructions()

# gameplay loop
while True:
    print(f'\nCurrent Position: {currm}\nInventory : {inv}\n{&quot;~&quot; * 27}')
    print(pin)

    #  Item pickup prompt
    if 'Item' in rooms[currm].keys():
        nitem = rooms[currm]['Item']
        if nitem not in inv:
            if nitem[0] not in vowels:
                print(f&quot;There's an {nitem} here. Take it?\n&quot;)
            else:
                print(f&quot;There's a {nitem} here. Take it?\n&quot;)

    if 'Boss' in rooms[currm].keys():  # Game End parameters
        if len(inv) == 6:
            print('''Congratulations!
            You've reached the Throne Room and defeated the Evil Sorceress, Daeris.
            And have therefore reclaimed the castle for the forces of Good!!''')
            break
        else:
            print('Daeris has defeated you and The Ultimate Evil is victorious!')
            break

    # Accepts player's move as input
    uin = input('Enter your move:\n')

    # Splits move into words
    move = uin.split(' ')

    # First word is the action
    action = move[0].title()

    # Second word is object or direction
    if len(move) &gt; 1:
        item = move[1:]
        direction = move[1].title()
        item = ' '.join(item).title()

        # Moving between rooms
    if action == 'Go':
        try:
            currm = rooms[currm][direction]
            pin = f'You head {direction}\n'
        except:
            pin = 'INVALID DIRECTION!\n'

            # Picking up items
    elif action == 'Get':
        try:
            if item == rooms[currm]['Item']:
                if item not in inv:
                    inv.append(rooms[currm]['Item'])
                    if item == ['Turkey Leg']:
                        pin = f'{item} obtained! Om nom nom.\n'
                    else:
                        pin = f'{item} obtained!\n'
                else:
                    del rooms[currm]['Item']  # deletes item if already obtained
            else:
                print('No item here')
        except:
            pin = f'That {item} is located elsewhere'
</code></pre>
<p>Initially picking up the item the result is:</p>
<pre><code>Enter your move:
get armor

Current Position: Guard House
Inventory : ['Armor']
~~~~~~~~~~~~~~~~~~~~~~~~~~~
Armor obtained!
</code></pre>
<p>However if I try and pick up the item again it acts like I never picked it up to begin with and returns the same result. I'm not 100% on what I expected to happen is but basically this is the result I'm wanting.</p>
<pre><code>Enter your move:
get armor

No item here

</code></pre>
<p>I tried moving this back to be in line with <code>if item == rooms[currm]['Item']:</code> to see if that was the issue but that didn't work either.</p>
<pre><code>else:
                    del rooms[currm]['Item']  # deletes item if already obtained
</code></pre>
",https://stackoverflow.com/questions/76020713/python-3-how-to-remove-item-from-room-after-adding-it-to-inventory,True,76021330
76020709,Detecting paragraphs in a PDF,"<p>How can I detect different &quot;blocks&quot; of text extracted from a PDF to split them into paragraphs? Could I try to use to use their position to do this?</p>
<p>PyMuPDF only puts one newline character between the blocks, and also one newline after one of the lines, making it not possible to distinguish between a separate block and a new line.</p>
<p><a href=""https://i.stack.imgur.com/ItrbC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ItrbC.png"" alt=""enter image description here"" /></a></p>
",https://stackoverflow.com/questions/76020709/detecting-paragraphs-in-a-pdf,False,
76020698,"Is it possible to create an online Python program which will process data on an online csv file, and then create a downloadable output file?","<p>I am attempting to create a Python program on the &quot;Programiz Python Online Compiler&quot; (<a href=""https://www.programiz.com/python-programming/online-compiler/"" rel=""nofollow noreferrer"">https://www.programiz.com/python-programming/online-compiler/</a>) which will read an online csv file, perform data processing on it using <code>pandas</code>, and then generate an output csv file with the results. Below is my attmept:</p>
<pre><code>import pandas as pd
import io

# define the input file name
input_file = 'example_online_file_name'

# read the input csv file into a Pandas DataFrame
df = pd.read_csv(input_file)

# calculate the average of each column
avg_unit = df['unit'].mean()
avg_cost = df['cost'].mean()
# create a new DataFrame with the average values
avg_df = pd.DataFrame({'average unit': [avg_unit], 'average cost': [avg_cost]})

# write the new DataFrame to a string buffer
output_buffer = io.StringIO()
avg_df.to_csv(output_buffer, index=False)

# convert the string buffer to bytes
output_bytes = output_buffer.getvalue().encode('utf-8')

# return the bytes with the appropriate content type and headers for download
print(&quot;Content-Type: application/octet-stream&quot;)
print(&quot;Content-Disposition: attachment; filename=output.csv&quot;)
print(&quot;Content-Length:&quot;, len(output_bytes))
print()
print(output_bytes.decode('utf-8'))
</code></pre>
<p>However, I get an array of errors in the shell of the online compiler (too many to fit here concisely). What am I doing wrong? Is it even possible?</p>
",https://stackoverflow.com/questions/76020698/is-it-possible-to-create-an-online-python-program-which-will-process-data-on-an,False,
76020687,Pythonic way to alter keyboard key characters,"<p>I want to build google inputs like offline app with python.</p>
<p>I want detect keyboard key press with python and alter the key pressed value to some other values and insert the altered character to screen. (input field (Notepad)).</p>
<p>How do insert value to input field with python like a virtual keyboard inputs?</p>
<p>Example:</p>
<p>If i pressed a -&gt; m will be shown in input field (Notepad or ..)
likewise b -&gt; d ,c -&gt; g, e -&gt; u, etc...</p>
",https://stackoverflow.com/questions/76020687/pythonic-way-to-alter-keyboard-key-characters,False,
76020684,Why is the output of this problem in Python different from the calculator?,"<p>Why, when printing the result of this mathematical problem, is the result different from the calculator?
Issue:</p>
<pre><code>print((1+1*0.005)*100)
</code></pre>
<p>The output in a regular calculator is:
100.5</p>
<p>While the output in Python is:
100.49999999999999</p>
",https://stackoverflow.com/questions/76020684/why-is-the-output-of-this-problem-in-python-different-from-the-calculator,False,
76020662,Creating a rule in outlook from python 3.10 to move to a different folder,"<p>I have created a function that uses the win32com library to create custom rules instead of creating them manually. I was not able to create the rule in outlook at first, so I tried to remove as much conditions as possible to find the error. This is the remaining Code snippet for this task, that has only a sender address condition and a moving to a folder action.</p>
<pre><code>import win32com.client

def find_or_create_folder(parent_folder, folder_name):
    for folder in parent_folder.Folders:
       if folder.Name == folder_name:
       return folder
     return parent_folder.Folders.Add(folder_name)

def create_rule(subject, sent_to, sender_address, from_address, importance, folder_name,     alert_text, mark_as_read, category):

    try:
        outlook = win32com.client.GetActiveObject(&quot;Outlook.Application&quot;)
    except:
        outlook = win32com.client.Dispatch(&quot;Outlook.Application&quot;)

    namespace = outlook.GetNamespace(&quot;MAPI&quot;)
    inbox = namespace.GetDefaultFolder(6)  # 6 corresponds to the Inbox folder
    target_folder = find_or_create_folder(inbox, folder_name)

    rules = namespace.DefaultStore.GetRules()

    # Create a new rule
    # 0 corresponds to the olRuleReceive type
    rule = rules.Create(&quot;MyCustomRule&quot;, 0)

    # Set conditions
    if sender_address:
       sender_address_condition = rule.Conditions.SenderAddress
       sender_address_condition.Enabled = True
       sender_address_condition.Address = [sender_address]

    #check if foldername exists and then create the rule to move to a specific folder
    if folder_name:
       move_action = rule.Actions.MoveToFolder
       move_action.Enabled = True
       move_action.Folder = target_folder

   # Save the rule
    rules.Save()  # True to show progress dialog

subject = &quot;&quot;
sent_to = &quot;&quot;
sender_address = &quot;example@gmail.com&quot;
from_address = &quot;&quot;
importance = None
folder_name = &quot;Test&quot;
alert_text = &quot;&quot;
mark_as_read = False
category = &quot;&quot;

create_rule(subject, sent_to, sender_address, from_address,
importance, folder_name, alert_text, mark_as_read, category)
</code></pre>
<p>When I remove the code snippet below, the code works and the rule is created in outlook with the sender condition:</p>
<pre><code>#check if foldername exists and then create the rule to move to a 
#specific folder     
if folder_name:         
   move_action = rule.Actions.MoveToFolder         
   move_action.Enabled = True         
   move_action.Folder = target_folder`
</code></pre>
<p>However I want to be able to move to a different folder, but this doesn't work.</p>
<p>I tried removing all the conditions to see what is the source of the problem, and it boiled down to moving to a different folder.</p>
<p>I tried creating the folder in a different way:</p>
<pre><code> try:          
     target_folder = inbox.Folders(folder_name)      
 except:          
     target_folder = inbox.Folders.Add(folder_name)`
</code></pre>
<p>But the problem persists, and it is not in creating the folder, because the code successfully creates the folder. The problem is precisely in
trying to move the email to this created folder.</p>
<p>The error message is the following:</p>
<p><code>Traceback (most recent call last): File &quot;c:\Users\houss\Desktop\scripts\mooreEmailRules.py&quot;, line 136, in &lt;module&gt; create_rule(subject, sent_to, sender_address, from_address, File &quot;c:\Users\houss\Desktop\scripts\mooreEmailRules.py&quot;, line 113, in create_rule rules.Save()  # True to show progress dialog File &quot;&lt;COMObject GetRules&gt;&quot;, line 2, in Save pywintypes.com_error: (-2147352567, 'Exception occurred.', (4096, 'Microsoft Outlook', 'One or more rules cannot be saved because of invalid actions or conditions.', None, 0, -2147467259), None)</code></p>
<p>I also wasn't able to find proper documentation for this library in python and I am still new to this com thing.</p>
",https://stackoverflow.com/questions/76020662/creating-a-rule-in-outlook-from-python-3-10-to-move-to-a-different-folder,False,
76020661,Discord.py Cogs “Command [ ] is not found”,"<p>I am recoding my discord.py bot using cogs as it wasnt very nice before.</p>
<p>I tried this code:</p>
<pre><code>import discord
import os
import keep_alive
from discord.ext import commands
from discord.ext.commands import BucketType, cog, BadArgument, command, cooldown


intents = discord.Intents().all()
bot = commands.Bot(command_prefix=',', intents=intents)
p = ','



class General(commands.Cog):
    def __init__(self, bot):
        self.bot = bot
  
    @commands.command()
    async def test(self, ctx):
        await ctx.channel.send(&quot;I'm **alive**.&quot;)

    @commands.command()
    async def ping(self, ctx):
        await ctx.send(f'⏱|** {round(bot.latency * 1000)} ms** Latency!')
        await bot.add_cog(General(bot))

if __name__ == '__main__':
    keep_alive.keep_alive()
bot.run(os.environ['token'])
</code></pre>
<p>But I get an error that says “Command ‘ping’ is not found”. What is wrong with the code?</p>
",https://stackoverflow.com/questions/76020661/discord-py-cogs-command-is-not-found,False,
76020652,Android build for QT C++ application that has python call,"<p>I have an QT C++ application that calls a python script. My application is working good when I creates the Executable in Ubuntu platform. However, i could not create the Andriod build for the application and it creates the below Error.</p>
<p>Environment: QT5.15.2 and Ubuntu20.04.</p>
<p>I have added the below lines in the Project file and I have added #include &lt;python.h&gt; in the CPP file where it calls the Python script.</p>
<pre><code>INCLUDEPATH = /usr/include/python3.8
LIBS += -lpython3.8
</code></pre>
<p>Error</p>
<blockquote>
<p>/usr/include/python3.8/pyconfig.h:15: error:
'arm-linux-gnueabi/python3.8/pyconfig.h' file not found In file
included from
/home/vinoth/qgroundcontrol_UI/src/QmlControls/AWSOperations.cpp:6: In
file included from /usr/include/python3.8/Python.h:8:
/usr/include/python3.8/pyconfig.h:15:12: fatal error:
'arm-linux-gnueabi/python3.8/pyconfig.h' file not found include
&lt;arm-linux-gnueabi/python3.8/pyconfig.h&gt;</p>
</blockquote>
<p>Please suggest how to build android build in this scenario</p>
",https://stackoverflow.com/questions/76020652/android-build-for-qt-c-application-that-has-python-call,False,
76020646,Python Planetscale DB MySQL Connection,"<p>I have been trying to make a Python program that connects to a Planetscale MySQL DB, I have used 2 libraries that are mysql-connector-python and mysqlclient.</p>
<p>I think I have entered the correct details every time with both but it hasn't worked.</p>
<p>I tried Planetscale's recommended way which is the following (it didn't work for me btw) :</p>
<pre><code>import MySQLdb # pip install mysqlclient

# This is planetscale's copy/paste method to connect to the DB

conn = MySQLdb.connect(
  host=[HOST],
  user=[USER],
  passwd=[PASSWORD],
  db=[DATABASE],
  ssl_mode = &quot;VERIFY_IDENTITY&quot;,
  ssl      = {
    &quot;ca&quot;: &quot;/etc/ssl/cert.pem&quot;
  }
)
</code></pre>
<p>This is the current code I'm using</p>
<pre><code>import mysql.connector # pip install mysql-connector-python

config = {
    &quot;user&quot;: hidden for security purposes,
    &quot;password&quot;: hidden for security purposes,
    &quot;host&quot;: hidden for security purposes,
    &quot;database&quot;: hidden for security purposes,
    &quot;ssl_verify_identity&quot;: True,
    &quot;ssl_ca&quot;: &quot;/etc/ssl/cert.pem&quot;,
}
conn = mysql.connector.connect(**config)

cursor = conn.cursor()

cursor.execute(&quot;&quot;&quot;CREATE TABLE IF NOT EXISTS qr_table (
    type VARCHAR(255),
    date VARCHAR(255),
    path VARCHAR(255),
    unique_id VARCHAR(255)
)&quot;&quot;&quot;)

conn.commit()
cursor.close()
conn.close()
</code></pre>
<p>This is the error I'm getting</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\Axelr\Python coding\lib\site-packages\mysql\connector\connection_cext.py&quot;, line 268, in _open_connection
    self._cmysql.connect(**cnx_kwargs)
_mysql_connector.MySQLInterfaceError: SSL connection error: SSL_CTX_set_default_verify_paths failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;C:\Users\Axelr\PycharmProjects\PC01\main\Special Main\ARC QR Generator\SQL Script.py&quot;, line 26, in &lt;module&gt;
    conn = mysql.connector.connect(**config)
  File &quot;C:\Users\Axelr\Python coding\lib\site-packages\mysql\connector\pooling.py&quot;, line 286, in connect
    return CMySQLConnection(*args, **kwargs)
  File &quot;C:\Users\Axelr\Python coding\lib\site-packages\mysql\connector\connection_cext.py&quot;, line 101, in __init__
    self.connect(**kwargs)
  File &quot;C:\Users\Axelr\Python coding\lib\site-packages\mysql\connector\abstracts.py&quot;, line 1108, in connect
    self._open_connection()
  File &quot;C:\Users\Axelr\Python coding\lib\site-packages\mysql\connector\connection_cext.py&quot;, line 273, in _open_connection
    raise get_mysql_exception(
mysql.connector.errors.InterfaceError: 2026 (HY000): SSL connection error: SSL_CTX_set_default_verify_paths failed

Process finished with exit code 1
</code></pre>
<p>Can someone please help me make a connection that works?</p>
<p>Also, it wouldn't be a problem to use another library to connect.</p>
<p>[Python 3.10]</p>
",https://stackoverflow.com/questions/76020646/python-planetscale-db-mysql-connection,True,76027144
76020626,AttributeError: &#39;DataFrame&#39; object has no attribute &#39;append&#39; for DataFrame,"<p>I am trying to create a DataFrame object for my spam classifier.It's supposed to contain two columns: 'messages' and 'class'. However when I use the <code>dataframe.append</code> function to add emails as 'messages' to my dataframe along with the folder name as 'class', I'm getting this error:</p>
<blockquote>
<p>AttributeError: 'DataFrame' object has no attribute 'append'</p>
</blockquote>
<p>For this I initially created a Dataframe as follow
<code>data = DataFrame({'message': [], 'class': []})</code></p>
<p>I tried to use the DataFrame.append() function for adding the spam and ham emails to the DataFrame. Here's the code I am using:</p>
<pre><code>data = DataFrame({'message': [], 'class': []})

data = data.append(dataFrameFromDirectory('D:\email_classifier\spam', 'spam'))
data = data.append(dataFrameFromDirectory('D:\email_classifier\ham', 'ham'))
</code></pre>
<p>In theory, this should add the emails and the folder name to data.
Is there a way to get around this without having to use an older version of pandas?</p>
",https://stackoverflow.com/questions/76020626/attributeerror-dataframe-object-has-no-attribute-append-for-dataframe,True,
76020620,How to get length of list from a json reponse from API request in python?,"<p>I'm trying to get json response from API get method and get a length of a list names <code>items</code> in the response.</p>
<p>This the API json reponse</p>
<pre><code>{
    &quot;identifier&quot;: &quot;id&quot;,
    &quot;items&quot;: [
        {
            &quot;deployPending&quot;: &quot;NONE&quot;,
            &quot;networkResource&quot;: {
                &quot;type&quot;: &quot;ManagedNetworkElement&quot;,
                &quot;id&quot;: 4424479,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.foundation.encapsulatedFunctionality.ManagedNetworkElement&quot;,
                &quot;url&quot;: &quot;../../ManagedNetworkElement/4424479&quot;
            },
            &quot;bridgeGroupName&quot;: &quot;Provisioned&quot;,
            &quot;networkResourceUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476198/related/networkResource&quot;,
            &quot;shortMaintenanceName&quot;: &quot;18&quot;,
            &quot;cfmServiceType&quot;: &quot;IEEE&quot;,
            &quot;megId&quot;: &quot;EVC18&quot;,
            &quot;shortMaintenanceNameType&quot;: &quot;UNSIGNED_INT16&quot;,
            &quot;bridgeDomainName&quot;: &quot;dppevplan&quot;,
            &quot;isInterfaceStatusTlvIncluded&quot;: true,
            &quot;isPortStatusTlvIncluded&quot;: true,
            &quot;maxMEPs&quot;: 0,
            &quot;peerMepAgingPeriod&quot;: 0,
            &quot;transmissionPeriod&quot;: &quot;_1_SEC&quot;,
            &quot;ethMDUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476198/related/ethMD&quot;,
            &quot;ethMD&quot;: {
                &quot;type&quot;: &quot;EthMaintenanceDomainSettings&quot;,
                &quot;id&quot;: 4478604,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.cfm.common.EthMaintenanceDomainSettings&quot;,
                &quot;url&quot;: &quot;../../EthMaintenanceDomainSettings/4478604&quot;
            },
            &quot;rootMEPUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476198/related/rootMEP&quot;,
            &quot;rootMEP&quot;: {
                &quot;rootMEP&quot;: [
                    {
                        &quot;type&quot;: &quot;EoamPmMep&quot;,
                        &quot;id&quot;: 4485928,
                        &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.pm.EoamPmMep&quot;,
                        &quot;url&quot;: &quot;../../EoamPmMep/4485928&quot;
                    }
                ]
            },
            &quot;name&quot;: &quot;dppevplan&quot;,
            &quot;displayName&quot;: &quot;8dfb4029[EVC18,4420459_10.104.120.130]&quot;,
            &quot;id&quot;: 4476198,
            &quot;uuid&quot;: &quot;7043487a-6d6b-4a89-998b-3a58dfea7978&quot;
        },
        {
            &quot;deployPending&quot;: &quot;NONE&quot;,
            &quot;networkResource&quot;: {
                &quot;type&quot;: &quot;ManagedNetworkElement&quot;,
                &quot;id&quot;: 4424479,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.foundation.encapsulatedFunctionality.ManagedNetworkElement&quot;,
                &quot;url&quot;: &quot;../../ManagedNetworkElement/4424479&quot;
            },
            &quot;networkResourceUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476196/related/networkResource&quot;,
            &quot;shortMaintenanceName&quot;: &quot;7&quot;,
            &quot;cfmServiceType&quot;: &quot;IEEE&quot;,
            &quot;megId&quot;: &quot;EVC7&quot;,
            &quot;shortMaintenanceNameType&quot;: &quot;UNSIGNED_INT16&quot;,
            &quot;crossConnectGroupName&quot;: &quot;Provisioned&quot;,
            &quot;isInterfaceStatusTlvIncluded&quot;: true,
            &quot;isPortStatusTlvIncluded&quot;: true,
            &quot;maxMEPs&quot;: 0,
            &quot;p2pName&quot;: &quot;test-y1731&quot;,
            &quot;peerMepAgingPeriod&quot;: 0,
            &quot;transmissionPeriod&quot;: &quot;_1_SEC&quot;,
            &quot;ethMDUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476196/related/ethMD&quot;,
            &quot;ethMD&quot;: {
                &quot;type&quot;: &quot;EthMaintenanceDomainSettings&quot;,
                &quot;id&quot;: 4478604,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.cfm.common.EthMaintenanceDomainSettings&quot;,
                &quot;url&quot;: &quot;../../EthMaintenanceDomainSettings/4478604&quot;
            },
            &quot;rootMEPUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476196/related/rootMEP&quot;,
            &quot;rootMEP&quot;: {
                &quot;rootMEP&quot;: [
                    {
                        &quot;type&quot;: &quot;EoamPmMep&quot;,
                        &quot;id&quot;: 4485927,
                        &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.pm.EoamPmMep&quot;,
                        &quot;url&quot;: &quot;../../EoamPmMep/4485927&quot;
                    }
                ]
            },
            &quot;name&quot;: &quot;test-y1731&quot;,
            &quot;displayName&quot;: &quot;8dfb4029[EVC7,4420459_10.104.120.130]&quot;,
            &quot;id&quot;: 4476196,
            &quot;uuid&quot;: &quot;b4d7623b-3392-4943-8762-51f28c0bdf8f&quot;
        },
        {
            &quot;deployPending&quot;: &quot;NONE&quot;,
            &quot;networkResource&quot;: {
                &quot;type&quot;: &quot;ManagedNetworkElement&quot;,
                &quot;id&quot;: 4424479,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.foundation.encapsulatedFunctionality.ManagedNetworkElement&quot;,
                &quot;url&quot;: &quot;../../ManagedNetworkElement/4424479&quot;
            },
            &quot;bridgeGroupName&quot;: &quot;Provisioned&quot;,
            &quot;networkResourceUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476197/related/networkResource&quot;,
            &quot;shortMaintenanceName&quot;: &quot;6&quot;,
            &quot;cfmServiceType&quot;: &quot;IEEE&quot;,
            &quot;megId&quot;: &quot;EVC6&quot;,
            &quot;shortMaintenanceNameType&quot;: &quot;UNSIGNED_INT16&quot;,
            &quot;bridgeDomainName&quot;: &quot;evplan&quot;,
            &quot;isInterfaceStatusTlvIncluded&quot;: true,
            &quot;isPortStatusTlvIncluded&quot;: true,
            &quot;maxMEPs&quot;: 0,
            &quot;peerMepAgingPeriod&quot;: 0,
            &quot;transmissionPeriod&quot;: &quot;_1_SEC&quot;,
            &quot;ethMDUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476197/related/ethMD&quot;,
            &quot;ethMD&quot;: {
                &quot;type&quot;: &quot;EthMaintenanceDomainSettings&quot;,
                &quot;id&quot;: 4478604,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.cfm.common.EthMaintenanceDomainSettings&quot;,
                &quot;url&quot;: &quot;../../EthMaintenanceDomainSettings/4478604&quot;
            },
            &quot;rootMEPUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476197/related/rootMEP&quot;,
            &quot;rootMEP&quot;: {
                &quot;rootMEP&quot;: [
                    {
                        &quot;type&quot;: &quot;EoamPmMep&quot;,
                        &quot;id&quot;: 4485926,
                        &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.pm.EoamPmMep&quot;,
                        &quot;url&quot;: &quot;../../EoamPmMep/4485926&quot;
                    }
                ]
            },
            &quot;name&quot;: &quot;evplan&quot;,
            &quot;displayName&quot;: &quot;8dfb4029[EVC6,4420459_10.104.120.130]&quot;,
            &quot;id&quot;: 4476197,
            &quot;uuid&quot;: &quot;8b79491a-80b1-4293-94c5-d62406c9ad33&quot;
        },
        {
            &quot;deployPending&quot;: &quot;NONE&quot;,
            &quot;networkResource&quot;: {
                &quot;type&quot;: &quot;ManagedNetworkElement&quot;,
                &quot;id&quot;: 4424479,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.foundation.encapsulatedFunctionality.ManagedNetworkElement&quot;,
                &quot;url&quot;: &quot;../../ManagedNetworkElement/4424479&quot;
            },
            &quot;networkResourceUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476199/related/networkResource&quot;,
            &quot;shortMaintenanceName&quot;: &quot;24&quot;,
            &quot;cfmServiceType&quot;: &quot;IEEE&quot;,
            &quot;megId&quot;: &quot;EVC24&quot;,
            &quot;shortMaintenanceNameType&quot;: &quot;UNSIGNED_INT16&quot;,
            &quot;crossConnectGroupName&quot;: &quot;Provisioned&quot;,
            &quot;isInterfaceStatusTlvIncluded&quot;: true,
            &quot;isPortStatusTlvIncluded&quot;: true,
            &quot;maxMEPs&quot;: 0,
            &quot;p2pName&quot;: &quot;dpp_test_evpl&quot;,
            &quot;peerMepAgingPeriod&quot;: 0,
            &quot;transmissionPeriod&quot;: &quot;_1_SEC&quot;,
            &quot;ethMDUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476199/related/ethMD&quot;,
            &quot;ethMD&quot;: {
                &quot;type&quot;: &quot;EthMaintenanceDomainSettings&quot;,
                &quot;id&quot;: 4478604,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.cfm.common.EthMaintenanceDomainSettings&quot;,
                &quot;url&quot;: &quot;../../EthMaintenanceDomainSettings/4478604&quot;
            },
            &quot;rootMEPUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476199/related/rootMEP&quot;,
            &quot;rootMEP&quot;: {
                &quot;rootMEP&quot;: [
                    {
                        &quot;type&quot;: &quot;EoamPmMep&quot;,
                        &quot;id&quot;: 4485921,
                        &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.pm.EoamPmMep&quot;,
                        &quot;url&quot;: &quot;../../EoamPmMep/4485921&quot;
                    }
                ]
            },
            &quot;name&quot;: &quot;dpp_test_evpl&quot;,
            &quot;displayName&quot;: &quot;8dfb4029[EVC24,4420459_10.104.120.130]&quot;,
            &quot;id&quot;: 4476199,
            &quot;uuid&quot;: &quot;ae72c328-f97e-4f92-889e-06ee72fe2883&quot;
        },
        {
            &quot;deployPending&quot;: &quot;NONE&quot;,
            &quot;networkResource&quot;: {
                &quot;type&quot;: &quot;ManagedNetworkElement&quot;,
                &quot;id&quot;: 4424479,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.foundation.encapsulatedFunctionality.ManagedNetworkElement&quot;,
                &quot;url&quot;: &quot;../../ManagedNetworkElement/4424479&quot;
            },
            &quot;networkResourceUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476194/related/networkResource&quot;,
            &quot;shortMaintenanceName&quot;: &quot;5&quot;,
            &quot;cfmServiceType&quot;: &quot;IEEE&quot;,
            &quot;megId&quot;: &quot;EVC5&quot;,
            &quot;shortMaintenanceNameType&quot;: &quot;UNSIGNED_INT16&quot;,
            &quot;crossConnectGroupName&quot;: &quot;Provisioned&quot;,
            &quot;isInterfaceStatusTlvIncluded&quot;: true,
            &quot;isPortStatusTlvIncluded&quot;: true,
            &quot;maxMEPs&quot;: 0,
            &quot;p2pName&quot;: &quot;EVPL-TEST1&quot;,
            &quot;peerMepAgingPeriod&quot;: 0,
            &quot;transmissionPeriod&quot;: &quot;_1_SEC&quot;,
            &quot;ethMDUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476194/related/ethMD&quot;,
            &quot;ethMD&quot;: {
                &quot;type&quot;: &quot;EthMaintenanceDomainSettings&quot;,
                &quot;id&quot;: 4478604,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.cfm.common.EthMaintenanceDomainSettings&quot;,
                &quot;url&quot;: &quot;../../EthMaintenanceDomainSettings/4478604&quot;
            },
            &quot;rootMEPUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476194/related/rootMEP&quot;,
            &quot;rootMEP&quot;: {
                &quot;rootMEP&quot;: [
                    {
                        &quot;type&quot;: &quot;EoamPmMep&quot;,
                        &quot;id&quot;: 4485925,
                        &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.pm.EoamPmMep&quot;,
                        &quot;url&quot;: &quot;../../EoamPmMep/4485925&quot;
                    }
                ]
            },
            &quot;name&quot;: &quot;EVPL-TEST1&quot;,
            &quot;displayName&quot;: &quot;8dfb4029[EVC5,4420459_10.104.120.130]&quot;,
            &quot;id&quot;: 4476194,
            &quot;uuid&quot;: &quot;189ce152-056d-4809-b8df-d15eb0545f42&quot;
        },
        {
            &quot;deployPending&quot;: &quot;NONE&quot;,
            &quot;networkResource&quot;: {
                &quot;type&quot;: &quot;ManagedNetworkElement&quot;,
                &quot;id&quot;: 4424479,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.foundation.encapsulatedFunctionality.ManagedNetworkElement&quot;,
                &quot;url&quot;: &quot;../../ManagedNetworkElement/4424479&quot;
            },
            &quot;networkResourceUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476195/related/networkResource&quot;,
            &quot;shortMaintenanceName&quot;: &quot;4&quot;,
            &quot;cfmServiceType&quot;: &quot;IEEE&quot;,
            &quot;megId&quot;: &quot;EVC4&quot;,
            &quot;shortMaintenanceNameType&quot;: &quot;UNSIGNED_INT16&quot;,
            &quot;crossConnectGroupName&quot;: &quot;Provisioned&quot;,
            &quot;isInterfaceStatusTlvIncluded&quot;: true,
            &quot;isPortStatusTlvIncluded&quot;: true,
            &quot;maxMEPs&quot;: 0,
            &quot;p2pName&quot;: &quot;Test-EVPL&quot;,
            &quot;peerMepAgingPeriod&quot;: 0,
            &quot;transmissionPeriod&quot;: &quot;_1_SEC&quot;,
            &quot;ethMDUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476195/related/ethMD&quot;,
            &quot;ethMD&quot;: {
                &quot;type&quot;: &quot;EthMaintenanceDomainSettings&quot;,
                &quot;id&quot;: 4478604,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.cfm.common.EthMaintenanceDomainSettings&quot;,
                &quot;url&quot;: &quot;../../EthMaintenanceDomainSettings/4478604&quot;
            },
            &quot;rootMEPUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476195/related/rootMEP&quot;,
            &quot;rootMEP&quot;: {
                &quot;rootMEP&quot;: [
                    {
                        &quot;type&quot;: &quot;EoamPmMep&quot;,
                        &quot;id&quot;: 4485924,
                        &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.pm.EoamPmMep&quot;,
                        &quot;url&quot;: &quot;../../EoamPmMep/4485924&quot;
                    }
                ]
            },
            &quot;name&quot;: &quot;Test-EVPL&quot;,
            &quot;displayName&quot;: &quot;8dfb4029[EVC4,4420459_10.104.120.130]&quot;,
            &quot;id&quot;: 4476195,
            &quot;uuid&quot;: &quot;585b1f48-81cb-4b67-b871-acd3d320fc60&quot;
        },
        {
            &quot;deployPending&quot;: &quot;NONE&quot;,
            &quot;networkResource&quot;: {
                &quot;type&quot;: &quot;ManagedNetworkElement&quot;,
                &quot;id&quot;: 4424479,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.foundation.encapsulatedFunctionality.ManagedNetworkElement&quot;,
                &quot;url&quot;: &quot;../../ManagedNetworkElement/4424479&quot;
            },
            &quot;networkResourceUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476192/related/networkResource&quot;,
            &quot;shortMaintenanceName&quot;: &quot;3&quot;,
            &quot;cfmServiceType&quot;: &quot;IEEE&quot;,
            &quot;megId&quot;: &quot;EVC3&quot;,
            &quot;shortMaintenanceNameType&quot;: &quot;UNSIGNED_INT16&quot;,
            &quot;crossConnectGroupName&quot;: &quot;Provisioned&quot;,
            &quot;isInterfaceStatusTlvIncluded&quot;: true,
            &quot;isPortStatusTlvIncluded&quot;: true,
            &quot;maxMEPs&quot;: 0,
            &quot;p2pName&quot;: &quot;EVPL-130-58-Y1731&quot;,
            &quot;peerMepAgingPeriod&quot;: 0,
            &quot;transmissionPeriod&quot;: &quot;_1_SEC&quot;,
            &quot;ethMDUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476192/related/ethMD&quot;,
            &quot;ethMD&quot;: {
                &quot;type&quot;: &quot;EthMaintenanceDomainSettings&quot;,
                &quot;id&quot;: 4478604,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.cfm.common.EthMaintenanceDomainSettings&quot;,
                &quot;url&quot;: &quot;../../EthMaintenanceDomainSettings/4478604&quot;
            },
            &quot;rootMEPUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476192/related/rootMEP&quot;,
            &quot;rootMEP&quot;: {
                &quot;rootMEP&quot;: [
                    {
                        &quot;type&quot;: &quot;EoamPmMep&quot;,
                        &quot;id&quot;: 4485923,
                        &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.pm.EoamPmMep&quot;,
                        &quot;url&quot;: &quot;../../EoamPmMep/4485923&quot;
                    }
                ]
            },
            &quot;name&quot;: &quot;EVPL-130-58-Y1731&quot;,
            &quot;displayName&quot;: &quot;8dfb4029[EVC3,4420459_10.104.120.130]&quot;,
            &quot;id&quot;: 4476192,
            &quot;uuid&quot;: &quot;d377a368-b7dd-4e72-91f1-0bc16e7203cb&quot;
        },
        {
            &quot;deployPending&quot;: &quot;NONE&quot;,
            &quot;networkResource&quot;: {
                &quot;type&quot;: &quot;ManagedNetworkElement&quot;,
                &quot;id&quot;: 4424479,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.foundation.encapsulatedFunctionality.ManagedNetworkElement&quot;,
                &quot;url&quot;: &quot;../../ManagedNetworkElement/4424479&quot;
            },
            &quot;networkResourceUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476193/related/networkResource&quot;,
            &quot;shortMaintenanceName&quot;: &quot;2&quot;,
            &quot;cfmServiceType&quot;: &quot;IEEE&quot;,
            &quot;megId&quot;: &quot;EVC2&quot;,
            &quot;shortMaintenanceNameType&quot;: &quot;UNSIGNED_INT16&quot;,
            &quot;crossConnectGroupName&quot;: &quot;Provisioned&quot;,
            &quot;isInterfaceStatusTlvIncluded&quot;: true,
            &quot;isPortStatusTlvIncluded&quot;: true,
            &quot;maxMEPs&quot;: 0,
            &quot;p2pName&quot;: &quot;evpl-130-70-1&quot;,
            &quot;peerMepAgingPeriod&quot;: 0,
            &quot;transmissionPeriod&quot;: &quot;_1_SEC&quot;,
            &quot;ethMDUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476193/related/ethMD&quot;,
            &quot;ethMD&quot;: {
                &quot;type&quot;: &quot;EthMaintenanceDomainSettings&quot;,
                &quot;id&quot;: 4478604,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.cfm.common.EthMaintenanceDomainSettings&quot;,
                &quot;url&quot;: &quot;../../EthMaintenanceDomainSettings/4478604&quot;
            },
            &quot;rootMEPUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476193/related/rootMEP&quot;,
            &quot;rootMEP&quot;: {
                &quot;rootMEP&quot;: [
                    {
                        &quot;type&quot;: &quot;EoamPmMep&quot;,
                        &quot;id&quot;: 4485922,
                        &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.pm.EoamPmMep&quot;,
                        &quot;url&quot;: &quot;../../EoamPmMep/4485922&quot;
                    }
                ]
            },
            &quot;name&quot;: &quot;evpl-130-70-1&quot;,
            &quot;displayName&quot;: &quot;8dfb4029[EVC2,4420459_10.104.120.130]&quot;,
            &quot;id&quot;: 4476193,
            &quot;uuid&quot;: &quot;778c18a0-9a51-47fe-998c-a13b11412c34&quot;
        },
        {
            &quot;deployPending&quot;: &quot;NONE&quot;,
            &quot;networkResource&quot;: {
                &quot;type&quot;: &quot;ManagedNetworkElement&quot;,
                &quot;id&quot;: 4424479,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.foundation.encapsulatedFunctionality.ManagedNetworkElement&quot;,
                &quot;url&quot;: &quot;../../ManagedNetworkElement/4424479&quot;
            },
            &quot;networkResourceUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476191/related/networkResource&quot;,
            &quot;shortMaintenanceName&quot;: &quot;1&quot;,
            &quot;cfmServiceType&quot;: &quot;IEEE&quot;,
            &quot;megId&quot;: &quot;EVC1&quot;,
            &quot;shortMaintenanceNameType&quot;: &quot;UNSIGNED_INT16&quot;,
            &quot;crossConnectGroupName&quot;: &quot;EVPN-Provisioned&quot;,
            &quot;isInterfaceStatusTlvIncluded&quot;: true,
            &quot;isPortStatusTlvIncluded&quot;: true,
            &quot;maxMEPs&quot;: 0,
            &quot;p2pName&quot;: &quot;l2vpn-evpn-130-70-1&quot;,
            &quot;peerMepAgingPeriod&quot;: 0,
            &quot;transmissionPeriod&quot;: &quot;_1_SEC&quot;,
            &quot;ethMDUrl&quot;: &quot;../../EthMaintenanceEntityGroupSettings/4476191/related/ethMD&quot;,
            &quot;ethMD&quot;: {
                &quot;type&quot;: &quot;EthMaintenanceDomainSettings&quot;,
                &quot;id&quot;: 4478604,
                &quot;longType&quot;: &quot;com.cisco.xmp.model.managed.standardTechnologies.cfm.common.EthMaintenanceDomainSettings&quot;,
                &quot;url&quot;: &quot;../../EthMaintenanceDomainSettings/4478604&quot;
            },
            &quot;name&quot;: &quot;l2vpn-evpn-130-70-1&quot;,
            &quot;displayName&quot;: &quot;8dfb4029[EVC1,4420459_10.104.120.130]&quot;,
            &quot;id&quot;: 4476191,
            &quot;uuid&quot;: &quot;4cdf848c-71b8-40b1-a134-01b21bef7f2c&quot;
        }
    ],
    &quot;status&quot;: {
        &quot;statusCode&quot;: 0,
        &quot;statusMessage&quot;: &quot;Get operation is successful&quot;,
        &quot;hideDialog&quot;: true
    }
}
</code></pre>
<p>Here <code>items</code> is a list. And I want to get the length of the list<br />
I have tried with following code</p>
<pre><code>            response = self.rest_oper_obj.get_operation(headers=headers_updated, url=self.base_url +
                                                                                     self.URL_getAllCFM.replace
                                                                                     (&quot;__evcId__&quot;, str(evcId)),
                                                        expected_return_code=[200], flag=True)
            logger.info(&quot;Response Body: {}&quot;.format(response.text))
            logger.info(&quot;Response code: {}&quot;.format(response.status_code))
            if response.status_code == 200:
                if &quot;success&quot; in response.text:
                    if &quot;items&quot; in response.text:
                        evc_data = response.json()
                        evc_list = evc_data('items') #getting error in this line
                        evc_count = len(evc_list)
                        print(evc_count)
</code></pre>
<p>But I'm not able to print the value, I'm getting error <code>'dict' object is not callable</code></p>
",https://stackoverflow.com/questions/76020620/how-to-get-length-of-list-from-a-json-reponse-from-api-request-in-python,True,76020716
76020608,Why is the efficiency of my neural network code reducing and what is causing the issue?,"<p>I am trying to implement a simple neural network from scratch to classify images from the MNIST dataset. However, I have noticed that the efficiency of the code decreases as I try to train the network on a larger number of examples. Specifically, the performance of the function train_dataset() is problematic. I have gone through the code and think that the issue lies with the variables sum_d_a and sum_d that are initialized and updated in the for-loop inside train_dataset(). Here is the relevant code snippet:</p>
<pre><code>def train_dataset(train_images, train_labels, activations, w, lr):
    m = len(train_images)
    deltas = [] # has deltas array for each training sample (yeah deltas in deltas)

    # getting deltas for each training sample
    for i in range(m):
        deltas.append(train_example(train_images[i], activations, w, train_labels[i]))

    # gradient descent
    for l in [3, 2, 1]:
        sum_d_a = np.zeros((w[0][l-1].shape)) 
        sum_d = np.zeros(w[1][l-1].shape)

        ####################### FINDING SUMS #####################################
        for i in range(m):  # for each training example
            sum_d_a += np.matmul(deltas[i][l-1], activations[l-1].T)
            sum_d += deltas[i][l-1]

        w[0][l-1] += lr * sum_d_a / m
        w[1][l-1] += lr * sum_d / m
</code></pre>
<p>I am not sure if this is the only part that is causing the inefficiency, but I have profiled the code and found that this loop takes a long time to execute. Could someone help me understand what is happening here and how I can fix it?</p>
<p>The whole code just in case:</p>
<pre><code>import idx2numpy
import numpy as np

# utils
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_prime(x):
    return sigmoid(x) * (1 - sigmoid(x))

# Get data
train_images = idx2numpy.convert_from_file(&quot;data/train-images.idx3-ubyte&quot;)
train_labels = idx2numpy.convert_from_file(&quot;data/train-labels.idx1-ubyte&quot;)
test_images = idx2numpy.convert_from_file(&quot;data/test-images.idx3-ubyte&quot;)
test_labels = idx2numpy.convert_from_file(&quot;data/test-labels.idx1-ubyte&quot;)

train_images = np.reshape(train_images, (train_images.shape[0], -1)) / 255
train_labels = np.reshape(train_labels, (train_labels.shape[0], -1))
test_images = np.reshape(test_images, (test_images.shape[0], -1)) / 255
test_labels = np.reshape(test_labels, (test_labels.shape[0], -1))

# initialises variables: &quot;activations&quot; and &quot;w&quot;
def init_vars():
    activations = [
        np.zeros((784,1)),# input layer
        np.zeros((16,1)), # hidden layer 1
        np.zeros((16,1)), # hidden layer 2
        np.zeros((10,1))  # output layer
    ]

    w_01 = np.random.uniform(low=-1, high=1, size=(16,784))
    w_12 = np.random.uniform(low=-1, high=1, size=(16,16))
    w_23 = np.random.uniform(low=-1, high=1, size=(10,16))

    b_1 = np.random.uniform(low=-1, high=1, size=(16,1))
    b_2 = np.random.uniform(low=-1, high=1, size=(16,1))
    b_3 = np.random.uniform(low=-1, high=1, size=(10,1))

    w = [
        [w_01, w_12, w_23], 
        [b_1, b_2, b_3]
    ]

    return activations, w

# train NN
def train_example(input_layer, activations, w, ideal_output):
    &quot;&quot;&quot;
    For each training example, we must go through each of the following steps:
    1. Feedforward: 
        - find z and a for each layer (layer indices: 1, 2, 3)
    2. Output error:
        - delta for last layer (the output layer)
    3. Backpropogate the error:
        - find delta for each layer with layer indices: 2, 1
    &quot;&quot;&quot;
    
    # activations array has 3 elements, each is a layer's activation values
    activations[0] = input_layer  
    activations[0] = activations[0].reshape((784,1))

    # initialise z
    z = [
        np.zeros((16,1)),
        np.zeros((16,1)),
        np.zeros((10,1)),
    ]

    # FEEDFORWARD 
    for l in [0, 1, 2]: # l here is (l_index - 1) since thats how things are stored in other arrays
        a_l = activations[l]
        w_l = w[0][l]
        b_l = w[1][l]

        z[l] = np.matmul(w_l, a_l) + b_l

        activations[l+1] = sigmoid(z[l])  
        # here we actually did find lth layer's z, a 

    # OUTPUT ERROR
    del_C_wrt_a = ideal_output.reshape((10,1)) - activations[3]  # from defn

    # initialise deltas
    deltas = [
        np.zeros((16, 1)),
        np.zeros((16, 1)),
        np.zeros((10, 1))
    ]

    deltas[2] = del_C_wrt_a * sigmoid_prime(z[2]) # last layer

    # BACKPROPOGATION
    for l in [1, 0]:  # here l is (l_index - 1)
        w_next_l = w[0][l+1]
        deltas[l] = np.matmul(w_next_l.T, deltas[l+1]) * sigmoid_prime(z[l])
    
    return deltas  # 3 rows for each layer

def train_dataset(train_images, train_labels, activations, w, lr):
    m = len(train_images)
    deltas = [] # has deltas array for each training sample (yeah deltas in deltas)

    # getting deltas for each training sample
    for i in range(m):
        deltas.append(train_example(train_images[i], activations, w, train_labels[i]))

    # gradient descent
    for l in [3, 2, 1]:
        sum_d_a = np.zeros((w[0][l-1].shape)) 
        sum_d = np.zeros(w[1][l-1].shape)

        ####################### FINDING SUMS #####################################
        for i in range(m):  # for each training example
            sum_d_a += np.matmul(deltas[i][l-1], activations[l-1].T)
            sum_d   += deltas[i][l-1]  # our lth layer is computer's (l-1)th index

        w[0][l-1] -= (lr / m) * sum_d_a 
        w[1][l-1] -= (lr / m) * sum_d

# test NN
def test_nn(test_inputs, test_labels, w, activations):
    count = 0
    total = len(test_inputs)

    for i in range(len(test_inputs)):
        activations[0] = test_inputs[i]
        activations[0] = activations[0].reshape((784,1))

        z = [
            np.zeros((16,1)),
            np.zeros((16,1)),
            np.zeros((10,1)),
        ]

        for l in [0, 1, 2]: 
            a_l = activations[l]
            w_l = w[0][l]
            b_l = w[1][l]
            
            z[l] = np.matmul(w_l, a_l) + b_l

            activations[l+1] = sigmoid(z[l])

        if np.argmax(activations[3]) == np.argmax(test_labels[i]):
            count += 1

    print(f&quot;accuracy = {count*100 / total}% i.e. {count} out of {total}&quot;)

# final output
def one_hot_encode(labels):
    result = np.zeros((labels.shape[0], 10))
    for i, label in enumerate(labels):
        result[i, label] = 1
    return result

if __name__ == &quot;__main__&quot;:
    activations, w = init_vars()

    train_labels = one_hot_encode(train_labels)
    test_labels = one_hot_encode(test_labels)

    for _ in range(5):
        # generate a permutation of indices
        perm = np.random.permutation(len(train_images))

        # use the permutation to shuffle both arrays
        train_images = train_images[perm]
        train_labels = train_labels[perm]
            
        # split the dataset into 10 parts
        num_parts = 10
        part_size = len(train_images) // num_parts
        parts = [train_images[i*part_size:(i+1)*part_size] for i in range(num_parts)]
        label_parts = [train_labels[i*part_size:(i+1)*part_size] for i in range(num_parts)]

        for i in range(num_parts):
            # use each part of the dataset in each iteration

            train_dataset(parts[i], label_parts[i], activations, w, lr=0.01)
            test_nn(test_images, test_labels, w, activations)
</code></pre>
<p>These are some of the outputs:</p>
<pre><code>accuracy = 9.87% i.e. 987 out of 10000
accuracy = 9.88% i.e. 988 out of 10000
accuracy = 9.87% i.e. 987 out of 10000
accuracy = 9.87% i.e. 987 out of 10000
accuracy = 9.88% i.e. 988 out of 10000
accuracy = 9.87% i.e. 987 out of 10000
accuracy = 9.87% i.e. 987 out of 10000
accuracy = 9.85% i.e. 985 out of 10000
accuracy = 9.83% i.e. 983 out of 10000
</code></pre>
<p>Also, let me know if you need to see other parts of the code to help diagnose the issue. Thanks in advance!</p>
",https://stackoverflow.com/questions/76020608/why-is-the-efficiency-of-my-neural-network-code-reducing-and-what-is-causing-the,False,
76020607,why Java code and Python code generate different signature of encode_jwt,"<p>#1. python code:</p>
<pre><code>payload = {&quot;login_user_key&quot;: &quot;b7c5443c-5395-46b0-b6c1-d940fd686880&quot;}
secret = 'abcdefghijklmnopqrstuvwxyz'
headers = {&quot;alg&quot;: &quot;HS512&quot;, &quot;typ&quot;: &quot;JWT&quot;}
encoded_jwt = jwt.encode(payload, secret, algorithm='HS512', headers=headers)
print(encoded_jwt)
</code></pre>
<p>#result:eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJsb2dpbl91c2VyX2tleSI6ImI3YzU0NDNjLTUzOTUtNDZiMC1iNmMxLWQ5NDBmZDY4Njg4MCJ9.fmhdCmM4BJBOASK_UXtRoszP0Q614mDlQ3iDU9_ZFCr--jh--OHG8o5aCG4Zgpcpmsry9NH1LY-4Bd_NN0mmQQ</p>
<p>//2. java code:</p>
<pre><code>        String secret = &quot;abcdefghijklmnopqrstuvwxyz&quot;;
        Map&lt;String, Object&gt; claims = new HashMap&lt;&gt;();
        claims.put(&quot;login_user_key&quot;, &quot;b7c5443c-5395-46b0-b6c1-d940fd686880&quot;);
        String jwttoken = Jwts.builder()
                .setHeaderParam(&quot;alg&quot;, &quot;HS512&quot;)
                .setHeaderParam(&quot;typ&quot;, &quot;JWT&quot;)
                .setClaims(claims)
                .signWith(SignatureAlgorithm.HS512, secret).compact();
        System.out.println(jwttoken);
</code></pre>
<p>//result:eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJsb2dpbl91c2VyX2tleSI6ImI3YzU0NDNjLTUzOTUtNDZiMC1iNmMxLWQ5NDBmZDY4Njg4MCJ9.Gat6BTTXtwC_XWN4pZjqC-x4Bw24xfpNXmLIMSysa3FJaPvpnXcjO0wp6FBlKtrfYyqZRQARQbE4rGwhAOY_eg</p>
<p>please tell me that Java code and Python code generate different signature of encode_jwt ,and modify the python code to make the final generated jwt the same. please....</p>
<p>I tried to use chatgpt to give an answer, but it didn't work, it may not be a problem with key encoding, maybe a problem with json data or string data?</p>
",https://stackoverflow.com/questions/76020607/why-java-code-and-python-code-generate-different-signature-of-encode-jwt,False,
76020588,Left align tkinter widgets using place,"<p>I'm having some trouble understanding tkinter place coordinates.</p>
<p>As demonstrated by the brilliant visual answer i found <a href=""https://stackoverflow.com/a/64545215/992644"">here</a>, anchors determine the corner/edge of the object that the coordinates apply to.</p>
<p>Given this fact, then why are my check boxes not vertically aligned when i give them the same x offset and the same anchor.</p>
<p>It seems as if the coordinates are being measured from the center of the object rather than the anchor point, and that center is being calculated based on the length of the text not the w specifed in the place. I have put red dots where I would expect the NW corner of each of the widgets to be.</p>
<p><a href=""https://i.stack.imgur.com/QzQe1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QzQe1.png"" alt=""The reason for my early hairloss"" /></a></p>
<pre><code>import tkinter as tk

class App:
    def __init__(self, root):
        root.title(&quot;Example&quot;)
        root.geometry(&quot;100x100&quot;)
        check_x           = 0
        check_w           = 100
        check_h           = 30
        check_anchor      = 'nw'
        check_j           = 'left'
        check_compound    = 'left'

        ch1               = tk.Checkbutton(root)
        ch1[&quot;justify&quot;]    = check_j
        ch1[&quot;text&quot;]       = &quot;Top &quot;
        ch1[&quot;compound&quot;]   = check_compound
        ch1.place(x       = check_x,
                  y       = 20,
                  width   = check_w,
                  height  = check_h,
                  anchor  = check_anchor)

        ch2              = tk.Checkbutton(root)
        ch2[&quot;justify&quot;]   = check_j
        ch2[&quot;text&quot;]      = &quot;Bottom&quot;
        ch2[&quot;compound&quot;]  = check_compound
        ch2.place(x      = check_x,
                  y      = 50,
                  width  = check_w,
                  height = check_h,
                  anchor = check_anchor)

if __name__ == &quot;__main__&quot;:
    root = tk.Tk()
    app = App(root)
    root.mainloop()
</code></pre>
<p>This is just an example, in practice i have a more complex GUI with around 30 widgets, I don't need anything at placed at x 0. I just have a few things which I want to be indented and vertically aligned to different horizontal offsets and I want to be able to control this by setting common values across the relevant widgets with a variable.</p>
<p>I can see that this is a common problem beginners have with tkinter, but I couldn't find anyone addressing it in the case of the place method.</p>
<ul>
<li><a href=""https://stackoverflow.com/q/44091064/992644"">Alignment of the widgets in Python Tkinter</a></li>
<li><a href=""https://stackoverflow.com/questions/43170320/how-to-align-tkinter-widgets"">How to align tkinter widgets?</a></li>
</ul>
",https://stackoverflow.com/questions/76020588/left-align-tkinter-widgets-using-place,True,76020699
76020567,"Near the end of deep learning, the console reports an error, or the prompt a.ny () or a.ll () is added to where","<p>Near the end of deep learning, the console reports an error, or the prompt a.ny () or a.ll () is added to where</p>
<pre class=""lang-py prettyprint-override""><code>def evaluate_performance_two(y_test, y_pred, y_score):
    &quot;&quot;&quot;Evaluate performance&quot;&quot;&quot;
    #the below line will report error
    sort_y_score = sorted(enumerate(y_score), key=lambda x: x[1], reverse=True)[:100]
    top_y_score = []
    top_y_score_index = []
    for x, y in sort_y_score:
        top_y_score_index.append(x)
        top_y_score.append(y)
    top_y_perd = [int(item &gt; 0.5) for item in top_y_score]
    top_y_test = [y_test[item] for item in top_y_score_index]
</code></pre>
<p>This is the error thrown:</p>
<pre><code>Traceback (most recent call last):
    sort_y_score = sorted(enumerate(y_score), key=lambda x: x[1], reverse=True)[:100]
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
</code></pre>
<p>I want to know how to solve this problem or the &quot;a.any&quot; &quot;a.all&quot; add where?</p>
",https://stackoverflow.com/questions/76020567/near-the-end-of-deep-learning-the-console-reports-an-error-or-the-prompt-a-ny,False,
76020560,How to scrape fields on eBay using beautifulSoup4 in Python?,"<p>I'm watching this <a href=""https://www.youtube.com/watch?v=csj1RoLTMIA"" rel=""nofollow noreferrer"">video</a> (which is dated) and adapting this <a href=""https://github.com/jhnwr/ebay-prices"" rel=""nofollow noreferrer"">code</a> (which is broken).  Learning a lot about web scraping from this and I've been able to adapt some of it to the changes in backend but not all of them.  For example, I can't figure out how to get the &quot;Sold date&quot;.  I understand that it's within here: <code>&lt;div class=&quot;s-item__title--tag&quot;&gt;</code> but I can't seem to scrape it out.</p>
<p>I found this (<a href=""https://stackoverflow.com/questions/39534371/storing-data-from-a-tag-in-python-with-beautifulsoup4"">Storing data from a tag in Python with BeautifulSoup4</a>) but was unable to adapt it to my problem.</p>
<p>Below you can see where I stopped, I was unable to scrape the Sold out date after I thought I figured out the pattern.</p>
<p><strong>Question: Can someone help me adapt my code to pull out the fields of interest?</strong></p>
<p>The fields I'm interested in are the following:</p>
<ul>
<li>Title</li>
<li>Sold date</li>
<li>Sold price</li>
<li>Region purchased (not sold from)</li>
</ul>
<p>Bonus:</p>
<ul>
<li>Link to listing</li>
</ul>
<p>This is the script I'm using: <code>ebay_scraper.py</code></p>
<p>Usage: <code>python ebay_scraper.py [search+term+with+spaces+as+plus+signs]</code></p>
<p>(e.g., <code>python ebay_scraper.py darth+vader &gt; output.tsv</code>)</p>
<pre class=""lang-py prettyprint-override""><code>import sys, requests
from bs4 import BeautifulSoup
import pandas as pd

def main():
    searchterm = sys.argv[1]
    url = f&quot;https://www.ebay.co.uk/sch/i.html?_from=R40&amp;_nkw={searchterm}+1&amp;_sacat=0&amp;LH_TitleDesc=0&amp;LH_Complete=1&amp;_ipg=200&amp;LH_Sold=1&amp;LH_PrefLoc=2&amp;rt=nc&amp;LH_BIN=1&quot;
    print(url, file=sys.stderr)
    soup = get_data(url)
    productlist = parse(soup)
    productsdf = pd.DataFrame(productlist)
    productsdf.to_csv(sys.stdout, sep=&quot;\t&quot;)

def get_data(url):
    r = requests.get(url)
    if r.status_code != 200:
        print('Failed to get data: ', r.status_code)
    else:
        soup = BeautifulSoup(r.text, 'html.parser')
        print(soup.title.text, file=sys.stderr)
    return soup

def parse(soup):
    productlist = []
    results = soup.find_all('div', {'class': 's-item__info clearfix'})
    for item in results:
        title = item.find('span', {'role':'heading'}).text
        terms = sys.argv[1].lower().split(&quot;+&quot;)
        query = title.lower().split(&quot; &quot;)
        n = len(set(terms) &amp; set(query))
        m = len(set(terms))
        if (n/m) &gt;= 0.75:
            if title not in {&quot;Shop on eBay&quot;}:
                sold = item.find('div', {'class': 's-item__title--tag'})     
                if sold is not None:
                    completed = sold.find('span', {'class': 's-item__title--tagblock'})
                    print(completed)
                    if completed is not None:
                        print(completed.find('span', {'class':'POSITIVE'}).text)

                    # products = {
                    # # 'title': item.find('h3', {'class':'s-item__title s-item__title--has-tags'}).text,
                    # 'title': item.find('span', {'role':'heading'}).text,
                    # # 'soldprice': float(item.find('span', {'class':'s-item__price'}).text.replace('£','').replace(',','').strip()),
                    # 'solddate': item.find('span', {'class': 's-item__title--tagblock__COMPLETED'}).find('span', {'class':'POSITIVE'}).text,
                    # # 'link': item.find('a', {'class': 's-item__link'})['href']        
                    # }
                    # productlist.append(products)
    return productlist


if __name__ == '__main__':
    main()
</code></pre>
",https://stackoverflow.com/questions/76020560/how-to-scrape-fields-on-ebay-using-beautifulsoup4-in-python,False,
76020491,Django REST Framework - Weird Parameter Shape due to JSON Parser?,"<p>Currently I'm contacting my APIViews through AJAX requests on my frontend.</p>
<pre class=""lang-js prettyprint-override""><code>config = {
    param1: 1,
    param2: 2,
    param3: 3
}

$.ajax(APIEndpoints.renderConfig.url, {
    type: APIEndpoints.renderConfig.method,
    headers: { &quot;X-CSRFToken&quot;: csrfToken },
    data: {
        &quot;config&quot;: config,
        &quot;useWorkingConfig&quot;: true
    }
}
</code></pre>
<p>However, when I receive the data in my APIView, I get an object with the following shape:</p>
<pre class=""lang-py prettyprint-override""><code>{
    &quot;config[param1]&quot;: 1,
    &quot;config[param2]&quot;: 2,
    &quot;config[param3]&quot;: 3,
    &quot;useWorkingConfig&quot;: true
}
</code></pre>
<p>According to different sources that I have checked, DRF does support nested JSON objects, so what am I missing? Is this something to do with the default behavior of the JSON Parser? It doesn't seem to be very useful, as you would basically need to preprocess the data before using your serializers (since they expect a nested structure). The same happens with lists, with your parameters being received as <code>list_param[]</code> instead of <code>list_param</code>.</p>
<p>If its useful for anything, these are my DRF settings</p>
<pre class=""lang-py prettyprint-override""><code>REST_FRAMEWORK = {
    'DEFAULT_SCHEMA_CLASS': 'drf_spectacular.openapi.AutoSchema',
    'EXCEPTION_HANDLER': 'django_project.api.exception_handler.custom_exception_handler',

    # Camel Case Package Settings
    'DEFAULT_RENDERER_CLASSES': (
        'djangorestframework_camel_case.render.CamelCaseJSONRenderer',
        'djangorestframework_camel_case.render.CamelCaseBrowsableAPIRenderer',
    ),
    'DEFAULT_PARSER_CLASSES': [
        'rest_framework.parsers.JSONParser',
        'djangorestframework_camel_case.parser.CamelCaseFormParser',
        'djangorestframework_camel_case.parser.CamelCaseMultiPartParser',
        'djangorestframework_camel_case.parser.CamelCaseJSONParser',
    ],
    'JSON_UNDERSCOREIZE': {
        'no_underscore_before_number': True,
    },
}
</code></pre>
<p>Its very probable that Im just missing something or its an intended behavior that Im not aware of.</p>
",https://stackoverflow.com/questions/76020491/django-rest-framework-weird-parameter-shape-due-to-json-parser,False,
76020481,how to fix the error: name &#39;event&#39; is not defined bot vk,"<pre><code>def loop2():
    while True:
        try:
            time.sleep(2)
            run=random.randint(10000,100000)
            file=open(str(event.obj.peer_id)+&quot;.txt&quot;,&quot;r&quot;)
            balans=file.read()
            file.close()
            balans=int(balans)
            balans+=run
            balans=str(balans)
            file=open(str(event.obj.peer_id)+&quot;.txt&quot;,&quot;w&quot;)
            file.write(balans)
            file.close()
            print(&quot;+ к балансу&quot;)
            print(run)
        except Exception as Y:
            print(Y)
Thread(target=loop1).start()
Thread(target=loop2).start()
</code></pre>
<p>I want the bot to add a random amount to the balance every minute and not interfere with the dialogue and save it in the database (the database is in my text file)</p>
",https://stackoverflow.com/questions/76020481/how-to-fix-the-error-name-event-is-not-defined-bot-vk,False,
76020437,Trouble unscaling data after predictions,"<p>I am working on a project where I am trying to predict mlb player stats from 1970 - 2022. I have 2 datasets, one for batters where I am predicting on 5 stats with 20 features and another for pitchers where I am predicting on 6 stats with 25 features. I am currently working on a Decision Tree Model, but also plan to work with Linear Regression and LSTM models as well.</p>
<p>Prior to initially scaling the dataset I removed the string columns, year, and columns I was using to compare results with.</p>
<pre><code>remove_bat_cols = ['Name', 'Tm', 'Year', 'Nxt_BA', 'Nxt_RBI', 'Nxt_HR', 'Nxt_BB', 'Nxt_SO']
remove_pitch_cols = ['Name', 'Tm', 'Year', 'Nxt_ERA', 'Nxt_SO', 'Nxt_WHIP', 'Nxt_BB', 'Nxt_W', 'Nxt_SV']

bat_cols = batting.columns[~batting.columns.isin(remove_bat_cols)]
pitch_cols = pitching.columns[~pitching.columns.isin(remove_pitch_cols)]
</code></pre>
<p>I then scaled my data</p>
<pre><code>scaler = MinMaxScaler()
batting.loc[:, bat_cols] = scaler.fit_transform(batting[bat_cols])
pitching.loc[:, pitch_cols] = scaler.fit_transform(pitching[pitch_cols])
</code></pre>
<p>I initially tried to just reverse my steps to unscale</p>
<pre><code>batting.loc[:, bat_cols] = scaler.inverse_transform(batting[bat_cols])
pitching.loc[:, pitch_cols] = scaler.inverse_transform(pitching[pitch_cols])
</code></pre>
<p>But I receive the below error:</p>
<pre><code>ValueError: operands could not be broadcast together with shapes (26768,29) (31,) (26768,29) 
</code></pre>
<p>I've also tried adding the predicted columns and even tried only predicted columns and receive the same.</p>
",https://stackoverflow.com/questions/76020437/trouble-unscaling-data-after-predictions,False,
76020424,How to display the blue warning message box of Windows In Python?,"<p><strong>PROBLEM DESCRIPTION :</strong></p>
<p>I want to generate the blue message box of Windows. The message box where it's writen &quot;you'll be disconnected&quot;.</p>
<p>I tried this code but it give me a different one (it's still a warning but that's not the message box I wanted) A good thing I know ctypes is the library to work with</p>
<p><strong>CODE :</strong></p>
<pre><code>import ctypes 

message_box_title = &quot;Warning&quot;
message_box_text = &quot;You will be disconnected in 30 seconds&quot;
message_box_flags = 0x00001030  


ctypes.windll.user32.MessageBoxW(None, message_box_text, message_box_title, message_box_flags)
</code></pre>
<p>That's a message box like this but with the message of disconnection soon like in x number of seconds/minutes</p>
<p><a href=""https://i.stack.imgur.com/Dlq2h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dlq2h.png"" alt=""publisher"" /></a></p>
",https://stackoverflow.com/questions/76020424/how-to-display-the-blue-warning-message-box-of-windows-in-python,True,76020625
76020422,Is there a way to truly delete an item from a list in python,"<p>I'm new to python so if this is obvious break it to me as nicely as possible.
Basically, every time I use list.remove() it removes it for the current use of the program but readds it whenever I reopen it, or I guess it never deletes it in the first place. I was wondering if there was a way to permanently alter the list and remove it from the code itself using only the program and not having to do it manually.</p>
",https://stackoverflow.com/questions/76020422/is-there-a-way-to-truly-delete-an-item-from-a-list-in-python,False,
76020402,EOFError when running a python script in docker container,"<p>I have a python script and it takes user input.</p>
<pre><code>numbers = input(&quot;Enter numbers&quot;)
</code></pre>
<p>I created a docker container but when I run the container I get EOFError: EOF when reading a line. I don't get this error when I run the script without the container. How would I fix this issue?</p>
",https://stackoverflow.com/questions/76020402/eoferror-when-running-a-python-script-in-docker-container,True,
76020376,How should I prepare a dataset to train a machine to track the motion of a robot [target]?,"<p>I have generated multiple scenarios for simulation and I need to use the results to train the machine. I am unsure whether to use a single dataset for each scenario and save multiple models (consider the fact that I have many scenarios), or combine all datasets from different scenarios and train one model. Which option do you think is the best approach and why?</p>
<p><strong>I have two options</strong></p>
<ol>
<li><p>Using a single dataset for each scenario and saving multiple models would require generating a separate dataset for each scenario and generate a separate model for each dataset and it is computationally expensive and time consuming. I would later call saved models depending on which I think should correspond to one I am trying to predict. (difficult, right?)</p>
</li>
<li><p>Combining all datasets and training one model: This approach can be computationally more efficient as only one model needs to be trained. However, combining different datasets of different nature can lead to unexpected errors, and the model may not perform as well as if it was specifically trained for each scenario.*</p>
</li>
</ol>
<p><strong>Which one is more favorable and why?</strong></p>
<p>I tried the <strong>first options</strong> but I think there must be something wrong so I need some clarifications and help</p>
",https://stackoverflow.com/questions/76020376/how-should-i-prepare-a-dataset-to-train-a-machine-to-track-the-motion-of-a-robot,False,
76020355,Cannot send embed in discord.py,"<p>I'm trying to make a simple discord bot with discord.py with slash commands and an embed, but I keep getting the error: <code>_Context.send() got an unexpected keyword argument 'embed'</code>.</p>
<p>Here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>intents = discord.Intents.default()
intents.message_content = True


bot = interactions.Client(token=TOKEN)


@bot.command(
  name=&quot;debug&quot;,
  description=&quot;Test for errors in bot&quot;,
  scope=&lt;id&gt;,
)
async def debug(ctx: interactions.CommandContext):
  help_embed = discord.Embed(title=&quot;Modules:&quot;)
  
  await ctx.send(embed=help_embed)


bot.start()
</code></pre>
<p>what am I doing wrong?</p>
",https://stackoverflow.com/questions/76020355/cannot-send-embed-in-discord-py,False,
76020335,Why isn&#39;t my networkx graph drawn correctly?,"<p>I'm trying to draw some graphs from an adjacency matrix stored in a text file (and loaded with Numpy), using <code>networkx</code> version 2.6.3 and matplotlib version 3.4.3.</p>
<p>I have this code:</p>
<pre><code>import networkx as nx
from prettytable import PrettyTable
import numpy as np
import matplotlib.pyplot as plt


def calculate_network_properties(file_path):
    # Read the adjacency matrix from the file
    adj_matrix = np.loadtxt(file_path)

    # Create a networkx graph from the adjacency matrix
    G = nx.from_numpy_matrix(adj_matrix)

    # Relabel nodes to start from 1 instead of 0
    mapping = {node: node + 1 for node in G.nodes()}
    G = nx.relabel_nodes(G, mapping)

    # Calculate network properties
    num_nodes = G.number_of_nodes()
    num_edges = G.number_of_edges()
    density = nx.density(G)
    diameter = nx.diameter(G)
    global_clustering_coefficient = nx.average_clustering(G)
    local_clustering_coefficients = nx.clustering(G)
    avg_local_clustering_coefficient = sum(local_clustering_coefficients.values()) / len(local_clustering_coefficients)
    mean_shortest_path = nx.average_shortest_path_length(G)
    assortativity = nx.degree_assortativity_coefficient(G)
    communities = nx.algorithms.community.modularity_max.greedy_modularity_communities(G)
    modularity = nx.algorithms.community.modularity(G, communities)
    num_communities = len(communities)

    # Calculate centrality measures
    degree_centrality = nx.degree_centrality(G)
    closeness_centrality = nx.closeness_centrality(G)
    betweenness_centrality = nx.betweenness_centrality(G)
    eigenvector_centrality = nx.eigenvector_centrality(G)

    # Check if the network is small-world
    small_world = nx.algorithms.smallworld.sigma(G)

    # Create a table with the results
    table = PrettyTable()
    table.field_names = ['Characteristic', 'Result']
    table.add_row(['Number of nodes', num_nodes])
    table.add_row(['Number of edges', num_edges])
    table.add_row(['Graph density', density])
    table.add_row(['Graph diameter', diameter])
    table.add_row(['Global clustering coefficient', global_clustering_coefficient])
    table.add_row(['Average of local clustering coefficients', avg_local_clustering_coefficient])
    table.add_row(['Mean shortest path', mean_shortest_path])
    table.add_row(['Assortativity (r coefficient)', assortativity])
    table.add_row(['Modularity (Q coefficient)', modularity])

    table.add_row(['Number of communities found', num_communities])
    table.add_row(['Small-world sigma', small_world])

    print(table)

    # Create tables for centrality measures
    degree_table = PrettyTable()
    degree_table.field_names = ['Node', 'Degree Centrality']
    for node, centrality in degree_centrality.items():
        degree_table.add_row([node, centrality])
    print('Degree Centrality')
    print(degree_table)

    closeness_table = PrettyTable()
    closeness_table.field_names = ['Node', 'Closeness Centrality']
    for node, centrality in closeness_centrality.items():
        closeness_table.add_row([node, centrality])
    print('Closeness Centrality')
    print(closeness_table)

    betweenness_table = PrettyTable()
    betweenness_table.field_names = ['Node', 'Betweenness Centrality']
    for node, centrality in betweenness_centrality.items():
        betweenness_table.add_row([node, centrality])

    print('Betweenness Centrality')
    print(betweenness_table)

    eigenvector_table = PrettyTable()
    eigenvector_table.field_names = ['Node', 'Eigenvector Centrality']
    for node, centrality in eigenvector_centrality.items():
        eigenvector_table.add_row([node, centrality])
    print('Eigenvector Centrality')
    print(eigenvector_table)

    # Set node size based on degree centrality
    node_size = [v * 1000 for v in degree_centrality.values()]

    # Draw the graph for degree centrality
    fig, ax = plt.subplots()
    nx.draw(G, pos=nx.kamada_kawai_layout(G), node_size=node_size, with_labels=True, ax=ax)
    ax.set_title('Degree Centrality')

    # Set node size based on closeness centrality
    node_size = [v * 1000 for v in closeness_centrality.values()]

    # Draw the graph for closeness centrality
    fig, ax = plt.subplots()

    nx.draw(G, pos=nx.kamada_kawai_layout(G), node_size=node_size, with_labels=True, ax=ax)
    ax.set_title('Closeness Centrality')

    # Set node size based on betweenness centrality
    node_size = [v * 1000 for v in betweenness_centrality.values()]

    # Draw the graph for betweenness centrality
    fig, ax = plt.subplots()
    nx.draw(G, pos=nx.kamada_kawai_layout(G), node_size=node_size, with_labels=True, ax=ax)
    ax.set_title('Betweenness Centrality')

    # Set node size based on eigenvector centrality
    node_size = [v * 1000 for v in eigenvector_centrality.values()]

    # Draw the graph for eigenvector centrality
    fig, ax = plt.subplots()
    nx.draw(G, pos=nx.kamada_kawai_layout(G), node_size=node_size, with_labels=True, ax=ax)
    ax.set_title('Eigenvector Centrality')

    # Draw the graph with communities shaded in different colors
    fig, ax = plt.subplots()
    pos = nx.kamada_kawai_layout(G)
    colors = ['r', 'g', 'b', 'y', 'c', 'm']
    for i, community in enumerate(communities):
        nx.draw_networkx_nodes(G, pos, nodelist=community, node_color=colors[i % len(colors)], ax=ax)

    nx.draw_networkx_edges(G, pos, ax=ax)
    nx.draw_networkx_labels(G, pos, ax=ax)
    ax.set_title('Communities')

    plt.show()
</code></pre>
<p>When I try calling the function with my input file:</p>
<pre><code>calculate_network_properties('adjacency matrix.txt')
</code></pre>
<p>I get an output images that looks like:
<img src=""https://i.stack.imgur.com/EGKvT.png"" alt="""" /></p>
<p>All the nodes are packed together and thus the architecture of my networks cannot be seen. How can I fix the problem?</p>
",https://stackoverflow.com/questions/76020335/why-isnt-my-networkx-graph-drawn-correctly,False,
76020323,Convert yolov7 weight and configuration file,"<p>How can I convert yolov7 .yaml and .pt files into .cfg and .weights files???!</p>
<p>I've already tried using pre-existing .cfg and .weights extensions and they worked fine.</p>
<p>I need to use these files with an OpenCV code, so I need those extensions.</p>
",https://stackoverflow.com/questions/76020323/convert-yolov7-weight-and-configuration-file,False,
76020318,Tranposing individual pieces of a DataFrame,"<p>I have a dataframe as follows (numbers are strings):</p>
<pre><code>Company Name  Total Amount Due  0-10 days  10-20 days  20-30 days
ABC Company   0                 0          0           0
$5,000.00     0                 0          0           0
$5,000.00     0                 0          0           0
$0.00         0                 0          0           0
$0.00         0                 0          0           0
XYZ Company   0                 0          0           0
$10,000.00    0                 0          0           0
$8,000.00     0                 0          0           0
$2,000.00     0                 0          0           0
$0.000        0                 0          0           0
...
</code></pre>
<p>I would like to know if there's a way to do miniature transposition inside of a DataFrame. For example, I would take the values $5,000.00, $5,000.00, $0.00, and $0.00 and flip them onto the other axis:</p>
<pre><code>Company Name  Total Amount Due  0-10 days  10-20 days  20-30 days
ABC Company  $5,000.00          $5,000.00  $0.00       $0.00
XYZ Company  $10,000.00         $8,000.00  $2,000.00   $0.00
...
</code></pre>
<p>I tried using shift and transpose, but I wasn't quite able to get the output I was expecting.</p>
",https://stackoverflow.com/questions/76020318/tranposing-individual-pieces-of-a-dataframe,True,
76020306,Tkinter showing strange artifact on Mac,"<p>I have built some GUI's using tkinter in python (3.7) and I am noticing that when some of the widgets are selected when run on mac (ventura 13.3.1 m2 mac mini) they show a strange spike in the highlight on the left side.</p>
<p><a href=""https://i.stack.imgur.com/Chz6Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Chz6Y.png"" alt=""Submit button with artifact"" /></a>
<a href=""https://i.stack.imgur.com/kysZ5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kysZ5.png"" alt=""Dropdown Menu with artifact"" /></a></p>
<p>My code is simple, but here's an example for the button just to let you know I'm not doing anything crazy:</p>
<pre class=""lang-py prettyprint-override""><code>import multiprocessing as mp
import os
import platform
from tkinter import ttk, Tk

def run_program():
    print(&quot;Run command ran&quot;)

def main():
    # This initializes the GUI
    root = Tk()
    root.title('COD_FISH Input')

    # create a notebook (the pages)
    notebook = ttk.Notebook(root)
    notebook.pack(pady=10, padx=10, expand=True, fill='both')

    main_frame = ttk.Frame(root)

    button = ttk.Button(main_frame, text='Submit', command=run_program)
    button.grid(column=0, row=13, sticky='se', padx=5, pady=5)
    # makes it so that hitting 'Enter' triggers the submit button.
    button.bind('&lt;Return&gt;', run_program)

    # This tries to see if the program is being run on windows and
    # if so changes a setting which improves the resolution of the GUI
    if os.name == 'nt':
        from ctypes import windll
        windll.shcore.SetProcessDpiAwareness(1)

    # This is what constantly updates the window.
    root.mainloop()

# This protects the script from running many times! Not sure why it would do that though
# since it isn't called anywhere else...
if __name__ == &quot;__main__&quot;:
    # And this prevents the pyinstaller .exe file from opening the window many times
    # due to incompatibility with multiprocessing
    mp.freeze_support()
    # Specific multithreading fix for Macs
    if platform.system() == &quot;Darwin&quot;:
        mp.set_start_method('spawn')
    main()
</code></pre>
<p>Is there something I'm doing wrong? Or is there something that I can add to remove this strange artifact?</p>
<p>I ran this on python 3.7. I tried with 3.11, and it was still present (as well as there being problems with the dark mode, with white text of a white background showing up)</p>
",https://stackoverflow.com/questions/76020306/tkinter-showing-strange-artifact-on-mac,True,76038355
76020302,How to handle non-ASCII characters when reading a file in a Python script?,"<p>I am working on a Python script that needs to read data from a file containing non-ASCII characters. However, when I run my script, I encounter the following error message:</p>
<blockquote>
<p>&quot;UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 10: invalid continuation byte&quot;</p>
</blockquote>
<p>I have tried to specify the encoding of the file as &quot;utf-8&quot; using the following code:</p>
<pre class=""lang-py prettyprint-override""><code>with open('data.txt', 'r', encoding='utf-8') as f:
    data = f.read()
</code></pre>
<p>Unfortunately, this still doesn't seem to work.</p>
<p>My expected outcome is to be able to read the data from the file without any errors and handle non-ASCII characters correctly.</p>
<p>Any help and suggestions would be greatly appreciated.</p>
<p>Edit:
<code>data.txt</code> is as follows:
(for my french assignment)</p>
<pre><code>Bonjour, comment ça va ?
Je suis en train d'apprendre le français.
J'aime bien écouter de la musique française.
Ça fait longtemps que je n'ai pas mangé de croissants frais.
Il y a beaucoup de sites web en français.
Je vais prendre un café au lait s'il vous plaît.
Les macarons sont délicieux.
Je rêve de visiter la Tour Eiffel un jour.
Le vin français est très bon.
</code></pre>
",https://stackoverflow.com/questions/76020302/how-to-handle-non-ascii-characters-when-reading-a-file-in-a-python-script,False,
76020286,"Call a function in another module, go through series of functions in that module, then pass the final result back to the original function and module?","<p>newbie here. I have a function in a module, let's say <code>data_command()</code> in <code>main.py</code>. <code>data_command()</code> calls another function in a separate module, let's say <code>get_data()</code> in <code>work.py</code>. <code>get_data()</code> requests and receives a json response and calls another function with <code>response</code> as the argument in the same module, <code>check_data(response)</code>. <code>check_data(response)</code> does something and returns a result, and then calls a final function with this result as the argument, <code>process_data(result)</code>. <code>process_data(result)</code> determines the data ultimately needed by <code>data_command()</code> in the original module <code>main.py</code>. How do I pass this data from <code>process_data(result)</code>, as a variable or otherwise, back to <code>data_command()</code> in a separate module?</p>
<p>The below code is a simplified example. The functions in the actual code actually do something, although I imagine there's probably a better way to do what I'm tying to do. Any advice, tips, or criticism appreciated.</p>
<pre><code>#main.py

import work

async def data_command():
    get_data()
    await message.channel.send(???work.process_data.data???) #not sure what to put here
</code></pre>
<pre><code>#work.py

def get_data():
    response = requests.get(&quot;www.api.com/test&quot;)
    check_data(response)

def check_data(response):
    result = ...
    process_data(result)

def process_data(result):
    data = ...
    return data
    

    
</code></pre>
",https://stackoverflow.com/questions/76020286/call-a-function-in-another-module-go-through-series-of-functions-in-that-module,False,
76020261,Box counting function for fractals not working,"<p>im making a python program to take an image and determine the fractal dimension with different grid sizes for the box counting. i had previously had it working and it got deleted and i cant remember exactly how i had it but i have something similar to it now. the main problem i have now is when the function count runs, N=1 for every size box.</p>
<p>this is my current code</p>
<pre><code>def count(image, size):
    N=0
    step=size
    for i in range(0, Lx, step):
       for j in range(0, Ly, step):
           if (img_matrix[i:step,j:step] == 0).any(): 
               N += 1
               return N
       
size=np.arange(0,10, 1)
N=0
Ns=[]
for s in size:
    N=count(img,s)
    Ns.append(N)`
</code></pre>
<p>it only gives 1 as the value of Ns.
how do i fix this?</p>
",https://stackoverflow.com/questions/76020261/box-counting-function-for-fractals-not-working,True,76020308
76020247,Natural language toolkit,"<p>The function <code>nltk.download()</code> returns this:</p>
<blockquote>
<p>Error connecting to server: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established  connection failed because connected host has failed to respond`</p>
</blockquote>
<p>I'm unable to solve this error? How do I need to solve this one?</p>
<p>I've tried installing NLTK and then downloading its data but failed to do it since it is showing <code>Win Error 10060 - error connecting to server</code> even when I have good server connection.</p>
",https://stackoverflow.com/questions/76020247/natural-language-toolkit,False,
76020231,Cannot update Anaconda Navigator and inconsistent environment in Mac OS,"<p>So I started using python again for my personal projects after an year but I saw an update for Anaconda Navigator version upgrade for 2.4.0 However I a not able to update the Anaconda Navigator. The loading circle loads infinitely.</p>
<p>I tried updating manually in terminal with</p>
<blockquote>
<p>conda update anaconda-navigator=2.4.0</p>
<p>conda install anaconda-navigator=2.4.0</p>
<p>conda update anaconda-navigator</p>
</blockquote>
<p>However I am getting following error</p>
<pre><code>The environment is inconsistent, please check the package plan carefully
The following packages are causing the inconsistency:

  - defaults/osx-64::pytest-astropy==0.5.0=py37_0
  - defaults/osx-64::imageio==2.6.0=py37_0
  - defaults/osx-64::pytables==3.5.2=py37h5bccee9_1
  - defaults/osx-64::scikit-image==0.15.0=py37h0a44026_0
  - defaults/noarch::numpydoc==0.9.1=py_0
  - defaults/noarch::anaconda-project==0.8.3=py_0
  - defaults/osx-64::conda==4.8.2=py37_0
  - defaults/osx-64::bottleneck==1.2.1=py37h1d22016_1
  - defaults/osx-64::pytest-arraydiff==0.3=py37h39e3cac_0
  - defaults/osx-64::pywavelets==1.0.3=py37h1d22016_1
  - defaults/osx-64::xlwings==0.15.10=py37_0
  - defaults/osx-64::numexpr==2.7.0=py37h7413580_0
  - defaults/noarch::sphinx==2.2.0=py_0
  - defaults/osx-64::bkcharts==0.2=py37_0
  - defaults/osx-64::anaconda==2019.10=py37_0
  - defaults/osx-64::anaconda-navigator==1.9.7=py37_0
  - defaults/osx-64::mkl_fft==1.0.14=py37h5e564d8_0
  - defaults/noarch::pytest-openfiles==0.4.0=py_0
  - defaults/osx-64::statsmodels==0.10.1=py37h1d22016_0
  - defaults/osx-64::seaborn==0.9.0=py37_0
  - defaults/osx-64::bokeh==1.3.4=py37_0
  - defaults/osx-64::numpy==1.17.2=py37h99e6662_0
  - defaults/osx-64::anaconda-client==1.7.2=py37_0
  - defaults/osx-64::patsy==0.5.1=py37_0
  - defaults/noarch::distributed==2.5.2=py_0
  - defaults/noarch::pytest-doctestplus==0.4.0=py_0
  - defaults/osx-64::mkl_random==1.1.0=py37ha771720_0
  - defaults/osx-64::astropy==3.2.2=py37h1de35cc_0
  - defaults/osx-64::spyder==3.3.6=py37_0
  - defaults/noarch::dask==2.5.2=py_0
  - defaults/osx-64::conda-build==3.18.9=py37_3
  - defaults/osx-64::numba==0.45.1=py37h6440ff4_0
</code></pre>
<p>Any solutions to update it?</p>
",https://stackoverflow.com/questions/76020231/cannot-update-anaconda-navigator-and-inconsistent-environment-in-mac-os,False,
76020210,how to connect smartwatch with my laptop and then collect physiological signals,"<p>I want to connect smartwatch with my laptop and then collect physiological signals such as heart rate variability, temperature, step count form it. I have no idea regarding this. kindly help me out?</p>
<p>A python code that help me to connect it with smartwatch and collect data from it</p>
",https://stackoverflow.com/questions/76020210/how-to-connect-smartwatch-with-my-laptop-and-then-collect-physiological-signals,False,
76020196,Scraping Links Off Google Search Results,"<p>I am trying to make a program that will output links for the search results of google queries. Here is my program:</p>
<pre><code># import necessary libraries
from bs4 import BeautifulSoup
import requests
import re

headers = {&quot;User-Agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582&quot;}

# assign URL
url_to_scrape = &quot;https://www.google.com/search?q=cats&quot;
  
# create document
html_document = requests.get(url_to_scrape, headers=headers).text
  
# create soap object
soup = BeautifulSoup(html_document, 'html.parser')
  
# find all the anchor tags with &quot;href&quot; 
# attribute starting with &quot;https://&quot;
for link in soup.find_all('a', attrs={'href': re.compile(&quot;^https://&quot;)}):
    # display the actual urls
    print(link.get('href'))  
</code></pre>
<p>I have tried a couple different things but I keep getting this output:</p>
<pre><code>maps.google.com/maps?q=cats&amp;um=1&amp;ie=UTF-8&amp;sa=X&amp;ved=0ahUKEwjv85nI8qr-AhXCjYkEHVQaAd0Q_AUICSgE

www.google.com/search?ei=TRM6ZK-ZE8KbptQP1LSE6A0&amp;q=cats&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwjv85nI8qr-AhXCjYkEHVQaAd0Q7Al6BAgMEAw

preferences?hl=en&amp;fg=1&amp;sa=X&amp;ved=0ahUKEwjv85nI8qr-AhXCjYkEHVQaAd0Q5fUCCPEB

policies.google.com/privacy?hl=en&amp;fg=1

policies.google.com/terms?hl=en&amp;fg=1
</code></pre>
<p>(I had to remove pieces of the links to not set off the spam fiter)</p>
<p>I'm not sure why this program doesn't just print out the first links that google returns from the search. I am a beginner at web scraping so any pointers are appreciated.</p>
",https://stackoverflow.com/questions/76020196/scraping-links-off-google-search-results,False,
76020194,Google API in Colab invalid request,"<p>I am trying to use the google API on my Colab notebook.  I was following <a href=""https://codelabs.developers.google.com/codelabs/gsuite-apis-intro/#0"" rel=""nofollow noreferrer"">this</a> but it seems to be out of date.</p>
<p>I ended up following <a href=""https://colab.research.google.com/github/kmcentush/site/blob/master/_notebooks/2020-08-16-fastapi_google_oauth_part1.ipynb#scrollTo=RlK2EGpAnlfW"" rel=""nofollow noreferrer"">this</a>  to know how to make my OAuth 2.0 Client IDs</p>
<p>but when I run my code  and I click on the link I am getting this error :
You can’t sign in because this app sent an invalid request. You can try again later, or contact the developer about this issue.</p>
<p>here is my code :</p>
<pre><code>!pip install httpx
!pip install starlette
!pip install fastapi

from typing import Optional
import os
from starlette.requests import Request
from fastapi import FastAPI
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# If modifying these SCOPES, delete the file token.json.
SCOPES = ['https://www.googleapis.com/auth/drive.metadata.readonly']

def get_authenticated_service():
 creds = None
 if os.path.exists('token.json'):
    creds = Credentials.from_authorized_user_file('token.json', SCOPES)
 if not creds or not creds.valid:
    if creds and creds.expired and creds.refresh_token:
     creds.refresh(Request())
    else:
     flow =InstalledAppFlow.from_client_secrets_file('/content/client_secret_99.apps.googleusercontent.com.json', SCOPES)
        creds = flow.run_local_server(port=0)
    with open('token.json', 'w') as token:
        token.write(creds.to_json())

  try:
   service = build('drive', 'v3', credentials=creds)
   return service
  except HttpError as error:
    print(f&quot;An error occurred: {error}&quot;)
    return None

 def list_drive_files(service, orderBy='modifiedByMeTime desc', pageSize=5):
  results = service.files().list(
    pageSize=pageSize, orderBy=orderBy, fields=&quot;nextPageToken, files(id, name, mimeType)&quot;).execute()
items = results.get('files', [])

if not items:
    print('No files found.')
else:
    print('Files:')
    for item in items:
     print(f&quot;{item['name']} ({item['mimeType']})&quot;)

 if __name__ == '__main__':
  os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'
  service = get_authenticated_service()
  list_drive_files(service, orderBy='modifiedByMeTime desc', pageSize=5)
</code></pre>
<p>So what I am missing ?</p>
",https://stackoverflow.com/questions/76020194/google-api-in-colab-invalid-request,False,
76020182,Pandas2.0 removed df.append,"<p>As the new version of pandas, pandas 2.0, removed the df.append method, how to modify the following code to add a dictionary to a pandas dataframe.</p>
<p>The old version of code is:</p>
<pre><code>record_score = {}
record_score[&quot;model_name&quot;] = model_name
record_score[&quot;time&quot;] = crt_time
record_score[&quot;epoch&quot;] = best_epoch
record_score[&quot;best_score&quot;] = best_score

# save best to file
record_path = &quot;./record.csv&quot;
if not os.path.exists(record_path):
    record_table = pd.DataFrame()
else:
    record_table = pd.read_csv(record_path)
record_table = record_table.append(record_score, ignore_index=True)
record_table.to_csv(record_path, index=False)
</code></pre>
<p>I think now i can just create a new dataframe from the record_score and use pandas.concat to concat the data.</p>
<p>I hope someone can give me some advice to modify the code with an efficient way. Thanks a lot.</p>
",https://stackoverflow.com/questions/76020182/pandas2-0-removed-df-append,False,
76020167,How to access the Date &amp; time in the Settings on Windows using pywinauto?,"<pre><code>English:
Settings &gt; Time &amp; language &gt; Date &amp; Time &gt; Set time automatically

Portuguese:
Configurações &gt; Hora e idioma &gt; Data e hora &gt; Definir horário automaticamente
</code></pre>
<p>My computer is not synchronizing the time when it restarts, so I want to automate this synchronization when I activate the Python code.</p>
<p>I want to perform the action of turning off and on the <code>Set time automatically</code> option inside a <code>Date &amp; Time</code>:</p>
<p><a href=""https://i.stack.imgur.com/ZGzm3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZGzm3.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/iSgYz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iSgYz.png"" alt=""enter image description here"" /></a></p>
<p>I have tried in English (Because I noticed that even my Windows 11 in other language, &quot;control system&quot; is used to open &quot;Settings&quot;):</p>
<pre class=""lang-python prettyprint-override""><code>from pywinauto import Application

#open Setting
app = Application(backend=&quot;uia&quot;).start(&quot;control system&quot;)
#open Time &amp; language
app = Application(backend=&quot;uia&quot;).connect(title=&quot;Time &amp; language&quot;, timeout=20)
#open Date &amp; Time
app.window(title=&quot;Time &amp; language&quot;).child_window(title=&quot;Date &amp; time&quot;).invoke()
#turn off
app.window(title=&quot;Date &amp; time&quot;).child_window(title=&quot;Set time automatically&quot;, control_type=&quot;CheckBox&quot;).click()
#turn on
app.window(title=&quot;Date &amp; time&quot;).child_window(title=&quot;Set time automatically&quot;, control_type=&quot;CheckBox&quot;).click()
</code></pre>
<p>And in portuguese (because my windows is Portuguese):</p>
<pre class=""lang-python prettyprint-override""><code>from pywinauto import Application

#open Setting
app = Application(backend=&quot;uia&quot;).start(&quot;control system&quot;)
#open Time &amp; language
app = Application(backend=&quot;uia&quot;).connect(title=&quot;Hora e idioma&quot;, timeout=20)
#open Date &amp; Time
app.window(title=&quot;Hora e idioma&quot;).child_window(title=&quot;Data e hora&quot;).invoke()
#turn off
app.window(title=&quot;Data e hora&quot;).child_window(title=&quot;Definir horário automaticamente&quot;, control_type=&quot;CheckBox&quot;).click()
#turn on
app.window(title=&quot;Data e hora&quot;).child_window(title=&quot;Definir horário automaticamente&quot;, control_type=&quot;CheckBox&quot;).click()
</code></pre>
<p>But the page always remains on the initial page of the Settings app, it doesn't move to where I want it to. What should I do?</p>
",https://stackoverflow.com/questions/76020167/how-to-access-the-date-time-in-the-settings-on-windows-using-pywinauto,False,
76020133,How to get Python tkinter application to run automatically (and reliably) at RPi startup?,"<p>I would like to run a Python tkinter application on a Raspberry PI (4b) at startup without the need to manually run the script once the device is turned on.</p>
<p>The target experience is to plug in the device and have it directly boot into the Python tkinter GUI.</p>
<p>There are a lot of questions and answers out there for how to run a Python program at RPi boot, however, there are some common issues that prevent it from working consistently with a GUI application. I have spent a lot of time testing the best way to do this an wanted to share the best approach I found. See below for details!</p>
",https://stackoverflow.com/questions/76020133/how-to-get-python-tkinter-application-to-run-automatically-and-reliably-at-rpi,False,
76020132,How to find files from a folder in an app file Mac,"<p>I have a .app file made with py2app which is supposed to show the contents of the folder on a localhost server but I realized that it only shows the files inside the .app and not the folder it's in, is there a way to make it so that it shows the files from the folder that it's in?
Here's my code:</p>
<pre><code>import socketserver

PORT = 8080
Handler = http.server.SimpleHTTPRequestHandler

with socketserver.TCPServer((&quot;&quot;, PORT), Handler) as httpd:
    print(&quot;serving at port&quot;, PORT)
    httpd.serve_forever()
</code></pre>
",https://stackoverflow.com/questions/76020132/how-to-find-files-from-a-folder-in-an-app-file-mac,False,
76020123,Error parsing font string in matplotlib stylesheet,"<p>I want to use a custom font in my matplotlib plots. I would like to use path of the .ttf file in the stylesheet, for example:</p>
<pre><code>mathtext.it: &quot;/path/to/fontname-Italic-VariableFont_wght.ttf&quot;
</code></pre>
<p>But when using this stylesheet the python script gives the following warning:</p>
<blockquote>
<p>Bad value in file '~/path/to/stylesheet.mplstyle', line 30
('mathtext.it:  &quot;/path/to/fontname-Italic-VariableFont_wght.ttf&quot;'):
Key mathtext.it: Could not parse font string:
'&quot;/path/to/fontname-Italic-VariableFont_wght.ttf&quot;' Expected end of
text, found '-'  (at char 36), (line:1, col:37)</p>
</blockquote>
",https://stackoverflow.com/questions/76020123/error-parsing-font-string-in-matplotlib-stylesheet,False,
76020081,How does this double loop list creation work?,"<p>The top answer to <a href=""https://stackoverflow.com/questions/7946798/interleave-multiple-lists-of-the-same-length-in-python"">Interleave multiple lists of the same length in Python</a> uses a really confusing syntax to interleave two lists <code>l1</code> and <code>l2</code>:</p>
<pre><code>l1 = [1,3,5,7]
l2 = [2,4,6,8]
l3 = [val for pair in zip(l1, l2) for val in pair]
</code></pre>
<p>and somehow you get <code>l3 = [1,2,3,4,5,6,7,8]</code></p>
<p>I understand how list comprehension with a single &quot;<code>for z in y</code>&quot; statement works, and I can see that <em>somehow</em> we're looping through the zip, and within that through each tuple. But how does that turn a list of four tuples into an interleaved list of eight? Totally bewildered by this line of code.</p>
",https://stackoverflow.com/questions/76020081/how-does-this-double-loop-list-creation-work,True,
76020065,detecting and segmenting squares from the image of a form,"<p>I'm trying to segment an image of a datasheet into the squares where data is written and save them as separate jpegs so that I can use them to train a neural network to read the handwriting. This is to save us from entering the data manually, which is a long, error prone process.</p>
<p>In an ideal world I would use tablets and skip the data entry all together however there are none that are both waterproof and work well.</p>
<p>I've tried with contour detect and adjusting threshholds as well as houghline transform but to no avail.</p>
<p>I've tried making a thicker outline around the region where data is entered and getting it to detect that then dividing the region by the number of cells but no luck</p>
<p>I was hoping someone could suggest a different approach that I could try.</p>
<p>The first image below is of the datasheet, the second is of the data sheet with the region I'm interested in highlighted in blue. Within the region, I want to extract the written categories ie the 5+, 2- etc as separate images. I'm also open getting a portable scanner to make it more consistent if anyone thinks that would help.</p>
<p>[<img src=""https://i.stack.imgur.com/6SL5L.jpg"" alt=""Image showing the data sheet1"" /></p>
<p><a href=""https://i.stack.imgur.com/YLQhB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YLQhB.jpg"" alt=""Image showing the region of interest within the datasheet"" /></a></p>
",https://stackoverflow.com/questions/76020065/detecting-and-segmenting-squares-from-the-image-of-a-form,False,
76020060,h2o sparkling: Error reading MOJO JSON: Object not supported:,"<p>I'm running the Sparkling Water automl example (<a href=""https://docs.h2o.ai/sparkling-water/3.3/latest-stable/doc/ml/sw_automl.html"" rel=""nofollow noreferrer"">https://docs.h2o.ai/sparkling-water/3.3/latest-stable/doc/ml/sw_automl.html</a>) on a local spark install on Windows (added code at the top setting up the SparkSession).</p>
<p>My versions:</p>
<pre><code>python:3.8.10 (default, May 19 2021, 13:12:57) [MSC v.1916 64 bit (AMD64)]
spark version:3.2.2
pyspark:3.2.2
h2o:3.40.0.2
pysparkling:3.40.0.2-1-3.2
</code></pre>
<p>When I step-over the line that calls automl.fit(), the training apparently works (details and leaderboard look good), but I get a long series of errors:</p>
<pre><code>04-14 19:17:25.105 192.168.0.229:54321   13836    Thread-3 ERROR water.default: Error reading MOJO JSON: Object not supported: 
04-14 19:17:25.105 192.168.0.229:54321   13836    Thread-3 ERROR water.default:  {&quot;__meta&quot;:{&quot;schema_version&quot;:3,&quot;schema_name&quot;:&quot;KeyV3&quot;,&quot;schema_type&quot;:&quot;Key&lt;Keyed&gt;&quot;},&quot;name&quot;:&quot;DeepLearning_grid_1_AutoML_1_20230414_191556_model_1&quot;,&quot;type&quot;:&quot;Key&lt;Keyed&gt;&quot;,&quot;URL&quot;:null} 
04-14 19:17:25.107 192.168.0.229:54321   13836    Thread-3 ERROR water.default: Error reading MOJO JSON: Object not supported: 
04-14 19:17:25.107 192.168.0.229:54321   13836    Thread-3 ERROR water.default:  {&quot;__meta&quot;:{&quot;schema_version&quot;:3,&quot;schema_name&quot;:&quot;KeyV3&quot;,&quot;schema_type&quot;:&quot;Key&lt;Keyed&gt;&quot;},&quot;name&quot;:&quot;XRT_1_AutoML_1_20230414_191556&quot;,&quot;type&quot;:&quot;Key&lt;Keyed&gt;&quot;,&quot;URL&quot;:null} 
...
</code></pre>
<p>Has anybody seen this?  Can I safely ignore these errors?</p>
",https://stackoverflow.com/questions/76020060/h2o-sparkling-error-reading-mojo-json-object-not-supported,False,
76020024,Application architecture planning multimedia applications,"<h1>SQL database</h1>
<ol>
<li>ID <em>(number in the database)</em></li>
<li>Name</li>
<li>Path to the logo <em>(local and web)</em></li>
<li>Screenshots and videos <em>(in array format) (local and web)</em></li>
<li>Number of views</li>
<li>Number of impressions</li>
<li>Type <em>(movie, anime, cartoon)</em></li>
<li>Type of title <em>(multi-series, full-length, OVA, etc.)</em></li>
<li>Average length of the series</li>
<li>Number of episodes</li>
<li>Studio creator</li>
<li>The ways to the titles themselves:
<code>{ &quot;AniLibria&quot;: { #The name of the voiceover 0: [ #urls series number  url ] }</code></li>
</ol>
<h1>Functional</h1>
<ol>
<li>Search by name</li>
<li>Random title by filters</li>
<li>Catalog with filters and sorting</li>
</ol>
<p>Filters – by type, title type, average length of the series, number of episodes, voiceover studio, creator studio</p>
<p>Sorting – number of episodes, number of impressions, number of views</p>
<p>Provide the function by filling out the submit for moderation form after it will be added to the catalog</p>
<h1>The structure of the application:</h1>
<ol>
<li>API <em>(stupid functions)</em></li>
<li>Data <em>(everything that is uploaded to the server except SQL – it lies in the API)</em></li>
<li>The application itself, which includes…
3.1. Telegram bot
3.2. Discord bot
All as separate programs, work independently <em>(we provide errors that some of them are easy)</em>. The structure where each service is independent is based on the ideas of Yandex.</li>
</ol>
<h1>ATTENTION QUESTION…</h1>
<ol>
<li>How to create filtering by voice-over studios if it is stored as a dictionary in the table</li>
<li>In general, how difficult is it to implement this system for a person who has experience only in game development + a little knowledge in python? <em>(in SQL, a complete layman)</em></li>
</ol>
<h1>1</h1>
<p>In general, my main question is about filters. Since I assume that there will be different voiceovers, different series (and several variants of the sources of these series. Then, as a person who has not worked with databases before, I see the simplest option: Dictionary -&gt; Dictionary -&gt; Array and write this structure as a unit in an SQL table. But if I want to implement filters, there are difficulties. I know that there is a built-in filter in SQL, but my structure obviously will not be able to work correctly with filters.
<code>{ &quot;AniLibria&quot;: { #The name of the voiceover 0: [ #urls series number  url ] }</code></p>
<h1>2</h1>
<p>And of course I wonder how difficult it is to write such a backend of an application <em>(and here we are not talking about filling the database and related problems, namely the code)</em>.</p>
",https://stackoverflow.com/questions/76020024/application-architecture-planning-multimedia-applications,False,
76020013,What are levels in metatrader indicator window and how do I code them in Python?,"<p>I'm trying to put level indicator (like those in metatrader5 indicator window) in my trading bot but I don't know the logic behind their algorithm.</p>
<p>I have tried using the highest candle in 74 frames as my pick and the lowest as my trough. Then work out the current price percentage using those bounds to find the level. This technique seems not to do the job. The following code shows how.</p>
<pre><code>def get_lvls(candles,ichmoku,window_size=73):
    
'''
    function to get price levels
    
@param: candles as a list of closing prices
    

@param: ichmoku as a list of derived ichimoku keisen values of the candles
    

@param : window_size as the frame (number of candles)to work with at a time
    '''
    

#wl as the current frame list
    
  wl=[]
    
    #the levels list
    lvls=[]
    
    
for i in range(len(candles)):
    
        #check if the frame is full. If its full delete the 1st candle so as to accommodate the next one
        if len(wl)&gt;window_size:
            
            del wl[0]
        
        wl.append(candles[i])
        
        # get the highest candle 
        maxi = max(wl)

    #get the lowest candle
        mini = min(wl)
        
            
        
        #ich value is the value we use to get the level percentage. we get it from current ichmuko value - the lowest candle
        
        ich_value = ichmoku[i] - mini
        
        #the quotient for getting the level wch is obtained by max - min 
        quotient = maxi - mini 
        
        #then use the two values to get the percentage
        
        try:
            lvl=(ich_value/quotient) * 100
        except ZeroDivisionError:
            lvl='0'
        
        #insert the percentage into the lvls list
        lvls.append(lvl)
    

    return lvls
</code></pre>
",https://stackoverflow.com/questions/76020013/what-are-levels-in-metatrader-indicator-window-and-how-do-i-code-them-in-python,False,
76019997,A Beginner Working on a Real Task with Python and EXIF,"<p>Please go easy on me. It is about 3 decades since I coded anything ans I knew nothing about python until yesterday.</p>
<p>I have a a problem that I have been working on. I've gone through various iterations of the code and I stall at the same point. Reading the exif data from one file of a group of files and writing that to the other three files in the group.</p>
<p><strong>More About the Problem</strong></p>
<p>I have a drone for agricultural work. It has a Seqouia multispectral (MS) camera. The MS camera takes four images at one time and writes the files to onboard SD card.
The camera records these four images plus one RGB image. Sometimes the write rate affects the time stamp on usually one but sometimes two of the images in each group of four MS images. This affects the way that the &quot;structure from motion&quot; application processes the files. In fact it means that the app cannot do so. The app looks for Image DateTime stamp (EXIF 306) tag that matches. If there is no match, the app processing fails. I should point out that other paid services can process the images fine because they apparently use a different method.</p>
<p><strong>The code and structure of the files</strong></p>
<p>I first wrote a simple script to take the images from one path and separate the MS from RGB images, and place each in a sperate folder. Great, feeling successful!</p>
<p>I then played around with various ways of searching the new path for the MS images for tif files, reporting the number found (in this case 2472), this is a great validation when dealing with lots of files, then group those files in groups of four according to the first 23 characters of the file name.</p>
<p>NOTE: files generated by the MS camera are always consistently named according to this convention below</p>
<p>IMG_YYYYMMDD_HHMMSS_INTE_BND.TIF</p>
<p>Where YYYY is year
MM is month
DD is date
INTEG is an integer of four digits
BND is BAND in three characters</p>
<p>Bands are GRE, RED, REG and NIR</p>
<p>Example: IMG_230406_233811_0001_GRE.TIF</p>
<p>So I wrote various iterations of scripts that searched for, found and correctly reported the number of files and also correctly grouped the files (by file name prefix) and reported the correct number of groups. They also reported on groups that had less than four files, due to the camera failing to write some images in that group of four. This apparently can happen when shooting rate is too high for the altitude and image overlap parameters.</p>
<p>The details of the groups of four files were placed in  a dictionary and also written to a .csv file in the output path.</p>
<p>A snippet of the .csv file is below</p>
<p><a href=""https://i.stack.imgur.com/GLIWp.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>So the area of difficulty I am running in to is this</p>
<p>I want to iterate over each of these groups of four (there are 618 groups) and do a number of things</p>
<ol>
<li>check for four files if not, skip that group and report skipped group (I had this working)</li>
<li>check that the four file names (path names) contain the four bands. It seems these are recorded as truncated labels in the dictionary RED = ED, GRE = RE, NIR = IR an REG = EG. this would seem the best to use but I could not get it tot work by searching for these. I also could not get the code to successfully find BND name in the path (tif_path) in the dictionary.</li>
<li>Read the exif 306 tag in the GRE file and write it to the other three files in that group of four.</li>
</ol>
<p>The reason for writing the from the GRE file to the others is that the GRE file eg. IMG_230406_233811_0001_GRE.TIF is the first to write to SD and has the correct timestamp
All we need to do is ensure the other three MS files match</p>
<p>The problems I am having are in this interation</p>
<p>Many versions of the code I have written, falsely report missing bands.</p>
<p>Or cannot find the exif tag either by searching for DateTime Image_DateTime or exif306</p>
<p>I have been able to read all of the tags i earlier versions of my script and write them to console, so I know the tag is called Image DateTime</p>
<p>One version of code which correctly identified file numbers and groups but falsley reported missing bands is below</p>
<pre><code>from datetime import datetime
import os
from pathlib import Path
from typing import List, Dict
from PIL import Image


def read_datetime_from_file(file_path: str) -&gt; datetime:
    with Image.open(file_path) as im:
        exif = im._getexif()
        date_str = exif.get(36867)
        return datetime.strptime(date_str, '%Y:%m:%d %H:%M:%S')


def write_datetime_to_files(file_paths: List[str], datetime_obj: datetime, datetime_tag: str = 'EXIF DateTimeOriginal'):
    for file_path in file_paths:
        with open(file_path, 'rb+') as file:
            tags = exifread.process_file(file, details=False)
            tags[datetime_tag] = datetime_obj.strftime('%Y:%m:%d %H:%M:%S')
            file.seek(0)
            for tag, value in tags.items():
                if tag.startswith(datetime_tag):
                    continue
                file.write(f'{tag}: {value}\n'.encode('utf-8'))
            file.truncate()


def main():
    input_folder = Path(input(&quot;Enter the path to the input folder containing TIF files: &quot;))
    output_folder = Path(input(&quot;Enter the path to the output folder: &quot;))

    print(f&quot;Searching for files with extensions ['.TIF'] in {input_folder}...&quot;)
    tif_paths = list(input_folder.glob('*.TIF'))
    print(f&quot;Found {len(tif_paths)} TIF files in {input_folder}&quot;)

    # Group files by prefix (assumes prefix is the same for all files in the group)
    tif_groups = {}
    for path in tif_paths:
        tif_prefix = path.name[:23]
        if tif_prefix not in tif_groups:
            tif_groups[tif_prefix] = {'RE': None, 'IR': None, 'ED': None, 'EG': None}
        for band in ['RE', 'IR', 'ED', 'EG']:
            if band in path.name:
                tif_groups[tif_prefix][band] = path

    print(f&quot;Grouping {len(tif_paths)} TIF files...&quot;)
    tif_groups = {k: v for k, v in tif_groups.items() if None not in v.values()}
    print(f&quot;Grouped into {len(tif_groups)} groups.&quot;)

    # Loop over the TIF groups
    for tif_prefix, tif_paths in tif_groups.items():
        if len(tif_paths) != 4:
            print(f&quot;Skipping group {tif_prefix}: incomplete group of TIF files&quot;)
            continue
        bands = [path.name[23:26] for path in tif_paths.values()]
        missing_bands = set(['GRE', 'RED', 'REG', 'NIR']) - set(bands)
        if missing_bands:
            print(f&quot;Skipping group {tif_prefix}: missing bands {', '.join(missing_bands)}&quot;)
            continue

        # Read DateTime value from GRE file
        gre_datetime = read_datetime_from_file(tif_paths['GRE'])

        # Write DateTime value to other files in group
        for band in ['RED', 'REG', 'NIR']:
            write_datetime_to_files([tif_paths[band]], gre_datetime)


if __name__ == '__main__':
    main()


</code></pre>
<p>The first few lines of the output are here</p>
<p>Enter the path to the input folder containing TIF files: D:/Original
Enter the path to the output folder: D:Corrected
Searching for files with extensions ['.TIF'] in D:\Original...
Found 2472 TIF files in D:\Original
Grouping 2472 TIF files...
Grouped into 618 groups.
Skipping group IMG_230406_233809_0000_: missing bands GRE
Skipping group IMG_230406_233811_0001_: missing bands GRE
Skipping group IMG_230406_233813_0002_: missing bands GRE
Skipping group IMG_230406_233816_0003_: missing bands GRE
Skipping group IMG_230406_233818_0004_: missing bands GRE</p>
<p>I have tried in later scripts to read the tag like this</p>
<pre><code>def main ()
# Loop over the TIF groups
    for tif_prefix, tif_paths in tif_groups.items():
        expected_labels = ['RE', 'ED', 'EG', 'IR']
        found_labels = []
        for label in expected_labels:
            for path in tif_paths.values():
                if str(path.name).startswith(label):
                    found_labels.append(label)
                    break
        if sorted(found_labels) != expected_labels:
            missing_labels = set(expected_labels) - set(found_labels)
            print(f&quot;Skipping group {tif_prefix}: missing labels {', '.join(missing_labels)}&quot;)
            continue
</code></pre>
<p>and looking for exif 306</p>
<pre><code>def read_datetime_from_file(file_path: str) -&gt; datetime:
    with open(file_path, 'rb') as file:
        tags = Image.open(file)._getexif()
        date_str = tags.get(306)
        if date_str is None:
            raise ValueError(&quot;Could not find datetime in EXIF tags&quot;)
        return datetime.strptime(date_str, '%Y:%m:%d %H:%M:%S')


def write_datetime_to_files(file_paths: List[str], datetime_obj: datetime):
    for file_path in file_paths:
        with open(file_path, 'rb+') as file:
            tags = Image.open(file)._getexif()
            tags[306] = datetime_obj.strftime('%Y:%m:%d %H:%M:%S')
            file.seek(0)
            for tag, value in tags.items():
                file.write(f'{tag}: {value}\n'.encode('utf-8'))
            file.truncate()
</code></pre>
<p>I'm sure in these early days I am committing all kinds of coding sins but I'm learning</p>
<p>Any help would be welcome.</p>
<p>The input files are here if you ant to play <a href=""https://www.dropbox.com/scl/fo/wa90ql8i9vx234zy6x0fn/h?dl=0&amp;rlkey=51t13ng8kxqcz10v1bewwxt0w"" rel=""nofollow noreferrer"">https://www.dropbox.com/scl/fo/wa90ql8i9vx234zy6x0fn/h?dl=0&amp;rlkey=51t13ng8kxqcz10v1bewwxt0w</a></p>
<p>I have tried various versions of the scripts but I seem to fail with false reports of missing bands, and not being able to read the tag or find the tag or the file associated with the tag.</p>
<p>My specific questions are can any one assist me to determine the approach I should use (with code example) to 1. Correctly identify the four bands when iterating 2. find and read the exif tag in the GRE file 3. write the value of this tag to the other three files in each group of four?</p>
",https://stackoverflow.com/questions/76019997/a-beginner-working-on-a-real-task-with-python-and-exif,False,
76019995,Get how many fields are null or empty string in DJANGO model,"<p>I've the next model:</p>
<pre class=""lang-py prettyprint-override""><code>class Iniciativa(models.Model):
    iniciativa      = models.CharField(max_length=150)
    canal           = models.ForeignKey(Canales, blank=True, null=True, on_delete=models.CASCADE)
    territorio      = models.JSONField(default=list, blank=True, null=True)
    bigbet          = models.ForeignKey(BigBet, blank=True, null=True, on_delete=models.CASCADE)
    objetivo        = models.TextField(blank=True, null=True)
    call2action     = models.TextField(blank=True, null=True)
    dialogo_valor   = models.TextField(blank=True, null=True)
    cobertura       = models.TextField(blank=True, null=True)
    kpi_medicion    = models.TextField(blank=True, null=True)
</code></pre>
<p>I made a command to pass the information from a file to the database, there is a field that carries the % of the filling progress, from the front I manage it with an event every time something changes but I don't know how to emulate that behavior from my command I thought if there is a simpler way other than manually checking each field to see the progress?</p>
",https://stackoverflow.com/questions/76019995/get-how-many-fields-are-null-or-empty-string-in-django-model,False,
76019985,Is there a way to set the angle mode to degrees in python math?,"<p>Is there a way to set the angle mode to <em>degrees</em> in python math?</p>
<p>You know that in python, the trigonometric functions are default in <em>radians</em> mode</p>
<pre class=""lang-py prettyprint-override""><code>from math import sin, radians
print(sin(radians(30)))
</code></pre>
<p>When I work with <em>degrees</em>, it seems redundant to keep typing <code>radians(...)</code>. <br></p>
<pre class=""lang-py prettyprint-override""><code>from math import sin, radians

def sin2(degrees):
    return sin(radians(degrees))

## or
sin2 = lambda degrees: sin(radians(degrees))

print(sin2(30))
</code></pre>
<p>It seems fine to just define functions like those,<br>
but, does anyone know a better way of doing this? Setting the mode in math maybe?</p>
",https://stackoverflow.com/questions/76019985/is-there-a-way-to-set-the-angle-mode-to-degrees-in-python-math,True,
76019972,How to get rid of annoying popups in Pycharm that make it hard to code fluidly?,"<p>How can I disable all of these popups? With them there I have to click to make them go away just so I can type on the next line. It's extremely annoying.<img src=""https://i.stack.imgur.com/wswn2.png"" alt=""!enter image description here"" /></p>
<p>I tried disable code completion but it did not get rid of these popups.</p>
",https://stackoverflow.com/questions/76019972/how-to-get-rid-of-annoying-popups-in-pycharm-that-make-it-hard-to-code-fluidly,False,
76019932,Django: &#39;bool&#39; object is not subscriptable when trying to get value from queryset,"<p>I am trying to get a value from a queryset, but i keep getting this error that says <code>'bool' object is not subscriptable</code>.</p>
<p>I have a queryset that looks like this <code>tax_rate_fee = TaxRate.objects.filter(country=cartorder.country).exists()</code> and i am checking to see if a specific country exists in the Queryset, then i want to grab a value from the queryset and perform some operation.</p>
<p>This is my code</p>
<pre><code>tax_rate_fee = TaxRate.objects.filter(country=cartorder.country).exists()
if tax_rate_fee:
    cartorderitem.vat = 5 * tax_rate_fee['rate']
</code></pre>
<p>this is the tax rate fee model</p>
<pre><code>class TaxRate(models.Model):
    country = models.CharField(max_length=200)
    rate = models.IntegerField(default=5, help_text=&quot;Numbers added here are in percentage (5 = 5%)&quot;)
    active = models.BooleanField(default=True)

</code></pre>
",https://stackoverflow.com/questions/76019932/django-bool-object-is-not-subscriptable-when-trying-to-get-value-from-queryse,True,76019980
76019931,Can VS Code suggest all dataframe column labels?,"<p>Is there any way to make VS Code suggest all dataframe column labels?</p>
<p><a href=""https://i.stack.imgur.com/ashxN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ashxN.png"" alt=""screenshot showing suggestions list"" /></a></p>
<p>From the above sample dataframe, I expect all the column labels (Country, Product, Price, Qty) is going to popup. But none show up after I select 'Country'.</p>
",https://stackoverflow.com/questions/76019931/can-vs-code-suggest-all-dataframe-column-labels,False,
76019929,pytorch transformer with different dimension of encoder output and decoder memory,"<p>The <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"" rel=""nofollow noreferrer"">Pytorch Transformer</a> takes in a <code>d_model</code> argument</p>
<p>They say <a href=""https://discuss.pytorch.org/t/using-different-feature-size-between-source-and-target-nn-transformer/139525/2?u=noam_salomonski"" rel=""nofollow noreferrer"">in the forums</a> that</p>
<blockquote>
<p>the <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">transformer model</a> is not based on encoder and decoder having
different output features</p>
</blockquote>
<p>That is correct, but shouldn't limit the Pytorch implementation to be more generic. Indeed, in the paper all data flows with the same dimension == <code>d_model</code>, but this shouldn't be a theoretical limitation.</p>
<p><strong>I am looking for the reason why Pytorch's transformer isn't generic in this regard, as I am sure there is a good reason</strong></p>
<hr />
<h2>My attempt at understanding this</h2>
<p><a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"" rel=""nofollow noreferrer"">Multi-Head Attention</a> takes in <code>query</code>, <code>key</code> and <code>value</code> matrices which are of orthogonal dimensions.<br />
To mu understanding, that fact alone should allow the transformer model to have one output size for the encoder (the size of its input, due to skip connections) and another for the decoder's input (and output due to skip connections).</p>
<p>Now, looking at the Multi-Head Attention layer in the decoder which takes in Q from the decoder, and K, V from the encoder. I fail to see why K, V can't be of different dimension than Q, even with the skip connection. We could just set <code>d_Q==d_decoder==layer_output_dim</code> and <code>d_K==d_V==encoder_output_dim</code>, and everything would still work, because Multi-Head Attention should be able to take care of the different embedding sizes.</p>
<p><strong>What am I missing, or, how to write a more generic transformer, without breaking Pytorch completely and writing ot all from scratch?</strong></p>
<p><a href=""https://i.stack.imgur.com/Usett.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Usett.png"" alt=""enter image description here"" /></a></p>
",https://stackoverflow.com/questions/76019929/pytorch-transformer-with-different-dimension-of-encoder-output-and-decoder-memor,False,
76019913,Calculating the Normalized Device Coordinates from World Coordinates,"<p>After researching, I tried to implement a Vector3 object that represents a 3D point. I want to get the Normalized device coordinates, which if I understood correctly, the x, y, z values should fall in range of [-1, 1]. But my current implementation does not always fall in range. Here is the object:</p>
<pre class=""lang-py prettyprint-override""><code>WIN_SIZE: int = 800  # square window
ASPECT_RATIO: float = 1  # square window

FOV: float = math.radians(60)
SCALING_FACTOR: float = 1 / math.tan(FOV / 2)
Z_FAR: float = 100
Z_NEAR: float = 1

class Vector3:
    def __init__(self, x: float, y: float, z: float) -&gt; None:
        self.x: float = x
        self.y: float = y
        self.z: float = z
    
    def xyzw(self) -&gt; np.array:
        return np.array([self.x, self.y, self.z, 1])
    
    def projection_matrix(self) -&gt; np.array:
        m = np.zeros((4, 4))
        m[0, 0] = ASPECT_RATIO * SCALING_FACTOR
        m[1, 1] = SCALING_FACTOR
        m[2, 2] = Z_FAR / (Z_FAR - Z_NEAR)
        m[2, 3] = (-Z_FAR * Z_NEAR) / (Z_FAR - Z_NEAR)
        m[3, 2] = 1
        return m

    def normalized_device_coordinates(self) -&gt; np.array:
        result = np.dot(self.projection_matrix(), self.xyzw())
        x, y, z, w = result

        if w != 0:
            result[0] /= w
            result[1] /= w
            result[2] /= w
        
        return result
</code></pre>
<p>A simple test case such as <code>Vector3(6, 0, 10).normalized_device_coordinates()</code> outputs <code>[ 1.03923048, 0, 0.90909091, 10]</code>, which shows the x value out of the range of [-1, 1]. Are there problems with maybe my projection matrix representation?</p>
<p>I followed <a href=""https://www.youtube.com/watch?v=EqNcqBdrNyI"" rel=""nofollow noreferrer"">this video</a> for the explaination of the projection matrix and NDC.</p>
",https://stackoverflow.com/questions/76019913/calculating-the-normalized-device-coordinates-from-world-coordinates,False,
76019893,How to Hide/Delete Index Column From Matplotlib Dataframe-to-Table,"<p>I am trying to illustrate a dataframe that aggregates values from various statistical models into a single table that is presentable. With the below code, I am able to get a table but I can't figure out how to get rid of the index column, nor how to gray out the grid lines. Is there anyway I can do this? Or are there better methods to achieve such a table image?</p>
<p>'''</p>
<pre><code>df_perf = df_perf.round(5)
dfig, ax = plt.subplots()

fig.patch.set_visible(True)
ax.axis('off')

tab = ax.table(cellText = df_perf.values,
               colLabels = df_perf.columns,
               rowLabels = df_perf.index,
               rowColours = [&quot;steelblue&quot;] * df_perf.shape[0],
               colColours = [&quot;steelblue&quot;] * df_perf.shape[1],
               cellColours=[[&quot;whitesmoke&quot;] * df_perf.shape[1]] * df_perf.shape[0],
               cellLoc = 'center',
               loc = 'upper center')

tab.auto_set_column_width(col=list(range(df_perf.shape[1])))
tab.auto_set_font_size(False)
plt.title('Figure 12 - Model Comparisons')
plt.show()
</code></pre>
<p>'''</p>
<p><a href=""https://i.stack.imgur.com/5tBNT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5tBNT.png"" alt=""This code returns this visual"" /></a></p>
",https://stackoverflow.com/questions/76019893/how-to-hide-delete-index-column-from-matplotlib-dataframe-to-table,False,
76019887,Problem with google cloud text to speech using python,"<p>Here are four versions of input texts to <a href=""https://cloud.google.com/text-to-speech"" rel=""nofollow noreferrer"">google cloud text to speech</a>:</p>
<p>Version 1 (This one works fine)</p>
<pre><code>&lt;speak version=&quot;1.1&quot;        xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;        xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;        xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis                  http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;        xml:lang=&quot;en-GB&quot;&gt;

The rain in Spain stays mainly in the plain.

How kind of you to let me come.

&lt;/speak&gt;
</code></pre>
<p>Version 2: Same as Version 1 except trying to insert external audio.
It results in an error</p>
<pre><code>&lt;speak version=&quot;1.1&quot;        xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;        xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;        xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis                  http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;        xml:lang=&quot;en-GB&quot;&gt;

The rain in Spain stays mainly in the plain.
&lt;audio src=&quot;uhm_male.mp3&quot; /&gt;
How kind of you to let me come.

&lt;/speak&gt;
</code></pre>
<p>Version 3: Same as version 2 but with simpler form of </p>
<pre><code>&lt;speak&gt;

The rain in Spain stays mainly in the plain.
&lt;audio src=&quot;uhm_male.mp3&quot; /&gt;
How kind of you to let me come.

&lt;/speak&gt;
</code></pre>
<p>Results in same error as version 3</p>
<p>And, finally, same as version 3 but excluding audio.
This again, works fine</p>
<p>Here is the python code:</p>
<pre><code>import os
from google.cloud import texttospeech_v1

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] =\
 'not_my_real_credentials.json'


def getText(infile_name):
    with open(infile_name, 'r') as fobj:
        intext = fobj.read()
    return intext    

def setVoice(language_code, name, ssml_gender):
    theVoice = tts.VoiceSelectionParams(
        language_code=language_code,
        name=name,
        ssml_gender=ssml_gender
    )
    return theVoice

def doAudioConfig(speaking_rate, pitch, volume_gain_db):
    audioConfig = tts.AudioConfig(
        audio_encoding = tts.AudioEncoding.MP3,
        speaking_rate = speaking_rate,
        pitch = pitch,
        volume_gain_db = volume_gain_db
    )
    return audioConfig

#We try each of these in turn:

#This one works fine
infile_name = './texts/test1.txt'

#This one gives an error message
#infile_name = './texts/test2.txt'

#This one gives same error message as prev
#infile_name = './texts/test3.txt'

#This one works fine
infile_name = './texts/test4.txt'

outfile_name = './audio/audioOutput.mp3'

tts = texttospeech_v1
client = tts.TextToSpeechClient()
language_code = &quot;en-GB&quot;
name = &quot;en-GB-Wavenet-F&quot;
ssml_gender = &quot;FEMALE&quot;
pitch = -8.0
speaking_rate = 0.9
volume_gain_db = 0

intext = getText(infile_name)
print(f'\n{intext}\n')

theVoice = setVoice(language_code, name, ssml_gender)
audioConfig = doAudioConfig(speaking_rate, pitch, volume_gain_db)
synthesis_input = tts.SynthesisInput(ssml=intext)

response = client.synthesize_speech(
input=synthesis_input, voice=theVoice, audio_config=audioConfig)

with open(outfile_name, 'wb') as output1:
    output1.write(response.audio_content)
</code></pre>
<p>And here is the error thrown by version 2 of the text:</p>
<pre><code>Traceback (most recent call last):
  File &quot;D:\py\_new\envo\lib\site-packages\google\api_core\grpc_helpers.py&quot;, line 66, in error_remapped_callable
    return callable_(*args, **kwargs)
  File &quot;D:\py\_new\envo\lib\site-packages\grpc\_channel.py&quot;, line 946, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File &quot;D:\py\_new\envo\lib\site-packages\grpc\_channel.py&quot;, line 849, in _end_unary_response_blocking
    raise _InactiveRpcError(state)
grpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:
        status = StatusCode.INTERNAL
        details = &quot;Internal error encountered.&quot;
        debug_error_string = &quot;{&quot;created&quot;:&quot;@1681519238.596000000&quot;,&quot;description&quot;:&quot;Error received from peer ipv4:142.250.70.170:443&quot;,&quot;file&quot;:&quot;src/core/lib/surface/call.cc&quot;,&quot;file_line&quot;:1075,&quot;grpc_message&quot;:&quot;Internal error encountered.&quot;,&quot;grpc_status&quot;:13}&quot;
&gt;

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;D:\py\_new\ttsgcp2\insertAudio.py&quot;, line 62, in &lt;module&gt;
    response = client.synthesize_speech(
  File &quot;D:\py\_new\envo\lib\site-packages\google\cloud\texttospeech_v1\services\text_to_speech\client.py&quot;, line 497, in synthesize_speech
    response = rpc(request, retry=retry, timeout=timeout, metadata=metadata,)
  File &quot;D:\py\_new\envo\lib\site-packages\google\api_core\gapic_v1\method.py&quot;, line 154, in __call__
    return wrapped_func(*args, **kwargs)
  File &quot;D:\py\_new\envo\lib\site-packages\google\api_core\grpc_helpers.py&quot;, line 68, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.InternalServerError: 500 Internal error encountered.
</code></pre>
<p>Using &quot;.wav&quot; and &quot;.ogg&quot; files results in the same error.</p>
<p>Also tried importing &quot;texttospeech&quot; instead of &quot;texttospeech_v1&quot; Same outcomes.</p>
<p>The external audio I'm trying to import sounds fine on all audio players I've tried. It's just an &quot;uhm&quot; sound. Very short. Duration 0.302 seconds</p>
<p>Can anyone help?</p>
",https://stackoverflow.com/questions/76019887/problem-with-google-cloud-text-to-speech-using-python,False,
76019862,Iteratively replace every cell in a dataframe using values from the original dataframe,"<p>Here's a sample dataframe:<br />
<a href=""https://i.stack.imgur.com/u8luX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u8luX.png"" alt=""example dataframe to update"" /></a></p>
<p>I need to be able to get &quot;2023-01-01&quot; <em>edit: (a string of random numbers, not a true Date object)</em> and &quot;Python is awesome&quot;, and send it through a function(<code>do_calculations(date, phrase)</code>), which will return a new value, which will be put in place where &quot;Python is awesome&quot; used to be.
Then I will send &quot;2023-01-01&quot; and &quot;Is the pizza&quot; through a function and the new returned value will be put in place of &quot;Is the pizza&quot;. Finally, I will get &quot;2023-01-01&quot; and &quot;Pizza&quot; and do the same.</p>
<p>I will then go down the column and do the same with &quot;2023-01-02&quot;, followed by &quot;2023-01-03&quot;, and so forth, until all the cells are replaced.</p>
<p>I've tried something along the lines of:</p>
<pre><code>for i, row in new_df.iterrows():
    print('index: ', i)
    print('row: ', row['Date'], row['Title1'], row.index)
    if row['Title1']:
        text = do_calculations(row['Date'], row['Title1'][0])
        #print(&quot;TEXT:&quot;, text)
        value = new_df.at[i, row.index[1]]
        print(&quot;VALUE:&quot;, value)
        
        new_df.at[i, row.index[2]] = text
</code></pre>
<p>But cannot get it working. I imagine there's another for loop required in there, and better use of the <code>i</code> index.</p>
<p>Whether a new dataframe is made, or the dataframe is updated in-place, does not matter - whichever is faster would be preferred.</p>
<hr />
<p>Here is code to generate the sample dataframe:</p>
<pre><code>import pandas as pd
import random
import datetime

# Create a list of dates
date_rng = pd.date_range(start='1/1/2023', end='1/10/2023', freq='D')

# Generate random phrases
phrases = ['Hello world', 'Python is awesome', None, 'Data science is fun', 'I love coding', 'Pandas is powerful', 'Pineapples', 'Pizza', 'Krusty', 'krab', 'Is the pizza']

# Create an empty DataFrame
df = pd.DataFrame(columns=['Date', 'title1', 'title2', 'title3'])

# Populate DataFrame with random phrases
for date in date_rng:
    # Generate random phrases for each column
    row = [date]
    row.extend(random.sample(phrases, 3))
    
    # Append row to DataFrame
    df = df.append(pd.Series(row, index=df.columns), ignore_index=True)

# Print DataFrame
print(df)
</code></pre>
<p>edit: I've clarified that one of the arguments being passed is a string of numbers, not a true date object, which most of the answers seem to be accounting for.</p>
",https://stackoverflow.com/questions/76019862/iteratively-replace-every-cell-in-a-dataframe-using-values-from-the-original-dat,False,
76019861,Add values into a list,"<p>Hope you're all good. So, I have this code where at the end, I'd like to append values to a global list, but when I call the list, it returns an empty list. It's not adding the values that the function throws.</p>
<pre class=""lang-py prettyprint-override""><code>den = []        # denominador
num_p = []      # numerador play
num_end = []    # numerador end

def Ocurrencias(Datos):
    episodios = []
    global den, num_end, num_p
    for _ in range(len(Datos)):
        x = Datos[_].split(',')
        episodios.extend(x)
    denominador = 0           # abajo
    nominador_p = 0           # arriba
    nominador_e = 0           # arriba
    for i in range(0, len(episodios)-3,4):
        episodios[i]          # estado fuente
        episodios[i+1]        # acción
        denominador += 1
            
        if 'play' in episodios[i+2]:   # estado destino
            nominador_p += 1
        else:
        #elif &quot;end&quot; in episodios[i+2]:
            nominador_e += 1
        episodios[i+3]        # recompensa
    den.append(denominador)     # se supone que debería guardar los valores a la lista
    num_p.append(nominador_p)
    num_end.append(nominador_e)
    x = f&quot;Veces que ocurre (play, continue) : {denominador}&quot; #type: ignore
    y = f&quot;Veces que ocurre (play, continue, play) : {nominador_p}&quot; #type: ignore
    z = f&quot;Veces que ocurre (play, continue, end)  : {nominador_e}&quot;  #type: ignore
    w = f&quot;Probabilidad de (play, continue, play)  : {nominador_p/denominador} &quot;   #type: ignore
    return x, y, z, w
</code></pre>
",https://stackoverflow.com/questions/76019861/add-values-into-a-list,False,
76019850,Extract information in JSON format from Pandas Dataframe and Append to Dataframe,"<p>Grateful for your help. I have data in JSON format within a dataframe. I'm trying to extract into new columns and append to the existing dataframe. Here's what my dataframe looks like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Company</th>
<th>Attribution</th>
</tr>
</thead>
<tbody>
<tr>
<td>Papa John's</td>
<td>Papa John's JSON data</td>
</tr>
<tr>
<td>KFC</td>
<td>JSON data</td>
</tr>
</tbody>
</table>
</div>
<p>Here's what the JSON data looks like:</p>
<pre><code>[{'domain':'papajohns.com','country':'USA'},{'domain':'papajohns.co.uk,'country':'UK}] [{'domain':'kfc.com','country':'USA'},{'domain':'kfc.co.uk,'country':'UK}]
</code></pre>
<p>And the results should be:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Company</th>
<th>domain</th>
<th>country</th>
</tr>
</thead>
<tbody>
<tr>
<td>Papa John's</td>
<td>papajohns</td>
<td>USA</td>
</tr>
<tr>
<td>Papa John's</td>
<td>papajohns UK</td>
<td>UK</td>
</tr>
<tr>
<td>KFC</td>
<td>kfc</td>
<td>USA</td>
</tr>
<tr>
<td>KFC</td>
<td>kfc UK</td>
<td>UK</td>
</tr>
</tbody>
</table>
</div>
<p>The closest I've got to this is:</p>
<pre><code>df_results = json_normalize(df1.to_dict('list'), ['Attribution'].unstack().apply(pd.Series)
</code></pre>
<p>But this does not append the Attribution columns to the Company columns. It does not seem to hit all rows, even when I've removed empty rows.</p>
<p>Other similar answers don't create new columns, based on the fields within the JSON.</p>
",https://stackoverflow.com/questions/76019850/extract-information-in-json-format-from-pandas-dataframe-and-append-to-dataframe,False,
